{"traceEvents": [{"ph": "M", "pid": 1644513, "tid": 1644513, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 1644513, "tid": 1644853, "name": "thread_name", "args": {"name": "EngineCoreOutputQueueThread"}}, {"ph": "M", "pid": 1644513, "tid": 1644513, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098624.793, "ph": "X", "dur": 1.0542748716957413, "name": "LLM.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:160)", "args": {"func_args": {}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098742.975, "ph": "X", "dur": 0.8009441437182642, "name": "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)", "args": {"func_args": {"name": "'current_platform'"}, "return_value": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098748.02, "ph": "X", "dur": 0.9990373445427576, "name": "Platform.pre_register_and_update (/home/jeromeku/vllm/vllm/platforms/interface.py:268)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "parser": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098811.371, "ph": "X", "dur": 71.70307210036194, "name": "check_gguf_file (/home/jeromeku/vllm/vllm/transformers_utils/utils.py:19)", "args": {"func_args": {"model": "'facebook/opt-125m'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098888.666, "ph": "X", "dur": 7.335162657453116, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:750)", "args": {"func_args": {}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098885.998, "ph": "X", "dur": 10.173228707727109, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_CI_USE_S3'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098792.861, "ph": "X", "dur": 11580163.111576943, "name": "EngineArgs.create_model_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:861)", "args": {"func_args": {"self": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')"}, "return_value": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180678997.178, "ph": "X", "dur": 10.719889752310086, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:589)", "args": {"func_args": {}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180678992.242, "ph": "X", "dur": 15.933169470162378, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_USE_V1'"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679102.057, "ph": "X", "dur": 0.9057049710773712, "name": "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)", "args": {"func_args": {"name": "'current_platform'"}, "return_value": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679111.27, "ph": "X", "dur": 2.5933066627168055, "name": "Platform.is_cuda (/home/jeromeku/vllm/vllm/platforms/interface.py:135)", "args": {"func_args": {"self": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679123.164, "ph": "X", "dur": 1.3390338478809505, "name": "_LoadNvmlLibrary (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2394)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679125.742, "ph": "X", "dur": 2.280928922955104, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlInitWithFlags'"}, "return_value": "<_FuncPtr object at 0x7f05debef5f0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180698248.334, "ph": "X", "dur": 5.484705497828161, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679121.084, "ph": "X", "dur": 19138.562218890238, "name": "nvmlInitWithFlags (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2373)", "args": {"func_args": {"flags": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679119.725, "ph": "X", "dur": 19141.31742864978, "name": "nvmlInit (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2390)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180698271.39, "ph": "X", "dur": 16.748399181247795, "name": "Platform.device_id_to_physical_device_id (/home/jeromeku/vllm/vllm/platforms/interface.py:166)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180698300.605, "ph": "X", "dur": 2.2828336652707244, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetHandleByIndex_v2'"}, "return_value": "<_FuncPtr object at 0x7f05ba9453d0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180705720.213, "ph": "X", "dur": 1.0076086849630481, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180698290.248, "ph": "X", "dur": 7431.318811401409, "name": "nvmlDeviceGetHandleByIndex (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2603)", "args": {"func_args": {"index": "0"}, "return_value": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfeddb50>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180705728.316, "ph": "X", "dur": 0.9514187866522543, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetCudaComputeCapability'"}, "return_value": "<_FuncPtr object at 0x7f05ba944a10>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180705732.502, "ph": "X", "dur": 0.7676111531949119, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180705724.521, "ph": "X", "dur": 9.841803544809204, "name": "nvmlDeviceGetCudaComputeCapability (/home/jeromeku/vllm/vllm/third_party/pynvml.py:3217)", "args": {"func_args": {"handle": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfeddb50>"}, "return_value": "(9, 0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180698263.965, "ph": "X", "dur": 7478.888798362864, "name": "NvmlCudaPlatform.get_device_capability (/home/jeromeku/vllm/vllm/platforms/cuda.py:321)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "DeviceCapability(major=9, minor=0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180705749.555, "ph": "X", "dur": 1.3323672497762802, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlShutdown'"}, "return_value": "<_FuncPtr object at 0x7f05debef930>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180708191.807, "ph": "X", "dur": 3.786627723452816, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180705748.596, "ph": "X", "dur": 2451.2290762812268, "name": "nvmlShutdown (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2428)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679115.643, "ph": "X", "dur": 29085.425635602107, "name": "with_nvml_context.<locals>.wrapper (/home/jeromeku/vllm/vllm/platforms/cuda.py:38)", "args": {"func_args": {"args": "(<class 'vllm.platforms.cuda.NvmlCudaPlatform'>,)", "kwargs": "{}"}, "return_value": "DeviceCapability(major=9, minor=0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180708219.305, "ph": "X", "dur": 2.9875883220501724, "name": "_LoadNvmlLibrary (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2394)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180708223.312, "ph": "X", "dur": 1.6361736491176908, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlInitWithFlags'"}, "return_value": "<_FuncPtr object at 0x7f05debef5f0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180709946.002, "ph": "X", "dur": 4.049482163008394, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180708215.55, "ph": "X", "dur": 1738.5773578612177, "name": "nvmlInitWithFlags (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2373)", "args": {"func_args": {"flags": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180708213.754, "ph": "X", "dur": 1741.0401896753147, "name": "nvmlInit (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2390)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180709962.143, "ph": "X", "dur": 14.425565927349046, "name": "Platform.device_id_to_physical_device_id (/home/jeromeku/vllm/vllm/platforms/interface.py:166)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180709981.504, "ph": "X", "dur": 2.0314076796088676, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetHandleByIndex_v2'"}, "return_value": "<_FuncPtr object at 0x7f05ba9453d0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180717282.918, "ph": "X", "dur": 1.1637975548438988, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180709977.996, "ph": "X", "dur": 7306.328668279259, "name": "nvmlDeviceGetHandleByIndex (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2603)", "args": {"func_args": {"index": "0"}, "return_value": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfeddc70>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180717289.424, "ph": "X", "dur": 1.197130545367251, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetCudaComputeCapability'"}, "return_value": "<_FuncPtr object at 0x7f05ba944a10>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180717293.404, "ph": "X", "dur": 0.6133270256296816, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180717287.154, "ph": "X", "dur": 7.710396893630282, "name": "nvmlDeviceGetCudaComputeCapability (/home/jeromeku/vllm/vllm/third_party/pynvml.py:3217)", "args": {"func_args": {"handle": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfeddc70>"}, "return_value": "(9, 0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180709956.72, "ph": "X", "dur": 7344.725416248688, "name": "NvmlCudaPlatform.get_device_capability (/home/jeromeku/vllm/vllm/platforms/cuda.py:321)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "DeviceCapability(major=9, minor=0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180717307.246, "ph": "X", "dur": 0.8657053824493486, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlShutdown'"}, "return_value": "<_FuncPtr object at 0x7f05debef930>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719242.926, "ph": "X", "dur": 1.9895033486652247, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180717305.56, "ph": "X", "dur": 1942.6409734740225, "name": "nvmlShutdown (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2428)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180708208.035, "ph": "X", "dur": 11040.731214551244, "name": "with_nvml_context.<locals>.wrapper (/home/jeromeku/vllm/vllm/platforms/cuda.py:38)", "args": {"func_args": {"args": "(<class 'vllm.platforms.cuda.NvmlCudaPlatform'>,)", "kwargs": "{}"}, "return_value": "DeviceCapability(major=9, minor=0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719715.426, "ph": "X", "dur": 1.3723668384043028, "name": "_ModelRegistry._normalize_archs.<locals>.<lambda> (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:452)", "args": {"func_args": {"model": "'OPTForCausalLM'"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719601.256, "ph": "X", "dur": 117.34545983840928, "name": "_ModelRegistry._normalize_archs (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:441)", "args": {"func_args": {"self": "_ModelRegistry(models={'AquilaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'AquilaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ArcticForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.arctic', class_name='ArcticForCausalLM'), 'MiniMaxText01ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_text_01', class_name='MiniMaxText01ForCausalLM'), 'BaiChuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaiChuanForCausalLM'), 'BaichuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaichuanForCausalLM'), 'BambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bamba', class_name='BambaForCausalLM'), 'BloomForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bloom', class_name='BloomForCausalLM'), 'ChatGLMModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'ChatGLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'CohereForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'Cohere2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'DbrxForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.dbrx', class_name='DbrxForCausalLM'), 'DeciLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron_nas', class_name='DeciLMForCausalLM'), 'DeepseekForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek', class_name='DeepseekForCausalLM'), 'DeepseekV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV2ForCausalLM'), 'DeepseekV3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV3ForCausalLM'), 'ExaoneForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.exaone', class_name='ExaoneForCausalLM'), 'FalconForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'Fairseq2LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fairseq2_llama', class_name='Fairseq2LlamaForCausalLM'), 'GemmaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma', class_name='GemmaForCausalLM'), 'Gemma2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'Gemma3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3', class_name='Gemma3ForCausalLM'), 'GlmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm', class_name='GlmForCausalLM'), 'Glm4ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4', class_name='Glm4ForCausalLM'), 'GPT2LMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt2', class_name='GPT2LMHeadModel'), 'GPTBigCodeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_bigcode', class_name='GPTBigCodeForCausalLM'), 'GPTJForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_j', class_name='GPTJForCausalLM'), 'GPTNeoXForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_neox', class_name='GPTNeoXForCausalLM'), 'GraniteForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite', class_name='GraniteForCausalLM'), 'GraniteMoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoe', class_name='GraniteMoeForCausalLM'), 'GraniteMoeHybridForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoehybrid', class_name='GraniteMoeHybridForCausalLM'), 'GraniteMoeSharedForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoeshared', class_name='GraniteMoeSharedForCausalLM'), 'GritLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gritlm', class_name='GritLM'), 'Grok1ModelForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.grok1', class_name='Grok1ForCausalLM'), 'InternLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'InternLM2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForCausalLM'), 'InternLM2VEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2_ve', class_name='InternLM2VEForCausalLM'), 'InternLM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'JAISLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.jais', class_name='JAISLMHeadModel'), 'JambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForCausalLM'), 'LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'LLaMAForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'FalconMambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'Mamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba2', class_name='Mamba2ForCausalLM'), 'MiniCPMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm', class_name='MiniCPMForCausalLM'), 'MiniCPM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm3', class_name='MiniCPM3ForCausalLM'), 'MistralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral', class_name='MixtralForCausalLM'), 'QuantMixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral_quant', class_name='MixtralForCausalLM'), 'MptForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MiMoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo', class_name='MiMoForCausalLM'), 'NemotronForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron', class_name='NemotronForCausalLM'), 'OlmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo', class_name='OlmoForCausalLM'), 'Olmo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo2', class_name='Olmo2ForCausalLM'), 'OlmoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmoe', class_name='OlmoeForCausalLM'), 'OPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.opt', class_name='OPTForCausalLM'), 'OrionForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.orion', class_name='OrionForCausalLM'), 'PersimmonForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.persimmon', class_name='PersimmonForCausalLM'), 'PhiForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi', class_name='PhiForCausalLM'), 'Phi3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3', class_name='Phi3ForCausalLM'), 'Phi3SmallForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3_small', class_name='Phi3SmallForCausalLM'), 'PhiMoEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phimoe', class_name='PhiMoEForCausalLM'), 'Plamo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.plamo2', class_name='Plamo2ForCausalLM'), 'QWenLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen', class_name='QWenLMHeadModel'), 'Qwen2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'Qwen2MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_moe', class_name='Qwen2MoeForCausalLM'), 'Qwen3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3', class_name='Qwen3ForCausalLM'), 'Qwen3MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3_moe', class_name='Qwen3MoeForCausalLM'), 'RWForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'StableLMEpochForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'StableLmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'Starcoder2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.starcoder2', class_name='Starcoder2ForCausalLM'), 'SolarForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.solar', class_name='SolarForCausalLM'), 'TeleChat2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.telechat2', class_name='TeleChat2ForCausalLM'), 'TeleFLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.teleflm', class_name='TeleFLMForCausalLM'), 'XverseForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'Zamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.zamba2', class_name='Zamba2ForCausalLM'), 'BartModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BartForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertEmbeddingModel'), 'Gemma2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'GteModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='SnowflakeGteNewModel'), 'GteNewModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='GteNewModel'), 'InternLM2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForRewardModel'), 'JambaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForSequenceClassification'), 'LlamaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MistralModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ModernBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertModel'), 'NomicBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='NomicBertModel'), 'Qwen2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2EmbeddingModel'), 'Qwen2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForRewardModel'), 'Qwen2ForProcessRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForProcessRewardModel'), 'RobertaForMaskedLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'RobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'XLMRobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'LlavaNextForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next', class_name='LlavaNextForConditionalGeneration'), 'Phi3VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3v', class_name='Phi3VForCausalLM'), 'Qwen2VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_vl', class_name='Qwen2VLForConditionalGeneration'), 'Qwen2ForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'PrithviGeoSpatialMAE': _LazyRegisteredModel(module_name='vllm.model_executor.models.prithvi_geospatial_mae', class_name='PrithviGeoSpatialMAE'), 'BertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertForSequenceClassification'), 'RobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'XLMRobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'ModernBertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertForSequenceClassification'), 'AriaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aria', class_name='AriaForConditionalGeneration'), 'AyaVisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aya_vision', class_name='AyaVisionForConditionalGeneration'), 'Blip2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.blip2', class_name='Blip2ForConditionalGeneration'), 'ChameleonForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chameleon', class_name='ChameleonForConditionalGeneration'), 'DeepseekVLV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_vl2', class_name='DeepseekVLV2ForCausalLM'), 'FuyuForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fuyu', class_name='FuyuForCausalLM'), 'Gemma3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3_mm', class_name='Gemma3ForConditionalGeneration'), 'GLM4VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4v', class_name='GLM4VForCausalLM'), 'GraniteSpeechForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite_speech', class_name='GraniteSpeechForConditionalGeneration'), 'H2OVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.h2ovl', class_name='H2OVLChatModel'), 'InternVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internvl', class_name='InternVLChatModel'), 'Idefics3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.idefics3', class_name='Idefics3ForConditionalGeneration'), 'SmolVLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.smolvlm', class_name='SmolVLMForConditionalGeneration'), 'KimiVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.kimi_vl', class_name='KimiVLForConditionalGeneration'), 'LlavaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='LlavaForConditionalGeneration'), 'LlavaNextVideoForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next_video', class_name='LlavaNextVideoForConditionalGeneration'), 'LlavaOnevisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_onevision', class_name='LlavaOnevisionForConditionalGeneration'), 'MantisForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='MantisForConditionalGeneration'), 'MiniMaxVL01ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_vl_01', class_name='MiniMaxVL01ForConditionalGeneration'), 'MiniCPMO': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmo', class_name='MiniCPMO'), 'MiniCPMV': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmv', class_name='MiniCPMV'), 'Mistral3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mistral3', class_name='Mistral3ForConditionalGeneration'), 'MolmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.molmo', class_name='MolmoForCausalLM'), 'NVLM_D': _LazyRegisteredModel(module_name='vllm.model_executor.models.nvlm_d', class_name='NVLM_D_Model'), 'Ovis': _LazyRegisteredModel(module_name='vllm.model_executor.models.ovis', class_name='Ovis'), 'PaliGemmaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.paligemma', class_name='PaliGemmaForConditionalGeneration'), 'PixtralForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.pixtral', class_name='PixtralForConditionalGeneration'), 'QwenVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen_vl', class_name='QwenVLForConditionalGeneration'), 'Qwen2_5_VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_vl', class_name='Qwen2_5_VLForConditionalGeneration'), 'Qwen2AudioForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_audio', class_name='Qwen2AudioForConditionalGeneration'), 'Qwen2_5OmniModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_omni_thinker', class_name='Qwen2_5OmniThinkerForConditionalGeneration'), 'UltravoxModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.ultravox', class_name='UltravoxModel'), 'Phi4MMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi4mm', class_name='Phi4MMForCausalLM'), 'Florence2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.florence2', class_name='Florence2ForConditionalGeneration'), 'MllamaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama', class_name='MllamaForConditionalGeneration'), 'Llama4ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama4', class_name='Llama4ForConditionalGeneration'), 'SkyworkR1VChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.skyworkr1v', class_name='SkyworkR1VChatModel'), 'WhisperForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.whisper', class_name='WhisperForConditionalGeneration'), 'MiMoMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo_mtp', class_name='MiMoMTP'), 'EAGLEModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.eagle', class_name='EAGLE'), 'EagleLlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle', class_name='EagleLlamaForCausalLM'), 'Eagle3LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle3', class_name='Eagle3LlamaForCausalLM'), 'DeepSeekMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_mtp', class_name='DeepSeekMTP'), 'MedusaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.medusa', class_name='Medusa'), 'MLPSpeculatorPreTrainedModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mlp_speculator', class_name='MLPSpeculator'), 'TransformersForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.transformers', class_name='TransformersForCausalLM')})", "architectures": "['OPTForCausalLM']"}, "return_value": "['OPTForCausalLM']"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719722.533, "ph": "X", "dur": 107.67317835969027, "name": "_ModelRegistry._try_inspect_model_cls (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:435)", "args": {"func_args": {"self": "_ModelRegistry(models={'AquilaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'AquilaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ArcticForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.arctic', class_name='ArcticForCausalLM'), 'MiniMaxText01ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_text_01', class_name='MiniMaxText01ForCausalLM'), 'BaiChuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaiChuanForCausalLM'), 'BaichuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaichuanForCausalLM'), 'BambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bamba', class_name='BambaForCausalLM'), 'BloomForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bloom', class_name='BloomForCausalLM'), 'ChatGLMModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'ChatGLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'CohereForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'Cohere2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'DbrxForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.dbrx', class_name='DbrxForCausalLM'), 'DeciLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron_nas', class_name='DeciLMForCausalLM'), 'DeepseekForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek', class_name='DeepseekForCausalLM'), 'DeepseekV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV2ForCausalLM'), 'DeepseekV3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV3ForCausalLM'), 'ExaoneForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.exaone', class_name='ExaoneForCausalLM'), 'FalconForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'Fairseq2LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fairseq2_llama', class_name='Fairseq2LlamaForCausalLM'), 'GemmaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma', class_name='GemmaForCausalLM'), 'Gemma2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'Gemma3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3', class_name='Gemma3ForCausalLM'), 'GlmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm', class_name='GlmForCausalLM'), 'Glm4ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4', class_name='Glm4ForCausalLM'), 'GPT2LMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt2', class_name='GPT2LMHeadModel'), 'GPTBigCodeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_bigcode', class_name='GPTBigCodeForCausalLM'), 'GPTJForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_j', class_name='GPTJForCausalLM'), 'GPTNeoXForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_neox', class_name='GPTNeoXForCausalLM'), 'GraniteForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite', class_name='GraniteForCausalLM'), 'GraniteMoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoe', class_name='GraniteMoeForCausalLM'), 'GraniteMoeHybridForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoehybrid', class_name='GraniteMoeHybridForCausalLM'), 'GraniteMoeSharedForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoeshared', class_name='GraniteMoeSharedForCausalLM'), 'GritLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gritlm', class_name='GritLM'), 'Grok1ModelForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.grok1', class_name='Grok1ForCausalLM'), 'InternLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'InternLM2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForCausalLM'), 'InternLM2VEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2_ve', class_name='InternLM2VEForCausalLM'), 'InternLM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'JAISLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.jais', class_name='JAISLMHeadModel'), 'JambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForCausalLM'), 'LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'LLaMAForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'FalconMambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'Mamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba2', class_name='Mamba2ForCausalLM'), 'MiniCPMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm', class_name='MiniCPMForCausalLM'), 'MiniCPM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm3', class_name='MiniCPM3ForCausalLM'), 'MistralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral', class_name='MixtralForCausalLM'), 'QuantMixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral_quant', class_name='MixtralForCausalLM'), 'MptForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MiMoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo', class_name='MiMoForCausalLM'), 'NemotronForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron', class_name='NemotronForCausalLM'), 'OlmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo', class_name='OlmoForCausalLM'), 'Olmo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo2', class_name='Olmo2ForCausalLM'), 'OlmoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmoe', class_name='OlmoeForCausalLM'), 'OPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.opt', class_name='OPTForCausalLM'), 'OrionForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.orion', class_name='OrionForCausalLM'), 'PersimmonForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.persimmon', class_name='PersimmonForCausalLM'), 'PhiForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi', class_name='PhiForCausalLM'), 'Phi3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3', class_name='Phi3ForCausalLM'), 'Phi3SmallForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3_small', class_name='Phi3SmallForCausalLM'), 'PhiMoEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phimoe', class_name='PhiMoEForCausalLM'), 'Plamo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.plamo2', class_name='Plamo2ForCausalLM'), 'QWenLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen', class_name='QWenLMHeadModel'), 'Qwen2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'Qwen2MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_moe', class_name='Qwen2MoeForCausalLM'), 'Qwen3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3', class_name='Qwen3ForCausalLM'), 'Qwen3MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3_moe', class_name='Qwen3MoeForCausalLM'), 'RWForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'StableLMEpochForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'StableLmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'Starcoder2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.starcoder2', class_name='Starcoder2ForCausalLM'), 'SolarForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.solar', class_name='SolarForCausalLM'), 'TeleChat2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.telechat2', class_name='TeleChat2ForCausalLM'), 'TeleFLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.teleflm', class_name='TeleFLMForCausalLM'), 'XverseForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'Zamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.zamba2', class_name='Zamba2ForCausalLM'), 'BartModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BartForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertEmbeddingModel'), 'Gemma2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'GteModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='SnowflakeGteNewModel'), 'GteNewModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='GteNewModel'), 'InternLM2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForRewardModel'), 'JambaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForSequenceClassification'), 'LlamaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MistralModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ModernBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertModel'), 'NomicBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='NomicBertModel'), 'Qwen2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2EmbeddingModel'), 'Qwen2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForRewardModel'), 'Qwen2ForProcessRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForProcessRewardModel'), 'RobertaForMaskedLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'RobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'XLMRobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'LlavaNextForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next', class_name='LlavaNextForConditionalGeneration'), 'Phi3VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3v', class_name='Phi3VForCausalLM'), 'Qwen2VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_vl', class_name='Qwen2VLForConditionalGeneration'), 'Qwen2ForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'PrithviGeoSpatialMAE': _LazyRegisteredModel(module_name='vllm.model_executor.models.prithvi_geospatial_mae', class_name='PrithviGeoSpatialMAE'), 'BertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertForSequenceClassification'), 'RobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'XLMRobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'ModernBertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertForSequenceClassification'), 'AriaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aria', class_name='AriaForConditionalGeneration'), 'AyaVisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aya_vision', class_name='AyaVisionForConditionalGeneration'), 'Blip2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.blip2', class_name='Blip2ForConditionalGeneration'), 'ChameleonForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chameleon', class_name='ChameleonForConditionalGeneration'), 'DeepseekVLV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_vl2', class_name='DeepseekVLV2ForCausalLM'), 'FuyuForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fuyu', class_name='FuyuForCausalLM'), 'Gemma3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3_mm', class_name='Gemma3ForConditionalGeneration'), 'GLM4VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4v', class_name='GLM4VForCausalLM'), 'GraniteSpeechForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite_speech', class_name='GraniteSpeechForConditionalGeneration'), 'H2OVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.h2ovl', class_name='H2OVLChatModel'), 'InternVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internvl', class_name='InternVLChatModel'), 'Idefics3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.idefics3', class_name='Idefics3ForConditionalGeneration'), 'SmolVLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.smolvlm', class_name='SmolVLMForConditionalGeneration'), 'KimiVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.kimi_vl', class_name='KimiVLForConditionalGeneration'), 'LlavaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='LlavaForConditionalGeneration'), 'LlavaNextVideoForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next_video', class_name='LlavaNextVideoForConditionalGeneration'), 'LlavaOnevisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_onevision', class_name='LlavaOnevisionForConditionalGeneration'), 'MantisForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='MantisForConditionalGeneration'), 'MiniMaxVL01ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_vl_01', class_name='MiniMaxVL01ForConditionalGeneration'), 'MiniCPMO': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmo', class_name='MiniCPMO'), 'MiniCPMV': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmv', class_name='MiniCPMV'), 'Mistral3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mistral3', class_name='Mistral3ForConditionalGeneration'), 'MolmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.molmo', class_name='MolmoForCausalLM'), 'NVLM_D': _LazyRegisteredModel(module_name='vllm.model_executor.models.nvlm_d', class_name='NVLM_D_Model'), 'Ovis': _LazyRegisteredModel(module_name='vllm.model_executor.models.ovis', class_name='Ovis'), 'PaliGemmaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.paligemma', class_name='PaliGemmaForConditionalGeneration'), 'PixtralForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.pixtral', class_name='PixtralForConditionalGeneration'), 'QwenVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen_vl', class_name='QwenVLForConditionalGeneration'), 'Qwen2_5_VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_vl', class_name='Qwen2_5_VLForConditionalGeneration'), 'Qwen2AudioForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_audio', class_name='Qwen2AudioForConditionalGeneration'), 'Qwen2_5OmniModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_omni_thinker', class_name='Qwen2_5OmniThinkerForConditionalGeneration'), 'UltravoxModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.ultravox', class_name='UltravoxModel'), 'Phi4MMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi4mm', class_name='Phi4MMForCausalLM'), 'Florence2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.florence2', class_name='Florence2ForConditionalGeneration'), 'MllamaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama', class_name='MllamaForConditionalGeneration'), 'Llama4ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama4', class_name='Llama4ForConditionalGeneration'), 'SkyworkR1VChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.skyworkr1v', class_name='SkyworkR1VChatModel'), 'WhisperForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.whisper', class_name='WhisperForConditionalGeneration'), 'MiMoMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo_mtp', class_name='MiMoMTP'), 'EAGLEModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.eagle', class_name='EAGLE'), 'EagleLlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle', class_name='EagleLlamaForCausalLM'), 'Eagle3LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle3', class_name='Eagle3LlamaForCausalLM'), 'DeepSeekMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_mtp', class_name='DeepSeekMTP'), 'MedusaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.medusa', class_name='Medusa'), 'MLPSpeculatorPreTrainedModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mlp_speculator', class_name='MLPSpeculator'), 'TransformersForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.transformers', class_name='TransformersForCausalLM')})", "model_arch": "'OPTForCausalLM'"}, "return_value": "_ModelInfo(architecture='OPTForCausalLM', is_text_generation_model=True, is_pooling_model=True, supports_cross_encoding=False, supports_multimodal=False, supports_pp=True, has_inner_state=False, is_attention_free=False, is_hybrid=False, has_noops=False, supports_transcription=False, supports_v0_only=False)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719484.488, "ph": "X", "dur": 352.8087525242118, "name": "_ModelRegistry.inspect_model_cls (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:459)", "args": {"func_args": {"self": "_ModelRegistry(models={'AquilaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'AquilaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ArcticForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.arctic', class_name='ArcticForCausalLM'), 'MiniMaxText01ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_text_01', class_name='MiniMaxText01ForCausalLM'), 'BaiChuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaiChuanForCausalLM'), 'BaichuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaichuanForCausalLM'), 'BambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bamba', class_name='BambaForCausalLM'), 'BloomForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bloom', class_name='BloomForCausalLM'), 'ChatGLMModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'ChatGLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'CohereForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'Cohere2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'DbrxForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.dbrx', class_name='DbrxForCausalLM'), 'DeciLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron_nas', class_name='DeciLMForCausalLM'), 'DeepseekForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek', class_name='DeepseekForCausalLM'), 'DeepseekV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV2ForCausalLM'), 'DeepseekV3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV3ForCausalLM'), 'ExaoneForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.exaone', class_name='ExaoneForCausalLM'), 'FalconForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'Fairseq2LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fairseq2_llama', class_name='Fairseq2LlamaForCausalLM'), 'GemmaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma', class_name='GemmaForCausalLM'), 'Gemma2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'Gemma3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3', class_name='Gemma3ForCausalLM'), 'GlmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm', class_name='GlmForCausalLM'), 'Glm4ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4', class_name='Glm4ForCausalLM'), 'GPT2LMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt2', class_name='GPT2LMHeadModel'), 'GPTBigCodeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_bigcode', class_name='GPTBigCodeForCausalLM'), 'GPTJForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_j', class_name='GPTJForCausalLM'), 'GPTNeoXForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_neox', class_name='GPTNeoXForCausalLM'), 'GraniteForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite', class_name='GraniteForCausalLM'), 'GraniteMoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoe', class_name='GraniteMoeForCausalLM'), 'GraniteMoeHybridForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoehybrid', class_name='GraniteMoeHybridForCausalLM'), 'GraniteMoeSharedForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoeshared', class_name='GraniteMoeSharedForCausalLM'), 'GritLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gritlm', class_name='GritLM'), 'Grok1ModelForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.grok1', class_name='Grok1ForCausalLM'), 'InternLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'InternLM2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForCausalLM'), 'InternLM2VEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2_ve', class_name='InternLM2VEForCausalLM'), 'InternLM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'JAISLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.jais', class_name='JAISLMHeadModel'), 'JambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForCausalLM'), 'LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'LLaMAForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'FalconMambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'Mamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba2', class_name='Mamba2ForCausalLM'), 'MiniCPMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm', class_name='MiniCPMForCausalLM'), 'MiniCPM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm3', class_name='MiniCPM3ForCausalLM'), 'MistralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral', class_name='MixtralForCausalLM'), 'QuantMixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral_quant', class_name='MixtralForCausalLM'), 'MptForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MiMoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo', class_name='MiMoForCausalLM'), 'NemotronForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron', class_name='NemotronForCausalLM'), 'OlmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo', class_name='OlmoForCausalLM'), 'Olmo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo2', class_name='Olmo2ForCausalLM'), 'OlmoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmoe', class_name='OlmoeForCausalLM'), 'OPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.opt', class_name='OPTForCausalLM'), 'OrionForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.orion', class_name='OrionForCausalLM'), 'PersimmonForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.persimmon', class_name='PersimmonForCausalLM'), 'PhiForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi', class_name='PhiForCausalLM'), 'Phi3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3', class_name='Phi3ForCausalLM'), 'Phi3SmallForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3_small', class_name='Phi3SmallForCausalLM'), 'PhiMoEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phimoe', class_name='PhiMoEForCausalLM'), 'Plamo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.plamo2', class_name='Plamo2ForCausalLM'), 'QWenLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen', class_name='QWenLMHeadModel'), 'Qwen2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'Qwen2MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_moe', class_name='Qwen2MoeForCausalLM'), 'Qwen3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3', class_name='Qwen3ForCausalLM'), 'Qwen3MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3_moe', class_name='Qwen3MoeForCausalLM'), 'RWForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'StableLMEpochForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'StableLmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'Starcoder2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.starcoder2', class_name='Starcoder2ForCausalLM'), 'SolarForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.solar', class_name='SolarForCausalLM'), 'TeleChat2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.telechat2', class_name='TeleChat2ForCausalLM'), 'TeleFLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.teleflm', class_name='TeleFLMForCausalLM'), 'XverseForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'Zamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.zamba2', class_name='Zamba2ForCausalLM'), 'BartModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BartForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertEmbeddingModel'), 'Gemma2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'GteModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='SnowflakeGteNewModel'), 'GteNewModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='GteNewModel'), 'InternLM2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForRewardModel'), 'JambaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForSequenceClassification'), 'LlamaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MistralModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ModernBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertModel'), 'NomicBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='NomicBertModel'), 'Qwen2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2EmbeddingModel'), 'Qwen2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForRewardModel'), 'Qwen2ForProcessRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForProcessRewardModel'), 'RobertaForMaskedLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'RobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'XLMRobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'LlavaNextForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next', class_name='LlavaNextForConditionalGeneration'), 'Phi3VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3v', class_name='Phi3VForCausalLM'), 'Qwen2VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_vl', class_name='Qwen2VLForConditionalGeneration'), 'Qwen2ForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'PrithviGeoSpatialMAE': _LazyRegisteredModel(module_name='vllm.model_executor.models.prithvi_geospatial_mae', class_name='PrithviGeoSpatialMAE'), 'BertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertForSequenceClassification'), 'RobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'XLMRobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'ModernBertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertForSequenceClassification'), 'AriaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aria', class_name='AriaForConditionalGeneration'), 'AyaVisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aya_vision', class_name='AyaVisionForConditionalGeneration'), 'Blip2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.blip2', class_name='Blip2ForConditionalGeneration'), 'ChameleonForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chameleon', class_name='ChameleonForConditionalGeneration'), 'DeepseekVLV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_vl2', class_name='DeepseekVLV2ForCausalLM'), 'FuyuForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fuyu', class_name='FuyuForCausalLM'), 'Gemma3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3_mm', class_name='Gemma3ForConditionalGeneration'), 'GLM4VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4v', class_name='GLM4VForCausalLM'), 'GraniteSpeechForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite_speech', class_name='GraniteSpeechForConditionalGeneration'), 'H2OVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.h2ovl', class_name='H2OVLChatModel'), 'InternVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internvl', class_name='InternVLChatModel'), 'Idefics3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.idefics3', class_name='Idefics3ForConditionalGeneration'), 'SmolVLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.smolvlm', class_name='SmolVLMForConditionalGeneration'), 'KimiVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.kimi_vl', class_name='KimiVLForConditionalGeneration'), 'LlavaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='LlavaForConditionalGeneration'), 'LlavaNextVideoForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next_video', class_name='LlavaNextVideoForConditionalGeneration'), 'LlavaOnevisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_onevision', class_name='LlavaOnevisionForConditionalGeneration'), 'MantisForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='MantisForConditionalGeneration'), 'MiniMaxVL01ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_vl_01', class_name='MiniMaxVL01ForConditionalGeneration'), 'MiniCPMO': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmo', class_name='MiniCPMO'), 'MiniCPMV': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmv', class_name='MiniCPMV'), 'Mistral3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mistral3', class_name='Mistral3ForConditionalGeneration'), 'MolmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.molmo', class_name='MolmoForCausalLM'), 'NVLM_D': _LazyRegisteredModel(module_name='vllm.model_executor.models.nvlm_d', class_name='NVLM_D_Model'), 'Ovis': _LazyRegisteredModel(module_name='vllm.model_executor.models.ovis', class_name='Ovis'), 'PaliGemmaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.paligemma', class_name='PaliGemmaForConditionalGeneration'), 'PixtralForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.pixtral', class_name='PixtralForConditionalGeneration'), 'QwenVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen_vl', class_name='QwenVLForConditionalGeneration'), 'Qwen2_5_VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_vl', class_name='Qwen2_5_VLForConditionalGeneration'), 'Qwen2AudioForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_audio', class_name='Qwen2AudioForConditionalGeneration'), 'Qwen2_5OmniModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_omni_thinker', class_name='Qwen2_5OmniThinkerForConditionalGeneration'), 'UltravoxModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.ultravox', class_name='UltravoxModel'), 'Phi4MMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi4mm', class_name='Phi4MMForCausalLM'), 'Florence2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.florence2', class_name='Florence2ForConditionalGeneration'), 'MllamaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama', class_name='MllamaForConditionalGeneration'), 'Llama4ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama4', class_name='Llama4ForConditionalGeneration'), 'SkyworkR1VChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.skyworkr1v', class_name='SkyworkR1VChatModel'), 'WhisperForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.whisper', class_name='WhisperForConditionalGeneration'), 'MiMoMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo_mtp', class_name='MiMoMTP'), 'EAGLEModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.eagle', class_name='EAGLE'), 'EagleLlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle', class_name='EagleLlamaForCausalLM'), 'Eagle3LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle3', class_name='Eagle3LlamaForCausalLM'), 'DeepSeekMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_mtp', class_name='DeepSeekMTP'), 'MedusaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.medusa', class_name='Medusa'), 'MLPSpeculatorPreTrainedModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mlp_speculator', class_name='MLPSpeculator'), 'TransformersForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.transformers', class_name='TransformersForCausalLM')})", "architectures": "['OPTForCausalLM']"}, "return_value": "(_ModelInfo(architecture='OPTForCausalLM', is_text_generation_model=True, is_pooling_model=True, supports_cross_encoding=False, supports_multimodal=False, supports_pp=True, has_inner_state=False, is_attention_free=False, is_hybrid=False, has_noops=False, supports_transcription=False, supports_v0_only=False), 'OPTForCausalLM')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719319.605, "ph": "X", "dur": 520.6584548612888, "name": "_ModelRegistry.is_v1_compatible (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:555)", "args": {"func_args": {"self": "_ModelRegistry(models={'AquilaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'AquilaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ArcticForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.arctic', class_name='ArcticForCausalLM'), 'MiniMaxText01ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_text_01', class_name='MiniMaxText01ForCausalLM'), 'BaiChuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaiChuanForCausalLM'), 'BaichuanForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.baichuan', class_name='BaichuanForCausalLM'), 'BambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bamba', class_name='BambaForCausalLM'), 'BloomForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.bloom', class_name='BloomForCausalLM'), 'ChatGLMModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'ChatGLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chatglm', class_name='ChatGLMForCausalLM'), 'CohereForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'Cohere2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.commandr', class_name='CohereForCausalLM'), 'DbrxForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.dbrx', class_name='DbrxForCausalLM'), 'DeciLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron_nas', class_name='DeciLMForCausalLM'), 'DeepseekForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek', class_name='DeepseekForCausalLM'), 'DeepseekV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV2ForCausalLM'), 'DeepseekV3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_v2', class_name='DeepseekV3ForCausalLM'), 'ExaoneForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.exaone', class_name='ExaoneForCausalLM'), 'FalconForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'Fairseq2LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fairseq2_llama', class_name='Fairseq2LlamaForCausalLM'), 'GemmaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma', class_name='GemmaForCausalLM'), 'Gemma2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'Gemma3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3', class_name='Gemma3ForCausalLM'), 'GlmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm', class_name='GlmForCausalLM'), 'Glm4ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4', class_name='Glm4ForCausalLM'), 'GPT2LMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt2', class_name='GPT2LMHeadModel'), 'GPTBigCodeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_bigcode', class_name='GPTBigCodeForCausalLM'), 'GPTJForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_j', class_name='GPTJForCausalLM'), 'GPTNeoXForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gpt_neox', class_name='GPTNeoXForCausalLM'), 'GraniteForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite', class_name='GraniteForCausalLM'), 'GraniteMoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoe', class_name='GraniteMoeForCausalLM'), 'GraniteMoeHybridForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoehybrid', class_name='GraniteMoeHybridForCausalLM'), 'GraniteMoeSharedForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.granitemoeshared', class_name='GraniteMoeSharedForCausalLM'), 'GritLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.gritlm', class_name='GritLM'), 'Grok1ModelForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.grok1', class_name='Grok1ForCausalLM'), 'InternLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'InternLM2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForCausalLM'), 'InternLM2VEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2_ve', class_name='InternLM2VEForCausalLM'), 'InternLM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'JAISLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.jais', class_name='JAISLMHeadModel'), 'JambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForCausalLM'), 'LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'LLaMAForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'FalconMambaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba', class_name='MambaForCausalLM'), 'Mamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mamba2', class_name='Mamba2ForCausalLM'), 'MiniCPMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm', class_name='MiniCPMForCausalLM'), 'MiniCPM3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpm3', class_name='MiniCPM3ForCausalLM'), 'MistralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral', class_name='MixtralForCausalLM'), 'QuantMixtralForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mixtral_quant', class_name='MixtralForCausalLM'), 'MptForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mpt', class_name='MPTForCausalLM'), 'MiMoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo', class_name='MiMoForCausalLM'), 'NemotronForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.nemotron', class_name='NemotronForCausalLM'), 'OlmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo', class_name='OlmoForCausalLM'), 'Olmo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmo2', class_name='Olmo2ForCausalLM'), 'OlmoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.olmoe', class_name='OlmoeForCausalLM'), 'OPTForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.opt', class_name='OPTForCausalLM'), 'OrionForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.orion', class_name='OrionForCausalLM'), 'PersimmonForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.persimmon', class_name='PersimmonForCausalLM'), 'PhiForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi', class_name='PhiForCausalLM'), 'Phi3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3', class_name='Phi3ForCausalLM'), 'Phi3SmallForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3_small', class_name='Phi3SmallForCausalLM'), 'PhiMoEForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phimoe', class_name='PhiMoEForCausalLM'), 'Plamo2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.plamo2', class_name='Plamo2ForCausalLM'), 'QWenLMHeadModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen', class_name='QWenLMHeadModel'), 'Qwen2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'Qwen2MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_moe', class_name='Qwen2MoeForCausalLM'), 'Qwen3ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3', class_name='Qwen3ForCausalLM'), 'Qwen3MoeForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen3_moe', class_name='Qwen3MoeForCausalLM'), 'RWForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.falcon', class_name='FalconForCausalLM'), 'StableLMEpochForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'StableLmForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.stablelm', class_name='StablelmForCausalLM'), 'Starcoder2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.starcoder2', class_name='Starcoder2ForCausalLM'), 'SolarForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.solar', class_name='SolarForCausalLM'), 'TeleChat2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.telechat2', class_name='TeleChat2ForCausalLM'), 'TeleFLMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.teleflm', class_name='TeleFLMForCausalLM'), 'XverseForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'Zamba2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.zamba2', class_name='Zamba2ForCausalLM'), 'BartModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BartForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.bart', class_name='BartForConditionalGeneration'), 'BertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertEmbeddingModel'), 'Gemma2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma2', class_name='Gemma2ForCausalLM'), 'GteModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='SnowflakeGteNewModel'), 'GteNewModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='GteNewModel'), 'InternLM2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internlm2', class_name='InternLM2ForRewardModel'), 'JambaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.jamba', class_name='JambaForSequenceClassification'), 'LlamaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'MistralModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama', class_name='LlamaForCausalLM'), 'ModernBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertModel'), 'NomicBertModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert_with_rope', class_name='NomicBertModel'), 'Qwen2Model': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2EmbeddingModel'), 'Qwen2ForRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForRewardModel'), 'Qwen2ForProcessRewardModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_rm', class_name='Qwen2ForProcessRewardModel'), 'RobertaForMaskedLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'RobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'XLMRobertaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaEmbeddingModel'), 'LlavaNextForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next', class_name='LlavaNextForConditionalGeneration'), 'Phi3VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi3v', class_name='Phi3VForCausalLM'), 'Qwen2VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_vl', class_name='Qwen2VLForConditionalGeneration'), 'Qwen2ForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2', class_name='Qwen2ForCausalLM'), 'PrithviGeoSpatialMAE': _LazyRegisteredModel(module_name='vllm.model_executor.models.prithvi_geospatial_mae', class_name='PrithviGeoSpatialMAE'), 'BertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.bert', class_name='BertForSequenceClassification'), 'RobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'XLMRobertaForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.roberta', class_name='RobertaForSequenceClassification'), 'ModernBertForSequenceClassification': _LazyRegisteredModel(module_name='vllm.model_executor.models.modernbert', class_name='ModernBertForSequenceClassification'), 'AriaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aria', class_name='AriaForConditionalGeneration'), 'AyaVisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.aya_vision', class_name='AyaVisionForConditionalGeneration'), 'Blip2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.blip2', class_name='Blip2ForConditionalGeneration'), 'ChameleonForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.chameleon', class_name='ChameleonForConditionalGeneration'), 'DeepseekVLV2ForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_vl2', class_name='DeepseekVLV2ForCausalLM'), 'FuyuForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.fuyu', class_name='FuyuForCausalLM'), 'Gemma3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.gemma3_mm', class_name='Gemma3ForConditionalGeneration'), 'GLM4VForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.glm4v', class_name='GLM4VForCausalLM'), 'GraniteSpeechForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.granite_speech', class_name='GraniteSpeechForConditionalGeneration'), 'H2OVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.h2ovl', class_name='H2OVLChatModel'), 'InternVLChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.internvl', class_name='InternVLChatModel'), 'Idefics3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.idefics3', class_name='Idefics3ForConditionalGeneration'), 'SmolVLMForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.smolvlm', class_name='SmolVLMForConditionalGeneration'), 'KimiVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.kimi_vl', class_name='KimiVLForConditionalGeneration'), 'LlavaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='LlavaForConditionalGeneration'), 'LlavaNextVideoForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_next_video', class_name='LlavaNextVideoForConditionalGeneration'), 'LlavaOnevisionForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava_onevision', class_name='LlavaOnevisionForConditionalGeneration'), 'MantisForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.llava', class_name='MantisForConditionalGeneration'), 'MiniMaxVL01ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.minimax_vl_01', class_name='MiniMaxVL01ForConditionalGeneration'), 'MiniCPMO': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmo', class_name='MiniCPMO'), 'MiniCPMV': _LazyRegisteredModel(module_name='vllm.model_executor.models.minicpmv', class_name='MiniCPMV'), 'Mistral3ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mistral3', class_name='Mistral3ForConditionalGeneration'), 'MolmoForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.molmo', class_name='MolmoForCausalLM'), 'NVLM_D': _LazyRegisteredModel(module_name='vllm.model_executor.models.nvlm_d', class_name='NVLM_D_Model'), 'Ovis': _LazyRegisteredModel(module_name='vllm.model_executor.models.ovis', class_name='Ovis'), 'PaliGemmaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.paligemma', class_name='PaliGemmaForConditionalGeneration'), 'PixtralForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.pixtral', class_name='PixtralForConditionalGeneration'), 'QwenVLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen_vl', class_name='QwenVLForConditionalGeneration'), 'Qwen2_5_VLForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_vl', class_name='Qwen2_5_VLForConditionalGeneration'), 'Qwen2AudioForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_audio', class_name='Qwen2AudioForConditionalGeneration'), 'Qwen2_5OmniModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.qwen2_5_omni_thinker', class_name='Qwen2_5OmniThinkerForConditionalGeneration'), 'UltravoxModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.ultravox', class_name='UltravoxModel'), 'Phi4MMForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.phi4mm', class_name='Phi4MMForCausalLM'), 'Florence2ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.florence2', class_name='Florence2ForConditionalGeneration'), 'MllamaForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama', class_name='MllamaForConditionalGeneration'), 'Llama4ForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.mllama4', class_name='Llama4ForConditionalGeneration'), 'SkyworkR1VChatModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.skyworkr1v', class_name='SkyworkR1VChatModel'), 'WhisperForConditionalGeneration': _LazyRegisteredModel(module_name='vllm.model_executor.models.whisper', class_name='WhisperForConditionalGeneration'), 'MiMoMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mimo_mtp', class_name='MiMoMTP'), 'EAGLEModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.eagle', class_name='EAGLE'), 'EagleLlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle', class_name='EagleLlamaForCausalLM'), 'Eagle3LlamaForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.llama_eagle3', class_name='Eagle3LlamaForCausalLM'), 'DeepSeekMTPModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.deepseek_mtp', class_name='DeepSeekMTP'), 'MedusaModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.medusa', class_name='Medusa'), 'MLPSpeculatorPreTrainedModel': _LazyRegisteredModel(module_name='vllm.model_executor.models.mlp_speculator', class_name='MLPSpeculator'), 'TransformersForCausalLM': _LazyRegisteredModel(module_name='vllm.model_executor.models.transformers', class_name='TransformersForCausalLM')})", "architectures": "['OPTForCausalLM']"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719265.191, "ph": "X", "dur": 575.2188461210696, "name": "ModelConfig.is_v1_compatible (/home/jeromeku/vllm/vllm/config.py:1369)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719847.568, "ph": "X", "dur": 10.120848294047555, "name": "is_set (/home/jeromeku/vllm/vllm/envs.py:830)", "args": {"func_args": {"name": "'VLLM_ATTENTION_BACKEND'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719860.365, "ph": "X", "dur": 11.344645231833487, "name": "CudaPlatformBase.supports_v1 (/home/jeromeku/vllm/vllm/platforms/cuda.py:306)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "model_config": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719878.162, "ph": "X", "dur": 2.9371126506862386, "name": "Platform.is_cuda (/home/jeromeku/vllm/vllm/platforms/interface.py:135)", "args": {"func_args": {"self": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719882.002, "ph": "X", "dur": 1.4676039541853092, "name": "Platform.is_tpu (/home/jeromeku/vllm/vllm/platforms/interface.py:141)", "args": {"func_args": {"self": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180679025.645, "ph": "X", "dur": 40858.71217340043, "name": "EngineArgs._is_v1_supported_oracle (/home/jeromeku/vllm/vllm/engine/arg_utils.py:1190)", "args": {"func_args": {"self": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')", "model_config": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719885.912, "ph": "X", "dur": 3.6409149363078765, "name": "is_set (/home/jeromeku/vllm/vllm/envs.py:830)", "args": {"func_args": {"name": "'VLLM_USE_V1'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719892.76, "ph": "X", "dur": 2.166644384017897, "name": "is_set (/home/jeromeku/vllm/vllm/envs.py:830)", "args": {"func_args": {"name": "'VLLM_USE_V1'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719891.86, "ph": "X", "dur": 12.658917429611376, "name": "set_vllm_use_v1 (/home/jeromeku/vllm/vllm/envs.py:837)", "args": {"func_args": {"use_v1": "True"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719963.512, "ph": "X", "dur": 0.902847857603941, "name": "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)", "args": {"func_args": {"name": "'current_platform'"}, "return_value": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719971.363, "ph": "X", "dur": 0.9657043540194052, "name": "_LoadNvmlLibrary (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2394)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719972.992, "ph": "X", "dur": 1.17998786452667, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlInitWithFlags'"}, "return_value": "<_FuncPtr object at 0x7f05debef5f0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180741502.98, "ph": "X", "dur": 3.474249983691115, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719969.762, "ph": "X", "dur": 21540.627991262994, "name": "nvmlInitWithFlags (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2373)", "args": {"func_args": {"flags": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719968.979, "ph": "X", "dur": 21542.301307387268, "name": "nvmlInit (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2390)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180741518.271, "ph": "X", "dur": 12.443681547946301, "name": "Platform.device_id_to_physical_device_id (/home/jeromeku/vllm/vllm/platforms/interface.py:166)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180741535.47, "ph": "X", "dur": 1.5323651929163937, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetHandleByIndex_v2'"}, "return_value": "<_FuncPtr object at 0x7f05ba9453d0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748805.66, "ph": "X", "dur": 0.8561816708712479, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180741532.03, "ph": "X", "dur": 7274.72518377849, "name": "nvmlDeviceGetHandleByIndex (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2603)", "args": {"func_args": {"index": "0"}, "return_value": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfeddb50>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748815.242, "ph": "X", "dur": 23.656899560002, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetMemoryInfo'"}, "return_value": "<_FuncPtr object at 0x7f05ba9454a0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748859.0, "ph": "X", "dur": 0.7333257915137497, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748811.768, "ph": "X", "dur": 48.112838521406644, "name": "nvmlDeviceGetMemoryInfo (/home/jeromeku/vllm/vllm/third_party/pynvml.py:3191)", "args": {"func_args": {"handle": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfeddb50>", "version": "None"}, "return_value": "<vllm.third_party.pynvml.c_nvmlMemory_t object at 0x7f05bfedd9a0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748863.781, "ph": "X", "dur": 6.017080975043987, "name": "_PrintableStructure.__getattribute__ (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1142)", "args": {"func_args": {"self": "<vllm.third_party.pynvml.c_nvmlMemory_t object at 0x7f05bfedd9a0>", "name": "'total'"}, "return_value": "85520809984"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180741514.412, "ph": "X", "dur": 7356.3576775701795, "name": "NvmlCudaPlatform.get_device_total_memory (/home/jeromeku/vllm/vllm/platforms/cuda.py:359)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "85520809984"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748872.783, "ph": "X", "dur": 0.8114202264541749, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlShutdown'"}, "return_value": "<_FuncPtr object at 0x7f05debef930>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180750765.832, "ph": "X", "dur": 2.307595315373786, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180748871.96, "ph": "X", "dur": 1899.0918975408417, "name": "nvmlShutdown (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2428)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719966.643, "ph": "X", "dur": 30805.109854434682, "name": "with_nvml_context.<locals>.wrapper (/home/jeromeku/vllm/vllm/platforms/cuda.py:38)", "args": {"func_args": {"args": "(<class 'vllm.platforms.cuda.NvmlCudaPlatform'>,)", "kwargs": "{}"}, "return_value": "85520809984"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180750782.783, "ph": "X", "dur": 1.0761794083253728, "name": "_LoadNvmlLibrary (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2394)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180750784.54, "ph": "X", "dur": 1.317129311251319, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlInitWithFlags'"}, "return_value": "<_FuncPtr object at 0x7f05debef5f0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180752319.34, "ph": "X", "dur": 10.891316560715897, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180750780.729, "ph": "X", "dur": 1552.2230839278911, "name": "nvmlInitWithFlags (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2373)", "args": {"func_args": {"flags": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180750779.621, "ph": "X", "dur": 1553.9706850024727, "name": "nvmlInit (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2390)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180752340.064, "ph": "X", "dur": 11.500834101714338, "name": "Platform.device_id_to_physical_device_id (/home/jeromeku/vllm/vllm/platforms/interface.py:166)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180752359.13, "ph": "X", "dur": 1.152369100950178, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetHandleByIndex_v2'"}, "return_value": "<_FuncPtr object at 0x7f05ba9453d0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759656.547, "ph": "X", "dur": 1.064750954431652, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180752355.979, "ph": "X", "dur": 7301.791572083452, "name": "nvmlDeviceGetHandleByIndex (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2603)", "args": {"func_args": {"index": "0"}, "return_value": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfedd9a0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759668.24, "ph": "X", "dur": 5.116137859755666, "name": "convertStrBytes.<locals>.wrapper.<locals>.<listcomp> (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2345)", "args": {"func_args": {".0": "<tuple_iterator object at 0x7f05ba954d30>"}, "return_value": "[<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfedd9a0>]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759688.473, "ph": "X", "dur": 16.316975046759833, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlDeviceGetName'"}, "return_value": "<_FuncPtr object at 0x7f05ba945160>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759711.59, "ph": "X", "dur": 0.6114222833140613, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759676.646, "ph": "X", "dur": 36.67486091610777, "name": "nvmlDeviceGetName (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2638)", "args": {"func_args": {"handle": "<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfedd9a0>"}, "return_value": "b'NVIDIA H100 80GB HBM3'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759662.41, "ph": "X", "dur": 54.774674770288044, "name": "convertStrBytes.<locals>.wrapper (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2342)", "args": {"func_args": {"args": "(<vllm.third_party.pynvml.LP_struct_c_nvmlDevice_t object at 0x7f05bfedd9a0>,)", "kwargs": "{}"}, "return_value": "'NVIDIA H100 80GB HBM3'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180752354.287, "ph": "X", "dur": 7363.748077754785, "name": "NvmlCudaPlatform._get_physical_device_name (/home/jeromeku/vllm/vllm/platforms/cuda.py:393)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "'NVIDIA H100 80GB HBM3'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180752336.207, "ph": "X", "dur": 7382.155507492938, "name": "NvmlCudaPlatform.get_device_name (/home/jeromeku/vllm/vllm/platforms/cuda.py:346)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "device_id": "0"}, "return_value": "'NVIDIA H100 80GB HBM3'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759723.045, "ph": "X", "dur": 0.9428474462319638, "name": "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)", "args": {"func_args": {"name": "'nvmlShutdown'"}, "return_value": "<_FuncPtr object at 0x7f05debef930>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761627.182, "ph": "X", "dur": 1.9999794314011354, "name": "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)", "args": {"func_args": {"ret": "0"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180759722.229, "ph": "X", "dur": 1909.6755982175848, "name": "nvmlShutdown (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2428)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180750774.781, "ph": "X", "dur": 10857.795953074456, "name": "with_nvml_context.<locals>.wrapper (/home/jeromeku/vllm/vllm/platforms/cuda.py:38)", "args": {"func_args": {"args": "(<class 'vllm.platforms.cuda.NvmlCudaPlatform'>,)", "kwargs": "{}"}, "return_value": "'NVIDIA H100 80GB HBM3'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761642.564, "ph": "X", "dur": 7.521827404383889, "name": "Platform.is_tpu (/home/jeromeku/vllm/vllm/platforms/interface.py:141)", "args": {"func_args": {"self": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761657.145, "ph": "X", "dur": 1.1723688952641893, "name": "Platform.is_tpu (/home/jeromeku/vllm/vllm/platforms/interface.py:141)", "args": {"func_args": {"self": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180719913.652, "ph": "X", "dur": 41845.99344837118, "name": "EngineArgs._set_default_args_v1 (/home/jeromeku/vllm/vllm/engine/arg_utils.py:1467)", "args": {"func_args": {"self": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')", "usage_context": "<UsageContext.LLM_CLASS: 'LLM_CLASS'>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761765.617, "ph": "X", "dur": 9.178953218973401, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:387)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761763.656, "ph": "X", "dur": 11.310359870152325, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_ATTENTION_BACKEND'"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761807.744, "ph": "X", "dur": 18.713140879809956, "name": "ModelConfig.get_hf_config_sliding_window (/home/jeromeku/vllm/vllm/config.py:1043)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761779.947, "ph": "X", "dur": 46.7366621983711, "name": "ModelConfig.get_sliding_window (/home/jeromeku/vllm/vllm/config.py:1055)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761855.941, "ph": "X", "dur": 14.552231291337785, "name": "is_in_ray_actor (/home/jeromeku/vllm/vllm/utils.py:2494)", "args": {"func_args": {}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761947.248, "ph": "X", "dur": 47.04523045350157, "name": "EngineArgs.create_speculative_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:928)", "args": {"func_args": {"self": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=16384, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=1024, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=True, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')", "target_model_config": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')", "target_parallel_config": "ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='auto', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0)", "enable_chunked_prefill": "True", "disable_log_stats": "True"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180761997.344, "ph": "X", "dur": 12.092256590714388, "name": "ModelConfig.runner_type (/home/jeromeku/vllm/vllm/config.py:1365)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "'generate'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180762012.477, "ph": "X", "dur": 6.80754903602634, "name": "ModelConfig.is_multimodal_model (/home/jeromeku/vllm/vllm/config.py:1349)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180762022.78, "ph": "X", "dur": 3.859960302604191, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:424)", "args": {"func_args": {}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180762021.078, "ph": "X", "dur": 5.702798492966665, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_USE_RAY_SPMD_WORKER'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180762370.82, "ph": "X", "dur": 38.90531416769894, "name": "EngineArgs.create_load_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:914)", "args": {"func_args": {"self": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=16384, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=1024, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=True, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')"}, "return_value": "LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098719.586, "ph": "X", "dur": 11664144.117406059, "name": "EngineArgs.create_engine_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:960)", "args": {"func_args": {"self": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')", "usage_context": "<UsageContext.LLM_CLASS: 'LLM_CLASS'>"}, "return_value": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180763248.021, "ph": "X", "dur": 3.6447244209391165, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:589)", "args": {"func_args": {}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815180763246.674, "ph": "X", "dur": 5.156137448383689, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_USE_V1'"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182190830.928, "ph": "X", "dur": 164.9087802017595, "name": "Executor.get_class (/home/jeromeku/vllm/vllm/v1/executor/abstract.py:26)", "args": {"func_args": {"vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')"}, "return_value": "<class 'vllm.v1.executor.abstract.UniProcExecutor'>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191003.906, "ph": "X", "dur": 8.967526821939568, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:664)", "args": {"func_args": {}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191000.164, "ph": "X", "dur": 12.894153105590462, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_ENABLE_V1_MULTIPROCESSING'"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191318.566, "ph": "X", "dur": 4.638999909692824, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:589)", "args": {"func_args": {}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191316.031, "ph": "X", "dur": 7.3618290498717975, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_USE_V1'"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191385.582, "ph": "X", "dur": 107.1465171094213, "name": "check_gguf_file (/home/jeromeku/vllm/vllm/transformers_utils/utils.py:19)", "args": {"func_args": {"model": "'facebook/opt-125m'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182392400.508, "ph": "X", "dur": 19.80360585550248, "name": "get_cached_tokenizer.<locals>.CachedTokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:107)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182378953.328, "ph": "X", "dur": 13514.03625427051, "name": "get_cached_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:83)", "args": {"func_args": {"tokenizer": "GPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191376.627, "ph": "X", "dur": 201140.88471897252, "name": "get_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:160)", "args": {"func_args": {"tokenizer_name": "'facebook/opt-125m'", "tokenizer_mode": "'auto'", "trust_remote_code": "False", "revision": "None", "download_dir": "None", "args": "()", "kwargs": "{'max_loras': 0, 'truncation_side': 'left'}"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182392557.74, "ph": "X", "dur": 45.9414322815997, "name": "LRUCache.__init__ (/home/jeromeku/vllm/vllm/utils.py:258)", "args": {"func_args": {"self": "Not Displayable", "capacity": "0", "getsizeof": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191353.416, "ph": "X", "dur": 201251.58072338713, "name": "TokenizerGroup.__init__ (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:17)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "tokenizer_id": "'facebook/opt-125m'", "enable_lora": "False", "max_num_seqs": "1024", "max_input_length": "None", "tokenizer_config": "{'max_loras': 0, 'tokenizer_mode': 'auto', 'trust_remote_code': False, 'revision': None, 'truncation_side': 'left'}"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191327.336, "ph": "X", "dur": 201279.70233893493, "name": "init_tokenizer_from_configs (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:107)", "args": {"func_args": {"model_config": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')", "scheduler_config": "SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler')", "lora_config": "None"}, "return_value": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182392963.193, "ph": "X", "dur": 337112.4358562545, "name": "try_get_generation_config (/home/jeromeku/vllm/vllm/transformers_utils/config.py:801)", "args": {"func_args": {"model": "'facebook/opt-125m'", "trust_remote_code": "False", "revision": "None"}, "return_value": "GenerationConfig {\n  \"bos_token_id\": 2,\n  \"eos_token_id\": 2,\n  \"pad_token_id\": 1\n}\n"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182392941.8, "ph": "X", "dur": 337558.4588882205, "name": "ModelConfig.try_get_generation_config (/home/jeromeku/vllm/vllm/config.py:1275)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "{'pad_token_id': 1, 'bos_token_id': 2, 'eos_token_id': 2, '_from_model_config': True, 'transformers_version': '4.51.3'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730509.736, "ph": "X", "dur": 36.09962873679049, "name": "InputPreprocessor.__init__ (/home/jeromeku/vllm/vllm/inputs/preprocess.py:30)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "model_config": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')", "tokenizer": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "mm_registry": "<vllm.multimodal.registry.MultiModalRegistry object at 0x7f05dff0d790>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730665.11, "ph": "X", "dur": 22.111201170876267, "name": "LRUCache.__init__ (/home/jeromeku/vllm/vllm/utils.py:258)", "args": {"func_args": {"self": "Not Displayable", "capacity": "4294967296", "getsizeof": "<function ProcessingCache.get_lru_cache.<locals>.get_item_size at 0x7f05b85fd940>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730571.361, "ph": "X", "dur": 116.31880373029003, "name": "ProcessingCache.get_lru_cache (/home/jeromeku/vllm/vllm/multimodal/processing.py:894)", "args": {"func_args": {"capacity_gb": "4", "value_type": "<class 'vllm.multimodal.inputs.MultiModalKwargs'>", "debug": "False"}, "return_value": "LRUCache({}, maxsize=4294967296, currsize=0)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730550.354, "ph": "X", "dur": 139.56237420780246, "name": "MirroredProcessingCache.__init__ (/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py:35)", "args": {"func_args": {"self": "<vllm.v1.engine.mm_input_cache.MirroredProcessingCache object at 0x7f05b88ce4d0>", "model_config": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182392617.457, "ph": "X", "dur": 338073.3745450002, "name": "Processor.__init__ (/home/jeromeku/vllm/vllm/v1/engine/processor.py:31)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "tokenizer": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "mm_registry": "<vllm.multimodal.registry.MultiModalRegistry object at 0x7f05dff0d790>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730701.713, "ph": "X", "dur": 1.4123664270323257, "name": "LoRARequestStates.__init__ (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:185)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730696.47, "ph": "X", "dur": 7.1561168797848245, "name": "OutputProcessor.__init__ (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:231)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "tokenizer": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "log_stats": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731297.533, "ph": "X", "dur": 9.366570337061983, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:795)", "args": {"func_args": {}, "return_value": "256"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731292.465, "ph": "X", "dur": 14.722705728585787, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_MSGPACK_ZERO_COPY_THRESHOLD'"}, "return_value": "256"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731316.416, "ph": "X", "dur": 3.6209151419938648, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:801)", "args": {"func_args": {}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731314.678, "ph": "X", "dur": 5.5247050864561835, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_ALLOW_INSECURE_SERIALIZATION'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731288.21, "ph": "X", "dur": 34.836784581534346, "name": "MsgpackEncoder.__init__ (/home/jeromeku/vllm/vllm/v1/serial_utils.py:58)", "args": {"func_args": {"self": "<vllm.v1.serial_utils.MsgpackEncoder object at 0x7f05b8792890>", "size_threshold": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731710.322, "ph": "X", "dur": 4.937092082087374, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:801)", "args": {"func_args": {}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731709.133, "ph": "X", "dur": 6.276125929968324, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_ALLOW_INSECURE_SERIALIZATION'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731326.758, "ph": "X", "dur": 388.89219095131955, "name": "MsgpackDecoder.__init__ (/home/jeromeku/vllm/vllm/v1/serial_utils.py:203)", "args": {"func_args": {"self": "<vllm.v1.serial_utils.MsgpackDecoder object at 0x7f05b87921d0>", "t": "<class 'vllm.v1.engine.EngineCoreOutputs'>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731857.418, "ph": "X", "dur": 7.182783272203506, "name": "CoreEngine.__init__ (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:286)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.CoreEngine object at 0x7f05b894f210>", "index": "0", "local": "True"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731884.9, "ph": "X", "dur": 13.539860750585685, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:261)", "args": {"func_args": {}, "return_value": "'/tmp'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731880.877, "ph": "X", "dur": 17.794102712523244, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_RPC_BASE_PATH'"}, "return_value": "'/tmp'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731878.26, "ph": "X", "dur": 42.10528125794076, "name": "get_open_zmq_ipc_path (/home/jeromeku/vllm/vllm/utils.py:626)", "args": {"func_args": {}, "return_value": "'ipc:///tmp/edec0891-6187-477b-bbe2-2c244126d941'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731922.746, "ph": "X", "dur": 4.12948134026444, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:261)", "args": {"func_args": {}, "return_value": "'/tmp'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731921.89, "ph": "X", "dur": 5.160899304172739, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_RPC_BASE_PATH'"}, "return_value": "'/tmp'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731920.98, "ph": "X", "dur": 11.537024205711122, "name": "get_open_zmq_ipc_path (/home/jeromeku/vllm/vllm/utils.py:626)", "args": {"func_args": {}, "return_value": "'ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731868.004, "ph": "X", "dur": 64.81171440244832, "name": "MPClient._get_zmq_addresses (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:441)", "args": {"func_args": {"parallel_config": "ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0)", "spmd_mode": "True"}, "return_value": "('ipc:///tmp/edec0891-6187-477b-bbe2-2c244126d941', 'ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732427.67, "ph": "X", "dur": 38.069132291141706, "name": "split_zmq_path (/home/jeromeku/vllm/vllm/utils.py:2378)", "args": {"func_args": {"path": "'ipc:///tmp/edec0891-6187-477b-bbe2-2c244126d941'"}, "return_value": "('ipc', '', '')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731942.424, "ph": "X", "dur": 637.0153534378912, "name": "make_zmq_socket (/home/jeromeku/vllm/vllm/utils.py:2418)", "args": {"func_args": {"ctx": "<zmq.Context() at 0x7f05b8415c70>", "path": "'ipc:///tmp/edec0891-6187-477b-bbe2-2c244126d941'", "socket_type": "<SocketType.ROUTER: 6>", "bind": "True", "identity": "None"}, "return_value": "<zmq.Socket(zmq.ROUTER) at 0x7f05b84110f0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732687.928, "ph": "X", "dur": 18.168384577542597, "name": "split_zmq_path (/home/jeromeku/vllm/vllm/utils.py:2378)", "args": {"func_args": {"path": "'ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8'"}, "return_value": "('ipc', '', '')"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732586.604, "ph": "X", "dur": 148.3213317461816, "name": "make_zmq_socket (/home/jeromeku/vllm/vllm/utils.py:2418)", "args": {"func_args": {"ctx": "<zmq.Context(1 socket) at 0x7f05b8415c70>", "path": "'ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8'", "socket_type": "<SocketType.PULL: 7>", "bind": "None", "identity": "None"}, "return_value": "<zmq.Socket(zmq.PULL) at 0x7f05b8411240>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732978.285, "ph": "X", "dur": 5.750417050857169, "name": "_maybe_force_spawn (/home/jeromeku/vllm/vllm/utils.py:2505)", "args": {"func_args": {}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732989.482, "ph": "X", "dur": 2.5666402702981235, "name": "<lambda> (/home/jeromeku/vllm/vllm/envs.py:456)", "args": {"func_args": {}, "return_value": "'spawn'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732986.39, "ph": "X", "dur": 5.901844064948969, "name": "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)", "args": {"func_args": {"name": "'VLLM_WORKER_MULTIPROC_METHOD'"}, "return_value": "'spawn'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732971.3, "ph": "X", "dur": 38.28532054396459, "name": "get_mp_context (/home/jeromeku/vllm/vllm/utils.py:2533)", "args": {"func_args": {}, "return_value": "<multiprocessing.context.SpawnContext object at 0x7f06d2e9c450>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182735798.014, "ph": "X", "dur": 11.357026056885019, "name": "CoreEngineProcManager.finished_procs.<locals>.<dictcomp> (/home/jeromeku/vllm/vllm/v1/utils.py:161)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b840d000>"}, "return_value": "{}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182735792.135, "ph": "X", "dur": 18.329335303212503, "name": "CoreEngineProcManager.finished_procs (/home/jeromeku/vllm/vllm/v1/utils.py:159)", "args": {"func_args": {"self": "<vllm.v1.utils.CoreEngineProcManager object at 0x7f05b894f890>"}, "return_value": "{}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182732747.33, "ph": "X", "dur": 3063.6103973512027, "name": "CoreEngineProcManager.__init__ (/home/jeromeku/vllm/vllm/v1/utils.py:104)", "args": {"func_args": {"self": "<vllm.v1.utils.CoreEngineProcManager object at 0x7f05b894f890>", "target_fn": "<function EngineCoreProc.run_engine_core at 0x7f05b8744720>", "local_engine_count": "1", "start_index": "0", "local_start_index": "0", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "on_head_node": "True", "input_address": "'ipc:///tmp/edec0891-6187-477b-bbe2-2c244126d941'", "executor_class": "<class 'vllm.v1.executor.abstract.UniProcExecutor'>", "log_stats": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182735901.78, "ph": "X", "dur": 3.5952011207329932, "name": "CoreEngineProcManager.sentinels.<locals>.<listcomp> (/home/jeromeku/vllm/vllm/v1/utils.py:157)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b840cc70>"}, "return_value": "[31]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182735899.643, "ph": "X", "dur": 6.638026969936149, "name": "CoreEngineProcManager.sentinels (/home/jeromeku/vllm/vllm/v1/utils.py:156)", "args": {"func_args": {"self": "<vllm.v1.utils.CoreEngineProcManager object at 0x7f05b894f890>"}, "return_value": "[31]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815189636381.572, "ph": "X", "dur": 9.402760441058767, "name": "MPClient._wait_for_engine_startup.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:501)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b840d090>"}, "return_value": "<vllm.v1.engine.core_client.CoreEngine object at 0x7f05b894f210>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815189636392.662, "ph": "X", "dur": 1.1923686895782006, "name": "MPClient._wait_for_engine_startup.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:501)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b840d090>"}, "return_value": "<NULL>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815189636414.201, "ph": "X", "dur": 13.096055791046195, "name": "MsgpackEncoder.encode (/home/jeromeku/vllm/vllm/v1/serial_utils.py:70)", "args": {"func_args": {"self": "<vllm.v1.serial_utils.MsgpackEncoder object at 0x7f05b8792890>", "obj": "{'output_socket_address': 'ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8', 'parallel_config': {'data_parallel_master_ip': '127.0.0.1', 'data_parallel_master_port': 0, 'data_parallel_size': 1}}"}, "return_value": "[b'\\x82\\xb5output_socket_address\\xd9/ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8\\xafparallel_config\\x83\\xb7data_parallel_master_ip\\xa9127.0.0.1\\xb9data_parallel_master_port\\x00\\xb2data_parallel_size\\x01']"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205516746.773, "ph": "X", "dur": 11.003696357337486, "name": "MPClient._wait_for_engine_startup.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:501)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b840cc70>"}, "return_value": "<vllm.v1.engine.core_client.CoreEngine object at 0x7f05b894f210>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205516760.191, "ph": "X", "dur": 1.5847456065959473, "name": "MPClient._wait_for_engine_startup.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:501)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b840cc70>"}, "return_value": "<NULL>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182735828.215, "ph": "X", "dur": 22781035.425838895, "name": "MPClient._wait_for_engine_startup (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:460)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "output_address": "'ipc:///tmp/271192f9-2177-480c-aac3-5790689ff0d8'", "parallel_config": "ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182731143.947, "ph": "X", "dur": 22785744.220268887, "name": "MPClient.__init__ (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:355)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "asyncio_mode": "False", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "executor_class": "<class 'vllm.v1.executor.abstract.UniProcExecutor'>", "log_stats": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205516982.46, "ph": "X", "dur": 29.89207353018449, "name": "get_open_zmq_inproc_path (/home/jeromeku/vllm/vllm/utils.py:631)", "args": {"func_args": {}, "return_value": "'inproc://0159c212-b0d1-4188-9062-f3b57ee9943a'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730979.418, "ph": "X", "dur": 22786407.87344361, "name": "SyncMPClient.__init__ (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:587)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "executor_class": "<class 'vllm.v1.executor.abstract.UniProcExecutor'>", "log_stats": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182730709.014, "ph": "X", "dur": 22786680.52968712, "name": "EngineCoreClient.make_client (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:62)", "args": {"func_args": {"multiprocess_mode": "True", "asyncio_mode": "False", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "executor_class": "<class 'vllm.v1.executor.abstract.UniProcExecutor'>", "log_stats": "False"}, "return_value": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517400.809, "ph": "X", "dur": 3.2647283289729008, "name": "Processor.mm_registry (/home/jeromeku/vllm/vllm/v1/engine/processor.py:57)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>"}, "return_value": "<vllm.multimodal.registry.MultiModalRegistry object at 0x7f05dff0d790>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517432.303, "ph": "X", "dur": 29.768265279669183, "name": "LRUCache.clear (/home/jeromeku/vllm/vllm/utils.py:427)", "args": {"func_args": {"self": "LRUCache({}, maxsize=4294967296, currsize=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517420.246, "ph": "X", "dur": 42.36242147054947, "name": "ProcessingCache.reset (/home/jeromeku/vllm/vllm/multimodal/processing.py:1032)", "args": {"func_args": {"self": "<vllm.multimodal.processing.ProcessingCache object at 0x7f05cb738b90>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517411.918, "ph": "X", "dur": 51.02614189314763, "name": "MultiModalRegistry.reset_processor_cache (/home/jeromeku/vllm/vllm/multimodal/registry.py:91)", "args": {"func_args": {"self": "<vllm.multimodal.registry.MultiModalRegistry object at 0x7f05dff0d790>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517468.676, "ph": "X", "dur": 6.258030877969934, "name": "LRUCache.clear (/home/jeromeku/vllm/vllm/utils.py:427)", "args": {"func_args": {"self": "LRUCache({}, maxsize=4294967296, currsize=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517465.928, "ph": "X", "dur": 9.361808481272934, "name": "MirroredProcessingCache.reset (/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py:87)", "args": {"func_args": {"self": "<vllm.v1.engine.mm_input_cache.MirroredProcessingCache object at 0x7f05b88ce4d0>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517597.021, "ph": "X", "dur": 1.6152214836458694, "name": "MPClient.ensure_alive (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:561)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517600.756, "ph": "X", "dur": 1.2752249803076763, "name": "MPClient.free_pending_messages (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:569)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517607.809, "ph": "X", "dur": 5.5847044693982175, "name": "MsgpackEncoder.encode (/home/jeromeku/vllm/vllm/v1/serial_utils.py:70)", "args": {"func_args": {"self": "<vllm.v1.serial_utils.MsgpackEncoder object at 0x7f05b8792890>", "obj": "(11522752091851919856, 'reset_mm_cache', ())"}, "return_value": "[b'\\x93\\xcf\\x9f\\xe9\\t\\xfa5\\xb9\\x11\\xf0\\xaereset_mm_cache\\x90']"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517586.953, "ph": "X", "dur": 71.24974342924435, "name": "SyncMPClient._send_input (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:659)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_type": "<EngineCoreRequestType.UTILITY: b'\\x03'>", "request": "(11522752091851919856, 'reset_mm_cache', ())"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517480.572, "ph": "X", "dur": 1361.5631399901056, "name": "SyncMPClient.call_utility (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:674)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "method": "'reset_mm_cache'", "args": "()"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517476.62, "ph": "X", "dur": 1368.569734598114, "name": "SyncMPClient.reset_mm_cache (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:693)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205517395.58, "ph": "X", "dur": 1452.3203018447734, "name": "LLMEngine.reset_mm_cache (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:246)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182191019.51, "ph": "X", "dur": 23327829.2861988, "name": "LLMEngine.__init__ (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:40)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "executor_class": "<class 'vllm.v1.executor.abstract.UniProcExecutor'>", "log_stats": "False", "usage_context": "<UsageContext.LLM_CLASS: 'LLM_CLASS'>", "stat_loggers": "None", "mm_registry": "<vllm.multimodal.registry.MultiModalRegistry object at 0x7f05dff0d790>", "use_cached_outputs": "False", "multiprocess_mode": "True"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815182190416.299, "ph": "X", "dur": 23328435.35806095, "name": "LLMEngine.from_vllm_config (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:107)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.llm_engine.LLMEngine'>", "vllm_config": "VllmConfig(model_config=ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto'), cache_config=CacheConfig(block_size=16, gpu_memory_utilization=0.9, swap_space=4, cache_dtype='auto', is_attention_free=False, num_gpu_blocks_override=None, sliding_window=None, enable_prefix_caching=True, prefix_caching_hash_algo='builtin', cpu_offload_gb=0, calculate_kv_scales=False, num_gpu_blocks=None, num_cpu_blocks=None), parallel_config=ParallelConfig(pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=1, data_parallel_rank=0, data_parallel_rank_local=0, data_parallel_master_ip='127.0.0.1', data_parallel_rpc_port=29550, data_parallel_master_port=0, enable_expert_parallel=False, max_parallel_loading_workers=None, disable_custom_all_reduce=False, tokenizer_pool_config=None, ray_workers_use_nsight=False, placement_group=None, distributed_executor_backend='uni', worker_cls='vllm.v1.worker.gpu_worker.Worker', sd_worker_cls='auto', worker_extension_cls='', world_size=1, rank=0), scheduler_config=SchedulerConfig(runner_type='generate', max_num_batched_tokens=16384, max_num_seqs=1024, max_model_len=2048, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, num_lookahead_slots=0, cuda_graph_sizes=[2], delay_factor=0.0, enable_chunked_prefill=True, is_multimodal_model=False, max_num_encoder_input_tokens=16384, encoder_cache_size=16384, preemption_mode=None, num_scheduler_steps=1, multi_step_stream_outputs=True, send_delta_data=False, policy='fcfs', chunked_prefill_enabled=True, disable_chunked_mm_input=False, scheduler_cls='vllm.v1.core.sched.scheduler.Scheduler'), device_config=DeviceConfig(device=device(type='cuda'), device_type='cuda'), load_config=LoadConfig(load_format=<LoadFormat.AUTO: 'auto'>, download_dir=None, model_loader_extra_config={}, ignore_patterns=['original/**/*'], use_tqdm_on_load=True, pt_load_map_location='cpu'), lora_config=None, speculative_config=None, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), prompt_adapter_config=None, quant_config=None, compilation_config={\"level\": 3, \"custom_ops\": [\"none\"], \"splitting_ops\": [\"vllm.unified_attention\", \"vllm.unified_attention_with_output\"], \"compile_sizes\": [], \"inductor_compile_config\": {\"enable_auto_functionalized_v2\": false}, \"use_cudagraph\": true, \"cudagraph_num_of_warmups\": 1, \"cudagraph_capture_sizes\": [4, 2, 1], \"max_capture_size\": 4}, kv_transfer_config=None, kv_events_config=None, additional_config=None, instance_id='c26b3')", "usage_context": "<UsageContext.LLM_CLASS: 'LLM_CLASS'>", "stat_loggers": "None", "disable_log_stats": "True"}, "return_value": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098674.31, "ph": "X", "dur": 36420180.997244015, "name": "LLMEngine.from_engine_args (/home/jeromeku/vllm/vllm/engine/llm_engine.py:495)", "args": {"func_args": {"cls": "<class 'vllm.engine.llm_engine.LLMEngine'>", "engine_args": "EngineArgs(model='facebook/opt-125m', served_model_name=None, tokenizer=None, hf_config_path=None, task='auto', skip_tokenizer_init=False, enable_prompt_embeds=False, tokenizer_mode='auto', trust_remote_code=False, allowed_local_media_path='', download_dir=None, load_format='auto', config_format='auto', dtype='auto', kv_cache_dtype='auto', seed=None, max_model_len=None, cuda_graph_sizes=[2], distributed_executor_backend=None, pipeline_parallel_size=1, tensor_parallel_size=1, data_parallel_size=1, data_parallel_size_local=None, data_parallel_address=None, data_parallel_rpc_port=None, enable_expert_parallel=False, max_parallel_loading_workers=None, block_size=None, enable_prefix_caching=None, prefix_caching_hash_algo='builtin', disable_sliding_window=False, disable_cascade_attn=False, use_v2_block_manager=True, swap_space=4, cpu_offload_gb=0, gpu_memory_utilization=0.9, max_num_batched_tokens=None, max_num_partial_prefills=1, max_long_partial_prefills=1, long_prefill_token_threshold=0, max_num_seqs=None, max_logprobs=20, disable_log_stats=True, revision=None, code_revision=None, rope_scaling={}, rope_theta=None, hf_token=None, hf_overrides=None, tokenizer_revision=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=8192, disable_custom_all_reduce=False, tokenizer_pool_size=0, tokenizer_pool_type='ray', tokenizer_pool_extra_config={}, limit_mm_per_prompt={}, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, enable_lora=False, enable_lora_bias=False, max_loras=1, max_lora_rank=16, fully_sharded_loras=False, max_cpu_loras=None, lora_dtype='auto', lora_extra_vocab_size=256, long_lora_scaling_factors=None, enable_prompt_adapter=False, max_prompt_adapters=1, max_prompt_adapter_token=0, device='auto', num_scheduler_steps=1, multi_step_stream_outputs=True, ray_workers_use_nsight=False, num_gpu_blocks_override=None, num_lookahead_slots=0, model_loader_extra_config={}, ignore_patterns=None, preemption_mode=None, scheduler_delay_factor=0.0, enable_chunked_prefill=None, disable_chunked_mm_input=False, guided_decoding_backend='auto', guided_decoding_disable_fallback=False, guided_decoding_disable_any_whitespace=False, guided_decoding_disable_additional_properties=False, logits_processor_pattern=None, speculative_config=None, qlora_adapter_name_or_path=None, show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, disable_async_output_proc=False, scheduling_policy='fcfs', scheduler_cls='vllm.core.scheduler.Scheduler', override_neuron_config={}, override_pooler_config=None, compilation_config=None, worker_cls='auto', worker_extension_cls='', kv_transfer_config=None, kv_events_config=None, generation_config='auto', enable_sleep_mode=False, override_generation_config={}, model_impl='auto', calculate_kv_scales=False, additional_config=None, enable_reasoning=None, reasoning_parser='', use_tqdm_on_load=True, pt_load_map_location='cpu')", "usage_context": "<UsageContext.LLM_CLASS: 'LLM_CLASS'>", "stat_loggers": "None"}, "return_value": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205518864.932, "ph": "X", "dur": 3.2875852367603424, "name": "Counter.__init__ (/home/jeromeku/vllm/vllm/utils.py:213)", "args": {"func_args": {"self": "<vllm.utils.Counter object at 0x7f05b9d63f10>", "start": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098635.04, "ph": "X", "dur": 36420234.6785967, "name": "LLM.__init__ (/home/jeromeku/vllm/vllm/entrypoints/llm.py:158)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "model": "'facebook/opt-125m'", "tokenizer": "None", "tokenizer_mode": "'auto'", "skip_tokenizer_init": "False", "trust_remote_code": "False", "allowed_local_media_path": "''", "tensor_parallel_size": "1", "dtype": "'auto'", "quantization": "None", "revision": "None", "tokenizer_revision": "None", "seed": "None", "gpu_memory_utilization": "0.9", "swap_space": "4", "cpu_offload_gb": "0", "enforce_eager": "False", "max_seq_len_to_capture": "8192", "disable_custom_all_reduce": "False", "disable_async_output_proc": "False", "hf_token": "None", "hf_overrides": "None", "mm_processor_kwargs": "None", "task": "'auto'", "override_pooler_config": "None", "compilation_config": "None", "kwargs": "{'cuda_graph_sizes': [2]}"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815169098612.1, "ph": "X", "dur": 36420265.450661175, "name": "deprecate_args.<locals>.wrapper.<locals>.inner (/home/jeromeku/vllm/vllm/utils.py:1161)", "args": {"func_args": {"args": "(<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>,)", "kwargs": "{'model': 'facebook/opt-125m', 'cuda_graph_sizes': [2]}"}, "return_value": "None"}, "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1733957976138160729}, "file_info": {"files": {"/home/jeromeku/vllm/vllm/entrypoints/llm.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport warnings\nfrom collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom typing import Any, Callable, ClassVar, Optional, Union, cast, overload\n\nimport cloudpickle\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nfrom typing_extensions import TypeVar, deprecated\n\nfrom vllm.beam_search import (BeamSearchInstance, BeamSearchOutput,\n                              BeamSearchSequence, get_beam_search_score)\nfrom vllm.config import (CompilationConfig, ModelDType, TokenizerMode,\n                         is_init_field)\nfrom vllm.engine.arg_utils import (EngineArgs, HfOverrides, PoolerConfig,\n                                   TaskOption)\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.entrypoints.chat_utils import (ChatCompletionMessageParam,\n                                         ChatTemplateContentFormatOption,\n                                         apply_hf_chat_template,\n                                         apply_mistral_chat_template,\n                                         parse_chat_messages,\n                                         resolve_chat_template_content_format)\nfrom vllm.entrypoints.score_utils import (_cosine_similarity,\n                                          _validate_score_input_lens)\nfrom vllm.entrypoints.utils import _validate_truncation_size\nfrom vllm.inputs import PromptType, SingletonPrompt, TextPrompt, TokensPrompt\nfrom vllm.inputs.parse import parse_and_batch_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.guided_decoding.guided_fields import (\n    GuidedDecodingRequest, LLMGuidedOptions)\nfrom vllm.model_executor.layers.quantization import QuantizationMethods\nfrom vllm.outputs import (ClassificationRequestOutput, EmbeddingRequestOutput,\n                          PoolingRequestOutput, RequestOutput,\n                          ScoringRequestOutput)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import (BeamSearchParams, GuidedDecodingParams,\n                                  RequestOutputKind, SamplingParams)\nfrom vllm.transformers_utils.tokenizer import (AnyTokenizer, MistralTokenizer,\n                                               get_cached_tokenizer)\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import (Counter, Device, deprecate_args, deprecate_kwargs,\n                        is_list_of)\n\nlogger = init_logger(__name__)\n\n_R = TypeVar(\"_R\", default=Any)\n\n\nclass LLM:\n    \"\"\"An LLM for generating texts from given prompts and sampling parameters.\n\n    This class includes a tokenizer, a language model (possibly distributed\n    across multiple GPUs), and GPU memory space allocated for intermediate\n    states (aka KV cache). Given a batch of prompts and sampling parameters,\n    this class generates texts from the model, using an intelligent batching\n    mechanism and efficient memory management.\n\n    Args:\n        model: The name or path of a HuggingFace Transformers model.\n        tokenizer: The name or path of a HuggingFace Transformers tokenizer.\n        tokenizer_mode: The tokenizer mode. \"auto\" will use the fast tokenizer\n            if available, and \"slow\" will always use the slow tokenizer.\n        skip_tokenizer_init: If true, skip initialization of tokenizer and\n            detokenizer. Expect valid prompt_token_ids and None for prompt\n            from the input.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        allowed_local_media_path: Allowing API requests to read local images\n            or videos from directories specified by the server file system.\n            This is a security risk. Should only be enabled in trusted\n            environments.\n        tensor_parallel_size: The number of GPUs to use for distributed\n            execution with tensor parallelism.\n        dtype: The data type for the model weights and activations. Currently,\n            we support `float32`, `float16`, and `bfloat16`. If `auto`, we use\n            the `torch_dtype` attribute specified in the model config file.\n            However, if the `torch_dtype` in the config is `float32`, we will\n            use `float16` instead.\n        quantization: The method used to quantize the model weights. Currently,\n            we support \"awq\", \"gptq\", and \"fp8\" (experimental).\n            If None, we first check the `quantization_config` attribute in the\n            model config file. If that is None, we assume the model weights are\n            not quantized and use `dtype` to determine the data type of\n            the weights.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id.\n        seed: The seed to initialize the random number generator for sampling.\n        gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to\n            reserve for the model weights, activations, and KV cache. Higher\n            values will increase the KV cache size and thus improve the model's\n            throughput. However, if the value is too high, it may cause out-of-\n            memory (OOM) errors.\n        swap_space: The size (GiB) of CPU memory per GPU to use as swap space.\n            This can be used for temporarily storing the states of the requests\n            when their `best_of` sampling parameters are larger than 1. If all\n            requests will have `best_of=1`, you can safely set this to 0.\n            Noting that `best_of` is only supported in V0. Otherwise, too small\n            values may cause out-of-memory (OOM) errors.\n        cpu_offload_gb: The size (GiB) of CPU memory to use for offloading\n            the model weights. This virtually increases the GPU memory space\n            you can use to hold the model weights, at the cost of CPU-GPU data\n            transfer for every forward pass.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n        max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode. Additionally for encoder-decoder models, if the\n            sequence length of the encoder input is larger than this, we fall\n            back to the eager mode.\n        disable_custom_all_reduce: See {class}`~vllm.config.ParallelConfig`\n        disable_async_output_proc: Disable async output processing.\n            This may result in lower performance.\n        hf_token: The token to use as HTTP bearer authorization for remote files\n            . If `True`, will use the token generated when running\n            `huggingface-cli login` (stored in `~/.huggingface`).\n        hf_overrides: If a dictionary, contains arguments to be forwarded to the\n            HuggingFace config. If a callable, it is called to update the\n            HuggingFace config.\n        compilation_config: Either an integer or a dictionary. If it is an\n            integer, it is used as the level of compilation optimization. If it\n            is a dictionary, it can specify the full compilation configuration.\n        **kwargs: Arguments for {class}`~vllm.EngineArgs`. (See\n            {ref}`engine-args`)\n\n    :::{note}\n    This class is intended to be used for offline inference. For online\n    serving, use the {class}`~vllm.AsyncLLMEngine` class instead.\n    :::\n    \"\"\"\n\n    DEPRECATE_LEGACY: ClassVar[bool] = True\n    \"\"\"A flag to toggle whether to deprecate the legacy generate/encode API.\"\"\"\n\n    DEPRECATE_INIT_POSARGS: ClassVar[bool] = True\n    \"\"\"\n    A flag to toggle whether to deprecate positional arguments in\n    {meth}`LLM.__init__`.\n    \"\"\"\n\n    @classmethod\n    @contextmanager\n    def deprecate_legacy_api(cls):\n        cls.DEPRECATE_LEGACY = True\n\n        yield\n\n        cls.DEPRECATE_LEGACY = False\n\n    @deprecate_args(\n        start_index=2,  # Ignore self and model\n        is_deprecated=lambda: LLM.DEPRECATE_INIT_POSARGS,\n        additional_message=(\n            \"All positional arguments other than `model` will be \"\n            \"replaced with keyword arguments in an upcoming version.\"),\n    )\n    def __init__(\n        self,\n        model: str,\n        tokenizer: Optional[str] = None,\n        tokenizer_mode: TokenizerMode = \"auto\",\n        skip_tokenizer_init: bool = False,\n        trust_remote_code: bool = False,\n        allowed_local_media_path: str = \"\",\n        tensor_parallel_size: int = 1,\n        dtype: ModelDType = \"auto\",\n        quantization: Optional[QuantizationMethods] = None,\n        revision: Optional[str] = None,\n        tokenizer_revision: Optional[str] = None,\n        seed: Optional[int] = None,\n        gpu_memory_utilization: float = 0.9,\n        swap_space: float = 4,\n        cpu_offload_gb: float = 0,\n        enforce_eager: bool = False,\n        max_seq_len_to_capture: int = 8192,\n        disable_custom_all_reduce: bool = False,\n        disable_async_output_proc: bool = False,\n        hf_token: Optional[Union[bool, str]] = None,\n        hf_overrides: Optional[HfOverrides] = None,\n        mm_processor_kwargs: Optional[dict[str, Any]] = None,\n        # After positional args are removed, move this right below `model`\n        task: TaskOption = \"auto\",\n        override_pooler_config: Optional[PoolerConfig] = None,\n        compilation_config: Optional[Union[int, dict[str, Any]]] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"LLM constructor.\"\"\"\n\n        if \"disable_log_stats\" not in kwargs:\n            kwargs[\"disable_log_stats\"] = True\n\n        if \"worker_cls\" in kwargs:\n            worker_cls = kwargs[\"worker_cls\"]\n            # if the worker_cls is not qualified string name,\n            # we serialize it using cloudpickle to avoid pickling issues\n            if isinstance(worker_cls, type):\n                kwargs[\"worker_cls\"] = cloudpickle.dumps(worker_cls)\n\n        if compilation_config is not None:\n            if isinstance(compilation_config, int):\n                compilation_config_instance = CompilationConfig(\n                    level=compilation_config)\n            elif isinstance(compilation_config, dict):\n                predicate = lambda x: is_init_field(CompilationConfig, x[0])\n                compilation_config_instance = CompilationConfig(\n                    **dict(filter(predicate, compilation_config.items())))\n            else:\n                compilation_config_instance = compilation_config\n        else:\n            compilation_config_instance = None\n\n        engine_args = EngineArgs(\n            model=model,\n            task=task,\n            tokenizer=tokenizer,\n            tokenizer_mode=tokenizer_mode,\n            skip_tokenizer_init=skip_tokenizer_init,\n            trust_remote_code=trust_remote_code,\n            allowed_local_media_path=allowed_local_media_path,\n            tensor_parallel_size=tensor_parallel_size,\n            dtype=dtype,\n            quantization=quantization,\n            revision=revision,\n            tokenizer_revision=tokenizer_revision,\n            seed=seed,\n            gpu_memory_utilization=gpu_memory_utilization,\n            swap_space=swap_space,\n            cpu_offload_gb=cpu_offload_gb,\n            enforce_eager=enforce_eager,\n            max_seq_len_to_capture=max_seq_len_to_capture,\n            disable_custom_all_reduce=disable_custom_all_reduce,\n            disable_async_output_proc=disable_async_output_proc,\n            hf_token=hf_token,\n            hf_overrides=hf_overrides,\n            mm_processor_kwargs=mm_processor_kwargs,\n            override_pooler_config=override_pooler_config,\n            compilation_config=compilation_config_instance,\n            **kwargs,\n        )\n\n        # Create the Engine (autoselects V0 vs V1)\n        self.llm_engine = LLMEngine.from_engine_args(\n            engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n        self.engine_class = type(self.llm_engine)\n\n        self.request_counter = Counter()\n        self.default_sampling_params: Union[dict[str, Any], None] = None\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.llm_engine.get_tokenizer_group().get_lora_tokenizer(\n            lora_request)\n\n    def set_tokenizer(self, tokenizer: AnyTokenizer) -> None:\n        tokenizer_group = self.llm_engine.get_tokenizer_group()\n\n        # While CachedTokenizer is dynamic, have no choice but\n        # compare class name. Misjudgment will arise from\n        # user-defined tokenizer started with 'Cached'\n        if tokenizer.__class__.__name__.startswith(\"Cached\"):\n            tokenizer_group.tokenizer = tokenizer\n        else:\n            tokenizer_group.tokenizer = get_cached_tokenizer(tokenizer)\n\n    def get_default_sampling_params(self) -> SamplingParams:\n        if self.default_sampling_params is None:\n            self.default_sampling_params = (\n                self.llm_engine.model_config.get_diff_sampling_param())\n        if self.default_sampling_params:\n            return SamplingParams.from_optional(**self.default_sampling_params)\n        return SamplingParams()\n\n    @overload\n    def generate(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        sampling_params: Optional[Union[SamplingParams,\n                                        Sequence[SamplingParams]]] = None,\n        *,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: str,\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        prompt_token_ids: Optional[list[int]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: list[str],\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        prompt_token_ids: Optional[list[list[int]]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: Optional[str] = None,\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        *,\n        prompt_token_ids: list[int],\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: Optional[list[str]] = None,\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        *,\n        prompt_token_ids: list[list[int]],\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: single or multi token ids [pos-only]\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: None,\n        sampling_params: None,\n        prompt_token_ids: Union[list[int], list[list[int]]],\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @deprecate_kwargs(\n        \"prompt_token_ids\",\n        is_deprecated=lambda: LLM.DEPRECATE_LEGACY,\n        additional_message=\"Please use the 'prompts' parameter instead.\",\n    )\n    def generate(\n        self,\n        prompts: Union[Union[PromptType, Sequence[PromptType]],\n                       Optional[Union[str, list[str]]]] = None,\n        sampling_params: Optional[Union[SamplingParams,\n                                        Sequence[SamplingParams]]] = None,\n        prompt_token_ids: Optional[Union[list[int], list[list[int]]]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n        priority: Optional[list[int]] = None,\n    ) -> list[RequestOutput]:\n        \"\"\"Generates the completions for the input prompts.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            sampling_params: The sampling parameters for text generation. If\n                None, we use the default sampling parameters.\n                When it is a single value, it is applied to every prompt.\n                When it is a list, the list must have the same length as the\n                prompts and it is paired one by one with the prompt.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n            priority: The priority of the requests, if any.\n                Only applicable when priority scheduling policy is enabled.\n\n        Returns:\n            A list of `RequestOutput` objects containing the\n            generated completions in the same order as the input prompts.\n\n        :::{note}\n        Using `prompts` and `prompt_token_ids` as keyword parameters is\n        considered legacy and may be deprecated in the future. You should\n        instead pass them via the `inputs` parameter.\n        :::\n        \"\"\"\n        runner_type = self.llm_engine.model_config.runner_type\n        if runner_type not in [\"generate\", \"transcription\"]:\n            messages = [\n                \"LLM.generate() is only supported for (conditional) generation \"\n                \"models (XForCausalLM, XForConditionalGeneration).\",\n            ]\n\n            supported_runner_types = self.llm_engine.model_config \\\n                .supported_runner_types\n            if \"generate\" in supported_runner_types:\n                messages.append(\n                    \"Your model supports the 'generate' runner, but is \"\n                    f\"currently initialized for the '{runner_type}' runner. \"\n                    \"Please initialize vLLM using `--task generate`.\")\n\n            raise ValueError(\" \".join(messages))\n\n        if prompt_token_ids is not None:\n            parsed_prompts = self._convert_v1_inputs(\n                prompts=cast(Optional[Union[str, list[str]]], prompts),\n                prompt_token_ids=prompt_token_ids,\n            )\n        else:\n            parsed_prompts = cast(Union[PromptType, Sequence[PromptType]],\n                                  prompts)\n\n        if isinstance(guided_options_request, dict):\n            if len(guided_options_request) > 1:\n                raise ValueError(\n                    \"You can only use one guided decoding but multiple is \"\n                    f\"specified: {guided_options_request}\")\n            guided_options_request = GuidedDecodingRequest(\n                **guided_options_request)\n\n        if sampling_params is None:\n            # Use default sampling params.\n            sampling_params = self.get_default_sampling_params()\n\n        self._validate_and_add_requests(\n            prompts=parsed_prompts,\n            params=sampling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            guided_options=guided_options_request,\n            priority=priority,\n        )\n\n        outputs = self._run_engine(use_tqdm=use_tqdm)\n        return self.engine_class.validate_outputs(outputs, RequestOutput)\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        \"\"\"\n        Execute an RPC call on all workers.\n\n        Args:\n            method: Name of the worker method to execute, or a callable that\n                is serialized and sent to all workers to execute.\n\n                If the method is a callable, it should accept an additional\n                `self` argument, in addition to the arguments passed in `args`\n                and `kwargs`. The `self` argument will be the worker object.\n            timeout: Maximum time in seconds to wait for execution. Raises a\n                {exc}`TimeoutError` on timeout. `None` means wait indefinitely.\n            args: Positional arguments to pass to the worker method.\n            kwargs: Keyword arguments to pass to the worker method.\n\n        Returns:\n            A list containing the results from each worker.\n\n        :::{note}\n        It is recommended to use this API to only pass control messages,\n        and set up data-plane communication to pass data.\n        :::\n        \"\"\"\n\n        return self.llm_engine.collective_rpc(method, timeout, args, kwargs)\n\n    def apply_model(self, func: Callable[[nn.Module], _R]) -> list[_R]:\n        \"\"\"\n        Run a function directly on the model inside each worker,\n        returning the result for each of them.\n        \"\"\"\n        executor = self.llm_engine.model_executor\n        return executor.apply_model(func)\n\n    def beam_search(\n        self,\n        prompts: list[Union[TokensPrompt, TextPrompt]],\n        params: BeamSearchParams,\n    ) -> list[BeamSearchOutput]:\n        \"\"\"\n        Generate sequences using beam search.\n\n        Args:\n            prompts: A list of prompts. Each prompt can be a string or a list\n                of token IDs.\n            params: The beam search parameters.\n        \"\"\"\n        # TODO: how does beam search work together with length penalty,\n        # frequency, penalty, and stopping criteria, etc.?\n        beam_width = params.beam_width\n        max_tokens = params.max_tokens\n        temperature = params.temperature\n        ignore_eos = params.ignore_eos\n        length_penalty = params.length_penalty\n\n        def sort_beams_key(x: BeamSearchSequence) -> float:\n            return get_beam_search_score(x.tokens, x.cum_logprob,\n                                         tokenizer.eos_token_id,\n                                         length_penalty)\n\n        def create_tokens_prompt_from_beam(\n                beam: BeamSearchSequence) -> TokensPrompt:\n            token_prompt_kwargs: TokensPrompt = {\n                \"prompt_token_ids\": beam.tokens\n            }\n            if beam.multi_modal_data is not None:\n                token_prompt_kwargs[\"multi_modal_data\"] = beam.multi_modal_data\n\n            if beam.mm_processor_kwargs is not None:\n                token_prompt_kwargs[\n                    \"mm_processor_kwargs\"] = beam.mm_processor_kwargs\n            return TokensPrompt(**token_prompt_kwargs)\n\n        tokenizer = self.get_tokenizer()\n        # generate 2 * beam_width candidates at each step\n        # following the huggingface transformers implementation\n        # at https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src/transformers/generation/beam_search.py#L534 # noqa\n        beam_search_params = SamplingParams(logprobs=2 * beam_width,\n                                            max_tokens=1,\n                                            temperature=temperature)\n        instances: list[BeamSearchInstance] = []\n\n        for prompt in prompts:\n            # Add multimodal processor kwargs & data\n            mm_kwargs = {}\n            if \"multi_modal_data\" in prompt:\n                mm_kwargs[\"multi_modal_data\"] = prompt[\"multi_modal_data\"]\n            if \"mm_processor_kwargs\" in prompt:\n                mm_kwargs[\"mm_processor_kwargs\"] = prompt[\n                    \"mm_processor_kwargs\"]\n\n            if \"prompt_token_ids\" in prompt:\n                prompt = cast(TokensPrompt, prompt)  # Needed for mypy\n                prompt_tokens = prompt[\"prompt_token_ids\"]\n            else:\n                prompt_tokens = tokenizer.encode(prompt[\"prompt\"])\n\n            instances.append(\n                BeamSearchInstance(prompt_tokens, logprobs=None, **mm_kwargs))\n\n        for _ in range(max_tokens):\n            all_beams: list[BeamSearchSequence] = list(\n                sum((instance.beams for instance in instances), []))\n            pos = [0] + list(\n                itertools.accumulate(\n                    len(instance.beams) for instance in instances))\n            instance_start_and_end: list[tuple[int, int]] = list(\n                zip(pos[:-1], pos[1:]))\n\n            if len(all_beams) == 0:\n                break\n\n            prompts_batch = [\n                create_tokens_prompt_from_beam(beam) for beam in all_beams\n            ]\n\n            # only runs for one step\n            # we don't need to use tqdm here\n            output = self.generate(prompts_batch,\n                                   sampling_params=beam_search_params,\n                                   use_tqdm=False)\n\n            for (start, end), instance in zip(instance_start_and_end,\n                                              instances):\n                instance_new_beams = []\n                for i in range(start, end):\n                    current_beam = all_beams[i]\n                    result = output[i]\n\n                    if result.outputs[0].logprobs is not None:\n                        # if `result.outputs[0].logprobs` is None, it means\n                        # the sequence is completed because of the max-model-len\n                        # or abortion. we don't need to add it to the new beams.\n                        logprobs = result.outputs[0].logprobs[0]\n                        for token_id, logprob_obj in logprobs.items():\n                            new_beam = BeamSearchSequence(\n                                tokens=current_beam.tokens + [token_id],\n                                logprobs=current_beam.logprobs + [logprobs],\n                                cum_logprob=current_beam.cum_logprob +\n                                logprob_obj.logprob,\n                                multi_modal_data=current_beam.multi_modal_data,\n                                mm_processor_kwargs=current_beam.\n                                mm_processor_kwargs)\n\n                            if token_id == tokenizer.eos_token_id and \\\n                                not ignore_eos:\n                                instance.completed.append(new_beam)\n                            else:\n                                instance_new_beams.append(new_beam)\n                sorted_beams = sorted(instance_new_beams,\n                                      key=sort_beams_key,\n                                      reverse=True)\n                instance.beams = sorted_beams[:beam_width]\n\n        outputs = []\n        for instance in instances:\n            instance.completed.extend(instance.beams)\n            sorted_completed = sorted(instance.completed,\n                                      key=sort_beams_key,\n                                      reverse=True)\n            best_beams = sorted_completed[:beam_width]\n\n            for beam in best_beams:\n                beam.text = tokenizer.decode(beam.tokens)\n            outputs.append(BeamSearchOutput(sequences=best_beams))\n\n        return outputs\n\n    def chat(\n        self,\n        messages: Union[list[ChatCompletionMessageParam],\n                        list[list[ChatCompletionMessageParam]]],\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[LoRARequest] = None,\n        chat_template: Optional[str] = None,\n        chat_template_content_format: ChatTemplateContentFormatOption = \"auto\",\n        add_generation_prompt: bool = True,\n        continue_final_message: bool = False,\n        tools: Optional[list[dict[str, Any]]] = None,\n        chat_template_kwargs: Optional[dict[str, Any]] = None,\n        mm_processor_kwargs: Optional[dict[str, Any]] = None,\n    ) -> list[RequestOutput]:\n        \"\"\"\n        Generate responses for a chat conversation.\n\n        The chat conversation is converted into a text prompt using the\n        tokenizer and calls the {meth}`generate` method to generate the\n        responses.\n\n        Multi-modal inputs can be passed in the same way you would pass them\n        to the OpenAI API.\n\n        Args:\n            messages: A list of conversations or a single conversation.\n\n              - Each conversation is represented as a list of messages.\n              - Each message is a dictionary with 'role' and 'content' keys.\n\n            sampling_params: The sampling parameters for text generation.\n                If None, we use the default sampling parameters. When it\n                is a single value, it is applied to every prompt. When it\n                is a list, the list must have the same length as the\n                prompts and it is paired one by one with the prompt.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            chat_template: The template to use for structuring the chat.\n              If not provided, the model's default chat template will be used.\n            chat_template_content_format: The format to render message content.\n\n              - \"string\" will render the content as a string.\n                Example: ``\"Who are you?\"``\n              - \"openai\" will render the content as a list of dictionaries,\n                similar to OpenAI schema.\n                Example: ``[{\"type\": \"text\", \"text\": \"Who are you?\"}]``\n\n            add_generation_prompt: If True, adds a generation template\n                to each message.\n            continue_final_message: If True, continues the final message in\n                the conversation instead of starting a new one. Cannot be\n                ``True`` if ``add_generation_prompt`` is also ``True``.\n            chat_template_kwargs: Additional kwargs to pass to the chat\n                template.\n            mm_processor_kwargs: Multimodal processor kwarg overrides for this\n                chat request. Only used for offline requests.\n\n        Returns:\n            A list of ``RequestOutput`` objects containing the generated\n            responses in the same order as the input messages.\n        \"\"\"\n        list_of_messages: list[list[ChatCompletionMessageParam]]\n\n        # Handle multi and single conversations\n        if is_list_of(messages, list):\n            # messages is list[list[...]]\n            list_of_messages = cast(list[list[ChatCompletionMessageParam]],\n                                    messages)\n        else:\n            # messages is list[...]\n            list_of_messages = [\n                cast(list[ChatCompletionMessageParam], messages)\n            ]\n\n        tokenizer = self.get_tokenizer(lora_request)\n        model_config = self.llm_engine.get_model_config()\n        resolved_content_format = resolve_chat_template_content_format(\n            chat_template,\n            tools,\n            chat_template_content_format,\n            tokenizer,\n            model_config=model_config,\n        )\n\n        _chat_template_kwargs: dict[str, Any] = dict(\n            chat_template=chat_template,\n            add_generation_prompt=add_generation_prompt,\n            continue_final_message=continue_final_message,\n            tools=tools,\n        )\n        _chat_template_kwargs.update(chat_template_kwargs or {})\n\n        prompts: list[Union[TokensPrompt, TextPrompt]] = []\n\n        for msgs in list_of_messages:\n            # NOTE: _parse_chat_message_content_parts() currently doesn't\n            # handle mm_processor_kwargs, since there is no implementation in\n            # the chat message parsing for it.\n            conversation, mm_data = parse_chat_messages(\n                msgs,\n                model_config,\n                tokenizer,\n                content_format=resolved_content_format,\n            )\n\n            if isinstance(tokenizer, MistralTokenizer):\n                prompt_token_ids = apply_mistral_chat_template(\n                    tokenizer,\n                    messages=msgs,\n                    **_chat_template_kwargs,\n                )\n            else:\n                prompt_str = apply_hf_chat_template(\n                    tokenizer=tokenizer,\n                    conversation=conversation,\n                    model_config=model_config,\n                    **_chat_template_kwargs,\n                )\n                # Special tokens are already included in chat templates so\n                # should not be added by the tokenizer in this case.\n                prompt_token_ids = tokenizer.encode(prompt_str,\n                                                    add_special_tokens=False)\n\n            prompt = TokensPrompt(prompt_token_ids=prompt_token_ids)\n\n            if mm_data is not None:\n                prompt[\"multi_modal_data\"] = mm_data\n\n            if mm_processor_kwargs is not None:\n                prompt[\"mm_processor_kwargs\"] = mm_processor_kwargs\n\n            prompts.append(prompt)\n\n        return self.generate(\n            prompts,\n            sampling_params=sampling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n        )\n\n    @overload\n    def encode(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        *,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: str,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        prompt_token_ids: Optional[list[int]] = None,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: list[str],\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        prompt_token_ids: Optional[list[list[int]]] = None,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: Optional[str] = None,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        *,\n        prompt_token_ids: list[int],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: Optional[list[str]] = None,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        *,\n        prompt_token_ids: list[list[int]],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: single or multi token ids [pos-only]\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: None,\n        pooling_params: None,\n        prompt_token_ids: Union[list[int], list[list[int]]],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @deprecate_kwargs(\n        \"prompt_token_ids\",\n        is_deprecated=lambda: LLM.DEPRECATE_LEGACY,\n        additional_message=\"Please use the 'prompts' parameter instead.\",\n    )\n    def encode(\n        self,\n        prompts: Union[Union[PromptType, Sequence[PromptType]],\n                       Optional[Union[str, list[str]]]] = None,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        prompt_token_ids: Optional[Union[list[int], list[list[int]]]] = None,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        \"\"\"Apply pooling to the hidden states corresponding to the input\n        prompts.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            pooling_params: The pooling parameters for pooling. If None, we\n                use the default pooling parameters.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of `PoolingRequestOutput` objects containing the\n            pooled hidden states in the same order as the input prompts.\n\n        :::{note}\n        Using `prompts` and `prompt_token_ids` as keyword parameters is\n        considered legacy and may be deprecated in the future. You should\n        instead pass them via the `inputs` parameter.\n        :::\n        \"\"\"\n        runner_type = self.llm_engine.model_config.runner_type\n        if runner_type != \"pooling\":\n            messages = [\"LLM.encode() is only supported for pooling models.\"]\n\n            supported_runner_types = self.llm_engine.model_config \\\n                .supported_runner_types\n            if \"pooling\" in supported_runner_types:\n                messages.append(\n                    \"Your model supports the 'pooling' runner, but is \"\n                    f\"currently initialized for the '{runner_type}' runner. \"\n                    \"Please initialize vLLM using `--task embed`, \"\n                    \"`--task classify`, `--task score` etc.\")\n\n            raise ValueError(\" \".join(messages))\n\n        if prompt_token_ids is not None:\n            parsed_prompts = self._convert_v1_inputs(\n                prompts=cast(Optional[Union[str, list[str]]], prompts),\n                prompt_token_ids=prompt_token_ids,\n            )\n        else:\n            parsed_prompts = cast(Union[PromptType, Sequence[PromptType]],\n                                  prompts)\n\n        if pooling_params is None:\n            # Use default pooling params.\n            pooling_params = PoolingParams()\n        elif isinstance(pooling_params, PoolingParams):\n            pooling_params.verify(self.llm_engine.model_config)\n        else:\n            for pooling_param in pooling_params:\n                pooling_param.verify(self.llm_engine.model_config)\n\n        tokenization_kwargs: dict[str, Any] = {}\n        _validate_truncation_size(self.llm_engine.model_config.max_model_len,\n                                  truncate_prompt_tokens, tokenization_kwargs)\n\n        self._validate_and_add_requests(\n            prompts=parsed_prompts,\n            params=pooling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            tokenization_kwargs=tokenization_kwargs,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        outputs = self._run_engine(use_tqdm=use_tqdm)\n        return self.engine_class.validate_outputs(outputs,\n                                                  PoolingRequestOutput)\n\n    def embed(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        *,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[EmbeddingRequestOutput]:\n        \"\"\"\n        Generate an embedding vector for each prompt.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            pooling_params: The pooling parameters for pooling. If None, we\n                use the default pooling parameters.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of ``EmbeddingRequestOutput`` objects containing the\n            embedding vectors in the same order as the input prompts.\n        \"\"\"\n        if self.llm_engine.model_config.task != \"embed\":\n            raise ValueError(\n                \"Embedding API is only enabled for `--task embed`\")\n\n        items = self.encode(prompts,\n                            truncate_prompt_tokens=truncate_prompt_tokens,\n                            use_tqdm=use_tqdm,\n                            pooling_params=pooling_params,\n                            lora_request=lora_request,\n                            prompt_adapter_request=prompt_adapter_request)\n\n        return [EmbeddingRequestOutput.from_base(item) for item in items]\n\n    def classify(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        *,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ClassificationRequestOutput]:\n        \"\"\"\n        Generate class logits for each prompt.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of ``ClassificationRequestOutput`` objects containing the\n            embedding vectors in the same order as the input prompts.\n        \"\"\"\n        if self.llm_engine.model_config.task != \"classify\":\n            raise ValueError(\n                \"Classification API is only enabled for `--task classify`\")\n\n        items = self.encode(prompts,\n                            use_tqdm=use_tqdm,\n                            lora_request=lora_request,\n                            prompt_adapter_request=prompt_adapter_request)\n\n        return [ClassificationRequestOutput.from_base(item) for item in items]\n\n    def _embedding_score(\n        self,\n        tokenizer: AnyTokenizer,\n        text_1: list[Union[str, TextPrompt, TokensPrompt]],\n        text_2: list[Union[str, TextPrompt, TokensPrompt]],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ScoringRequestOutput]:\n\n        encoded_output: list[PoolingRequestOutput] = self.encode(\n            text_1 + text_2,\n            truncate_prompt_tokens=truncate_prompt_tokens,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request)\n\n        encoded_output_1: list[PoolingRequestOutput] = encoded_output[\n            0:len(text_1)]\n        encoded_output_2: list[PoolingRequestOutput] = encoded_output[\n            len(text_1):]\n\n        if len(encoded_output_1) == 1:\n            encoded_output_1 = encoded_output_1 * len(encoded_output_2)\n\n        scores = _cosine_similarity(tokenizer=tokenizer,\n                                    embed_1=encoded_output_1,\n                                    embed_2=encoded_output_2)\n\n        items = self.engine_class.validate_outputs(scores,\n                                                   PoolingRequestOutput)\n        return [ScoringRequestOutput.from_base(item) for item in items]\n\n    def _cross_encoding_score(\n        self,\n        tokenizer: AnyTokenizer,\n        text_1: list[str],\n        text_2: list[str],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ScoringRequestOutput]:\n\n        if isinstance(tokenizer, MistralTokenizer):\n            raise ValueError(\n                \"Score API is only enabled for `--task embed or score`\")\n\n        if len(text_1) == 1:\n            text_1 = text_1 * len(text_2)\n\n        input_pairs = [(t1, t2) for t1, t2 in zip(text_1, text_2)]\n\n        pooling_params = PoolingParams()\n\n        tokenization_kwargs: dict[str, Any] = {}\n        _validate_truncation_size(self.llm_engine.model_config.max_model_len,\n                                  truncate_prompt_tokens, tokenization_kwargs)\n\n        parsed_prompts = []\n\n        for q, t in input_pairs:\n            prompt_inputs = tokenizer(text=q,\n                                      text_pair=t,\n                                      **tokenization_kwargs)\n            engine_prompt = TokensPrompt(\n                prompt_token_ids=prompt_inputs[\"input_ids\"],\n                token_type_ids=prompt_inputs.get(\"token_type_ids\"))\n            parsed_prompts.append(engine_prompt)\n\n        self._validate_and_add_requests(\n            prompts=parsed_prompts,\n            params=pooling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        outputs = self._run_engine(use_tqdm=use_tqdm)\n        items = self.engine_class.validate_outputs(outputs,\n                                                   PoolingRequestOutput)\n\n        return [ScoringRequestOutput.from_base(item) for item in items]\n\n    def score(\n        self,\n        text_1: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n        text_2: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n        /,\n        *,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ScoringRequestOutput]:\n        \"\"\"Generate similarity scores for all pairs ``<text,text_pair>``.\n\n        The inputs can be ``1 -> 1``, ``1 -> N`` or ``N -> N``.\n        In the ``1 - N`` case the ``text_1`` sentence will be replicated ``N``\n        times to pair with the ``text_2`` sentences.\n        The input pairs are used to build a list of prompts for the\n        cross encoder model. This class automatically batches the prompts,\n        considering the memory constraint. For the best performance, put all\n        of your texts into a single list and pass it to this method.\n\n        Args:\n            text_1: can be a single prompt or a list of prompts, in which\n                case it has to have the same length as the ``text_2`` list\n            text_2: The texts to pair with the query to form the input\n                to the LLM. See {class}`~vllm.inputs.PromptType` for\n                more details about the format of each prompts.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of ``ScoringRequestOutput`` objects containing the\n            generated scores in the same order as the input prompts.\n        \"\"\"\n        runner_type = self.llm_engine.model_config.runner_type\n        if runner_type != \"pooling\":\n            messages = [\"LLM.score() is only supported for pooling models.\"]\n\n            supported_runner_types = self.llm_engine.model_config \\\n                .supported_runner_types\n            if \"pooling\" in supported_runner_types:\n                messages.append(\n                    \"Your model supports the 'pooling' runner, but is \"\n                    f\"currently initialized for the '{runner_type}' runner. \"\n                    \"Please initialize vLLM using `--task embed`, \"\n                    \"`--task classify`, `--task score` etc.\")\n\n            raise ValueError(\" \".join(messages))\n\n        if self.llm_engine.model_config.task not in (\"embed\", \"score\"):\n            raise ValueError(\n                \"Score API is only enabled for `--task embed or --task score`\")\n\n        # the tokenizer for models such as\n        # \"cross-encoder/ms-marco-MiniLM-L-6-v2\" doesn't support passing\n        # lists of tokens to the `text` and `text_pair` kwargs\n        tokenizer = self.llm_engine.get_tokenizer()\n\n        def ensure_str(prompt: SingletonPrompt):\n            if isinstance(prompt, dict):\n                if \"multi_modal_data\" in prompt:\n                    raise ValueError(\"Multi-modal prompt is not \"\n                                     \"supported for scoring\")\n                elif \"prompt_token_ids\" in prompt:\n                    prompt = tokenizer.decode(\n                        cast(TokensPrompt, prompt)[\"prompt_token_ids\"])\n                elif \"prompt\" in prompt:\n                    prompt = cast(TextPrompt, prompt)[\"prompt\"]\n            assert type(prompt) is str\n            return prompt\n\n        if isinstance(text_1, (str, dict)):\n            # Convert a single prompt to a list.\n            text_1 = [text_1]\n        input_text_1: list[str] = [ensure_str(t) for t in text_1]\n\n        if isinstance(text_2, (str, dict)):\n            # Convert a single prompt to a list.\n            text_2 = [text_2]\n        input_text_2: list[str] = [ensure_str(t) for t in text_2]\n\n        _validate_score_input_lens(input_text_1, input_text_2)\n\n        if self.llm_engine.model_config.is_cross_encoder:\n            return self._cross_encoding_score(tokenizer, input_text_1,\n                                              input_text_2,\n                                              truncate_prompt_tokens, use_tqdm,\n                                              lora_request,\n                                              prompt_adapter_request)\n        else:\n            return self._embedding_score(\n                tokenizer,\n                input_text_1,  # type: ignore[arg-type]\n                input_text_2,  # type: ignore[arg-type]\n                truncate_prompt_tokens,\n                use_tqdm,\n                lora_request,\n                prompt_adapter_request)\n\n    def start_profile(self) -> None:\n        self.llm_engine.start_profile()\n\n    def stop_profile(self) -> None:\n        self.llm_engine.stop_profile()\n\n    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:\n        return self.llm_engine.reset_prefix_cache(device)\n\n    def sleep(self, level: int = 1):\n        \"\"\"\n        Put the engine to sleep. The engine should not process any requests.\n        The caller should guarantee that no requests are being processed\n        during the sleep period, before `wake_up` is called.\n\n        Args:\n            level: The sleep level. Level 1 sleep will offload the model \n                weights and discard the kv cache. The content of kv cache \n                is forgotten. Level 1 sleep is good for sleeping and waking\n                up the engine to run the same model again. The model weights \n                are backed up in CPU memory. Please make sure there's enough \n                CPU memory to store the model weights. Level 2 sleep will \n                discard both the model weights and the kv cache. The content \n                of both the model weights and kv cache is forgotten. Level 2 \n                sleep is good for sleeping and waking up the engine to run a\n                different model or update the model, where previous model \n                weights are not needed. It reduces CPU memory pressure.\n        \"\"\"\n        self.reset_prefix_cache()\n        self.llm_engine.sleep(level=level)\n\n    def wake_up(self, tags: Optional[list[str]] = None):\n        \"\"\"\n        Wake up the engine from sleep mode. See the {meth}`sleep` method\n        for more details.\n        \n        Args:\n            tags: An optional list of tags to reallocate the engine memory \n                for specific memory allocations. Values must be in \n                (\"weights\", \"kv_cache\",). If None, all memory is reallocated.\n                wake_up should be called with all tags (or None) before the \n                engine is used again.\n        \"\"\"\n        self.llm_engine.wake_up(tags)\n\n    # LEGACY\n    def _convert_v1_inputs(\n        self,\n        prompts: Optional[Union[str, list[str]]],\n        prompt_token_ids: Optional[Union[list[int], list[list[int]]]],\n    ):\n        # skip_tokenizer_init is now checked in engine\n\n        if prompts is not None:\n            prompts = [p[\"content\"] for p in parse_and_batch_prompt(prompts)]\n        if prompt_token_ids is not None:\n            prompt_token_ids = [\n                p[\"content\"] for p in parse_and_batch_prompt(prompt_token_ids)\n            ]\n\n        num_requests = None\n        if prompts is not None:\n            num_requests = len(prompts)\n        if prompt_token_ids is not None:\n            if (num_requests is not None\n                    and num_requests != len(prompt_token_ids)):\n                raise ValueError(\"The lengths of prompts and prompt_token_ids \"\n                                 \"must be the same.\")\n\n            num_requests = len(prompt_token_ids)\n        if num_requests is None:\n            raise ValueError(\"Either prompts or prompt_token_ids must be \"\n                             \"provided.\")\n\n        parsed_prompts: list[PromptType] = []\n        for i in range(num_requests):\n            item: PromptType\n\n            if prompts is not None:\n                item = TextPrompt(prompt=prompts[i])\n            elif prompt_token_ids is not None:\n                item = TokensPrompt(prompt_token_ids=prompt_token_ids[i])\n            else:\n                raise AssertionError\n\n            parsed_prompts.append(item)\n\n        return parsed_prompts\n\n    def _validate_and_add_requests(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        params: Union[SamplingParams, Sequence[SamplingParams], PoolingParams,\n                      Sequence[PoolingParams]],\n        *,\n        use_tqdm: bool,\n        lora_request: Optional[Union[Sequence[LoRARequest], LoRARequest]],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        guided_options: Optional[GuidedDecodingRequest] = None,\n        priority: Optional[list[int]] = None,\n    ) -> None:\n        if guided_options is not None:\n            warnings.warn(\n                \"guided_options_request is deprecated, use \"\n                \"SamplingParams.guided_decoding instead\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        if isinstance(prompts, (str, dict)):\n            # Convert a single prompt to a list.\n            prompts = [prompts]\n\n        num_requests = len(prompts)\n        if isinstance(params, list) and len(params) != num_requests:\n            raise ValueError(\"The lengths of prompts and params \"\n                             \"must be the same.\")\n        if isinstance(lora_request,\n                      list) and len(lora_request) != num_requests:\n            raise ValueError(\"The lengths of prompts and lora_request \"\n                             \"must be the same.\")\n\n        for sp in params if isinstance(params, list) else (params, ):\n            if isinstance(sp, SamplingParams):\n                self._add_guided_params(sp, guided_options)\n\n                # We only care about the final output\n                sp.output_kind = RequestOutputKind.FINAL_ONLY\n\n        # Add requests to the engine.\n        it = prompts\n        if use_tqdm:\n            it = tqdm(it, desc=\"Adding requests\")\n\n        for i, prompt in enumerate(it):\n            self._add_request(\n                prompt,\n                params[i] if isinstance(params, Sequence) else params,\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request[i] if isinstance(\n                    lora_request, Sequence) else lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                priority=priority[i] if priority else 0,\n            )\n\n    def _add_request(\n        self,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        request_id = str(next(self.request_counter))\n        self.llm_engine.add_request(\n            request_id,\n            prompt,\n            params,\n            lora_request=lora_request,\n            tokenization_kwargs=tokenization_kwargs,\n            prompt_adapter_request=prompt_adapter_request,\n            priority=priority,\n        )\n\n    def _add_guided_params(\n            self,\n            params: SamplingParams,\n            guided_options: Optional[GuidedDecodingRequest] = None):\n        if guided_options is None:\n            return params\n\n        if params.guided_decoding is not None:\n            raise ValueError(\"Cannot set both guided_options_request and \"\n                             \"params.guided_decoding.\")\n\n        params.guided_decoding = GuidedDecodingParams(\n            json=guided_options.guided_json,\n            regex=guided_options.guided_regex,\n            choice=guided_options.guided_choice,\n            grammar=guided_options.guided_grammar,\n            json_object=guided_options.guided_json_object,\n            backend=guided_options.guided_decoding_backend,\n            whitespace_pattern=guided_options.guided_whitespace_pattern,\n            structural_tag=guided_options.structural_tag,\n        )\n        return params\n\n    def _run_engine(\n            self, *, use_tqdm: bool\n    ) -> list[Union[RequestOutput, PoolingRequestOutput]]:\n        # Initialize tqdm.\n        if use_tqdm:\n            num_requests = self.llm_engine.get_num_unfinished_requests()\n            pbar = tqdm(\n                total=num_requests,\n                desc=\"Processed prompts\",\n                dynamic_ncols=True,\n                postfix=(f\"est. speed input: {0:.2f} toks/s, \"\n                         f\"output: {0:.2f} toks/s\"),\n            )\n\n        # Run the engine.\n        outputs: list[Union[RequestOutput, PoolingRequestOutput]] = []\n        total_in_toks = 0\n        total_out_toks = 0\n        while self.llm_engine.has_unfinished_requests():\n            step_outputs = self.llm_engine.step()\n            for output in step_outputs:\n                if output.finished:\n                    outputs.append(output)\n                    if use_tqdm:\n                        if isinstance(output, RequestOutput):\n                            # Calculate tokens only for RequestOutput\n                            n = len(output.outputs)\n                            assert output.prompt_token_ids is not None\n                            total_in_toks += len(output.prompt_token_ids) * n\n                            in_spd = total_in_toks / pbar.format_dict[\"elapsed\"]\n                            total_out_toks += sum(\n                                len(stp.token_ids) for stp in output.outputs)\n                            out_spd = (total_out_toks /\n                                       pbar.format_dict[\"elapsed\"])\n                            pbar.postfix = (\n                                f\"est. speed input: {in_spd:.2f} toks/s, \"\n                                f\"output: {out_spd:.2f} toks/s\")\n                            pbar.update(n)\n                        else:\n                            pbar.update(1)\n\n        if use_tqdm:\n            pbar.close()\n        # Sort the outputs by request ID.\n        # This is necessary because some requests may be finished earlier than\n        # its previous requests.\n        return sorted(outputs, key=lambda x: int(x.request_id))\n", 1491], "/home/jeromeku/vllm/vllm/platforms/__init__.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport traceback\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Optional\n\nfrom vllm.plugins import load_plugins_by_group\nfrom vllm.utils import resolve_obj_by_qualname\n\nfrom .interface import _Backend  # noqa: F401\nfrom .interface import CpuArchEnum, Platform, PlatformEnum\n\nlogger = logging.getLogger(__name__)\n\n\ndef vllm_version_matches_substr(substr: str) -> bool:\n    \"\"\"\n    Check to see if the vLLM version matches a substring.\n    \"\"\"\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        vllm_version = version(\"vllm\")\n    except PackageNotFoundError as e:\n        logger.warning(\n            \"The vLLM package was not found, so its version could not be \"\n            \"inspected. This may cause platform detection to fail.\")\n        raise e\n    return substr in vllm_version\n\n\ndef tpu_platform_plugin() -> Optional[str]:\n    is_tpu = False\n    logger.debug(\"Checking if TPU platform is available.\")\n    try:\n        # While it's technically possible to install libtpu on a\n        # non-TPU machine, this is a very uncommon scenario. Therefore,\n        # we assume that libtpu is installed if and only if the machine\n        # has TPUs.\n        import libtpu  # noqa: F401\n        is_tpu = True\n        logger.debug(\"Confirmed TPU platform is available.\")\n    except Exception as e:\n        logger.debug(\"TPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.tpu.TpuPlatform\" if is_tpu else None\n\n\ndef cuda_platform_plugin() -> Optional[str]:\n    is_cuda = False\n    logger.debug(\"Checking if CUDA platform is available.\")\n    try:\n        from vllm.utils import import_pynvml\n        pynvml = import_pynvml()\n        pynvml.nvmlInit()\n        try:\n            # NOTE: Edge case: vllm cpu build on a GPU machine.\n            # Third-party pynvml can be imported in cpu build,\n            # we need to check if vllm is built with cpu too.\n            # Otherwise, vllm will always activate cuda plugin\n            # on a GPU machine, even if in a cpu build.\n            is_cuda = (pynvml.nvmlDeviceGetCount() > 0\n                       and not vllm_version_matches_substr(\"cpu\"))\n            if pynvml.nvmlDeviceGetCount() <= 0:\n                logger.debug(\n                    \"CUDA platform is not available because no GPU is found.\")\n            if vllm_version_matches_substr(\"cpu\"):\n                logger.debug(\"CUDA platform is not available because\"\n                             \" vLLM is built with CPU.\")\n            if is_cuda:\n                logger.debug(\"Confirmed CUDA platform is available.\")\n        finally:\n            pynvml.nvmlShutdown()\n    except Exception as e:\n        logger.debug(\"Exception happens when checking CUDA platform: %s\",\n                     str(e))\n        if \"nvml\" not in e.__class__.__name__.lower():\n            # If the error is not related to NVML, re-raise it.\n            raise e\n\n        # CUDA is supported on Jetson, but NVML may not be.\n        import os\n\n        def cuda_is_jetson() -> bool:\n            return os.path.isfile(\"/etc/nv_tegra_release\") \\\n                or os.path.exists(\"/sys/class/tegra-firmware\")\n\n        if cuda_is_jetson():\n            logger.debug(\"Confirmed CUDA platform is available on Jetson.\")\n            is_cuda = True\n        else:\n            logger.debug(\"CUDA platform is not available because: %s\", str(e))\n\n    return \"vllm.platforms.cuda.CudaPlatform\" if is_cuda else None\n\n\ndef rocm_platform_plugin() -> Optional[str]:\n    is_rocm = False\n    logger.debug(\"Checking if ROCm platform is available.\")\n    try:\n        import amdsmi\n        amdsmi.amdsmi_init()\n        try:\n            if len(amdsmi.amdsmi_get_processor_handles()) > 0:\n                is_rocm = True\n                logger.debug(\"Confirmed ROCm platform is available.\")\n            else:\n                logger.debug(\"ROCm platform is not available because\"\n                             \" no GPU is found.\")\n        finally:\n            amdsmi.amdsmi_shut_down()\n    except Exception as e:\n        logger.debug(\"ROCm platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.rocm.RocmPlatform\" if is_rocm else None\n\n\ndef hpu_platform_plugin() -> Optional[str]:\n    is_hpu = False\n    logger.debug(\"Checking if HPU platform is available.\")\n    try:\n        from importlib import util\n        is_hpu = util.find_spec('habana_frameworks') is not None\n        if is_hpu:\n            logger.debug(\"Confirmed HPU platform is available.\")\n        else:\n            logger.debug(\"HPU platform is not available because \"\n                         \"habana_frameworks is not found.\")\n    except Exception as e:\n        logger.debug(\"HPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.hpu.HpuPlatform\" if is_hpu else None\n\n\ndef xpu_platform_plugin() -> Optional[str]:\n    is_xpu = False\n    logger.debug(\"Checking if XPU platform is available.\")\n    try:\n        # installed IPEX if the machine has XPUs.\n        import intel_extension_for_pytorch  # noqa: F401\n        import oneccl_bindings_for_pytorch  # noqa: F401\n        import torch\n        if hasattr(torch, 'xpu') and torch.xpu.is_available():\n            is_xpu = True\n            logger.debug(\"Confirmed XPU platform is available.\")\n    except Exception as e:\n        logger.debug(\"XPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.xpu.XPUPlatform\" if is_xpu else None\n\n\ndef cpu_platform_plugin() -> Optional[str]:\n    is_cpu = False\n    logger.debug(\"Checking if CPU platform is available.\")\n    try:\n        is_cpu = vllm_version_matches_substr(\"cpu\")\n        if is_cpu:\n            logger.debug(\"Confirmed CPU platform is available because\"\n                         \" vLLM is built with CPU.\")\n        if not is_cpu:\n            import sys\n            is_cpu = sys.platform.startswith(\"darwin\")\n            if is_cpu:\n                logger.debug(\"Confirmed CPU platform is available\"\n                             \" because the machine is MacOS.\")\n\n    except Exception as e:\n        logger.debug(\"CPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.cpu.CpuPlatform\" if is_cpu else None\n\n\ndef neuron_platform_plugin() -> Optional[str]:\n    tnx_installed = False\n    nxd_installed = False\n    logger.debug(\"Checking if Neuron platform is available.\")\n    try:\n        import transformers_neuronx  # noqa: F401\n        tnx_installed = True\n        logger.debug(\"Confirmed Neuron platform is available because\"\n                     \" transformers_neuronx is found.\")\n    except ImportError:\n        pass\n\n    try:\n        import neuronx_distributed_inference  # noqa: F401\n        nxd_installed = True\n        logger.debug(\"Confirmed Neuron platform is available because\"\n                     \" neuronx_distributed_inference is found.\")\n    except ImportError:\n        pass\n\n    is_neuron = tnx_installed or nxd_installed\n    return \"vllm.platforms.neuron.NeuronPlatform\" if is_neuron else None\n\n\nbuiltin_platform_plugins = {\n    'tpu': tpu_platform_plugin,\n    'cuda': cuda_platform_plugin,\n    'rocm': rocm_platform_plugin,\n    'hpu': hpu_platform_plugin,\n    'xpu': xpu_platform_plugin,\n    'cpu': cpu_platform_plugin,\n    'neuron': neuron_platform_plugin,\n}\n\n\ndef resolve_current_platform_cls_qualname() -> str:\n    platform_plugins = load_plugins_by_group('vllm.platform_plugins')\n\n    activated_plugins = []\n\n    for name, func in chain(builtin_platform_plugins.items(),\n                            platform_plugins.items()):\n        try:\n            assert callable(func)\n            platform_cls_qualname = func()\n            if platform_cls_qualname is not None:\n                activated_plugins.append(name)\n        except Exception:\n            pass\n\n    activated_builtin_plugins = list(\n        set(activated_plugins) & set(builtin_platform_plugins.keys()))\n    activated_oot_plugins = list(\n        set(activated_plugins) & set(platform_plugins.keys()))\n\n    if len(activated_oot_plugins) >= 2:\n        raise RuntimeError(\n            \"Only one platform plugin can be activated, but got: \"\n            f\"{activated_oot_plugins}\")\n    elif len(activated_oot_plugins) == 1:\n        platform_cls_qualname = platform_plugins[activated_oot_plugins[0]]()\n        logger.info(\"Platform plugin %s is activated\",\n                    activated_oot_plugins[0])\n    elif len(activated_builtin_plugins) >= 2:\n        raise RuntimeError(\n            \"Only one platform plugin can be activated, but got: \"\n            f\"{activated_builtin_plugins}\")\n    elif len(activated_builtin_plugins) == 1:\n        platform_cls_qualname = builtin_platform_plugins[\n            activated_builtin_plugins[0]]()\n        logger.info(\"Automatically detected platform %s.\",\n                    activated_builtin_plugins[0])\n    else:\n        platform_cls_qualname = \"vllm.platforms.interface.UnspecifiedPlatform\"\n        logger.info(\n            \"No platform detected, vLLM is running on UnspecifiedPlatform\")\n    return platform_cls_qualname\n\n\n_current_platform = None\n_init_trace: str = ''\n\nif TYPE_CHECKING:\n    current_platform: Platform\n\n\ndef __getattr__(name: str):\n    if name == 'current_platform':\n        # lazy init current_platform.\n        # 1. out-of-tree platform plugins need `from vllm.platforms import\n        #    Platform` so that they can inherit `Platform` class. Therefore,\n        #    we cannot resolve `current_platform` during the import of\n        #    `vllm.platforms`.\n        # 2. when users use out-of-tree platform plugins, they might run\n        #    `import vllm`, some vllm internal code might access\n        #    `current_platform` during the import, and we need to make sure\n        #    `current_platform` is only resolved after the plugins are loaded\n        #    (we have tests for this, if any developer violate this, they will\n        #    see the test failures).\n        global _current_platform\n        if _current_platform is None:\n            platform_cls_qualname = resolve_current_platform_cls_qualname()\n            _current_platform = resolve_obj_by_qualname(\n                platform_cls_qualname)()\n            global _init_trace\n            _init_trace = \"\".join(traceback.format_stack())\n        return _current_platform\n    elif name in globals():\n        return globals()[name]\n    else:\n        raise AttributeError(\n            f\"No attribute named '{name}' exists in {__name__}.\")\n\n\n__all__ = [\n    'Platform', 'PlatformEnum', 'current_platform', 'CpuArchEnum',\n    \"_init_trace\"\n]\n", 295], "/home/jeromeku/vllm/vllm/platforms/interface.py": ["# SPDX-License-Identifier: Apache-2.0\nimport enum\nimport os\nimport platform\nimport random\nfrom platform import uname\nfrom typing import TYPE_CHECKING, NamedTuple, Optional, Union\n\nimport numpy as np\nimport torch\n\nfrom vllm.inputs import ProcessorInputs, PromptType\nfrom vllm.logger import init_logger\n\nif TYPE_CHECKING:\n    from vllm.config import ModelConfig, VllmConfig\n    from vllm.lora.request import LoRARequest\n    from vllm.pooling_params import PoolingParams\n    from vllm.sampling_params import SamplingParams\n    from vllm.utils import FlexibleArgumentParser\nelse:\n    ModelConfig = None\n    VllmConfig = None\n    LoRARequest = None\n    PoolingParams = None\n    SamplingParams = None\n    FlexibleArgumentParser = None\n\nlogger = init_logger(__name__)\n\n\ndef in_wsl() -> bool:\n    # Reference: https://github.com/microsoft/WSL/issues/4071\n    return \"microsoft\" in \" \".join(uname()).lower()\n\n\nclass _Backend(enum.Enum):\n    FLASH_ATTN = enum.auto()\n    FLASH_ATTN_VLLM_V1 = enum.auto()\n    TRITON_ATTN_VLLM_V1 = enum.auto()\n    XFORMERS = enum.auto()\n    ROCM_FLASH = enum.auto()\n    ROCM_AITER_MLA = enum.auto()  # Supported by V1\n    ROCM_AITER_MLA_VLLM_V1 = enum.auto()\n    TORCH_SDPA = enum.auto()\n    FLASHINFER = enum.auto()\n    TRITON_MLA = enum.auto()  # Supported by V1\n    FLASHMLA = enum.auto()  # Supported by V1\n    HPU_ATTN = enum.auto()\n    PALLAS = enum.auto()\n    PALLAS_VLLM_V1 = enum.auto()\n    IPEX = enum.auto()\n    BLOCK_SPARSE_FLASH_ATTN = enum.auto()\n    DUAL_CHUNK_FLASH_ATTN = enum.auto()\n    NO_ATTENTION = enum.auto()\n\n\nclass PlatformEnum(enum.Enum):\n    CUDA = enum.auto()\n    ROCM = enum.auto()\n    TPU = enum.auto()\n    HPU = enum.auto()\n    XPU = enum.auto()\n    CPU = enum.auto()\n    NEURON = enum.auto()\n    OOT = enum.auto()\n    UNSPECIFIED = enum.auto()\n\n\nclass CpuArchEnum(enum.Enum):\n    X86 = enum.auto()\n    ARM = enum.auto()\n    POWERPC = enum.auto()\n    OTHER = enum.auto()\n    UNKNOWN = enum.auto()\n\n\nclass DeviceCapability(NamedTuple):\n    major: int\n    minor: int\n\n    def as_version_str(self) -> str:\n        return f\"{self.major}.{self.minor}\"\n\n    def to_int(self) -> int:\n        \"\"\"\n        Express device capability as an integer ``<major><minor>``.\n\n        It is assumed that the minor version is always a single digit.\n        \"\"\"\n        assert 0 <= self.minor < 10\n        return self.major * 10 + self.minor\n\n\nclass Platform:\n    _enum: PlatformEnum\n    device_name: str\n    device_type: str\n\n    # available dispatch keys:\n    # check https://github.com/pytorch/pytorch/blob/313dac6c1ca0fa0cde32477509cce32089f8532a/torchgen/model.py#L134 # noqa\n    # use \"CPU\" as a fallback for platforms not registered in PyTorch\n    dispatch_key: str = \"CPU\"\n\n    # available ray device keys:\n    # https://github.com/ray-project/ray/blob/10ba5adadcc49c60af2c358a33bb943fb491a171/python/ray/_private/ray_constants.py#L438 # noqa\n    # empty string means the device does not support ray\n    ray_device_key: str = \"\"\n\n    # platform-agnostic way to specify the device control environment variable,\n    # .e.g. CUDA_VISIBLE_DEVICES for CUDA.\n    # hint: search for \"get_visible_accelerator_ids_env_var\" in\n    # https://github.com/ray-project/ray/tree/master/python/ray/_private/accelerators # noqa\n    device_control_env_var: str = \"VLLM_DEVICE_CONTROL_ENV_VAR_PLACEHOLDER\"\n\n    # The torch.compile backend for compiling simple and\n    # standalone functions. The default value is \"inductor\" to keep\n    # the same behavior as PyTorch.\n    # NOTE: for the forward part of the model, vLLM has another separate\n    # compilation strategy.\n    simple_compile_backend: str = \"inductor\"\n\n    supported_quantization: list[str] = []\n\n    additional_env_vars: list[str] = []\n\n    @property\n    def supported_dtypes(self) -> list[torch.dtype]:\n        \"\"\"Returns the supported dtypes for the current platform.\"\"\"\n        # Be careful with the order of the dtypes. The first dtype will\n        # be used as the default dtype fallback for the current platform,\n        # when encountering unsupported dtypes in \"auto\" dtype.\n        return [torch.bfloat16, torch.float16, torch.float32]\n\n    def is_cuda(self) -> bool:\n        return self._enum == PlatformEnum.CUDA\n\n    def is_rocm(self) -> bool:\n        return self._enum == PlatformEnum.ROCM\n\n    def is_tpu(self) -> bool:\n        return self._enum == PlatformEnum.TPU\n\n    def is_hpu(self) -> bool:\n        return self._enum == PlatformEnum.HPU\n\n    def is_xpu(self) -> bool:\n        return self._enum == PlatformEnum.XPU\n\n    def is_cpu(self) -> bool:\n        return self._enum == PlatformEnum.CPU\n\n    def is_neuron(self) -> bool:\n        return self._enum == PlatformEnum.NEURON\n\n    def is_out_of_tree(self) -> bool:\n        return self._enum == PlatformEnum.OOT\n\n    def is_cuda_alike(self) -> bool:\n        \"\"\"Stateless version of {func}`torch.cuda.is_available`.\"\"\"\n        return self._enum in (PlatformEnum.CUDA, PlatformEnum.ROCM)\n\n    def is_sleep_mode_available(self) -> bool:\n        return self._enum == PlatformEnum.CUDA\n\n    @classmethod\n    def device_id_to_physical_device_id(cls, device_id: int):\n        if cls.device_control_env_var in os.environ:\n            device_ids = os.environ[cls.device_control_env_var].split(\",\")\n            if device_ids == [\"\"]:\n                msg = (f\"{cls.device_control_env_var} is set to empty string, \"\n                       \"which means current platform support is disabled. If \"\n                       \"you are using ray, please unset the environment \"\n                       f\"variable `{cls.device_control_env_var}` inside the \"\n                       \"worker/actor. Check \"\n                       \"https://github.com/vllm-project/vllm/issues/8402 for \"\n                       \"more information.\")\n                raise RuntimeError(msg)\n            physical_device_id = device_ids[device_id]\n            return int(physical_device_id)\n        else:\n            return device_id\n\n    @classmethod\n    def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,\n                             dtype: torch.dtype, kv_cache_dtype: Optional[str],\n                             block_size: int, use_v1: bool,\n                             use_mla: bool) -> str:\n        \"\"\"Get the attention backend class of a device.\"\"\"\n        return \"\"\n\n    @classmethod\n    def get_device_capability(\n        cls,\n        device_id: int = 0,\n    ) -> Optional[DeviceCapability]:\n        \"\"\"Stateless version of {func}`torch.cuda.get_device_capability`.\"\"\"\n        return None\n\n    @classmethod\n    def has_device_capability(\n        cls,\n        capability: Union[tuple[int, int], int],\n        device_id: int = 0,\n    ) -> bool:\n        \"\"\"\n        Test whether this platform is compatible with a device capability.\n\n        The ``capability`` argument can either be:\n\n        - A tuple ``(major, minor)``.\n        - An integer ``<major><minor>``. (See {meth}`DeviceCapability.to_int`)\n        \"\"\"\n        current_capability = cls.get_device_capability(device_id=device_id)\n        if current_capability is None:\n            return False\n\n        if isinstance(capability, tuple):\n            return current_capability >= capability\n\n        return current_capability.to_int() >= capability\n\n    @classmethod\n    def get_device_name(cls, device_id: int = 0) -> str:\n        \"\"\"Get the name of a device.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_device_uuid(cls, device_id: int = 0) -> str:\n        \"\"\"Get the uuid of a device, e.g. the PCI bus ID.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_device_total_memory(cls, device_id: int = 0) -> int:\n        \"\"\"Get the total memory of a device in bytes.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def is_async_output_supported(cls, enforce_eager: Optional[bool]) -> bool:\n        \"\"\"\n        Check if the current platform supports async output.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def inference_mode(cls):\n        \"\"\"A device-specific wrapper of `torch.inference_mode`.\n\n        This wrapper is recommended because some hardware backends such as TPU\n        do not support `torch.inference_mode`. In such a case, they will fall\n        back to `torch.no_grad` by overriding this method.\n        \"\"\"\n        return torch.inference_mode(mode=True)\n\n    @classmethod\n    def seed_everything(cls, seed: Optional[int] = None) -> None:\n        \"\"\"\n        Set the seed of each random module.\n        `torch.manual_seed` will set seed on all devices.\n\n        Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20\n        \"\"\"\n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n\n    @classmethod\n    def pre_register_and_update(cls,\n                                parser: Optional[FlexibleArgumentParser] = None\n                                ) -> None:\n        \"\"\"\n        Do some pre-registration or update action for the current platform.\n\n        This function is called before global VllmConfig is initialized or cli\n        arguments are parsed. It's used for out-of-tree platforms to register or\n        update the configuration.\n\n        For example, the out-of-tree quantization config can be imported and\n        registered here dynamically.\n        \"\"\"\n        pass\n\n    @classmethod\n    def check_and_update_config(cls, vllm_config: VllmConfig) -> None:\n        \"\"\"\n        Check and update the configuration for the current platform.\n\n        It can raise an exception if the configuration is not compatible with\n        the current platform, or it can update the configuration to make it\n        compatible with the current platform.\n\n        The config is passed by reference, so it can be modified in place.\n        \"\"\"\n        pass\n\n    @classmethod\n    def verify_model_arch(cls, model_arch: str) -> None:\n        \"\"\"\n        Verify whether the current platform supports the specified model\n        architecture.\n\n        - This will raise an Error or Warning based on the model support on\n        the current platform.\n        - By default all models are considered supported.\n        \"\"\"\n        pass\n\n    @classmethod\n    def verify_quantization(cls, quant: str) -> None:\n        \"\"\"\n        Verify whether the quantization is supported by the current platform.\n        \"\"\"\n        if cls.supported_quantization and \\\n            quant not in cls.supported_quantization:\n            raise ValueError(\n                f\"{quant} quantization is currently not supported in \"\n                f\"{cls.device_name}.\")\n\n    @classmethod\n    def get_cpu_architecture(cls) -> CpuArchEnum:\n        \"\"\"\n        Determine the CPU architecture of the current system.\n        Returns CpuArchEnum indicating the architecture type.\n        \"\"\"\n        machine = platform.machine().lower()\n\n        if machine in (\"x86_64\", \"amd64\", \"i386\", \"i686\"):\n            return CpuArchEnum.X86\n        elif machine.startswith(\"arm\") or machine.startswith(\"aarch\"):\n            return CpuArchEnum.ARM\n        elif machine.startswith(\"ppc\"):\n            return CpuArchEnum.POWERPC\n\n        return CpuArchEnum.OTHER if machine else CpuArchEnum.UNKNOWN\n\n    @classmethod\n    def is_pin_memory_available(cls) -> bool:\n        \"\"\"Checks whether pin memory is available on the current platform.\"\"\"\n        if in_wsl():\n            # Pinning memory in WSL is not supported.\n            # https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications\n            logger.warning(\"Using 'pin_memory=False' as WSL is detected. \"\n                           \"This may slow down the performance.\")\n            return False\n        return True\n\n    @classmethod\n    def get_current_memory_usage(cls,\n                                 device: Optional[torch.types.Device] = None\n                                 ) -> float:\n        \"\"\"\n        Return the memory usage in bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_punica_wrapper(cls) -> str:\n        \"\"\"\n        Return the punica wrapper for current platform.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_infinity_values(cls, dtype: torch.dtype) -> tuple[float, float]:\n        \"\"\"\n        Return the platform specific values for (-inf, inf)\n        \"\"\"\n        return float(\"-inf\"), float(\"inf\")\n\n    @classmethod\n    def can_update_inplace(cls) -> bool:\n        \"\"\"\n        Checks if the platform allows inplace memory updates\n        \"\"\"\n        return True\n\n    @classmethod\n    def get_lora_vocab_padding_size(cls) -> int:\n        \"\"\"\n        Returns how much padding the LoRA logits need for kernels\n        \"\"\"\n        return 256\n\n    @classmethod\n    def get_device_communicator_cls(cls) -> str:\n        \"\"\"\n        Get device specific communicator class for distributed communication.\n        \"\"\"\n        return \"vllm.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase\"  # noqa\n\n    @classmethod\n    def supports_mx(cls) -> bool:\n        \"\"\"\n        Returns whether the current platform supports MX types.\n        \"\"\"\n        return False\n\n    @classmethod\n    def supports_fp8(cls) -> bool:\n        \"\"\"\n        Returns whether the current platform supports FP8 types.\n        \"\"\"\n        return False\n\n    @classmethod\n    def is_fp8_fnuz(cls) -> bool:\n        \"\"\"\n        Returns whether the preferred FP8 type is FNUZ on the current platform.\n\n        There are two representations of FP8, OCP FP8 and FNUZ FP8.\n        The OCP specification can be found at https://tinyurl.com/b7jvwpft.\n        The FNUZ specification can be found at https://tinyurl.com/5n6hwwu5.\n\n        AMD's MI300 and MI325 have native hardware support for FNUZ. All other\n        hardware has converged on the OCP FP8 standard.\n        \"\"\"\n        return False\n\n    @classmethod\n    def fp8_dtype(cls) -> torch.dtype:\n        \"\"\"\n        Returns the preferred FP8 type on the current platform.\n\n        See the documentation for is_fp8_fnuz for details.\n        \"\"\"\n        return torch.float8_e4m3fn\n\n    @classmethod\n    def use_all_gather(cls) -> bool:\n        \"\"\"\n        Whether to use allgather in LogitsProcessor to gather the logits.\n        \"\"\"\n        import vllm.envs as envs\n        from vllm.config import get_current_vllm_config\n\n        parallel_config = get_current_vllm_config().parallel_config\n        return (envs.VLLM_USE_V1\n                or parallel_config.distributed_executor_backend\n                == \"external_launcher\")\n\n    @classmethod\n    def supports_v1(cls, model_config: ModelConfig) -> bool:\n        \"\"\"Returns whether the current platform can support v1 for the supplied\n        model configuration.\n        \"\"\"\n        return False\n\n    @classmethod\n    def use_custom_allreduce(cls) -> bool:\n        \"\"\"\n        Returns if custom allreduce is supported on the current platform\n        \"\"\"\n        return False\n\n    @classmethod\n    def validate_request(\n        cls,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        processed_inputs: ProcessorInputs,\n    ) -> None:\n        \"\"\"Raises if this request is unsupported on this platform\"\"\"\n\n    def __getattr__(self, key: str):\n        device = getattr(torch, self.device_type, None)\n        if device is not None and hasattr(device, key):\n            return getattr(device, key)\n        else:\n            logger.warning(\"Current platform %s does not have '%s'\" \\\n            \" attribute.\", self.device_type, key)\n            return None\n\n    @classmethod\n    def get_cu_count(cls, device_id: int = 0) -> int:\n        \"\"\"\n        Returns the total number of compute units (CU) on single GPU.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass UnspecifiedPlatform(Platform):\n    _enum = PlatformEnum.UNSPECIFIED\n    device_type = \"\"\n", 484], "/home/jeromeku/vllm/vllm/transformers_utils/utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport json\nfrom functools import cache\nfrom os import PathLike\nfrom pathlib import Path\nfrom typing import Optional, Union\n\nfrom vllm.envs import VLLM_MODEL_REDIRECT_PATH\nfrom vllm.logger import init_logger\n\nlogger = init_logger(__name__)\n\n\ndef is_s3(model_or_path: str) -> bool:\n    return model_or_path.lower().startswith('s3://')\n\n\ndef check_gguf_file(model: Union[str, PathLike]) -> bool:\n    \"\"\"Check if the file is a GGUF model.\"\"\"\n    model = Path(model)\n    if not model.is_file():\n        return False\n    elif model.suffix == \".gguf\":\n        return True\n\n    try:\n        with model.open(\"rb\") as f:\n            header = f.read(4)\n\n        return header == b\"GGUF\"\n    except Exception as e:\n        logger.debug(\"Error reading file %s: %s\", model, e)\n        return False\n\n\ndef modelscope_list_repo_files(\n    repo_id: str,\n    revision: Optional[str] = None,\n    token: Union[str, bool, None] = None,\n) -> list[str]:\n    \"\"\"List files in a modelscope repo.\"\"\"\n    from modelscope.hub.api import HubApi\n    api = HubApi()\n    api.login(token)\n    # same as huggingface_hub.list_repo_files\n    files = [\n        file['Path'] for file in api.get_model_files(\n            model_id=repo_id, revision=revision, recursive=True)\n        if file['Type'] == 'blob'\n    ]\n    return files\n\n\ndef _maybe_json_dict(path: Union[str, PathLike]) -> dict[str, str]:\n    with open(path) as f:\n        try:\n            return json.loads(f.read())\n        except Exception:\n            return dict[str, str]()\n\n\ndef _maybe_space_split_dict(path: Union[str, PathLike]) -> dict[str, str]:\n    parsed_dict = dict[str, str]()\n    with open(path) as f:\n        for line in f.readlines():\n            try:\n                model_name, redirect_name = line.strip().split()\n                parsed_dict[model_name] = redirect_name\n            except Exception:\n                pass\n    return parsed_dict\n\n\n@cache\ndef maybe_model_redirect(model: str) -> str:\n    \"\"\"\n    Use model_redirect to redirect the model name to a local folder.\n\n    :param model: hf model name\n    :return: maybe redirect to a local folder\n    \"\"\"\n\n    model_redirect_path = VLLM_MODEL_REDIRECT_PATH\n\n    if not model_redirect_path:\n        return model\n\n    if not Path(model_redirect_path).exists():\n        return model\n\n    redirect_dict = (_maybe_json_dict(model_redirect_path)\n                     or _maybe_space_split_dict(model_redirect_path))\n    if (redirect_model := redirect_dict.get(model)):\n        logger.info(\"model redirect: [ %s ] -> [ %s ]\", model, redirect_model)\n        return redirect_model\n\n    return model\n", 98], "/home/jeromeku/vllm/vllm/envs.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport hashlib\nimport os\nimport sys\nimport tempfile\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nif TYPE_CHECKING:\n    VLLM_HOST_IP: str = \"\"\n    VLLM_PORT: Optional[int] = None\n    VLLM_RPC_BASE_PATH: str = tempfile.gettempdir()\n    VLLM_USE_MODELSCOPE: bool = False\n    VLLM_RINGBUFFER_WARNING_INTERVAL: int = 60\n    VLLM_NCCL_SO_PATH: Optional[str] = None\n    LD_LIBRARY_PATH: Optional[str] = None\n    VLLM_USE_TRITON_FLASH_ATTN: bool = False\n    VLLM_FLASH_ATTN_VERSION: Optional[int] = None\n    LOCAL_RANK: int = 0\n    CUDA_VISIBLE_DEVICES: Optional[str] = None\n    VLLM_ENGINE_ITERATION_TIMEOUT_S: int = 60\n    VLLM_API_KEY: Optional[str] = None\n    S3_ACCESS_KEY_ID: Optional[str] = None\n    S3_SECRET_ACCESS_KEY: Optional[str] = None\n    S3_ENDPOINT_URL: Optional[str] = None\n    VLLM_MODEL_REDIRECT_PATH: Optional[str] = None\n    VLLM_CACHE_ROOT: str = os.path.expanduser(\"~/.cache/vllm\")\n    VLLM_CONFIG_ROOT: str = os.path.expanduser(\"~/.config/vllm\")\n    VLLM_USAGE_STATS_SERVER: str = \"https://stats.vllm.ai\"\n    VLLM_NO_USAGE_STATS: bool = False\n    VLLM_DO_NOT_TRACK: bool = False\n    VLLM_USAGE_SOURCE: str = \"\"\n    VLLM_CONFIGURE_LOGGING: int = 1\n    VLLM_LOGGING_LEVEL: str = \"INFO\"\n    VLLM_LOGGING_PREFIX: str = \"\"\n    VLLM_LOGGING_CONFIG_PATH: Optional[str] = None\n    VLLM_LOGITS_PROCESSOR_THREADS: Optional[int] = None\n    VLLM_TRACE_FUNCTION: int = 0\n    VLLM_ATTENTION_BACKEND: Optional[str] = None\n    VLLM_USE_FLASHINFER_SAMPLER: Optional[bool] = None\n    VLLM_FLASHINFER_FORCE_TENSOR_CORES: bool = False\n    VLLM_PP_LAYER_PARTITION: Optional[str] = None\n    VLLM_CPU_KVCACHE_SPACE: int = 0\n    VLLM_CPU_OMP_THREADS_BIND: str = \"\"\n    VLLM_CPU_MOE_PREPACK: bool = True\n    VLLM_XLA_CACHE_PATH: str = os.path.join(VLLM_CACHE_ROOT, \"xla_cache\")\n    VLLM_XLA_CHECK_RECOMPILATION: bool = False\n    VLLM_FUSED_MOE_CHUNK_SIZE: int = 64 * 1024\n    VLLM_USE_RAY_SPMD_WORKER: bool = False\n    VLLM_USE_RAY_COMPILED_DAG: bool = False\n    VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE: str = \"auto\"\n    VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM: bool = False\n    VLLM_WORKER_MULTIPROC_METHOD: str = \"fork\"\n    VLLM_ASSETS_CACHE: str = os.path.join(VLLM_CACHE_ROOT, \"assets\")\n    VLLM_IMAGE_FETCH_TIMEOUT: int = 5\n    VLLM_VIDEO_FETCH_TIMEOUT: int = 30\n    VLLM_AUDIO_FETCH_TIMEOUT: int = 10\n    VLLM_VIDEO_LOADER_BACKEND: str = \"opencv\"\n    VLLM_MM_INPUT_CACHE_GIB: int = 8\n    VLLM_TARGET_DEVICE: str = \"cuda\"\n    MAX_JOBS: Optional[str] = None\n    NVCC_THREADS: Optional[str] = None\n    VLLM_USE_PRECOMPILED: bool = False\n    VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL: bool = False\n    VLLM_NO_DEPRECATION_WARNING: bool = False\n    VLLM_KEEP_ALIVE_ON_ENGINE_DEATH: bool = False\n    CMAKE_BUILD_TYPE: Optional[str] = None\n    VERBOSE: bool = False\n    VLLM_ALLOW_LONG_MAX_MODEL_LEN: bool = False\n    VLLM_RPC_TIMEOUT: int = 10000  # ms\n    VLLM_PLUGINS: Optional[list[str]] = None\n    VLLM_LORA_RESOLVER_CACHE_DIR: Optional[str] = None\n    VLLM_TORCH_PROFILER_DIR: Optional[str] = None\n    VLLM_USE_TRITON_AWQ: bool = False\n    VLLM_ALLOW_RUNTIME_LORA_UPDATING: bool = False\n    VLLM_SKIP_P2P_CHECK: bool = False\n    VLLM_DISABLED_KERNELS: list[str] = []\n    VLLM_USE_V1: bool = True\n    VLLM_ROCM_USE_AITER: bool = False\n    VLLM_ROCM_USE_AITER_PAGED_ATTN: bool = False\n    VLLM_ROCM_USE_AITER_LINEAR: bool = True\n    VLLM_ROCM_USE_AITER_MOE: bool = True\n    VLLM_ROCM_USE_AITER_RMSNORM: bool = True\n    VLLM_ROCM_USE_AITER_MLA: bool = True\n    VLLM_ROCM_USE_SKINNY_GEMM: bool = True\n    VLLM_ROCM_FP8_PADDING: bool = True\n    VLLM_ROCM_MOE_PADDING: bool = True\n    VLLM_ROCM_CUSTOM_PAGED_ATTN: bool = True\n    VLLM_QUARK_EMU_MEM_OPT: bool = False\n    VLLM_ENABLE_V1_MULTIPROCESSING: bool = True\n    VLLM_LOG_BATCHSIZE_INTERVAL: float = -1\n    VLLM_DISABLE_COMPILE_CACHE: bool = False\n    Q_SCALE_CONSTANT: int = 200\n    K_SCALE_CONSTANT: int = 200\n    V_SCALE_CONSTANT: int = 100\n    VLLM_SERVER_DEV_MODE: bool = False\n    VLLM_V1_OUTPUT_PROC_CHUNK_SIZE: int = 128\n    VLLM_MLA_DISABLE: bool = False\n    VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON: bool = False\n    VLLM_RAY_PER_WORKER_GPUS: float = 1.0\n    VLLM_RAY_BUNDLE_INDICES: str = \"\"\n    VLLM_CUDART_SO_PATH: Optional[str] = None\n    VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH: bool = True\n    VLLM_HPU_USE_DELAYED_SAMPLING: bool = False\n    VLLM_DP_RANK: int = 0\n    VLLM_DP_RANK_LOCAL: int = -1\n    VLLM_DP_SIZE: int = 1\n    VLLM_DP_MASTER_IP: str = \"\"\n    VLLM_DP_MASTER_PORT: int = 0\n    VLLM_MARLIN_USE_ATOMIC_ADD: bool = False\n    VLLM_V0_USE_OUTLINES_CACHE: bool = False\n    VLLM_TPU_BUCKET_PADDING_GAP: int = 0\n    VLLM_USE_DEEP_GEMM: bool = False\n    VLLM_XGRAMMAR_CACHE_MB: int = 0\n    VLLM_MSGPACK_ZERO_COPY_THRESHOLD: int = 256\n    VLLM_ALLOW_INSECURE_SERIALIZATION: bool = False\n    VLLM_NIXL_SIDE_CHANNEL_HOST: str = \"localhost\"\n    VLLM_NIXL_SIDE_CHANNEL_PORT: int = 5557\n    VLLM_ALL2ALL_BACKEND: str = \"naive\"\n\n\ndef get_default_cache_root():\n    return os.getenv(\n        \"XDG_CACHE_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".cache\"),\n    )\n\n\ndef get_default_config_root():\n    return os.getenv(\n        \"XDG_CONFIG_HOME\",\n        os.path.join(os.path.expanduser(\"~\"), \".config\"),\n    )\n\n\ndef maybe_convert_int(value: Optional[str]) -> Optional[int]:\n    if value is None:\n        return None\n    return int(value)\n\n\ndef get_vllm_port() -> Optional[int]:\n    \"\"\"Get the port from VLLM_PORT environment variable.\n    \n    Returns:\n        The port number as an integer if VLLM_PORT is set, None otherwise.\n        \n    Raises:\n        ValueError: If VLLM_PORT is a URI, suggest k8s service discovery issue.\n    \"\"\"\n    if 'VLLM_PORT' not in os.environ:\n        return None\n\n    port = os.getenv('VLLM_PORT', '0')\n\n    try:\n        return int(port)\n    except ValueError as err:\n        from urllib.parse import urlparse\n        try:\n            parsed = urlparse(port)\n            if parsed.scheme:\n                raise ValueError(\n                    f\"VLLM_PORT '{port}' appears to be a URI. \"\n                    \"This may be caused by a Kubernetes service discovery issue\"\n                    \"check the warning in: https://docs.vllm.ai/en/stable/serving/env_vars.html\"\n                )\n        except Exception:\n            pass\n\n        raise ValueError(\n            f\"VLLM_PORT '{port}' must be a valid integer\") from err\n\n\n# The begin-* and end* here are used by the documentation generator\n# to extract the used env vars.\n\n# begin-env-vars-definition\n\nenvironment_variables: dict[str, Callable[[], Any]] = {\n\n    # ================== Installation Time Env Vars ==================\n\n    # Target device of vLLM, supporting [cuda (by default),\n    # rocm, neuron, cpu]\n    \"VLLM_TARGET_DEVICE\":\n    lambda: os.getenv(\"VLLM_TARGET_DEVICE\", \"cuda\"),\n\n    # Maximum number of compilation jobs to run in parallel.\n    # By default this is the number of CPUs\n    \"MAX_JOBS\":\n    lambda: os.getenv(\"MAX_JOBS\", None),\n\n    # Number of threads to use for nvcc\n    # By default this is 1.\n    # If set, `MAX_JOBS` will be reduced to avoid oversubscribing the CPU.\n    \"NVCC_THREADS\":\n    lambda: os.getenv(\"NVCC_THREADS\", None),\n\n    # If set, vllm will use precompiled binaries (*.so)\n    \"VLLM_USE_PRECOMPILED\":\n    lambda: bool(os.environ.get(\"VLLM_USE_PRECOMPILED\")) or bool(\n        os.environ.get(\"VLLM_PRECOMPILED_WHEEL_LOCATION\")),\n\n    # Whether to force using nightly wheel in python build.\n    # This is used for testing the nightly wheel in python build.\n    \"VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL\":\n    lambda: bool(int(os.getenv(\"VLLM_TEST_USE_PRECOMPILED_NIGHTLY_WHEEL\", \"0\"))\n                 ),\n\n    # CMake build type\n    # If not set, defaults to \"Debug\" or \"RelWithDebInfo\"\n    # Available options: \"Debug\", \"Release\", \"RelWithDebInfo\"\n    \"CMAKE_BUILD_TYPE\":\n    lambda: os.getenv(\"CMAKE_BUILD_TYPE\"),\n\n    # If set, vllm will print verbose logs during installation\n    \"VERBOSE\":\n    lambda: bool(int(os.getenv('VERBOSE', '0'))),\n\n    # Root directory for vLLM configuration files\n    # Defaults to `~/.config/vllm` unless `XDG_CONFIG_HOME` is set\n    # Note that this not only affects how vllm finds its configuration files\n    # during runtime, but also affects how vllm installs its configuration\n    # files during **installation**.\n    \"VLLM_CONFIG_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CONFIG_ROOT\",\n            os.path.join(get_default_config_root(), \"vllm\"),\n        )),\n\n    # ================== Runtime Env Vars ==================\n\n    # Root directory for vLLM cache files\n    # Defaults to `~/.cache/vllm` unless `XDG_CACHE_HOME` is set\n    \"VLLM_CACHE_ROOT\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_CACHE_ROOT\",\n            os.path.join(get_default_cache_root(), \"vllm\"),\n        )),\n\n    # used in distributed environment to determine the ip address\n    # of the current node, when the node has multiple network interfaces.\n    # If you are using multi-node inference, you should set this differently\n    # on each node.\n    'VLLM_HOST_IP':\n    lambda: os.getenv('VLLM_HOST_IP', \"\"),\n\n    # used in distributed environment to manually set the communication port\n    # Note: if VLLM_PORT is set, and some code asks for multiple ports, the\n    # VLLM_PORT will be used as the first port, and the rest will be generated\n    # by incrementing the VLLM_PORT value.\n    'VLLM_PORT':\n    get_vllm_port,\n\n    # path used for ipc when the frontend api server is running in\n    # multi-processing mode to communicate with the backend engine process.\n    'VLLM_RPC_BASE_PATH':\n    lambda: os.getenv('VLLM_RPC_BASE_PATH', tempfile.gettempdir()),\n\n    # If true, will load models from ModelScope instead of Hugging Face Hub.\n    # note that the value is true or false, not numbers\n    \"VLLM_USE_MODELSCOPE\":\n    lambda: os.environ.get(\"VLLM_USE_MODELSCOPE\", \"False\").lower() == \"true\",\n\n    # Interval in seconds to log a warning message when the ring buffer is full\n    \"VLLM_RINGBUFFER_WARNING_INTERVAL\":\n    lambda: int(os.environ.get(\"VLLM_RINGBUFFER_WARNING_INTERVAL\", \"60\")),\n\n    # path to cudatoolkit home directory, under which should be bin, include,\n    # and lib directories.\n    \"CUDA_HOME\":\n    lambda: os.environ.get(\"CUDA_HOME\", None),\n\n    # Path to the NCCL library file. It is needed because nccl>=2.19 brought\n    # by PyTorch contains a bug: https://github.com/NVIDIA/nccl/issues/1234\n    \"VLLM_NCCL_SO_PATH\":\n    lambda: os.environ.get(\"VLLM_NCCL_SO_PATH\", None),\n\n    # when `VLLM_NCCL_SO_PATH` is not set, vllm will try to find the nccl\n    # library file in the locations specified by `LD_LIBRARY_PATH`\n    \"LD_LIBRARY_PATH\":\n    lambda: os.environ.get(\"LD_LIBRARY_PATH\", None),\n\n    # flag to control if vllm should use triton flash attention\n    \"VLLM_USE_TRITON_FLASH_ATTN\":\n    lambda: (os.environ.get(\"VLLM_USE_TRITON_FLASH_ATTN\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Force vllm to use a specific flash-attention version (2 or 3), only valid\n    # when using the flash-attention backend.\n    \"VLLM_FLASH_ATTN_VERSION\":\n    lambda: maybe_convert_int(os.environ.get(\"VLLM_FLASH_ATTN_VERSION\", None)),\n\n    # Internal flag to enable Dynamo fullgraph capture\n    \"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\":\n    lambda: bool(\n        os.environ.get(\"VLLM_TEST_DYNAMO_FULLGRAPH_CAPTURE\", \"1\") != \"0\"),\n\n    # Internal flag to enable/disable Inductor standalone compile\n    \"VLLM_TEST_STANDALONE_COMPILE\":\n    lambda: os.environ.get(\"VLLM_TEST_STANDALONE_COMPILE\", \"0\") != \"0\",\n\n    # local rank of the process in the distributed setting, used to determine\n    # the GPU device id\n    \"LOCAL_RANK\":\n    lambda: int(os.environ.get(\"LOCAL_RANK\", \"0\")),\n\n    # used to control the visible devices in the distributed setting\n    \"CUDA_VISIBLE_DEVICES\":\n    lambda: os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n\n    # timeout for each iteration in the engine\n    \"VLLM_ENGINE_ITERATION_TIMEOUT_S\":\n    lambda: int(os.environ.get(\"VLLM_ENGINE_ITERATION_TIMEOUT_S\", \"60\")),\n\n    # API key for vLLM API server\n    \"VLLM_API_KEY\":\n    lambda: os.environ.get(\"VLLM_API_KEY\", None),\n\n    # Whether to log responses from API Server for debugging\n    \"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\":\n    lambda: os.environ.get(\"VLLM_DEBUG_LOG_API_SERVER_RESPONSE\", \"False\").\n    lower() == \"true\",\n\n    # S3 access information, used for tensorizer to load model from S3\n    \"S3_ACCESS_KEY_ID\":\n    lambda: os.environ.get(\"S3_ACCESS_KEY_ID\", None),\n    \"S3_SECRET_ACCESS_KEY\":\n    lambda: os.environ.get(\"S3_SECRET_ACCESS_KEY\", None),\n    \"S3_ENDPOINT_URL\":\n    lambda: os.environ.get(\"S3_ENDPOINT_URL\", None),\n\n    # Usage stats collection\n    \"VLLM_USAGE_STATS_SERVER\":\n    lambda: os.environ.get(\"VLLM_USAGE_STATS_SERVER\", \"https://stats.vllm.ai\"),\n    \"VLLM_NO_USAGE_STATS\":\n    lambda: os.environ.get(\"VLLM_NO_USAGE_STATS\", \"0\") == \"1\",\n    \"VLLM_DO_NOT_TRACK\":\n    lambda: (os.environ.get(\"VLLM_DO_NOT_TRACK\", None) or os.environ.get(\n        \"DO_NOT_TRACK\", None) or \"0\") == \"1\",\n    \"VLLM_USAGE_SOURCE\":\n    lambda: os.environ.get(\"VLLM_USAGE_SOURCE\", \"production\"),\n\n    # Logging configuration\n    # If set to 0, vllm will not configure logging\n    # If set to 1, vllm will configure logging using the default configuration\n    #    or the configuration file specified by VLLM_LOGGING_CONFIG_PATH\n    \"VLLM_CONFIGURE_LOGGING\":\n    lambda: int(os.getenv(\"VLLM_CONFIGURE_LOGGING\", \"1\")),\n    \"VLLM_LOGGING_CONFIG_PATH\":\n    lambda: os.getenv(\"VLLM_LOGGING_CONFIG_PATH\"),\n\n    # this is used for configuring the default logging level\n    \"VLLM_LOGGING_LEVEL\":\n    lambda: os.getenv(\"VLLM_LOGGING_LEVEL\", \"INFO\").upper(),\n\n    # if set, VLLM_LOGGING_PREFIX will be prepended to all log messages\n    \"VLLM_LOGGING_PREFIX\":\n    lambda: os.getenv(\"VLLM_LOGGING_PREFIX\", \"\"),\n\n    # if set, vllm will call logits processors in a thread pool with this many\n    # threads. This is useful when using custom logits processors that either\n    # (a) launch additional CUDA kernels or (b) do significant CPU-bound work\n    # while not holding the python GIL, or both.\n    \"VLLM_LOGITS_PROCESSOR_THREADS\":\n    lambda: int(os.getenv(\"VLLM_LOGITS_PROCESSOR_THREADS\", \"0\"))\n    if \"VLLM_LOGITS_PROCESSOR_THREADS\" in os.environ else None,\n\n    # Trace function calls\n    # If set to 1, vllm will trace function calls\n    # Useful for debugging\n    \"VLLM_TRACE_FUNCTION\":\n    lambda: int(os.getenv(\"VLLM_TRACE_FUNCTION\", \"0\")),\n\n    # Backend for attention computation\n    # Available options:\n    # - \"TORCH_SDPA\": use torch.nn.MultiheadAttention\n    # - \"FLASH_ATTN\": use FlashAttention\n    # - \"XFORMERS\": use XFormers\n    # - \"ROCM_FLASH\": use ROCmFlashAttention\n    # - \"FLASHINFER\": use flashinfer\n    # - \"FLASHMLA\": use FlashMLA\n    \"VLLM_ATTENTION_BACKEND\":\n    lambda: os.getenv(\"VLLM_ATTENTION_BACKEND\", None),\n\n    # If set, vllm will use flashinfer sampler\n    \"VLLM_USE_FLASHINFER_SAMPLER\":\n    lambda: bool(int(os.environ[\"VLLM_USE_FLASHINFER_SAMPLER\"]))\n    if \"VLLM_USE_FLASHINFER_SAMPLER\" in os.environ else None,\n\n    # If set, vllm will force flashinfer to use tensor cores;\n    # otherwise will use heuristic based on model architecture.\n    \"VLLM_FLASHINFER_FORCE_TENSOR_CORES\":\n    lambda: bool(int(os.getenv(\"VLLM_FLASHINFER_FORCE_TENSOR_CORES\", \"0\"))),\n\n    # Pipeline stage partition strategy\n    \"VLLM_PP_LAYER_PARTITION\":\n    lambda: os.getenv(\"VLLM_PP_LAYER_PARTITION\", None),\n\n    # (CPU backend only) CPU key-value cache space.\n    # default is 4 GiB\n    \"VLLM_CPU_KVCACHE_SPACE\":\n    lambda: int(os.getenv(\"VLLM_CPU_KVCACHE_SPACE\", \"0\")),\n\n    # (CPU backend only) CPU core ids bound by OpenMP threads, e.g., \"0-31\",\n    # \"0,1,2\", \"0-31,33\". CPU cores of different ranks are separated by '|'.\n    \"VLLM_CPU_OMP_THREADS_BIND\":\n    lambda: os.getenv(\"VLLM_CPU_OMP_THREADS_BIND\", \"all\"),\n\n    # (CPU backend only) whether to use prepack for MoE layer. This will be\n    # passed to ipex.llm.modules.GatedMLPMOE. On unsupported CPUs, you might\n    # need to set this to \"0\" (False).\n    \"VLLM_CPU_MOE_PREPACK\":\n    lambda: bool(int(os.getenv(\"VLLM_CPU_MOE_PREPACK\", \"1\"))),\n\n    # If the env var is set, then all workers will execute as separate\n    # processes from the engine, and we use the same mechanism to trigger\n    # execution on all workers.\n    # Run vLLM with VLLM_USE_RAY_SPMD_WORKER=1 to enable it.\n    \"VLLM_USE_RAY_SPMD_WORKER\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_SPMD_WORKER\", \"0\"))),\n\n    # If the env var is set, it uses the Ray's Compiled Graph\n    # (previously known as ADAG) API which optimizes the\n    # control plane overhead.\n    # Run vLLM with VLLM_USE_RAY_COMPILED_DAG=1 to enable it.\n    # Note that this variable is set to 1 in V1 by default\n    # when ray distributed executor is used.\n    \"VLLM_USE_RAY_COMPILED_DAG\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG\", \"0\"))),\n\n    # If the env var is set, Ray Compiled Graph uses the specified\n    # channel type to communicate between workers belonging to\n    # different pipeline-parallel stages.\n    # Available options:\n    # - \"auto\": use the default channel type\n    # - \"nccl\": use NCCL for communication\n    # - \"shm\": use shared memory and gRPC for communication\n    # This flag is ignored if VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE\":\n    lambda: os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_CHANNEL_TYPE\", \"auto\"),\n\n    # If the env var is set, it enables GPU communication overlap\n    # (experimental feature) in Ray's Compiled Graph. This flag is ignored if\n    # VLLM_USE_RAY_COMPILED_DAG is not set.\n    \"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_RAY_COMPILED_DAG_OVERLAP_COMM\", \"0\"))\n                 ),\n\n    # Use dedicated multiprocess context for workers.\n    # Both spawn and fork work\n    \"VLLM_WORKER_MULTIPROC_METHOD\":\n    lambda: os.getenv(\"VLLM_WORKER_MULTIPROC_METHOD\", \"fork\"),\n\n    # Path to the cache for storing downloaded assets\n    \"VLLM_ASSETS_CACHE\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_ASSETS_CACHE\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"assets\"),\n        )),\n\n    # Timeout for fetching images when serving multimodal models\n    # Default is 5 seconds\n    \"VLLM_IMAGE_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_IMAGE_FETCH_TIMEOUT\", \"5\")),\n\n    # Timeout for fetching videos when serving multimodal models\n    # Default is 30 seconds\n    \"VLLM_VIDEO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_VIDEO_FETCH_TIMEOUT\", \"30\")),\n\n    # Timeout for fetching audio when serving multimodal models\n    # Default is 10 seconds\n    \"VLLM_AUDIO_FETCH_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_AUDIO_FETCH_TIMEOUT\", \"10\")),\n\n    # Backend for Video IO\n    # - \"opencv\": Default backend that uses OpenCV stream buffered backend.\n    #\n    # Custom backend implementations can be registered\n    # via `@VIDEO_LOADER_REGISTRY.register(\"my_custom_video_loader\")` and\n    # imported at runtime.\n    # If a non-existing backend is used, an AssertionError will be thrown.\n    \"VLLM_VIDEO_LOADER_BACKEND\":\n    lambda: os.getenv(\"VLLM_VIDEO_LOADER_BACKEND\", \"opencv\"),\n\n    # Cache size (in GiB) for multimodal input cache\n    # Default is 4 GiB\n    \"VLLM_MM_INPUT_CACHE_GIB\":\n    lambda: int(os.getenv(\"VLLM_MM_INPUT_CACHE_GIB\", \"4\")),\n\n    # Path to the XLA persistent cache directory.\n    # Only used for XLA devices such as TPUs.\n    \"VLLM_XLA_CACHE_PATH\":\n    lambda: os.path.expanduser(\n        os.getenv(\n            \"VLLM_XLA_CACHE_PATH\",\n            os.path.join(get_default_cache_root(), \"vllm\", \"xla_cache\"),\n        )),\n\n    # If set, assert on XLA recompilation after each execution step.\n    \"VLLM_XLA_CHECK_RECOMPILATION\":\n    lambda: bool(int(os.getenv(\"VLLM_XLA_CHECK_RECOMPILATION\", \"0\"))),\n    \"VLLM_FUSED_MOE_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_FUSED_MOE_CHUNK_SIZE\", \"32768\")),\n\n    # If set, vllm will skip the deprecation warnings.\n    \"VLLM_NO_DEPRECATION_WARNING\":\n    lambda: bool(int(os.getenv(\"VLLM_NO_DEPRECATION_WARNING\", \"0\"))),\n\n    # If set, the OpenAI API server will stay alive even after the underlying\n    # AsyncLLMEngine errors and stops serving requests\n    \"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\":\n    lambda: bool(os.getenv(\"VLLM_KEEP_ALIVE_ON_ENGINE_DEATH\", 0)),\n\n    # If the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN is set, it allows\n    # the user to specify a max sequence length greater than\n    # the max length derived from the model's config.json.\n    # To enable this, set VLLM_ALLOW_LONG_MAX_MODEL_LEN=1.\n    \"VLLM_ALLOW_LONG_MAX_MODEL_LEN\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_LONG_MAX_MODEL_LEN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # If set, forces FP8 Marlin to be used for FP8 quantization regardless\n    # of the hardware support for FP8 compute.\n    \"VLLM_TEST_FORCE_FP8_MARLIN\":\n    lambda:\n    (os.environ.get(\"VLLM_TEST_FORCE_FP8_MARLIN\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n    \"VLLM_TEST_FORCE_LOAD_FORMAT\":\n    lambda: os.getenv(\"VLLM_TEST_FORCE_LOAD_FORMAT\", \"dummy\"),\n\n    # Time in ms for the zmq client to wait for a response from the backend\n    # server for simple data operations\n    \"VLLM_RPC_TIMEOUT\":\n    lambda: int(os.getenv(\"VLLM_RPC_TIMEOUT\", \"10000\")),\n\n    # a list of plugin names to load, separated by commas.\n    # if this is not set, it means all plugins will be loaded\n    # if this is set to an empty string, no plugins will be loaded\n    \"VLLM_PLUGINS\":\n    lambda: None if \"VLLM_PLUGINS\" not in os.environ else os.environ[\n        \"VLLM_PLUGINS\"].split(\",\"),\n\n    # a local directory to look in for unrecognized LoRA adapters.\n    # only works if plugins are enabled and\n    # VLLM_ALLOW_RUNTIME_LORA_UPDATING is enabled.\n    \"VLLM_LORA_RESOLVER_CACHE_DIR\":\n    lambda: os.getenv(\"VLLM_LORA_RESOLVER_CACHE_DIR\", None),\n\n    # Enables torch profiler if set. Path to the directory where torch profiler\n    # traces are saved. Note that it must be an absolute path.\n    \"VLLM_TORCH_PROFILER_DIR\":\n    lambda: (None if os.getenv(\"VLLM_TORCH_PROFILER_DIR\", None) is None else os\n             .path.expanduser(os.getenv(\"VLLM_TORCH_PROFILER_DIR\", \".\"))),\n\n    # If set, vLLM will use Triton implementations of AWQ.\n    \"VLLM_USE_TRITON_AWQ\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_TRITON_AWQ\", \"0\"))),\n\n    # If set, allow loading or unloading lora adapters in runtime,\n    \"VLLM_ALLOW_RUNTIME_LORA_UPDATING\":\n    lambda:\n    (os.environ.get(\"VLLM_ALLOW_RUNTIME_LORA_UPDATING\", \"0\").strip().lower() in\n     (\"1\", \"true\")),\n\n    # By default, vLLM will check the peer-to-peer capability itself,\n    # in case of broken drivers. See https://github.com/vllm-project/vllm/blob/a9b15c606fea67a072416ea0ea115261a2756058/vllm/distributed/device_communicators/custom_all_reduce_utils.py#L101-L108 for details. # noqa\n    # If this env var is set to 1, vLLM will skip the peer-to-peer check,\n    # and trust the driver's peer-to-peer capability report.\n    \"VLLM_SKIP_P2P_CHECK\":\n    lambda: os.getenv(\"VLLM_SKIP_P2P_CHECK\", \"0\") == \"1\",\n\n    # List of quantization kernels that should be disabled, used for testing\n    # and performance comparisons. Currently only affects MPLinearKernel\n    # selection\n    # (kernels: MacheteLinearKernel, MarlinLinearKernel, ExllamaLinearKernel)\n    \"VLLM_DISABLED_KERNELS\":\n    lambda: [] if \"VLLM_DISABLED_KERNELS\" not in os.environ else os.environ[\n        \"VLLM_DISABLED_KERNELS\"].split(\",\"),\n\n    # If set, use the V1 code path.\n    \"VLLM_USE_V1\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_V1\", \"1\"))),\n\n    # Disable aiter ops unless specifically enabled.\n    # Acts as a parent switch to enable the rest of the other operations.\n    \"VLLM_ROCM_USE_AITER\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_AITER\", \"False\").lower() in\n             (\"true\", \"1\")),\n\n    # Whether to use aiter paged attention.\n    # By default is disabled.\n    \"VLLM_ROCM_USE_AITER_PAGED_ATTN\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_AITER_PAGED_ATTN\", \"False\").lower() in\n             (\"true\", \"1\")),\n\n    # use aiter linear op if aiter ops are enabled\n    # The following list of related ops\n    # - scaled_mm (per-tensor / rowwise)\n    \"VLLM_ROCM_USE_AITER_LINEAR\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_AITER_LINEAR\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Whether to use aiter moe ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MOE\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_AITER_MOE\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # use aiter rms norm op if aiter ops are enabled.\n    \"VLLM_ROCM_USE_AITER_RMSNORM\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_AITER_RMSNORM\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Whether to use aiter mla ops.\n    # By default is enabled.\n    \"VLLM_ROCM_USE_AITER_MLA\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_AITER_MLA\", \"True\").lower() in\n             (\"true\", \"1\")),\n    # use rocm skinny gemms\n    \"VLLM_ROCM_USE_SKINNY_GEMM\":\n    lambda: (os.getenv(\"VLLM_ROCM_USE_SKINNY_GEMM\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # Pad the fp8 weights to 256 bytes for ROCm\n    \"VLLM_ROCM_FP8_PADDING\":\n    lambda: bool(int(os.getenv(\"VLLM_ROCM_FP8_PADDING\", \"1\"))),\n\n    # Pad the weights for the moe kernel\n    \"VLLM_ROCM_MOE_PADDING\":\n    lambda: bool(int(os.getenv(\"VLLM_ROCM_MOE_PADDING\", \"1\"))),\n\n    # custom paged attention kernel for MI3* cards\n    \"VLLM_ROCM_CUSTOM_PAGED_ATTN\":\n    lambda: (os.getenv(\"VLLM_ROCM_CUSTOM_PAGED_ATTN\", \"True\").lower() in\n             (\"true\", \"1\")),\n\n    # If set, when running in Quark emulation mode, do not dequantize the\n    # weights at load time. Instead, dequantize weights on-the-fly during\n    # kernel execution.\n    # This allows running larger models at the cost of slower inference.\n    # This flag has no effect when not running in Quark emulation mode.\n    \"VLLM_QUARK_EMU_MEM_OPT\":\n    lambda: bool(int(os.getenv(\"VLLM_QUARK_EMU_MEM_OPT\", \"0\"))),\n\n    # Divisor for dynamic query scale factor calculation for FP8 KV Cache\n    \"Q_SCALE_CONSTANT\":\n    lambda: int(os.getenv(\"Q_SCALE_CONSTANT\", \"200\")),\n    # Divisor for dynamic key scale factor calculation for FP8 KV Cache\n    \"K_SCALE_CONSTANT\":\n    lambda: int(os.getenv(\"K_SCALE_CONSTANT\", \"200\")),\n    # Divisor for dynamic value scale factor calculation for FP8 KV Cache\n    \"V_SCALE_CONSTANT\":\n    lambda: int(os.getenv(\"V_SCALE_CONSTANT\", \"100\")),\n\n    # If set, enable multiprocessing in LLM for the V1 code path.\n    \"VLLM_ENABLE_V1_MULTIPROCESSING\":\n    lambda: bool(int(os.getenv(\"VLLM_ENABLE_V1_MULTIPROCESSING\", \"1\"))),\n    \"VLLM_LOG_BATCHSIZE_INTERVAL\":\n    lambda: float(os.getenv(\"VLLM_LOG_BATCHSIZE_INTERVAL\", \"-1\")),\n    \"VLLM_DISABLE_COMPILE_CACHE\":\n    lambda: bool(int(os.getenv(\"VLLM_DISABLE_COMPILE_CACHE\", \"0\"))),\n\n    # If set, vllm will run in development mode, which will enable\n    # some additional endpoints for developing and debugging,\n    # e.g. `/reset_prefix_cache`\n    \"VLLM_SERVER_DEV_MODE\":\n    lambda: bool(int(os.getenv(\"VLLM_SERVER_DEV_MODE\", \"0\"))),\n\n    # Controls the maximum number of requests to handle in a\n    # single asyncio task when processing per-token outputs in the\n    # V1 AsyncLLM interface. It is applicable when handling a high\n    # concurrency of streaming requests.\n    # Setting this too high can result in a higher variance of\n    # inter-message latencies. Setting it too low can negatively impact\n    # TTFT and overall throughput.\n    \"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\":\n    lambda: int(os.getenv(\"VLLM_V1_OUTPUT_PROC_CHUNK_SIZE\", \"128\")),\n\n    # If set, vLLM will disable the MLA attention optimizations.\n    \"VLLM_MLA_DISABLE\":\n    lambda: bool(int(os.getenv(\"VLLM_MLA_DISABLE\", \"0\"))),\n\n    # If set, vLLM will use the Triton implementation of moe_align_block_size,\n    # i.e. moe_align_block_size_triton in fused_moe.py.\n    \"VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON\":\n    lambda: bool(int(os.getenv(\"VLLM_ENABLE_MOE_ALIGN_BLOCK_SIZE_TRITON\", \"0\"))\n                 ),\n\n    # Number of GPUs per worker in Ray, if it is set to be a fraction,\n    # it allows ray to schedule multiple actors on a single GPU,\n    # so that users can colocate other actors on the same GPUs as vLLM.\n    \"VLLM_RAY_PER_WORKER_GPUS\":\n    lambda: float(os.getenv(\"VLLM_RAY_PER_WORKER_GPUS\", \"1.0\")),\n\n    # Bundle indices for Ray, if it is set, it can control precisely\n    # which indices are used for the Ray bundle, for every worker.\n    # Format: comma-separated list of integers, e.g. \"0,1,2,3\"\n    \"VLLM_RAY_BUNDLE_INDICES\":\n    lambda: os.getenv(\"VLLM_RAY_BUNDLE_INDICES\", \"\"),\n\n    # In some system, find_loaded_library() may not work. So we allow users to\n    # specify the path through environment variable VLLM_CUDART_SO_PATH.\n    \"VLLM_CUDART_SO_PATH\":\n    lambda: os.getenv(\"VLLM_CUDART_SO_PATH\", None),\n\n    # Contiguous cache fetching to avoid using costly gather operation on\n    # Gaudi3. This is only applicable to HPU contiguous cache. If set to true,\n    # contiguous cache fetch will be used.\n    \"VLLM_USE_HPU_CONTIGUOUS_CACHE_FETCH\":\n    lambda: os.environ.get(\"VLLM_CONTIGUOUS_PA\", \"true\").lower() in\n    (\"1\", \"true\"),\n\n    # Use delayed sampling for HPU to reduce host cpu overhead\n    # between each step.\n    \"VLLM_HPU_USE_DELAYED_SAMPLING\":\n    lambda: os.environ.get(\"VLLM_DELAYED_SAMPLING\", \"false\").lower() in\n    (\"1\", \"true\"),\n\n    # Rank of the process in the data parallel setting\n    \"VLLM_DP_RANK\":\n    lambda: int(os.getenv(\"VLLM_DP_RANK\", \"0\")),\n\n    # Rank of the process in the data parallel setting.\n    # Defaults to VLLM_DP_RANK when not set.\n    \"VLLM_DP_RANK_LOCAL\":\n    lambda: int(\n        os.getenv(\"VLLM_DP_RANK_LOCAL\", sys.modules[__name__].VLLM_DP_RANK)),\n\n    # World size of the data parallel setting\n    \"VLLM_DP_SIZE\":\n    lambda: int(os.getenv(\"VLLM_DP_SIZE\", \"1\")),\n\n    # IP address of the master node in the data parallel setting\n    \"VLLM_DP_MASTER_IP\":\n    lambda: os.getenv(\"VLLM_DP_MASTER_IP\", \"127.0.0.1\"),\n\n    # Port of the master node in the data parallel setting\n    \"VLLM_DP_MASTER_PORT\":\n    lambda: int(os.getenv(\"VLLM_DP_MASTER_PORT\", \"0\")),\n\n    # Whether to use S3 path for model loading in CI via RunAI Streamer\n    \"VLLM_CI_USE_S3\":\n    lambda: os.environ.get(\"VLLM_CI_USE_S3\", \"0\") == \"1\",\n\n    # Use model_redirect to redirect the model name to a local folder.\n    # `model_redirect` can be a json file mapping the model between\n    # repo_id and local folder:\n    # {\"meta-llama/Llama-3.2-1B\": \"/tmp/Llama-3.2-1B\"}\n    # or a space separated values table file:\n    # meta-llama/Llama-3.2-1B   /tmp/Llama-3.2-1B\n    \"VLLM_MODEL_REDIRECT_PATH\":\n    lambda: os.environ.get(\"VLLM_MODEL_REDIRECT_PATH\", None),\n\n    # Whether to use atomicAdd reduce in gptq/awq marlin kernel.\n    \"VLLM_MARLIN_USE_ATOMIC_ADD\":\n    lambda: os.environ.get(\"VLLM_MARLIN_USE_ATOMIC_ADD\", \"0\") == \"1\",\n\n    # Whether to turn on the outlines cache for V0\n    # This cache is unbounded and on disk, so it's not safe to use in\n    # an environment with potentially malicious users.\n    \"VLLM_V0_USE_OUTLINES_CACHE\":\n    lambda: os.environ.get(\"VLLM_V0_USE_OUTLINES_CACHE\", \"0\") == \"1\",\n\n    # Gap between padding buckets for the forward pass. So we have\n    # 8, we will run forward pass with [16, 24, 32, ...].\n    \"VLLM_TPU_BUCKET_PADDING_GAP\":\n    lambda: int(os.environ[\"VLLM_TPU_BUCKET_PADDING_GAP\"])\n    if \"VLLM_TPU_BUCKET_PADDING_GAP\" in os.environ else 0,\n\n    # Allow use of DeepGemm kernels for fused moe ops.\n    \"VLLM_USE_DEEP_GEMM\":\n    lambda: bool(int(os.getenv(\"VLLM_USE_DEEP_GEMM\", \"0\"))),\n\n    # Control the cache sized used by the xgrammar compiler. The default\n    # of 512 MB should be enough for roughly 1000 JSON schemas.\n    # It can be changed with this variable if needed for some reason.\n    \"VLLM_XGRAMMAR_CACHE_MB\":\n    lambda: int(os.getenv(\"VLLM_XGRAMMAR_CACHE_MB\", \"512\")),\n\n    # Control the threshold for msgspec to use 'zero copy' for\n    # serialization/deserialization of tensors. Tensors below\n    # this limit will be encoded into the msgpack buffer, and\n    # tensors above will instead be sent via a separate message.\n    # While the sending side still actually copies the tensor\n    # in all cases, on the receiving side, tensors above this\n    # limit will actually be zero-copy decoded.\n    \"VLLM_MSGPACK_ZERO_COPY_THRESHOLD\":\n    lambda: int(os.getenv(\"VLLM_MSGPACK_ZERO_COPY_THRESHOLD\", \"256\")),\n\n    # If set, allow insecure serialization using pickle.\n    # This is useful for environments where it is deemed safe to use the\n    # insecure method and it is needed for some reason.\n    \"VLLM_ALLOW_INSECURE_SERIALIZATION\":\n    lambda: bool(int(os.getenv(\"VLLM_ALLOW_INSECURE_SERIALIZATION\", \"0\"))),\n\n    # IP address used for NIXL handshake between remote agents.\n    \"VLLM_NIXL_SIDE_CHANNEL_HOST\":\n    lambda: os.getenv(\"VLLM_NIXL_SIDE_CHANNEL_HOST\", \"localhost\"),\n\n    # Port used for NIXL handshake between remote agents.\n    \"VLLM_NIXL_SIDE_CHANNEL_PORT\":\n    lambda: int(os.getenv(\"VLLM_NIXL_SIDE_CHANNEL_PORT\", \"5557\")),\n\n    # all2all backend for vllm's expert parallel communication\n    \"VLLM_ALL2ALL_BACKEND\":\n    lambda: os.getenv(\"VLLM_ALL2ALL_BACKEND\", \"naive\"),\n}\n\n# end-env-vars-definition\n\n\ndef __getattr__(name: str):\n    # lazy evaluation of environment variables\n    if name in environment_variables:\n        return environment_variables[name]()\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef __dir__():\n    return list(environment_variables.keys())\n\n\ndef is_set(name: str):\n    \"\"\"Check if an environment variable is explicitly set.\"\"\"\n    if name in environment_variables:\n        return name in os.environ\n    raise AttributeError(f\"module {__name__!r} has no attribute {name!r}\")\n\n\ndef set_vllm_use_v1(use_v1: bool):\n    if is_set(\"VLLM_USE_V1\"):\n        raise ValueError(\n            \"Should not call set_vllm_use_v1() if VLLM_USE_V1 is set \"\n            \"explicitly by the user. Please raise this as a Github \"\n            \"Issue and explicitly set VLLM_USE_V1=0 or 1.\")\n    os.environ[\"VLLM_USE_V1\"] = \"1\" if use_v1 else \"0\"\n\n\ndef compute_hash() -> str:\n    \"\"\"\n    WARNING: Whenever a new key is added to this environment\n    variables, ensure that it is included in the factors list if\n    it affects the computation graph. For example, different values\n    of VLLM_PP_LAYER_PARTITION will generate different computation\n    graphs, so it is included in the factors list. The env vars that\n    affect the choice of different kernels or attention backends should\n    also be included in the factors list.\n    \"\"\"\n    factors: list[Any] = []\n\n    # summarize environment variables\n    def factorize(name: str):\n        if __getattr__(name):\n            factors.append(__getattr__(name))\n        else:\n            factors.append(\"None\")\n\n    # The values of envs may affects the computation graph.\n    # TODO(DefTruth): hash all environment variables?\n    # for key in environment_variables:\n    #     factorize(key)\n    environment_variables_to_hash = [\n        \"VLLM_PP_LAYER_PARTITION\",\n        \"VLLM_MLA_DISABLE\",\n        \"VLLM_USE_TRITON_FLASH_ATTN\",\n        \"VLLM_USE_TRITON_AWQ\",\n        \"VLLM_DP_RANK\",\n        \"VLLM_DP_SIZE\",\n        \"VLLM_TEST_STANDALONE_COMPILE\",\n    ]\n    for key in environment_variables_to_hash:\n        if key in environment_variables:\n            factorize(key)\n\n    hash_str = hashlib.md5(str(factors).encode(),\n                           usedforsecurity=False).hexdigest()\n\n    return hash_str\n", 885], "/home/jeromeku/vllm/vllm/engine/arg_utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\n# yapf: disable\nimport argparse\nimport dataclasses\nimport json\nimport re\nimport sys\nimport threading\nimport warnings\nfrom dataclasses import MISSING, dataclass, fields, is_dataclass\nfrom itertools import permutations\nfrom typing import (Annotated, Any, Callable, Dict, List, Literal, Optional,\n                    Type, TypeVar, Union, cast, get_args, get_origin)\n\nimport torch\nfrom typing_extensions import TypeIs, deprecated\n\nimport vllm.envs as envs\nfrom vllm.config import (BlockSize, CacheConfig, CacheDType, CompilationConfig,\n                         ConfigFormat, ConfigType, DecodingConfig,\n                         DetailedTraceModules, Device, DeviceConfig,\n                         DistributedExecutorBackend, GuidedDecodingBackend,\n                         GuidedDecodingBackendV1, HfOverrides, KVEventsConfig,\n                         KVTransferConfig, LoadConfig, LoadFormat, LoRAConfig,\n                         ModelConfig, ModelDType, ModelImpl, MultiModalConfig,\n                         ObservabilityConfig, ParallelConfig, PoolerConfig,\n                         PrefixCachingHashAlgo, PromptAdapterConfig,\n                         SchedulerConfig, SchedulerPolicy, SpeculativeConfig,\n                         TaskOption, TokenizerMode, TokenizerPoolConfig,\n                         VllmConfig, get_attr_docs, get_field)\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import QuantizationMethods\nfrom vllm.plugins import load_general_plugins\nfrom vllm.reasoning import ReasoningParserManager\nfrom vllm.test_utils import MODEL_WEIGHTS_S3_BUCKET, MODELS_ON_S3\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import (STR_DUAL_CHUNK_FLASH_ATTN_VAL, FlexibleArgumentParser,\n                        GiB_bytes, is_in_doc_build, is_in_ray_actor)\n\n# yapf: enable\n\nlogger = init_logger(__name__)\n\n# object is used to allow for special typing forms\nT = TypeVar(\"T\")\nTypeHint = Union[type[Any], object]\nTypeHintT = Union[type[T], object]\n\n\ndef parse_type(return_type: Callable[[str], T]) -> Callable[[str], T]:\n\n    def _parse_type(val: str) -> T:\n        try:\n            if return_type is json.loads and not re.match(\"^{.*}$\", val):\n                return cast(T, nullable_kvs(val))\n            return return_type(val)\n        except ValueError as e:\n            raise argparse.ArgumentTypeError(\n                f\"Value {val} cannot be converted to {return_type}.\") from e\n\n    return _parse_type\n\n\ndef optional_type(\n        return_type: Callable[[str], T]) -> Callable[[str], Optional[T]]:\n\n    def _optional_type(val: str) -> Optional[T]:\n        if val == \"\" or val == \"None\":\n            return None\n        return parse_type(return_type)(val)\n\n    return _optional_type\n\n\ndef union_dict_and_str(val: str) -> Optional[Union[str, dict[str, str]]]:\n    if not re.match(\"^{.*}$\", val):\n        return str(val)\n    return optional_type(json.loads)(val)\n\n\n@deprecated(\n    \"Passing a JSON argument as a string containing comma separated key=value \"\n    \"pairs is deprecated. This will be removed in v0.10.0. Please use a JSON \"\n    \"string instead.\")\ndef nullable_kvs(val: str) -> dict[str, int]:\n    \"\"\"Parses a string containing comma separate key [str] to value [int]\n    pairs into a dictionary.\n\n    Args:\n        val: String value to be parsed.\n\n    Returns:\n        Dictionary with parsed values.\n    \"\"\"\n    out_dict: dict[str, int] = {}\n    for item in val.split(\",\"):\n        kv_parts = [part.lower().strip() for part in item.split(\"=\")]\n        if len(kv_parts) != 2:\n            raise argparse.ArgumentTypeError(\n                \"Each item should be in the form KEY=VALUE\")\n        key, value = kv_parts\n\n        try:\n            parsed_value = int(value)\n        except ValueError as exc:\n            msg = f\"Failed to parse value of item {key}={value}\"\n            raise argparse.ArgumentTypeError(msg) from exc\n\n        if key in out_dict and out_dict[key] != parsed_value:\n            raise argparse.ArgumentTypeError(\n                f\"Conflicting values specified for key: {key}\")\n        out_dict[key] = parsed_value\n\n    return out_dict\n\n\ndef is_type(type_hint: TypeHint, type: TypeHintT) -> TypeIs[TypeHintT]:\n    \"\"\"Check if the type hint is a specific type.\"\"\"\n    return type_hint is type or get_origin(type_hint) is type\n\n\ndef contains_type(type_hints: set[TypeHint], type: TypeHintT) -> bool:\n    \"\"\"Check if the type hints contain a specific type.\"\"\"\n    return any(is_type(type_hint, type) for type_hint in type_hints)\n\n\ndef get_type(type_hints: set[TypeHint], type: TypeHintT) -> TypeHintT:\n    \"\"\"Get the specific type from the type hints.\"\"\"\n    return next((th for th in type_hints if is_type(th, type)), None)\n\n\ndef literal_to_kwargs(type_hints: set[TypeHint]) -> dict[str, Any]:\n    \"\"\"Convert Literal type hints to argparse kwargs.\"\"\"\n    type_hint = get_type(type_hints, Literal)\n    choices = get_args(type_hint)\n    choice_type = type(choices[0])\n    if not all(isinstance(choice, choice_type) for choice in choices):\n        raise ValueError(\n            \"All choices must be of the same type. \"\n            f\"Got {choices} with types {[type(c) for c in choices]}\")\n    return {\"type\": choice_type, \"choices\": sorted(choices)}\n\n\ndef is_not_builtin(type_hint: TypeHint) -> bool:\n    \"\"\"Check if the class is not a built-in type.\"\"\"\n    return type_hint.__module__ != \"builtins\"\n\n\ndef get_kwargs(cls: ConfigType) -> dict[str, Any]:\n    cls_docs = get_attr_docs(cls)\n    kwargs = {}\n    for field in fields(cls):\n        # Get the set of possible types for the field\n        type_hints: set[TypeHint] = set()\n        if get_origin(field.type) in {Union, Annotated}:\n            type_hints.update(get_args(field.type))\n        else:\n            type_hints.add(field.type)\n\n        # If the field is a dataclass, we can use the model_validate_json\n        generator = (th for th in type_hints if is_dataclass(th))\n        dataclass_cls = next(generator, None)\n\n        # Get the default value of the field\n        if field.default is not MISSING:\n            default = field.default\n        elif field.default_factory is not MISSING:\n            if is_dataclass(field.default_factory) and is_in_doc_build():\n                default = {}\n            else:\n                default = field.default_factory()\n\n        # Get the help text for the field\n        name = field.name\n        help = cls_docs[name].strip()\n        # Escape % for argparse\n        help = help.replace(\"%\", \"%%\")\n\n        # Initialise the kwargs dictionary for the field\n        kwargs[name] = {\"default\": default, \"help\": help}\n\n        # Set other kwargs based on the type hints\n        json_tip = \"\"\"\\n\\nShould either be a valid JSON string or JSON keys\n        passed individually. For example, the following sets of arguments are\n        equivalent:\\n\\n\n        - `--json-arg '{\"key1\": \"value1\", \"key2\": {\"key3\": \"value2\"}}'`\\n\n        - `--json-arg.key1 value1 --json-arg.key2.key3 value2`\\n\\n\"\"\"\n        if dataclass_cls is not None:\n            dataclass_init = lambda x, f=dataclass_cls: f(**json.loads(x))\n            # Special case for configs with a from_cli method\n            if hasattr(dataclass_cls, \"from_cli\"):\n                from_cli = dataclass_cls.from_cli\n                dataclass_init = lambda x, f=from_cli: f(x)\n            kwargs[name][\"type\"] = dataclass_init\n            kwargs[name][\"help\"] += json_tip\n        elif contains_type(type_hints, bool):\n            # Creates --no-<name> and --<name> flags\n            kwargs[name][\"action\"] = argparse.BooleanOptionalAction\n        elif contains_type(type_hints, Literal):\n            kwargs[name].update(literal_to_kwargs(type_hints))\n        elif contains_type(type_hints, tuple):\n            type_hint = get_type(type_hints, tuple)\n            types = get_args(type_hint)\n            tuple_type = types[0]\n            assert all(t is tuple_type for t in types if t is not Ellipsis), (\n                \"All non-Ellipsis tuple elements must be of the same \"\n                f\"type. Got {types}.\")\n            kwargs[name][\"type\"] = tuple_type\n            kwargs[name][\"nargs\"] = \"+\" if Ellipsis in types else len(types)\n        elif contains_type(type_hints, list):\n            type_hint = get_type(type_hints, list)\n            types = get_args(type_hint)\n            assert len(types) == 1, (\n                \"List type must have exactly one type. Got \"\n                f\"{type_hint} with types {types}\")\n            kwargs[name][\"type\"] = types[0]\n            kwargs[name][\"nargs\"] = \"+\"\n        elif contains_type(type_hints, int):\n            kwargs[name][\"type\"] = int\n            # Special case for large integers\n            if name in {\"max_model_len\"}:\n                kwargs[name][\"type\"] = human_readable_int\n        elif contains_type(type_hints, float):\n            kwargs[name][\"type\"] = float\n        elif contains_type(type_hints,\n                           dict) and (contains_type(type_hints, str) or any(\n                               is_not_builtin(th) for th in type_hints)):\n            kwargs[name][\"type\"] = union_dict_and_str\n        elif contains_type(type_hints, dict):\n            # Dict arguments will always be optional\n            kwargs[name][\"type\"] = parse_type(json.loads)\n            kwargs[name][\"help\"] += json_tip\n        elif (contains_type(type_hints, str)\n              or any(is_not_builtin(th) for th in type_hints)):\n            kwargs[name][\"type\"] = str\n        else:\n            raise ValueError(\n                f\"Unsupported type {type_hints} for argument {name}.\")\n\n        # If the type hint was a sequence of literals, use the helper function\n        # to update the type and choices\n        if get_origin(kwargs[name].get(\"type\")) is Literal:\n            kwargs[name].update(literal_to_kwargs({kwargs[name][\"type\"]}))\n\n        # If None is in type_hints, make the argument optional.\n        # But not if it's a bool, argparse will handle this better.\n        if type(None) in type_hints and not contains_type(type_hints, bool):\n            kwargs[name][\"type\"] = optional_type(kwargs[name][\"type\"])\n            if kwargs[name].get(\"choices\"):\n                kwargs[name][\"choices\"].append(\"None\")\n    return kwargs\n\n\n@dataclass\nclass EngineArgs:\n    \"\"\"Arguments for vLLM engine.\"\"\"\n    model: str = ModelConfig.model\n    served_model_name: Optional[Union[\n        str, List[str]]] = ModelConfig.served_model_name\n    tokenizer: Optional[str] = ModelConfig.tokenizer\n    hf_config_path: Optional[str] = ModelConfig.hf_config_path\n    task: TaskOption = ModelConfig.task\n    skip_tokenizer_init: bool = ModelConfig.skip_tokenizer_init\n    enable_prompt_embeds: bool = ModelConfig.enable_prompt_embeds\n    tokenizer_mode: TokenizerMode = ModelConfig.tokenizer_mode\n    trust_remote_code: bool = ModelConfig.trust_remote_code\n    allowed_local_media_path: str = ModelConfig.allowed_local_media_path\n    download_dir: Optional[str] = LoadConfig.download_dir\n    load_format: str = LoadConfig.load_format\n    config_format: str = ModelConfig.config_format\n    dtype: ModelDType = ModelConfig.dtype\n    kv_cache_dtype: CacheDType = CacheConfig.cache_dtype\n    seed: Optional[int] = ModelConfig.seed\n    max_model_len: Optional[int] = ModelConfig.max_model_len\n    cuda_graph_sizes: list[int] = get_field(SchedulerConfig,\n                                            \"cuda_graph_sizes\")\n    # Note: Specifying a custom executor backend by passing a class\n    # is intended for expert use only. The API may change without\n    # notice.\n    distributed_executor_backend: Optional[Union[\n        DistributedExecutorBackend,\n        Type[ExecutorBase]]] = ParallelConfig.distributed_executor_backend\n    # number of P/D disaggregation (or other disaggregation) workers\n    pipeline_parallel_size: int = ParallelConfig.pipeline_parallel_size\n    tensor_parallel_size: int = ParallelConfig.tensor_parallel_size\n    data_parallel_size: int = ParallelConfig.data_parallel_size\n    data_parallel_size_local: Optional[int] = None\n    data_parallel_address: Optional[str] = None\n    data_parallel_rpc_port: Optional[int] = None\n    enable_expert_parallel: bool = ParallelConfig.enable_expert_parallel\n    max_parallel_loading_workers: Optional[\n        int] = ParallelConfig.max_parallel_loading_workers\n    block_size: Optional[BlockSize] = CacheConfig.block_size\n    enable_prefix_caching: Optional[bool] = CacheConfig.enable_prefix_caching\n    prefix_caching_hash_algo: PrefixCachingHashAlgo = \\\n        CacheConfig.prefix_caching_hash_algo\n    disable_sliding_window: bool = ModelConfig.disable_sliding_window\n    disable_cascade_attn: bool = ModelConfig.disable_cascade_attn\n    use_v2_block_manager: bool = True\n    swap_space: float = CacheConfig.swap_space\n    cpu_offload_gb: float = CacheConfig.cpu_offload_gb\n    gpu_memory_utilization: float = CacheConfig.gpu_memory_utilization\n    max_num_batched_tokens: Optional[\n        int] = SchedulerConfig.max_num_batched_tokens\n    max_num_partial_prefills: int = SchedulerConfig.max_num_partial_prefills\n    max_long_partial_prefills: int = SchedulerConfig.max_long_partial_prefills\n    long_prefill_token_threshold: int = \\\n        SchedulerConfig.long_prefill_token_threshold\n    max_num_seqs: Optional[int] = SchedulerConfig.max_num_seqs\n    max_logprobs: int = ModelConfig.max_logprobs\n    disable_log_stats: bool = False\n    revision: Optional[str] = ModelConfig.revision\n    code_revision: Optional[str] = ModelConfig.code_revision\n    rope_scaling: dict[str, Any] = get_field(ModelConfig, \"rope_scaling\")\n    rope_theta: Optional[float] = ModelConfig.rope_theta\n    hf_token: Optional[Union[bool, str]] = ModelConfig.hf_token\n    hf_overrides: Optional[HfOverrides] = \\\n        get_field(ModelConfig, \"hf_overrides\")\n    tokenizer_revision: Optional[str] = ModelConfig.tokenizer_revision\n    quantization: Optional[QuantizationMethods] = ModelConfig.quantization\n    enforce_eager: bool = ModelConfig.enforce_eager\n    max_seq_len_to_capture: int = ModelConfig.max_seq_len_to_capture\n    disable_custom_all_reduce: bool = ParallelConfig.disable_custom_all_reduce\n    # The following three fields are deprecated and will be removed in a future\n    # release. Setting them will have no effect. Please remove them from your\n    # configurations.\n    tokenizer_pool_size: int = TokenizerPoolConfig.pool_size\n    tokenizer_pool_type: str = TokenizerPoolConfig.pool_type\n    tokenizer_pool_extra_config: dict = \\\n        get_field(TokenizerPoolConfig, \"extra_config\")\n    limit_mm_per_prompt: dict[str, int] = \\\n        get_field(MultiModalConfig, \"limit_per_prompt\")\n    mm_processor_kwargs: Optional[Dict[str, Any]] = \\\n        MultiModalConfig.mm_processor_kwargs\n    disable_mm_preprocessor_cache: bool = \\\n        MultiModalConfig.disable_mm_preprocessor_cache\n    # LoRA fields\n    enable_lora: bool = False\n    enable_lora_bias: bool = LoRAConfig.bias_enabled\n    max_loras: int = LoRAConfig.max_loras\n    max_lora_rank: int = LoRAConfig.max_lora_rank\n    fully_sharded_loras: bool = LoRAConfig.fully_sharded_loras\n    max_cpu_loras: Optional[int] = LoRAConfig.max_cpu_loras\n    lora_dtype: Optional[Union[str, torch.dtype]] = LoRAConfig.lora_dtype\n    lora_extra_vocab_size: int = LoRAConfig.lora_extra_vocab_size\n    long_lora_scaling_factors: Optional[tuple[float, ...]] = \\\n        LoRAConfig.long_lora_scaling_factors\n    # PromptAdapter fields\n    enable_prompt_adapter: bool = False\n    max_prompt_adapters: int = PromptAdapterConfig.max_prompt_adapters\n    max_prompt_adapter_token: int = \\\n        PromptAdapterConfig.max_prompt_adapter_token\n\n    device: Device = DeviceConfig.device\n    num_scheduler_steps: int = SchedulerConfig.num_scheduler_steps\n    multi_step_stream_outputs: bool = SchedulerConfig.multi_step_stream_outputs\n    ray_workers_use_nsight: bool = ParallelConfig.ray_workers_use_nsight\n    num_gpu_blocks_override: Optional[\n        int] = CacheConfig.num_gpu_blocks_override\n    num_lookahead_slots: int = SchedulerConfig.num_lookahead_slots\n    model_loader_extra_config: dict = \\\n        get_field(LoadConfig, \"model_loader_extra_config\")\n    ignore_patterns: Optional[Union[str,\n                                    List[str]]] = LoadConfig.ignore_patterns\n    preemption_mode: Optional[str] = SchedulerConfig.preemption_mode\n\n    scheduler_delay_factor: float = SchedulerConfig.delay_factor\n    enable_chunked_prefill: Optional[\n        bool] = SchedulerConfig.enable_chunked_prefill\n    disable_chunked_mm_input: bool = SchedulerConfig.disable_chunked_mm_input\n\n    guided_decoding_backend: GuidedDecodingBackend = DecodingConfig.backend\n    guided_decoding_disable_fallback: bool = DecodingConfig.disable_fallback\n    guided_decoding_disable_any_whitespace: bool = \\\n        DecodingConfig.disable_any_whitespace\n    guided_decoding_disable_additional_properties: bool = \\\n        DecodingConfig.disable_additional_properties\n    logits_processor_pattern: Optional[\n        str] = ModelConfig.logits_processor_pattern\n\n    speculative_config: Optional[Dict[str, Any]] = None\n\n    qlora_adapter_name_or_path: Optional[str] = None\n    show_hidden_metrics_for_version: Optional[str] = \\\n        ObservabilityConfig.show_hidden_metrics_for_version\n    otlp_traces_endpoint: Optional[str] = \\\n        ObservabilityConfig.otlp_traces_endpoint\n    collect_detailed_traces: Optional[list[DetailedTraceModules]] = \\\n        ObservabilityConfig.collect_detailed_traces\n    disable_async_output_proc: bool = not ModelConfig.use_async_output_proc\n    scheduling_policy: SchedulerPolicy = SchedulerConfig.policy\n    scheduler_cls: Union[str, Type[object]] = SchedulerConfig.scheduler_cls\n\n    override_neuron_config: dict[str, Any] = \\\n        get_field(ModelConfig, \"override_neuron_config\")\n    override_pooler_config: Optional[Union[dict, PoolerConfig]] = \\\n        ModelConfig.override_pooler_config\n    compilation_config: Optional[CompilationConfig] = None\n    worker_cls: str = ParallelConfig.worker_cls\n    worker_extension_cls: str = ParallelConfig.worker_extension_cls\n\n    kv_transfer_config: Optional[KVTransferConfig] = None\n    kv_events_config: Optional[KVEventsConfig] = None\n\n    generation_config: str = ModelConfig.generation_config\n    enable_sleep_mode: bool = ModelConfig.enable_sleep_mode\n    override_generation_config: dict[str, Any] = \\\n        get_field(ModelConfig, \"override_generation_config\")\n    model_impl: str = ModelConfig.model_impl\n\n    calculate_kv_scales: bool = CacheConfig.calculate_kv_scales\n\n    additional_config: Optional[Dict[str, Any]] = None\n    enable_reasoning: Optional[bool] = None  # DEPRECATED\n    reasoning_parser: str = DecodingConfig.reasoning_backend\n\n    use_tqdm_on_load: bool = LoadConfig.use_tqdm_on_load\n    pt_load_map_location: str = LoadConfig.pt_load_map_location\n\n    def __post_init__(self):\n        # support `EngineArgs(compilation_config={...})`\n        # without having to manually construct a\n        # CompilationConfig object\n        if isinstance(self.compilation_config, (int, dict)):\n            self.compilation_config = CompilationConfig.from_cli(\n                str(self.compilation_config))\n        if self.qlora_adapter_name_or_path is not None:\n            warnings.warn(\n                \"The `qlora_adapter_name_or_path` is deprecated \"\n                \"and will be removed in v0.10.0. \",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n        # Setup plugins\n        from vllm.plugins import load_general_plugins\n        load_general_plugins()\n\n    @staticmethod\n    def add_cli_args(parser: FlexibleArgumentParser) -> FlexibleArgumentParser:\n        \"\"\"Shared CLI arguments for vLLM engine.\"\"\"\n\n        # Model arguments\n        model_kwargs = get_kwargs(ModelConfig)\n        model_group = parser.add_argument_group(\n            title=\"ModelConfig\",\n            description=ModelConfig.__doc__,\n        )\n        if 'serve' not in sys.argv[1:] and '--help' not in sys.argv[1:]:\n            model_group.add_argument(\"--model\", **model_kwargs[\"model\"])\n        model_group.add_argument(\"--task\", **model_kwargs[\"task\"])\n        model_group.add_argument(\"--tokenizer\", **model_kwargs[\"tokenizer\"])\n        model_group.add_argument(\"--tokenizer-mode\",\n                                 **model_kwargs[\"tokenizer_mode\"])\n        model_group.add_argument(\"--trust-remote-code\",\n                                 **model_kwargs[\"trust_remote_code\"])\n        model_group.add_argument(\"--dtype\", **model_kwargs[\"dtype\"])\n        model_group.add_argument(\"--seed\", **model_kwargs[\"seed\"])\n        model_group.add_argument(\"--hf-config-path\",\n                                 **model_kwargs[\"hf_config_path\"])\n        model_group.add_argument(\"--allowed-local-media-path\",\n                                 **model_kwargs[\"allowed_local_media_path\"])\n        model_group.add_argument(\"--revision\", **model_kwargs[\"revision\"])\n        model_group.add_argument(\"--code-revision\",\n                                 **model_kwargs[\"code_revision\"])\n        model_group.add_argument(\"--rope-scaling\",\n                                 **model_kwargs[\"rope_scaling\"])\n        model_group.add_argument(\"--rope-theta\", **model_kwargs[\"rope_theta\"])\n        model_group.add_argument(\"--tokenizer-revision\",\n                                 **model_kwargs[\"tokenizer_revision\"])\n        model_group.add_argument(\"--max-model-len\",\n                                 **model_kwargs[\"max_model_len\"])\n        model_group.add_argument(\"--quantization\", \"-q\",\n                                 **model_kwargs[\"quantization\"])\n        model_group.add_argument(\"--enforce-eager\",\n                                 **model_kwargs[\"enforce_eager\"])\n        model_group.add_argument(\"--max-seq-len-to-capture\",\n                                 **model_kwargs[\"max_seq_len_to_capture\"])\n        model_group.add_argument(\"--max-logprobs\",\n                                 **model_kwargs[\"max_logprobs\"])\n        model_group.add_argument(\"--disable-sliding-window\",\n                                 **model_kwargs[\"disable_sliding_window\"])\n        model_group.add_argument(\"--disable-cascade-attn\",\n                                 **model_kwargs[\"disable_cascade_attn\"])\n        model_group.add_argument(\"--skip-tokenizer-init\",\n                                 **model_kwargs[\"skip_tokenizer_init\"])\n        model_group.add_argument(\"--enable-prompt-embeds\",\n                                 **model_kwargs[\"enable_prompt_embeds\"])\n        model_group.add_argument(\"--served-model-name\",\n                                 **model_kwargs[\"served_model_name\"])\n        # This one is a special case because it is the\n        # opposite of ModelConfig.use_async_output_proc\n        model_group.add_argument(\n            \"--disable-async-output-proc\",\n            action=\"store_true\",\n            default=EngineArgs.disable_async_output_proc,\n            help=\"Disable async output processing. This may result in \"\n            \"lower performance.\")\n        model_group.add_argument(\"--config-format\",\n                                 choices=[f.value for f in ConfigFormat],\n                                 **model_kwargs[\"config_format\"])\n        # This one is a special case because it can bool\n        # or str. TODO: Handle this in get_kwargs\n        model_group.add_argument(\"--hf-token\",\n                                 type=str,\n                                 nargs=\"?\",\n                                 const=True,\n                                 default=model_kwargs[\"hf_token\"][\"default\"],\n                                 help=model_kwargs[\"hf_token\"][\"help\"])\n        model_group.add_argument(\"--hf-overrides\",\n                                 **model_kwargs[\"hf_overrides\"])\n        model_group.add_argument(\"--override-neuron-config\",\n                                 **model_kwargs[\"override_neuron_config\"])\n        model_group.add_argument(\"--override-pooler-config\",\n                                 **model_kwargs[\"override_pooler_config\"])\n        model_group.add_argument(\"--logits-processor-pattern\",\n                                 **model_kwargs[\"logits_processor_pattern\"])\n        model_group.add_argument(\"--generation-config\",\n                                 **model_kwargs[\"generation_config\"])\n        model_group.add_argument(\"--override-generation-config\",\n                                 **model_kwargs[\"override_generation_config\"])\n        model_group.add_argument(\"--enable-sleep-mode\",\n                                 **model_kwargs[\"enable_sleep_mode\"])\n        model_group.add_argument(\"--model-impl\",\n                                 choices=[f.value for f in ModelImpl],\n                                 **model_kwargs[\"model_impl\"])\n\n        # Model loading arguments\n        load_kwargs = get_kwargs(LoadConfig)\n        load_group = parser.add_argument_group(\n            title=\"LoadConfig\",\n            description=LoadConfig.__doc__,\n        )\n        load_group.add_argument(\"--load-format\",\n                                choices=[f.value for f in LoadFormat],\n                                **load_kwargs[\"load_format\"])\n        load_group.add_argument(\"--download-dir\",\n                                **load_kwargs[\"download_dir\"])\n        load_group.add_argument(\"--model-loader-extra-config\",\n                                **load_kwargs[\"model_loader_extra_config\"])\n        load_group.add_argument(\"--ignore-patterns\",\n                                **load_kwargs[\"ignore_patterns\"])\n        load_group.add_argument(\"--use-tqdm-on-load\",\n                                **load_kwargs[\"use_tqdm_on_load\"])\n        load_group.add_argument(\n            \"--qlora-adapter-name-or-path\",\n            type=str,\n            default=None,\n            help=\"The `--qlora-adapter-name-or-path` has no effect, do not set\"\n            \" it, and it  will be removed in v0.10.0.\",\n            deprecated=True,\n        )\n        load_group.add_argument('--pt-load-map-location',\n                                **load_kwargs[\"pt_load_map_location\"])\n\n        # Guided decoding arguments\n        guided_decoding_kwargs = get_kwargs(DecodingConfig)\n        guided_decoding_group = parser.add_argument_group(\n            title=\"DecodingConfig\",\n            description=DecodingConfig.__doc__,\n        )\n        guided_decoding_group.add_argument(\"--guided-decoding-backend\",\n                                           **guided_decoding_kwargs[\"backend\"])\n        guided_decoding_group.add_argument(\n            \"--guided-decoding-disable-fallback\",\n            **guided_decoding_kwargs[\"disable_fallback\"])\n        guided_decoding_group.add_argument(\n            \"--guided-decoding-disable-any-whitespace\",\n            **guided_decoding_kwargs[\"disable_any_whitespace\"])\n        guided_decoding_group.add_argument(\n            \"--guided-decoding-disable-additional-properties\",\n            **guided_decoding_kwargs[\"disable_additional_properties\"])\n        guided_decoding_group.add_argument(\n            \"--enable-reasoning\",\n            action=argparse.BooleanOptionalAction,\n            deprecated=True,\n            help=\"[DEPRECATED] The `--enable-reasoning` flag is deprecated as \"\n            \"of v0.8.6. Use `--reasoning-parser` to specify the reasoning \"\n            \"parser backend instead. This flag (`--enable-reasoning`) will be \"\n            \"removed in v0.10.0. When `--reasoning-parser` is specified, \"\n            \"reasoning mode is automatically enabled.\")\n        guided_decoding_group.add_argument(\n            \"--reasoning-parser\",\n            # This choices is a special case because it's not static\n            choices=list(ReasoningParserManager.reasoning_parsers),\n            **guided_decoding_kwargs[\"reasoning_backend\"])\n\n        # Parallel arguments\n        parallel_kwargs = get_kwargs(ParallelConfig)\n        parallel_group = parser.add_argument_group(\n            title=\"ParallelConfig\",\n            description=ParallelConfig.__doc__,\n        )\n        parallel_group.add_argument(\n            \"--distributed-executor-backend\",\n            **parallel_kwargs[\"distributed_executor_backend\"])\n        parallel_group.add_argument(\n            \"--pipeline-parallel-size\", \"-pp\",\n            **parallel_kwargs[\"pipeline_parallel_size\"])\n        parallel_group.add_argument(\"--tensor-parallel-size\", \"-tp\",\n                                    **parallel_kwargs[\"tensor_parallel_size\"])\n        parallel_group.add_argument(\"--data-parallel-size\", \"-dp\",\n                                    **parallel_kwargs[\"data_parallel_size\"])\n        parallel_group.add_argument('--data-parallel-size-local',\n                                    '-dpl',\n                                    type=int,\n                                    help='Number of data parallel replicas '\n                                    'to run on this node.')\n        parallel_group.add_argument('--data-parallel-address',\n                                    '-dpa',\n                                    type=str,\n                                    help='Address of data parallel cluster '\n                                    'head-node.')\n        parallel_group.add_argument('--data-parallel-rpc-port',\n                                    '-dpp',\n                                    type=int,\n                                    help='Port for data parallel RPC '\n                                    'communication.')\n        parallel_group.add_argument(\n            \"--enable-expert-parallel\",\n            **parallel_kwargs[\"enable_expert_parallel\"])\n        parallel_group.add_argument(\n            \"--max-parallel-loading-workers\",\n            **parallel_kwargs[\"max_parallel_loading_workers\"])\n        parallel_group.add_argument(\n            \"--ray-workers-use-nsight\",\n            **parallel_kwargs[\"ray_workers_use_nsight\"])\n        parallel_group.add_argument(\n            \"--disable-custom-all-reduce\",\n            **parallel_kwargs[\"disable_custom_all_reduce\"])\n        parallel_group.add_argument(\"--worker-cls\",\n                                    **parallel_kwargs[\"worker_cls\"])\n        parallel_group.add_argument(\"--worker-extension-cls\",\n                                    **parallel_kwargs[\"worker_extension_cls\"])\n\n        # KV cache arguments\n        cache_kwargs = get_kwargs(CacheConfig)\n        cache_group = parser.add_argument_group(\n            title=\"CacheConfig\",\n            description=CacheConfig.__doc__,\n        )\n        cache_group.add_argument(\"--block-size\", **cache_kwargs[\"block_size\"])\n        cache_group.add_argument(\"--gpu-memory-utilization\",\n                                 **cache_kwargs[\"gpu_memory_utilization\"])\n        cache_group.add_argument(\"--swap-space\", **cache_kwargs[\"swap_space\"])\n        cache_group.add_argument(\"--kv-cache-dtype\",\n                                 **cache_kwargs[\"cache_dtype\"])\n        cache_group.add_argument(\"--num-gpu-blocks-override\",\n                                 **cache_kwargs[\"num_gpu_blocks_override\"])\n        cache_group.add_argument(\"--enable-prefix-caching\",\n                                 **cache_kwargs[\"enable_prefix_caching\"])\n        cache_group.add_argument(\"--prefix-caching-hash-algo\",\n                                 **cache_kwargs[\"prefix_caching_hash_algo\"])\n        cache_group.add_argument(\"--cpu-offload-gb\",\n                                 **cache_kwargs[\"cpu_offload_gb\"])\n        cache_group.add_argument(\"--calculate-kv-scales\",\n                                 **cache_kwargs[\"calculate_kv_scales\"])\n\n        # Tokenizer arguments\n        tokenizer_kwargs = get_kwargs(TokenizerPoolConfig)\n        tokenizer_group = parser.add_argument_group(\n            title=\"TokenizerPoolConfig\",\n            description=TokenizerPoolConfig.__doc__,\n        )\n        tokenizer_group.add_argument(\"--tokenizer-pool-size\",\n                                     **tokenizer_kwargs[\"pool_size\"])\n        tokenizer_group.add_argument(\"--tokenizer-pool-type\",\n                                     **tokenizer_kwargs[\"pool_type\"])\n        tokenizer_group.add_argument(\"--tokenizer-pool-extra-config\",\n                                     **tokenizer_kwargs[\"extra_config\"])\n\n        # Multimodal related configs\n        multimodal_kwargs = get_kwargs(MultiModalConfig)\n        multimodal_group = parser.add_argument_group(\n            title=\"MultiModalConfig\",\n            description=MultiModalConfig.__doc__,\n        )\n        multimodal_group.add_argument(\"--limit-mm-per-prompt\",\n                                      **multimodal_kwargs[\"limit_per_prompt\"])\n        multimodal_group.add_argument(\n            \"--mm-processor-kwargs\",\n            **multimodal_kwargs[\"mm_processor_kwargs\"])\n        multimodal_group.add_argument(\n            \"--disable-mm-preprocessor-cache\",\n            **multimodal_kwargs[\"disable_mm_preprocessor_cache\"])\n\n        # LoRA related configs\n        lora_kwargs = get_kwargs(LoRAConfig)\n        lora_group = parser.add_argument_group(\n            title=\"LoRAConfig\",\n            description=LoRAConfig.__doc__,\n        )\n        lora_group.add_argument(\n            \"--enable-lora\",\n            action=argparse.BooleanOptionalAction,\n            help=\"If True, enable handling of LoRA adapters.\")\n        lora_group.add_argument(\"--enable-lora-bias\",\n                                **lora_kwargs[\"bias_enabled\"])\n        lora_group.add_argument(\"--max-loras\", **lora_kwargs[\"max_loras\"])\n        lora_group.add_argument(\"--max-lora-rank\",\n                                **lora_kwargs[\"max_lora_rank\"])\n        lora_group.add_argument(\"--lora-extra-vocab-size\",\n                                **lora_kwargs[\"lora_extra_vocab_size\"])\n        lora_group.add_argument(\n            \"--lora-dtype\",\n            **lora_kwargs[\"lora_dtype\"],\n        )\n        lora_group.add_argument(\"--long-lora-scaling-factors\",\n                                **lora_kwargs[\"long_lora_scaling_factors\"])\n        lora_group.add_argument(\"--max-cpu-loras\",\n                                **lora_kwargs[\"max_cpu_loras\"])\n        lora_group.add_argument(\"--fully-sharded-loras\",\n                                **lora_kwargs[\"fully_sharded_loras\"])\n\n        # PromptAdapter related configs\n        prompt_adapter_kwargs = get_kwargs(PromptAdapterConfig)\n        prompt_adapter_group = parser.add_argument_group(\n            title=\"PromptAdapterConfig\",\n            description=PromptAdapterConfig.__doc__,\n        )\n        prompt_adapter_group.add_argument(\n            \"--enable-prompt-adapter\",\n            action=argparse.BooleanOptionalAction,\n            help=\"If True, enable handling of PromptAdapters.\")\n        prompt_adapter_group.add_argument(\n            \"--max-prompt-adapters\",\n            **prompt_adapter_kwargs[\"max_prompt_adapters\"])\n        prompt_adapter_group.add_argument(\n            \"--max-prompt-adapter-token\",\n            **prompt_adapter_kwargs[\"max_prompt_adapter_token\"])\n\n        # Device arguments\n        device_kwargs = get_kwargs(DeviceConfig)\n        device_group = parser.add_argument_group(\n            title=\"DeviceConfig\",\n            description=DeviceConfig.__doc__,\n        )\n        device_group.add_argument(\"--device\", **device_kwargs[\"device\"])\n\n        # Speculative arguments\n        speculative_group = parser.add_argument_group(\n            title=\"SpeculativeConfig\",\n            description=SpeculativeConfig.__doc__,\n        )\n        speculative_group.add_argument(\n            \"--speculative-config\",\n            type=json.loads,\n            default=None,\n            help=\"The configurations for speculative decoding. Should be a \"\n            \"JSON string.\")\n\n        # Observability arguments\n        observability_kwargs = get_kwargs(ObservabilityConfig)\n        observability_group = parser.add_argument_group(\n            title=\"ObservabilityConfig\",\n            description=ObservabilityConfig.__doc__,\n        )\n        observability_group.add_argument(\n            \"--show-hidden-metrics-for-version\",\n            **observability_kwargs[\"show_hidden_metrics_for_version\"])\n        observability_group.add_argument(\n            \"--otlp-traces-endpoint\",\n            **observability_kwargs[\"otlp_traces_endpoint\"])\n        # TODO: generalise this special case\n        choices = observability_kwargs[\"collect_detailed_traces\"][\"choices\"]\n        metavar = f\"{{{','.join(choices)}}}\"\n        observability_kwargs[\"collect_detailed_traces\"][\"metavar\"] = metavar\n        observability_kwargs[\"collect_detailed_traces\"][\"choices\"] += [\n            \",\".join(p)\n            for p in permutations(get_args(DetailedTraceModules), r=2)\n        ]\n        observability_group.add_argument(\n            \"--collect-detailed-traces\",\n            **observability_kwargs[\"collect_detailed_traces\"])\n\n        # Scheduler arguments\n        scheduler_kwargs = get_kwargs(SchedulerConfig)\n        scheduler_group = parser.add_argument_group(\n            title=\"SchedulerConfig\",\n            description=SchedulerConfig.__doc__,\n        )\n        scheduler_group.add_argument(\n            \"--max-num-batched-tokens\",\n            **scheduler_kwargs[\"max_num_batched_tokens\"])\n        scheduler_group.add_argument(\"--max-num-seqs\",\n                                     **scheduler_kwargs[\"max_num_seqs\"])\n        scheduler_group.add_argument(\n            \"--max-num-partial-prefills\",\n            **scheduler_kwargs[\"max_num_partial_prefills\"])\n        scheduler_group.add_argument(\n            \"--max-long-partial-prefills\",\n            **scheduler_kwargs[\"max_long_partial_prefills\"])\n        scheduler_group.add_argument('--cuda-graph-sizes',\n                                     **scheduler_kwargs[\"cuda_graph_sizes\"])\n        scheduler_group.add_argument(\n            \"--long-prefill-token-threshold\",\n            **scheduler_kwargs[\"long_prefill_token_threshold\"])\n        scheduler_group.add_argument(\"--num-lookahead-slots\",\n                                     **scheduler_kwargs[\"num_lookahead_slots\"])\n        scheduler_group.add_argument(\"--scheduler-delay-factor\",\n                                     **scheduler_kwargs[\"delay_factor\"])\n        scheduler_group.add_argument(\"--preemption-mode\",\n                                     **scheduler_kwargs[\"preemption_mode\"])\n        scheduler_group.add_argument(\"--num-scheduler-steps\",\n                                     **scheduler_kwargs[\"num_scheduler_steps\"])\n        scheduler_group.add_argument(\n            \"--multi-step-stream-outputs\",\n            **scheduler_kwargs[\"multi_step_stream_outputs\"])\n        scheduler_group.add_argument(\"--scheduling-policy\",\n                                     **scheduler_kwargs[\"policy\"])\n        scheduler_group.add_argument(\n            \"--enable-chunked-prefill\",\n            **scheduler_kwargs[\"enable_chunked_prefill\"])\n        scheduler_group.add_argument(\n            \"--disable-chunked-mm-input\",\n            **scheduler_kwargs[\"disable_chunked_mm_input\"])\n        scheduler_group.add_argument(\"--scheduler-cls\",\n                                     **scheduler_kwargs[\"scheduler_cls\"])\n\n        # vLLM arguments\n        vllm_kwargs = get_kwargs(VllmConfig)\n        vllm_group = parser.add_argument_group(\n            title=\"VllmConfig\",\n            description=VllmConfig.__doc__,\n        )\n        vllm_group.add_argument(\"--kv-transfer-config\",\n                                **vllm_kwargs[\"kv_transfer_config\"])\n        vllm_group.add_argument('--kv-events-config',\n                                **vllm_kwargs[\"kv_events_config\"])\n        vllm_group.add_argument(\"--compilation-config\", \"-O\",\n                                **vllm_kwargs[\"compilation_config\"])\n        vllm_group.add_argument(\"--additional-config\",\n                                **vllm_kwargs[\"additional_config\"])\n\n        # Other arguments\n        parser.add_argument('--use-v2-block-manager',\n                            action='store_true',\n                            default=True,\n                            deprecated=True,\n                            help='[DEPRECATED] block manager v1 has been '\n                            'removed and SelfAttnBlockSpaceManager (i.e. '\n                            'block manager v2) is now the default. '\n                            'Setting this flag to True or False'\n                            ' has no effect on vLLM behavior.')\n        parser.add_argument('--disable-log-stats',\n                            action='store_true',\n                            help='Disable logging statistics.')\n\n        return parser\n\n    @classmethod\n    def from_cli_args(cls, args: argparse.Namespace):\n        # Get the list of attributes of this dataclass.\n        attrs = [attr.name for attr in dataclasses.fields(cls)]\n        # Set the attributes from the parsed arguments.\n        engine_args = cls(**{attr: getattr(args, attr) for attr in attrs})\n        return engine_args\n\n    def create_model_config(self) -> ModelConfig:\n        # gguf file needs a specific model loader and doesn't use hf_repo\n        if check_gguf_file(self.model):\n            self.quantization = self.load_format = \"gguf\"\n\n        # NOTE: This is to allow model loading from S3 in CI\n        if (not isinstance(self, AsyncEngineArgs) and envs.VLLM_CI_USE_S3\n                and self.model in MODELS_ON_S3\n                and self.load_format == LoadFormat.AUTO):  # noqa: E501\n            self.model = f\"{MODEL_WEIGHTS_S3_BUCKET}/{self.model}\"\n            self.load_format = LoadFormat.RUNAI_STREAMER\n\n        return ModelConfig(\n            model=self.model,\n            hf_config_path=self.hf_config_path,\n            task=self.task,\n            tokenizer=self.tokenizer,\n            tokenizer_mode=self.tokenizer_mode,\n            trust_remote_code=self.trust_remote_code,\n            allowed_local_media_path=self.allowed_local_media_path,\n            dtype=self.dtype,\n            seed=self.seed,\n            revision=self.revision,\n            code_revision=self.code_revision,\n            rope_scaling=self.rope_scaling,\n            rope_theta=self.rope_theta,\n            hf_token=self.hf_token,\n            hf_overrides=self.hf_overrides,\n            tokenizer_revision=self.tokenizer_revision,\n            max_model_len=self.max_model_len,\n            quantization=self.quantization,\n            enforce_eager=self.enforce_eager,\n            max_seq_len_to_capture=self.max_seq_len_to_capture,\n            max_logprobs=self.max_logprobs,\n            disable_sliding_window=self.disable_sliding_window,\n            disable_cascade_attn=self.disable_cascade_attn,\n            skip_tokenizer_init=self.skip_tokenizer_init,\n            enable_prompt_embeds=self.enable_prompt_embeds,\n            served_model_name=self.served_model_name,\n            limit_mm_per_prompt=self.limit_mm_per_prompt,\n            use_async_output_proc=not self.disable_async_output_proc,\n            config_format=self.config_format,\n            mm_processor_kwargs=self.mm_processor_kwargs,\n            disable_mm_preprocessor_cache=self.disable_mm_preprocessor_cache,\n            override_neuron_config=self.override_neuron_config,\n            override_pooler_config=self.override_pooler_config,\n            logits_processor_pattern=self.logits_processor_pattern,\n            generation_config=self.generation_config,\n            override_generation_config=self.override_generation_config,\n            enable_sleep_mode=self.enable_sleep_mode,\n            model_impl=self.model_impl,\n        )\n\n    def create_load_config(self) -> LoadConfig:\n\n        if self.quantization == \"bitsandbytes\":\n            self.load_format = \"bitsandbytes\"\n\n        return LoadConfig(\n            load_format=self.load_format,\n            download_dir=self.download_dir,\n            model_loader_extra_config=self.model_loader_extra_config,\n            ignore_patterns=self.ignore_patterns,\n            use_tqdm_on_load=self.use_tqdm_on_load,\n            pt_load_map_location=self.pt_load_map_location,\n        )\n\n    def create_speculative_config(\n        self,\n        target_model_config: ModelConfig,\n        target_parallel_config: ParallelConfig,\n        enable_chunked_prefill: bool,\n        disable_log_stats: bool,\n    ) -> Optional[\"SpeculativeConfig\"]:\n        \"\"\"Initializes and returns a SpeculativeConfig object based on\n        `speculative_config`.\n\n        This function utilizes `speculative_config` to create a\n        SpeculativeConfig object. The `speculative_config` can either be\n        provided as a JSON string input via CLI arguments or directly as a\n        dictionary from the engine.\n        \"\"\"\n        if self.speculative_config is None:\n            return None\n\n        # Note(Shangming): These parameters are not obtained from the cli arg\n        # '--speculative-config' and must be passed in when creating the engine\n        # config.\n        self.speculative_config.update({\n            \"target_model_config\": target_model_config,\n            \"target_parallel_config\": target_parallel_config,\n            \"enable_chunked_prefill\": enable_chunked_prefill,\n            \"disable_log_stats\": disable_log_stats,\n        })\n        speculative_config = SpeculativeConfig.from_dict(\n            self.speculative_config)\n\n        return speculative_config\n\n    def create_engine_config(\n        self,\n        usage_context: Optional[UsageContext] = None,\n    ) -> VllmConfig:\n        \"\"\"\n        Create the VllmConfig.\n\n        NOTE: for autoselection of V0 vs V1 engine, we need to\n        create the ModelConfig first, since ModelConfig's attrs\n        (e.g. the model arch) are needed to make the decision.\n\n        This function set VLLM_USE_V1=X if VLLM_USE_V1 is\n        unspecified by the user.\n\n        If VLLM_USE_V1 is specified by the user but the VllmConfig\n        is incompatible, we raise an error.\n        \"\"\"\n        from vllm.platforms import current_platform\n        current_platform.pre_register_and_update()\n\n        device_config = DeviceConfig(device=self.device)\n        model_config = self.create_model_config()\n\n        # * If VLLM_USE_V1 is unset, we enable V1 for \"supported features\"\n        #   and fall back to V0 for experimental or unsupported features.\n        # * If VLLM_USE_V1=1, we enable V1 for supported + experimental\n        #   features and raise error for unsupported features.\n        # * If VLLM_USE_V1=0, we disable V1.\n        use_v1 = False\n        try_v1 = envs.VLLM_USE_V1 or not envs.is_set(\"VLLM_USE_V1\")\n        if try_v1 and self._is_v1_supported_oracle(model_config):\n            use_v1 = True\n\n        # If user explicitly set VLLM_USE_V1, sanity check we respect it.\n        if envs.is_set(\"VLLM_USE_V1\"):\n            assert use_v1 == envs.VLLM_USE_V1\n        # Otherwise, set the VLLM_USE_V1 variable globally.\n        else:\n            envs.set_vllm_use_v1(use_v1)\n\n        # Set default arguments for V0 or V1 Engine.\n        if use_v1:\n            self._set_default_args_v1(usage_context)\n        else:\n            self._set_default_args_v0(model_config)\n\n        assert self.enable_chunked_prefill is not None\n\n        if envs.VLLM_ATTENTION_BACKEND in [STR_DUAL_CHUNK_FLASH_ATTN_VAL]:\n            assert self.enforce_eager, (\n                \"Cuda graph is not supported with DualChunkFlashAttention. \"\n                \"To run the model in eager mode, set 'enforce_eager=True' \"\n                \"or use '--enforce-eager' in the CLI.\")\n            assert current_platform.is_cuda(), (\n                \"DualChunkFlashAttention is only supported on CUDA platform.\")\n            assert not use_v1, (\n                \"DualChunkFlashAttention is not supported on V1 engine. \"\n                \"To run the model in V0 engine, try set 'VLLM_USE_V1=0'\")\n\n        cache_config = CacheConfig(\n            block_size=self.block_size,\n            gpu_memory_utilization=self.gpu_memory_utilization,\n            swap_space=self.swap_space,\n            cache_dtype=self.kv_cache_dtype,\n            is_attention_free=model_config.is_attention_free,\n            num_gpu_blocks_override=self.num_gpu_blocks_override,\n            sliding_window=model_config.get_sliding_window(),\n            enable_prefix_caching=self.enable_prefix_caching,\n            prefix_caching_hash_algo=self.prefix_caching_hash_algo,\n            cpu_offload_gb=self.cpu_offload_gb,\n            calculate_kv_scales=self.calculate_kv_scales,\n        )\n\n        # Get the current placement group if Ray is initialized and\n        # we are in a Ray actor. If so, then the placement group will be\n        # passed to spawned processes.\n        placement_group = None\n        if is_in_ray_actor():\n            import ray\n\n            # This call initializes Ray automatically if it is not initialized,\n            # but we should not do this here.\n            placement_group = ray.util.get_current_placement_group()\n\n        # Local DP size defaults to global DP size if not set.\n        data_parallel_size_local = self.data_parallel_size if (\n            self.data_parallel_size_local\n            is None) else self.data_parallel_size_local\n\n        # DP address, used in multi-node case for torch distributed group\n        # and ZMQ sockets.\n        data_parallel_address = self.data_parallel_address if (\n            self.data_parallel_address\n            is not None) else ParallelConfig.data_parallel_master_ip\n\n        # This port is only used when there are remote data parallel engines,\n        # otherwise the local IPC transport is used.\n        data_parallel_rpc_port = self.data_parallel_rpc_port if (\n            self.data_parallel_rpc_port\n            is not None) else ParallelConfig.data_parallel_rpc_port\n\n        parallel_config = ParallelConfig(\n            pipeline_parallel_size=self.pipeline_parallel_size,\n            tensor_parallel_size=self.tensor_parallel_size,\n            data_parallel_size=self.data_parallel_size,\n            data_parallel_size_local=data_parallel_size_local,\n            data_parallel_master_ip=data_parallel_address,\n            data_parallel_rpc_port=data_parallel_rpc_port,\n            enable_expert_parallel=self.enable_expert_parallel,\n            max_parallel_loading_workers=self.max_parallel_loading_workers,\n            disable_custom_all_reduce=self.disable_custom_all_reduce,\n            ray_workers_use_nsight=self.ray_workers_use_nsight,\n            placement_group=placement_group,\n            distributed_executor_backend=self.distributed_executor_backend,\n            worker_cls=self.worker_cls,\n            worker_extension_cls=self.worker_extension_cls,\n        )\n\n        speculative_config = self.create_speculative_config(\n            target_model_config=model_config,\n            target_parallel_config=parallel_config,\n            enable_chunked_prefill=self.enable_chunked_prefill,\n            disable_log_stats=self.disable_log_stats,\n        )\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        if self.num_scheduler_steps > 1:\n            if speculative_config is not None:\n                raise ValueError(\"Speculative decoding is not supported with \"\n                                 \"multi-step (--num-scheduler-steps > 1)\")\n            if self.enable_chunked_prefill and self.pipeline_parallel_size > 1:\n                raise ValueError(\"Multi-Step Chunked-Prefill is not supported \"\n                                 \"for pipeline-parallel-size > 1\")\n            from vllm.platforms import current_platform\n            if current_platform.is_cpu():\n                logger.warning(\"Multi-Step (--num-scheduler-steps > 1) is \"\n                               \"currently not supported for CPUs and has been \"\n                               \"disabled.\")\n                self.num_scheduler_steps = 1\n\n        # make sure num_lookahead_slots is set the higher value depending on\n        # if we are using speculative decoding or multi-step\n        num_lookahead_slots = max(self.num_lookahead_slots,\n                                  self.num_scheduler_steps - 1)\n        num_lookahead_slots = num_lookahead_slots \\\n            if speculative_config is None \\\n            else speculative_config.num_lookahead_slots\n\n        scheduler_config = SchedulerConfig(\n            runner_type=model_config.runner_type,\n            max_num_batched_tokens=self.max_num_batched_tokens,\n            max_num_seqs=self.max_num_seqs,\n            max_model_len=model_config.max_model_len,\n            cuda_graph_sizes=self.cuda_graph_sizes,\n            num_lookahead_slots=num_lookahead_slots,\n            delay_factor=self.scheduler_delay_factor,\n            enable_chunked_prefill=self.enable_chunked_prefill,\n            disable_chunked_mm_input=self.disable_chunked_mm_input,\n            is_multimodal_model=model_config.is_multimodal_model,\n            preemption_mode=self.preemption_mode,\n            num_scheduler_steps=self.num_scheduler_steps,\n            multi_step_stream_outputs=self.multi_step_stream_outputs,\n            send_delta_data=(envs.VLLM_USE_RAY_SPMD_WORKER\n                             and parallel_config.use_ray),\n            policy=self.scheduling_policy,\n            scheduler_cls=self.scheduler_cls,\n            max_num_partial_prefills=self.max_num_partial_prefills,\n            max_long_partial_prefills=self.max_long_partial_prefills,\n            long_prefill_token_threshold=self.long_prefill_token_threshold,\n        )\n\n        lora_config = LoRAConfig(\n            bias_enabled=self.enable_lora_bias,\n            max_lora_rank=self.max_lora_rank,\n            max_loras=self.max_loras,\n            fully_sharded_loras=self.fully_sharded_loras,\n            lora_extra_vocab_size=self.lora_extra_vocab_size,\n            long_lora_scaling_factors=self.long_lora_scaling_factors,\n            lora_dtype=self.lora_dtype,\n            max_cpu_loras=self.max_cpu_loras if self.max_cpu_loras\n            and self.max_cpu_loras > 0 else None) if self.enable_lora else None\n\n        # bitsandbytes pre-quantized model need a specific model loader\n        if model_config.quantization == \"bitsandbytes\":\n            self.quantization = self.load_format = \"bitsandbytes\"\n\n        load_config = self.create_load_config()\n\n        prompt_adapter_config = PromptAdapterConfig(\n            max_prompt_adapters=self.max_prompt_adapters,\n            max_prompt_adapter_token=self.max_prompt_adapter_token) \\\n                                        if self.enable_prompt_adapter else None\n\n        decoding_config = DecodingConfig(\n            backend=self.guided_decoding_backend,\n            disable_fallback=self.guided_decoding_disable_fallback,\n            disable_any_whitespace=self.guided_decoding_disable_any_whitespace,\n            disable_additional_properties=\\\n                self.guided_decoding_disable_additional_properties,\n            reasoning_backend=self.reasoning_parser\n        )\n\n        observability_config = ObservabilityConfig(\n            show_hidden_metrics_for_version=self.\n            show_hidden_metrics_for_version,\n            otlp_traces_endpoint=self.otlp_traces_endpoint,\n            collect_detailed_traces=self.collect_detailed_traces,\n        )\n\n        config = VllmConfig(\n            model_config=model_config,\n            cache_config=cache_config,\n            parallel_config=parallel_config,\n            scheduler_config=scheduler_config,\n            device_config=device_config,\n            lora_config=lora_config,\n            speculative_config=speculative_config,\n            load_config=load_config,\n            decoding_config=decoding_config,\n            observability_config=observability_config,\n            prompt_adapter_config=prompt_adapter_config,\n            compilation_config=self.compilation_config,\n            kv_transfer_config=self.kv_transfer_config,\n            kv_events_config=self.kv_events_config,\n            additional_config=self.additional_config,\n        )\n\n        return config\n\n    def _is_v1_supported_oracle(self, model_config: ModelConfig) -> bool:\n        \"\"\"Oracle for whether to use V0 or V1 Engine by default.\"\"\"\n\n        #############################################################\n        # Unsupported Feature Flags on V1.\n\n        if (self.load_format == LoadFormat.TENSORIZER.value\n                or self.load_format == LoadFormat.SHARDED_STATE.value):\n            _raise_or_fallback(\n                feature_name=f\"--load_format {self.load_format}\",\n                recommend_to_remove=False)\n            return False\n\n        if (self.logits_processor_pattern\n                != EngineArgs.logits_processor_pattern):\n            _raise_or_fallback(feature_name=\"--logits-processor-pattern\",\n                               recommend_to_remove=False)\n            return False\n\n        if self.preemption_mode != SchedulerConfig.preemption_mode:\n            _raise_or_fallback(feature_name=\"--preemption-mode\",\n                               recommend_to_remove=True)\n            return False\n\n        if (self.disable_async_output_proc\n                != EngineArgs.disable_async_output_proc):\n            _raise_or_fallback(feature_name=\"--disable-async-output-proc\",\n                               recommend_to_remove=True)\n            return False\n\n        if self.scheduling_policy != SchedulerConfig.policy:\n            _raise_or_fallback(feature_name=\"--scheduling-policy\",\n                               recommend_to_remove=False)\n            return False\n\n        if self.num_scheduler_steps != SchedulerConfig.num_scheduler_steps:\n            _raise_or_fallback(feature_name=\"--num-scheduler-steps\",\n                               recommend_to_remove=True)\n            return False\n\n        if self.scheduler_delay_factor != SchedulerConfig.delay_factor:\n            _raise_or_fallback(feature_name=\"--scheduler-delay-factor\",\n                               recommend_to_remove=True)\n            return False\n\n        if self.guided_decoding_backend not in get_args(\n                GuidedDecodingBackendV1):\n            _raise_or_fallback(\n                feature_name=\n                f\"--guided-decoding-backend={self.guided_decoding_backend}\",\n                recommend_to_remove=False)\n            return False\n\n        # Need at least Ampere for now (FA support required).\n        # Skip this check if we are running on a non-GPU platform,\n        # or if the device capability is not available\n        # (e.g. in a Ray actor without GPUs).\n        from vllm.platforms import current_platform\n        if (current_platform.is_cuda()\n                and current_platform.get_device_capability()\n                and current_platform.get_device_capability().major < 8):\n            _raise_or_fallback(feature_name=\"Compute Capability < 8.0\",\n                               recommend_to_remove=False)\n            return False\n\n        # No Fp8 KV cache so far.\n        if self.kv_cache_dtype != \"auto\":\n            fp8_attention = self.kv_cache_dtype.startswith(\"fp8\")\n            will_use_fa = (\n                current_platform.is_cuda()\n                and not envs.is_set(\"VLLM_ATTENTION_BACKEND\")\n            ) or envs.VLLM_ATTENTION_BACKEND == \"FLASH_ATTN_VLLM_V1\"\n            supported = False\n            if current_platform.is_rocm():\n                supported = True\n            elif fp8_attention and will_use_fa:\n                from vllm.attention.utils.fa_utils import (\n                    flash_attn_supports_fp8)\n                supported = flash_attn_supports_fp8()\n            if not supported:\n                _raise_or_fallback(feature_name=\"--kv-cache-dtype\",\n                                   recommend_to_remove=False)\n                return False\n\n        # No Prompt Adapter so far.\n        if self.enable_prompt_adapter:\n            _raise_or_fallback(feature_name=\"--enable-prompt-adapter\",\n                               recommend_to_remove=False)\n            return False\n\n        # No text embedding inputs so far.\n        if self.enable_prompt_embeds:\n            _raise_or_fallback(feature_name=\"--enable-prompt-embeds\",\n                               recommend_to_remove=False)\n            return False\n\n        # Only Fp16 and Bf16 dtypes since we only support FA.\n        V1_SUPPORTED_DTYPES = [torch.bfloat16, torch.float16]\n        if model_config.dtype not in V1_SUPPORTED_DTYPES:\n            _raise_or_fallback(feature_name=f\"--dtype {model_config.dtype}\",\n                               recommend_to_remove=False)\n            return False\n\n        # Some quantization is not compatible with torch.compile.\n        V1_UNSUPPORTED_QUANT = [\"gguf\"]\n        if model_config.quantization in V1_UNSUPPORTED_QUANT:\n            _raise_or_fallback(\n                feature_name=f\"--quantization {model_config.quantization}\",\n                recommend_to_remove=False)\n            return False\n\n        # No Embedding Models so far.\n        if model_config.task not in [\"generate\"]:\n            _raise_or_fallback(feature_name=f\"--task {model_config.task}\",\n                               recommend_to_remove=False)\n            return False\n\n        # No Mamba or Encoder-Decoder so far.\n        if not model_config.is_v1_compatible:\n            _raise_or_fallback(feature_name=model_config.architectures,\n                               recommend_to_remove=False)\n            return False\n\n        # No Concurrent Partial Prefills so far.\n        if (self.max_num_partial_prefills\n                != SchedulerConfig.max_num_partial_prefills\n                or self.max_long_partial_prefills\n                != SchedulerConfig.max_long_partial_prefills):\n            _raise_or_fallback(feature_name=\"Concurrent Partial Prefill\",\n                               recommend_to_remove=False)\n            return False\n\n        # No OTLP observability so far.\n        if (self.otlp_traces_endpoint or self.collect_detailed_traces):\n            _raise_or_fallback(feature_name=\"--otlp-traces-endpoint\",\n                               recommend_to_remove=False)\n            return False\n\n        # V1 supports N-gram, Medusa, and Eagle speculative decoding.\n        is_ngram_enabled = False\n        is_eagle_enabled = False\n        is_medusa_enabled = False\n        if self.speculative_config is not None:\n            # This is supported but experimental (handled below).\n            speculative_method = self.speculative_config.get(\"method\")\n            if speculative_method:\n                if speculative_method in (\"ngram\", \"[ngram]\"):\n                    is_ngram_enabled = True\n                elif speculative_method == \"medusa\":\n                    is_medusa_enabled = True\n                elif speculative_method in (\"eagle\", \"eagle3\"):\n                    is_eagle_enabled = True\n            else:\n                speculative_model = self.speculative_config.get(\"model\")\n                if speculative_model in (\"ngram\", \"[ngram]\"):\n                    is_ngram_enabled = True\n            if not (is_ngram_enabled or is_eagle_enabled or is_medusa_enabled):\n                # Other speculative decoding methods are not supported yet.\n                _raise_or_fallback(feature_name=\"Speculative Decoding\",\n                                   recommend_to_remove=False)\n                return False\n\n        # No XFormers so far.\n        V1_BACKENDS = [\n            \"FLASH_ATTN_VLLM_V1\",\n            \"FLASH_ATTN\",\n            \"PALLAS\",\n            \"PALLAS_VLLM_V1\",\n            \"TRITON_ATTN_VLLM_V1\",\n            \"TRITON_MLA\",\n            \"FLASHMLA\",\n            \"FLASHINFER\",\n            \"FLASHINFER_VLLM_V1\",\n            \"ROCM_AITER_MLA\",\n        ]\n        if (envs.is_set(\"VLLM_ATTENTION_BACKEND\")\n                and envs.VLLM_ATTENTION_BACKEND not in V1_BACKENDS):\n            name = f\"VLLM_ATTENTION_BACKEND={envs.VLLM_ATTENTION_BACKEND}\"\n            _raise_or_fallback(feature_name=name, recommend_to_remove=True)\n            return False\n\n        # Platforms must decide if they can support v1 for this model\n        if not current_platform.supports_v1(model_config=model_config):\n            _raise_or_fallback(\n                feature_name=f\"device type={current_platform.device_type}\",\n                recommend_to_remove=False)\n            return False\n        #############################################################\n        # Experimental Features - allow users to opt in.\n\n        # Signal Handlers requires running in main thread.\n        if (threading.current_thread() != threading.main_thread()\n                and _warn_or_fallback(\"Engine in background thread\")):\n            return False\n\n        if (self.pipeline_parallel_size > 1\n                and self.distributed_executor_backend\n                not in (\"ray\", \"mp\", \"external_launcher\")):\n            name = \"Pipeline Parallelism without Ray distributed executor \" \\\n                    \"or multiprocessing executor or external launcher\"\n            _raise_or_fallback(feature_name=name, recommend_to_remove=False)\n            return False\n\n        # Non-[CUDA, TPU] may be supported on V1, but off by default for now.\n        v0_hardware = not any(\n            (current_platform.is_cuda(), current_platform.is_tpu()))\n        if v0_hardware and _warn_or_fallback(  # noqa: SIM103\n                current_platform.device_name):\n            return False\n        #############################################################\n\n        return True\n\n    def _set_default_args_v0(self, model_config: ModelConfig) -> None:\n        \"\"\"Set Default Arguments for V0 Engine.\"\"\"\n\n        max_model_len = model_config.max_model_len\n        use_long_context = max_model_len > 32768\n        if self.enable_chunked_prefill is None:\n            # Chunked prefill not supported for Multimodal or MLA in V0.\n            if model_config.is_multimodal_model or model_config.use_mla:\n                self.enable_chunked_prefill = False\n\n            # Enable chunked prefill by default for long context (> 32K)\n            # models to avoid OOM errors in initial memory profiling phase.\n            elif use_long_context:\n                from vllm.platforms import current_platform\n                is_gpu = current_platform.is_cuda()\n                use_sliding_window = (model_config.get_sliding_window()\n                                      is not None)\n                use_spec_decode = self.speculative_config is not None\n\n                if (is_gpu and not use_sliding_window and not use_spec_decode\n                        and not self.enable_lora\n                        and not self.enable_prompt_adapter\n                        and model_config.runner_type != \"pooling\"):\n                    self.enable_chunked_prefill = True\n                    logger.warning(\n                        \"Chunked prefill is enabled by default for models \"\n                        \"with max_model_len > 32K. Chunked prefill might \"\n                        \"not work with some features or models. If you \"\n                        \"encounter any issues, please disable by launching \"\n                        \"with --enable-chunked-prefill=False.\")\n\n            if self.enable_chunked_prefill is None:\n                self.enable_chunked_prefill = False\n\n        if not self.enable_chunked_prefill and use_long_context:\n            logger.warning(\n                \"The model has a long context length (%s). This may cause\"\n                \"OOM during the initial memory profiling phase, or result \"\n                \"in low performance due to small KV cache size. Consider \"\n                \"setting --max-model-len to a smaller value.\", max_model_len)\n        elif (self.enable_chunked_prefill\n              and model_config.runner_type == \"pooling\"):\n            msg = \"Chunked prefill is not supported for pooling models\"\n            raise ValueError(msg)\n\n        # if using prefix caching, we must set a hash algo\n        if self.enable_prefix_caching:\n            # Disable prefix caching for multimodal models for VLLM_V0.\n            if model_config.is_multimodal_model:\n                logger.warning(\n                    \"--enable-prefix-caching is not supported for multimodal \"\n                    \"models in V0 and has been disabled.\")\n                self.enable_prefix_caching = False\n\n            # VLLM_V0 only supports builtin hash algo for prefix caching.\n            if self.prefix_caching_hash_algo == \"sha256\":\n                raise ValueError(\n                    \"sha256 is not supported for prefix caching in V0 engine. \"\n                    \"Please use 'builtin'.\")\n\n        # Set max_num_seqs to 256 for VLLM_V0.\n        if self.max_num_seqs is None:\n            self.max_num_seqs = 256\n\n    def _set_default_args_v1(self, usage_context: UsageContext) -> None:\n        \"\"\"Set Default Arguments for V1 Engine.\"\"\"\n\n        # V1 always uses chunked prefills.\n        self.enable_chunked_prefill = True\n\n        # V1 enables prefix caching by default.\n        if self.enable_prefix_caching is None:\n            self.enable_prefix_caching = True\n\n        # V1 should use the new scheduler by default.\n        # Swap it only if this arg is set to the original V0 default\n        if self.scheduler_cls == EngineArgs.scheduler_cls:\n            self.scheduler_cls = \"vllm.v1.core.sched.scheduler.Scheduler\"\n\n        # When no user override, set the default values based on the usage\n        # context.\n        # Use different default values for different hardware.\n\n        # Try to query the device name on the current platform. If it fails,\n        # it may be because the platform that imports vLLM is not the same\n        # as the platform that vLLM is running on (e.g. the case of scaling\n        # vLLM with Ray) and has no GPUs. In this case we use the default\n        # values for non-H100/H200 GPUs.\n        from vllm.platforms import current_platform\n        try:\n            device_memory = current_platform.get_device_total_memory()\n            device_name = current_platform.get_device_name().lower()\n        except Exception:\n            # This is only used to set default_max_num_batched_tokens\n            device_memory = 0\n\n        # NOTE(Kuntai): Setting large `max_num_batched_tokens` for A100 reduces\n        # throughput, see PR #17885 for more details.\n        # So here we do an extra device name check to prevent such regression.\n        if device_memory >= 70 * GiB_bytes and \"a100\" not in device_name:\n            # For GPUs like H100 and MI300x, use larger default values.\n            default_max_num_batched_tokens = {\n                UsageContext.LLM_CLASS: 16384,\n                UsageContext.OPENAI_API_SERVER: 8192,\n            }\n            default_max_num_seqs = 1024\n        else:\n            # TODO(woosuk): Tune the default values for other hardware.\n            default_max_num_batched_tokens = {\n                UsageContext.LLM_CLASS: 8192,\n                UsageContext.OPENAI_API_SERVER: 2048,\n            }\n            default_max_num_seqs = 256\n\n        # tpu specific default values.\n        if current_platform.is_tpu():\n            default_max_num_batched_tokens_tpu = {\n                UsageContext.LLM_CLASS: {\n                    'V6E': 2048,\n                    'V5E': 1024,\n                    'V5P': 512,\n                },\n                UsageContext.OPENAI_API_SERVER: {\n                    'V6E': 1024,\n                    'V5E': 512,\n                    'V5P': 256,\n                }\n            }\n\n        use_context_value = usage_context.value if usage_context else None\n        if (self.max_num_batched_tokens is None\n                and usage_context in default_max_num_batched_tokens):\n            if current_platform.is_tpu():\n                chip_name = current_platform.get_device_name()\n                if chip_name in default_max_num_batched_tokens_tpu[\n                        usage_context]:\n                    self.max_num_batched_tokens = \\\n                        default_max_num_batched_tokens_tpu[\n                            usage_context][chip_name]\n                else:\n                    self.max_num_batched_tokens = \\\n                        default_max_num_batched_tokens[usage_context]\n            else:\n                self.max_num_batched_tokens = default_max_num_batched_tokens[\n                    usage_context]\n            logger.debug(\n                \"Setting max_num_batched_tokens to %d for %s usage context.\",\n                self.max_num_batched_tokens, use_context_value)\n\n        if self.max_num_seqs is None:\n            self.max_num_seqs = default_max_num_seqs\n\n            logger.debug(\"Setting max_num_seqs to %d for %s usage context.\",\n                         self.max_num_seqs, use_context_value)\n\n\n@dataclass\nclass AsyncEngineArgs(EngineArgs):\n    \"\"\"Arguments for asynchronous vLLM engine.\"\"\"\n    disable_log_requests: bool = False\n\n    @staticmethod\n    def add_cli_args(parser: FlexibleArgumentParser,\n                     async_args_only: bool = False) -> FlexibleArgumentParser:\n        # Initialize plugin to update the parser, for example, The plugin may\n        # adding a new kind of quantization method to --quantization argument or\n        # a new device to --device argument.\n        load_general_plugins()\n        if not async_args_only:\n            parser = EngineArgs.add_cli_args(parser)\n        parser.add_argument('--disable-log-requests',\n                            action='store_true',\n                            help='Disable logging requests.')\n        from vllm.platforms import current_platform\n        current_platform.pre_register_and_update(parser)\n        return parser\n\n\ndef _raise_or_fallback(feature_name: str, recommend_to_remove: bool):\n    if envs.is_set(\"VLLM_USE_V1\") and envs.VLLM_USE_V1:\n        raise NotImplementedError(\n            f\"VLLM_USE_V1=1 is not supported with {feature_name}.\")\n    msg = f\"{feature_name} is not supported by the V1 Engine. \"\n    msg += \"Falling back to V0. \"\n    if recommend_to_remove:\n        msg += f\"We recommend to remove {feature_name} from your config \"\n        msg += \"in favor of the V1 Engine.\"\n    logger.warning(msg)\n\n\ndef _warn_or_fallback(feature_name: str) -> bool:\n    if envs.is_set(\"VLLM_USE_V1\") and envs.VLLM_USE_V1:\n        logger.warning(\n            \"Detected VLLM_USE_V1=1 with %s. Usage should \"\n            \"be considered experimental. Please report any \"\n            \"issues on Github.\", feature_name)\n        should_exit = False\n    else:\n        logger.info(\n            \"%s is experimental on VLLM_USE_V1=1. \"\n            \"Falling back to V0 Engine.\", feature_name)\n        should_exit = True\n    return should_exit\n\n\ndef human_readable_int(value):\n    \"\"\"Parse human-readable integers like '1k', '2M', etc.\n    Including decimal values with decimal multipliers.\n\n    Examples:\n    - '1k' -> 1,000\n    - '1K' -> 1,024\n    - '25.6k' -> 25,600\n    \"\"\"\n    value = value.strip()\n    match = re.fullmatch(r'(\\d+(?:\\.\\d+)?)([kKmMgGtT])', value)\n    if match:\n        decimal_multiplier = {\n            'k': 10**3,\n            'm': 10**6,\n            'g': 10**9,\n        }\n        binary_multiplier = {\n            'K': 2**10,\n            'M': 2**20,\n            'G': 2**30,\n        }\n\n        number, suffix = match.groups()\n        if suffix in decimal_multiplier:\n            mult = decimal_multiplier[suffix]\n            return int(float(number) * mult)\n        elif suffix in binary_multiplier:\n            mult = binary_multiplier[suffix]\n            # Do not allow decimals with binary multipliers\n            try:\n                return int(number) * mult\n            except ValueError as e:\n                raise argparse.ArgumentTypeError(\"Decimals are not allowed \" \\\n                f\"with binary suffixes like {suffix}. Did you mean to use \" \\\n                f\"{number}{suffix.lower()} instead?\") from e\n\n    # Regular plain number.\n    return int(value)\n\n\n# These functions are used by sphinx to build the documentation\ndef _engine_args_parser():\n    return EngineArgs.add_cli_args(FlexibleArgumentParser())\n\n\ndef _async_engine_args_parser():\n    return AsyncEngineArgs.add_cli_args(FlexibleArgumentParser(),\n                                        async_args_only=True)\n", 1656], "/home/jeromeku/vllm/vllm/third_party/pynvml.py": ["# SPDX-License-Identifier: Apache-2.0\n# copied from https://pypi.org/project/nvidia-ml-py\n# version 12.570.86\n\n#####\n# Copyright (c) 2011-2023, NVIDIA Corporation.  All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n#    * Redistributions of source code must retain the above copyright notice,\n#      this list of conditions and the following disclaimer.\n#    * Redistributions in binary form must reproduce the above copyright\n#      notice, this list of conditions and the following disclaimer in the\n#      documentation and/or other materials provided with the distribution.\n#    * Neither the name of the NVIDIA Corporation nor the names of its\n#      contributors may be used to endorse or promote products derived from\n#      this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n# ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n# LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n# CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n# SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n# INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n# CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n# ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n# THE POSSIBILITY OF SUCH DAMAGE.\n#####\n\n##\n# Python bindings for the NVML library\n##\nfrom ctypes import *\nfrom ctypes.util import find_library\nfrom functools import wraps\nimport sys\nimport os\nimport threading\nimport string\n\n## C Type mappings ##\n## Enums\n_nvmlEnableState_t = c_uint\nNVML_FEATURE_DISABLED    = 0\nNVML_FEATURE_ENABLED     = 1\n\n_nvmlBrandType_t = c_uint\nNVML_BRAND_UNKNOWN             = 0\nNVML_BRAND_QUADRO              = 1\nNVML_BRAND_TESLA               = 2\nNVML_BRAND_NVS                 = 3\nNVML_BRAND_GRID                = 4   # Deprecated from API reporting. Keeping definition for backward compatibility.\nNVML_BRAND_GEFORCE             = 5\nNVML_BRAND_TITAN               = 6\nNVML_BRAND_NVIDIA_VAPPS        = 7   # NVIDIA Virtual Applications\nNVML_BRAND_NVIDIA_VPC          = 8   # NVIDIA Virtual PC\nNVML_BRAND_NVIDIA_VCS          = 9   # NVIDIA Virtual Compute Server\nNVML_BRAND_NVIDIA_VWS          = 10  # NVIDIA RTX Virtual Workstation\nNVML_BRAND_NVIDIA_CLOUD_GAMING = 11  # NVIDIA Cloud Gaming\nNVML_BRAND_NVIDIA_VGAMING      = NVML_BRAND_NVIDIA_CLOUD_GAMING # Deprecated from API reporting. Keeping definition for backward compatibility.\nNVML_BRAND_QUADRO_RTX          = 12\nNVML_BRAND_NVIDIA_RTX          = 13\nNVML_BRAND_NVIDIA              = 14\nNVML_BRAND_GEFORCE_RTX         = 15  # Unused\nNVML_BRAND_TITAN_RTX           = 16  # Unused\nNVML_BRAND_COUNT               = 17\n\n_nvmlTemperatureThresholds_t = c_uint\nNVML_TEMPERATURE_THRESHOLD_SHUTDOWN      = 0\nNVML_TEMPERATURE_THRESHOLD_SLOWDOWN      = 1\nNVML_TEMPERATURE_THRESHOLD_MEM_MAX       = 2\nNVML_TEMPERATURE_THRESHOLD_GPU_MAX       = 3\nNVML_TEMPERATURE_THRESHOLD_ACOUSTIC_MIN  = 4\nNVML_TEMPERATURE_THRESHOLD_ACOUSTIC_CURR = 5\nNVML_TEMPERATURE_THRESHOLD_ACOUSTIC_MAX  = 6\nNVML_TEMPERATURE_THRESHOLD_GPS_CURR      = 7\nNVML_TEMPERATURE_THRESHOLD_COUNT         = 8\n\n_nvmlTemperatureSensors_t = c_uint\nNVML_TEMPERATURE_GPU     = 0\nNVML_TEMPERATURE_COUNT   = 1\n\n\n_nvmlComputeMode_t = c_uint\nNVML_COMPUTEMODE_DEFAULT           = 0\nNVML_COMPUTEMODE_EXCLUSIVE_THREAD  = 1  ## Support Removed\nNVML_COMPUTEMODE_PROHIBITED        = 2\nNVML_COMPUTEMODE_EXCLUSIVE_PROCESS = 3\nNVML_COMPUTEMODE_COUNT             = 4\n\n_nvmlMemoryLocation_t = c_uint\nNVML_MEMORY_LOCATION_L1_CACHE = 0\nNVML_MEMORY_LOCATION_L2_CACHE = 1\nNVML_MEMORY_LOCATION_DEVICE_MEMORY = 2\nNVML_MEMORY_LOCATION_DRAM = 2\nNVML_MEMORY_LOCATION_REGISTER_FILE = 3\nNVML_MEMORY_LOCATION_TEXTURE_MEMORY = 4\nNVML_MEMORY_LOCATION_TEXTURE_SHM = 5\nNVML_MEMORY_LOCATION_CBU = 6\nNVML_MEMORY_LOCATION_SRAM = 7\nNVML_MEMORY_LOCATION_COUNT = 8\n\nNVML_NVLINK_MAX_LINKS = 18\n\n# For backwards compatibility, maintain the incorrectly-named \"LANES\" define\nNVML_NVLINK_MAX_LANES = NVML_NVLINK_MAX_LINKS\n\n_nvmlNvLinkErrorCounter_t = c_uint\nNVML_NVLINK_ERROR_DL_REPLAY = 0\nNVML_NVLINK_ERROR_DL_RECOVERY = 1\nNVML_NVLINK_ERROR_DL_CRC_FLIT = 2\nNVML_NVLINK_ERROR_DL_CRC_DATA = 3\nNVML_NVLINK_ERROR_DL_ECC_DATA = 4\nNVML_NVLINK_ERROR_COUNT = 5\n\n_nvmlNvLinkEccLaneErrorCounter_t = c_uint\nNVML_NVLINK_ERROR_DL_ECC_LANE0 = 0\nNVML_NVLINK_ERROR_DL_ECC_LANE1 = 1\nNVML_NVLINK_ERROR_DL_ECC_LANE2 = 2\nNVML_NVLINK_ERROR_DL_ECC_LANE3 = 3\nNVML_NVLINK_ERROR_DL_ECC_COUNT = 5\n\n_nvmlNvLinkCapability_t = c_uint\nNVML_NVLINK_CAP_P2P_SUPPORTED = 0\nNVML_NVLINK_CAP_SYSMEM_ACCESS = 1\nNVML_NVLINK_CAP_P2P_ATOMICS   = 2\nNVML_NVLINK_CAP_SYSMEM_ATOMICS= 3\nNVML_NVLINK_CAP_SLI_BRIDGE    = 4\nNVML_NVLINK_CAP_VALID         = 5\nNVML_NVLINK_CAP_COUNT         = 6\n\n_nvmlNvLinkUtilizationCountPktTypes_t = c_uint\nNVML_NVLINK_COUNTER_PKTFILTER_NOP        = 0x1\nNVML_NVLINK_COUNTER_PKTFILTER_READ       = 0x2\nNVML_NVLINK_COUNTER_PKTFILTER_WRITE      = 0x4\nNVML_NVLINK_COUNTER_PKTFILTER_RATOM      = 0x8\nNVML_NVLINK_COUNTER_PKTFILTER_NRATOM     = 0x10\nNVML_NVLINK_COUNTER_PKTFILTER_FLUSH      = 0x20\nNVML_NVLINK_COUNTER_PKTFILTER_RESPDATA   = 0x40\nNVML_NVLINK_COUNTER_PKTFILTER_RESPNODATA = 0x80\nNVML_NVLINK_COUNTER_PKTFILTER_ALL        = 0xFF\n\n_nvmlNvLinkUtilizationCountUnits_t = c_uint\nNVML_NVLINK_COUNTER_UNIT_CYCLES   = 0\nNVML_NVLINK_COUNTER_UNIT_PACKETS  = 1\nNVML_NVLINK_COUNTER_UNIT_BYTES    = 2\nNVML_NVLINK_COUNTER_UNIT_RESERVED = 3\nNVML_NVLINK_COUNTER_UNIT_COUNT    = 4\n\n_nvmlNvLinkDeviceType_t = c_uint\nNVML_NVLINK_DEVICE_TYPE_GPU     = 0x00\nNVML_NVLINK_DEVICE_TYPE_IBMNPU  = 0x01\nNVML_NVLINK_DEVICE_TYPE_SWITCH  = 0x02\nNVML_NVLINK_DEVICE_TYPE_UNKNOWN = 0xFF\n\n# These are deprecated, instead use _nvmlMemoryErrorType_t\n_nvmlEccBitType_t = c_uint\nNVML_SINGLE_BIT_ECC    = 0\nNVML_DOUBLE_BIT_ECC    = 1\nNVML_ECC_ERROR_TYPE_COUNT = 2\n\n_nvmlEccCounterType_t = c_uint\nNVML_VOLATILE_ECC      = 0\nNVML_AGGREGATE_ECC     = 1\nNVML_ECC_COUNTER_TYPE_COUNT = 2\n\n_nvmlMemoryErrorType_t = c_uint\nNVML_MEMORY_ERROR_TYPE_CORRECTED   = 0\nNVML_MEMORY_ERROR_TYPE_UNCORRECTED = 1\nNVML_MEMORY_ERROR_TYPE_COUNT       = 2\n\n_nvmlClockType_t = c_uint\nNVML_CLOCK_GRAPHICS  = 0\nNVML_CLOCK_SM        = 1\nNVML_CLOCK_MEM       = 2\nNVML_CLOCK_VIDEO     = 3\nNVML_CLOCK_COUNT     = 4\n\n_nvmlClockId_t = c_uint\nNVML_CLOCK_ID_CURRENT            = 0\nNVML_CLOCK_ID_APP_CLOCK_TARGET   = 1\nNVML_CLOCK_ID_APP_CLOCK_DEFAULT  = 2\nNVML_CLOCK_ID_CUSTOMER_BOOST_MAX = 3\nNVML_CLOCK_ID_COUNT              = 4\n\n_nvmlDriverModel_t = c_uint\nNVML_DRIVER_WDDM       = 0\nNVML_DRIVER_WDM        = 1\nNVML_DRIVER_MCDM       = 2\n\nNVML_MAX_GPU_PERF_PSTATES = 16\n\n_nvmlPstates_t = c_uint\nNVML_PSTATE_0               = 0\nNVML_PSTATE_1               = 1\nNVML_PSTATE_2               = 2\nNVML_PSTATE_3               = 3\nNVML_PSTATE_4               = 4\nNVML_PSTATE_5               = 5\nNVML_PSTATE_6               = 6\nNVML_PSTATE_7               = 7\nNVML_PSTATE_8               = 8\nNVML_PSTATE_9               = 9\nNVML_PSTATE_10              = 10\nNVML_PSTATE_11              = 11\nNVML_PSTATE_12              = 12\nNVML_PSTATE_13              = 13\nNVML_PSTATE_14              = 14\nNVML_PSTATE_15              = 15\nNVML_PSTATE_UNKNOWN         = 32\n\n_nvmlInforomObject_t = c_uint\nNVML_INFOROM_OEM            = 0\nNVML_INFOROM_ECC            = 1\nNVML_INFOROM_POWER          = 2\nNVML_INFOROM_DEN            = 3\nNVML_INFOROM_COUNT          = 4\n\n_nvmlReturn_t = c_uint\nNVML_SUCCESS                         = 0\nNVML_ERROR_UNINITIALIZED             = 1\nNVML_ERROR_INVALID_ARGUMENT          = 2\nNVML_ERROR_NOT_SUPPORTED             = 3\nNVML_ERROR_NO_PERMISSION             = 4\nNVML_ERROR_ALREADY_INITIALIZED       = 5\nNVML_ERROR_NOT_FOUND                 = 6\nNVML_ERROR_INSUFFICIENT_SIZE         = 7\nNVML_ERROR_INSUFFICIENT_POWER        = 8\nNVML_ERROR_DRIVER_NOT_LOADED         = 9\nNVML_ERROR_TIMEOUT                   = 10\nNVML_ERROR_IRQ_ISSUE                 = 11\nNVML_ERROR_LIBRARY_NOT_FOUND         = 12\nNVML_ERROR_FUNCTION_NOT_FOUND        = 13\nNVML_ERROR_CORRUPTED_INFOROM         = 14\nNVML_ERROR_GPU_IS_LOST               = 15\nNVML_ERROR_RESET_REQUIRED            = 16\nNVML_ERROR_OPERATING_SYSTEM          = 17\nNVML_ERROR_LIB_RM_VERSION_MISMATCH   = 18\nNVML_ERROR_IN_USE                    = 19\nNVML_ERROR_MEMORY                    = 20\nNVML_ERROR_NO_DATA                   = 21\nNVML_ERROR_VGPU_ECC_NOT_SUPPORTED    = 22\nNVML_ERROR_INSUFFICIENT_RESOURCES    = 23\nNVML_ERROR_FREQ_NOT_SUPPORTED        = 24\nNVML_ERROR_ARGUMENT_VERSION_MISMATCH = 25\nNVML_ERROR_DEPRECATED                = 26\nNVML_ERROR_NOT_READY                 = 27\nNVML_ERROR_GPU_NOT_FOUND             = 28\nNVML_ERROR_INVALID_STATE             = 29\nNVML_ERROR_UNKNOWN                   = 999\n\n_nvmlFanState_t = c_uint\nNVML_FAN_NORMAL             = 0\nNVML_FAN_FAILED             = 1\n\n_nvmlFanControlPolicy_t = c_uint\nNVML_FAN_POLICY_TEMPERATURE_CONTINOUS_SW = 0\nNVML_FAN_POLICY_MANUAL                   = 1\n\n_nvmlLedColor_t = c_uint\nNVML_LED_COLOR_GREEN        = 0\nNVML_LED_COLOR_AMBER        = 1\n\n_nvmlGpuOperationMode_t = c_uint\nNVML_GOM_ALL_ON                 = 0\nNVML_GOM_COMPUTE                = 1\nNVML_GOM_LOW_DP                 = 2\n\n_nvmlPageRetirementCause_t = c_uint\nNVML_PAGE_RETIREMENT_CAUSE_MULTIPLE_SINGLE_BIT_ECC_ERRORS = 0\nNVML_PAGE_RETIREMENT_CAUSE_DOUBLE_BIT_ECC_ERROR           = 1\nNVML_PAGE_RETIREMENT_CAUSE_COUNT                          = 2\n\n_nvmlRestrictedAPI_t = c_uint\nNVML_RESTRICTED_API_SET_APPLICATION_CLOCKS                = 0\nNVML_RESTRICTED_API_SET_AUTO_BOOSTED_CLOCKS               = 1\nNVML_RESTRICTED_API_COUNT                                 = 2\n\n_nvmlBridgeChipType_t = c_uint\nNVML_BRIDGE_CHIP_PLX = 0\nNVML_BRIDGE_CHIP_BRO4 = 1\nNVML_MAX_PHYSICAL_BRIDGE = 128\n\n_nvmlValueType_t = c_uint\nNVML_VALUE_TYPE_DOUBLE = 0\nNVML_VALUE_TYPE_UNSIGNED_INT = 1\nNVML_VALUE_TYPE_UNSIGNED_LONG = 2\nNVML_VALUE_TYPE_UNSIGNED_LONG_LONG = 3\nNVML_VALUE_TYPE_SIGNED_LONG_LONG = 4\nNVML_VALUE_TYPE_SIGNED_INT = 5\nNVML_VALUE_TYPE_UNSIGNED_SHORT = 6\nNVML_VALUE_TYPE_COUNT = 7\n\n_nvmlNvlinkVersion_t = c_uint\nNVML_NVLINK_VERSION_INVALID = 0\nNVML_NVLINK_VERSION_1_0 = 1\nNVML_NVLINK_VERSION_2_0 = 2\nNVML_NVLINK_VERSION_2_2 = 3\nNVML_NVLINK_VERSION_3_0 = 4\nNVML_NVLINK_VERSION_3_1 = 5\nNVML_NVLINK_VERSION_4_0 = 6\nNVML_NVLINK_VERSION_5_0 = 7\n\n_nvmlPerfPolicyType_t = c_uint\nNVML_PERF_POLICY_POWER = 0\nNVML_PERF_POLICY_THERMAL = 1\nNVML_PERF_POLICY_SYNC_BOOST = 2\nNVML_PERF_POLICY_BOARD_LIMIT = 3\nNVML_PERF_POLICY_LOW_UTILIZATION = 4\nNVML_PERF_POLICY_RELIABILITY = 5\nNVML_PERF_POLICY_TOTAL_APP_CLOCKS = 10\nNVML_PERF_POLICY_TOTAL_BASE_CLOCKS = 11\nNVML_PERF_POLICY_COUNT = 12\n\n_nvmlEncoderQueryType_t = c_uint\nNVML_ENCODER_QUERY_H264 = 0\nNVML_ENCODER_QUERY_HEVC = 1\nNVML_ENCODER_QUERY_AV1 = 2\nNVML_ENCODER_QUERY_UNKNOWN = 255\n\n_nvmlFBCSessionType_t = c_uint\nNVML_FBC_SESSION_TYPE_UNKNOWN = 0\nNVML_FBC_SESSION_TYPE_TOSYS = 1\nNVML_FBC_SESSION_TYPE_CUDA = 2\nNVML_FBC_SESSION_TYPE_VID = 3\nNVML_FBC_SESSION_TYPE_HWENC = 4\n\n_nvmlDetachGpuState_t = c_uint\nNVML_DETACH_GPU_KEEP = 0\nNVML_DETACH_GPU_REMOVE = 1\n\n_nvmlPcieLinkState_t = c_uint\nNVML_PCIE_LINK_KEEP = 0\nNVML_PCIE_LINK_SHUT_DOWN = 1\n\n_nvmlSamplingType_t = c_uint\nNVML_TOTAL_POWER_SAMPLES = 0\nNVML_GPU_UTILIZATION_SAMPLES = 1\nNVML_MEMORY_UTILIZATION_SAMPLES = 2\nNVML_ENC_UTILIZATION_SAMPLES = 3\nNVML_DEC_UTILIZATION_SAMPLES = 4\nNVML_PROCESSOR_CLK_SAMPLES = 5\nNVML_MEMORY_CLK_SAMPLES = 6\nNVML_MODULE_POWER_SAMPLES = 7\nNVML_JPG_UTILIZATION_SAMPLES = 8\nNVML_OFA_UTILIZATION_SAMPLES = 9\nNVML_SAMPLINGTYPE_COUNT = 10\n\n_nvmlPcieUtilCounter_t = c_uint\nNVML_PCIE_UTIL_TX_BYTES = 0\nNVML_PCIE_UTIL_RX_BYTES = 1\nNVML_PCIE_UTIL_COUNT = 2\n\n_nvmlGpuTopologyLevel_t = c_uint\nNVML_TOPOLOGY_INTERNAL = 0\nNVML_TOPOLOGY_SINGLE = 10\nNVML_TOPOLOGY_MULTIPLE = 20\nNVML_TOPOLOGY_HOSTBRIDGE = 30\nNVML_TOPOLOGY_NODE = 40\nNVML_TOPOLOGY_CPU = NVML_TOPOLOGY_NODE\nNVML_TOPOLOGY_SYSTEM = 50\n\n_nvmlGpuP2PCapsIndex_t = c_uint\nNVML_P2P_CAPS_INDEX_READ = 0,\nNVML_P2P_CAPS_INDEX_WRITE = 1\nNVML_P2P_CAPS_INDEX_NVLINK =2\nNVML_P2P_CAPS_INDEX_ATOMICS = 3\n#\n# NVML_P2P_CAPS_INDEX_PROP is deprecated.\n# Use NVML_P2P_CAPS_INDEX_PCI instead.\n#\nNVML_P2P_CAPS_INDEX_PROP = 4\nNVML_P2P_CAPS_INDEX_PCI = 4\nNVML_P2P_CAPS_INDEX_UNKNOWN = 5\n\n_nvmlGpuP2PStatus_t = c_uint\nNVML_P2P_STATUS_OK     = 0\nNVML_P2P_STATUS_CHIPSET_NOT_SUPPORED = 1\nNVML_P2P_STATUS_CHIPSET_NOT_SUPPORTED = NVML_P2P_STATUS_CHIPSET_NOT_SUPPORED\nNVML_P2P_STATUS_GPU_NOT_SUPPORTED = 2\nNVML_P2P_STATUS_IOH_TOPOLOGY_NOT_SUPPORTED =3\nNVML_P2P_STATUS_DISABLED_BY_REGKEY =4\nNVML_P2P_STATUS_NOT_SUPPORTED =5\nNVML_P2P_STATUS_UNKNOWN =6\n\n_nvmlDeviceArchitecture_t = c_uint\nNVML_DEVICE_ARCH_KEPLER   = 2\nNVML_DEVICE_ARCH_MAXWELL  = 3\nNVML_DEVICE_ARCH_PASCAL   = 4\nNVML_DEVICE_ARCH_VOLTA    = 5\nNVML_DEVICE_ARCH_TURING   = 6\nNVML_DEVICE_ARCH_AMPERE   = 7\nNVML_DEVICE_ARCH_ADA      = 8\nNVML_DEVICE_ARCH_HOPPER   = 9\nNVML_DEVICE_ARCH_BLACKWELL   = 10\nNVML_DEVICE_ARCH_T23X     = 11\nNVML_DEVICE_ARCH_UNKNOWN  = 0xffffffff\n\n# PCI bus Types\n_nvmlBusType_t = c_uint\nNVML_BUS_TYPE_UNKNOWN = 0\nNVML_BUS_TYPE_PCI     = 1\nNVML_BUS_TYPE_PCIE    = 2\nNVML_BUS_TYPE_FPCI    = 3\nNVML_BUS_TYPE_AGP     = 4\n\n_nvmlPowerSource_t = c_uint\nNVML_POWER_SOURCE_AC         = 0x00000000\nNVML_POWER_SOURCE_BATTERY    = 0x00000001\nNVML_POWER_SOURCE_UNDERSIZED = 0x00000002\n\n_nvmlAdaptiveClockInfoStatus_t = c_uint\nNVML_ADAPTIVE_CLOCKING_INFO_STATUS_DISABLED = 0x00000000\nNVML_ADAPTIVE_CLOCKING_INFO_STATUS_ENABLED = 0x00000001\n\n_nvmlClockLimitId_t = c_uint\nNVML_CLOCK_LIMIT_ID_RANGE_START = 0xffffff00\nNVML_CLOCK_LIMIT_ID_TDP         = 0xffffff01\nNVML_CLOCK_LIMIT_ID_UNLIMITED   = 0xffffff02\n\n_nvmlPcieLinkMaxSpeed_t = c_uint\nNVML_PCIE_LINK_MAX_SPEED_INVALID   = 0x00000000\nNVML_PCIE_LINK_MAX_SPEED_2500MBPS  = 0x00000001\nNVML_PCIE_LINK_MAX_SPEED_5000MBPS  = 0x00000002\nNVML_PCIE_LINK_MAX_SPEED_8000MBPS  = 0x00000003\nNVML_PCIE_LINK_MAX_SPEED_16000MBPS = 0x00000004\nNVML_PCIE_LINK_MAX_SPEED_32000MBPS = 0x00000005\nNVML_PCIE_LINK_MAX_SPEED_64000MBPS = 0x00000006\n\n_nvmlPcieAtomicsCapability_t = c_uint\nNVML_PCIE_ATOMICS_CAP_FETCHADD32  = 0x01\nNVML_PCIE_ATOMICS_CAP_FETCHADD64  = 0x02\nNVML_PCIE_ATOMICS_CAP_SWAP32      = 0x04\nNVML_PCIE_ATOMICS_CAP_SWAP64      = 0x08\nNVML_PCIE_ATOMICS_CAP_CAS32       = 0x10\nNVML_PCIE_ATOMICS_CAP_CAS64       = 0x20\nNVML_PCIE_ATOMICS_CAP_CAS128      = 0x40\nNVML_PCIE_ATOMICS_OPS_MAX         = 7\n\n_nvmlAffinityScope_t = c_uint\nNVML_AFFINITY_SCOPE_NODE   = 0\nNVML_AFFINITY_SCOPE_SOCKET = 1\n\n_nvmlDeviceGpuRecoveryAction_t = c_uint\nNVML_GPU_RECOVERY_ACTION_NONE        = 0\nNVML_GPU_RECOVERY_ACTION_GPU_RESET   = 1\nNVML_GPU_RECOVERY_ACTION_NODE_REBOOT = 2\nNVML_GPU_RECOVERY_ACTION_DRAIN_P2P   = 3\nNVML_GPU_RECOVERY_ACTION_DRAIN_AND_RESET = 4\n\n# C preprocessor defined values\nnvmlFlagDefault             = 0\nnvmlFlagForce               = 1\nNVML_INIT_FLAG_NO_GPUS      = 1\nNVML_INIT_FLAG_NO_ATTACH    = 2\n\nNVML_MAX_GPC_COUNT          = 32\n\n# buffer size\nNVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE      = 16\nNVML_DEVICE_UUID_BUFFER_SIZE                 = 80\nNVML_DEVICE_UUID_V2_BUFFER_SIZE              = 96\nNVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE       = 80\nNVML_SYSTEM_NVML_VERSION_BUFFER_SIZE         = 80\nNVML_DEVICE_NAME_BUFFER_SIZE                 = 64\nNVML_DEVICE_NAME_V2_BUFFER_SIZE              = 96\nNVML_DEVICE_SERIAL_BUFFER_SIZE               = 30\nNVML_DEVICE_PART_NUMBER_BUFFER_SIZE          = 80\nNVML_DEVICE_GPU_PART_NUMBER_BUFFER_SIZE      = 80\nNVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE        = 32\nNVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE           = 32\nNVML_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE        = 16\nNVML_GRID_LICENSE_BUFFER_SIZE                = 128\nNVML_VGPU_NAME_BUFFER_SIZE                   = 64\nNVML_GRID_LICENSE_FEATURE_MAX_COUNT          = 3\nNVML_VGPU_METADATA_OPAQUE_DATA_SIZE          = sizeof(c_uint) + 256\nNVML_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE     = 256\nNVML_DEVICE_GPU_FRU_PART_NUMBER_BUFFER_SIZE  = 0x14 # NV2080_GPU_MAX_PRODUCT_PART_NUMBER_LENGTH\nNVML_PERF_MODES_BUFFER_SIZE                  = 2048\n\n# Format strings\nNVML_DEVICE_PCI_BUS_ID_LEGACY_FMT   = \"%04X:%02X:%02X.0\"\nNVML_DEVICE_PCI_BUS_ID_FMT          = \"%08X:%02X:%02X.0\"\n\nNVML_VALUE_NOT_AVAILABLE_ulonglong = c_ulonglong(-1)\nNVML_VALUE_NOT_AVAILABLE_uint = c_uint(-1)\n\n'''\n Field Identifiers.\n\n All Identifiers pertain to a device. Each ID is only used once and is guaranteed never to change.\n'''\nNVML_FI_DEV_ECC_CURRENT          = 1   # Current ECC mode. 1=Active. 0=Inactive\nNVML_FI_DEV_ECC_PENDING          = 2   # Pending ECC mode. 1=Active. 0=Inactive\n\n#ECC Count Totals\nNVML_FI_DEV_ECC_SBE_VOL_TOTAL    = 3   # Total single bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_TOTAL    = 4   # Total double bit volatile ECC errors\nNVML_FI_DEV_ECC_SBE_AGG_TOTAL    = 5   # Total single bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_TOTAL    = 6   # Total double bit aggregate (persistent) ECC errors\n#Individual ECC locations\nNVML_FI_DEV_ECC_SBE_VOL_L1       = 7   # L1 cache single bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_L1       = 8   # L1 cache double bit volatile ECC errors\nNVML_FI_DEV_ECC_SBE_VOL_L2       = 9   # L2 cache single bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_L2       = 10  # L2 cache double bit volatile ECC errors\nNVML_FI_DEV_ECC_SBE_VOL_DEV      = 11  # Device memory single bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_DEV      = 12  # Device memory double bit volatile ECC errors\nNVML_FI_DEV_ECC_SBE_VOL_REG      = 13  # Register file single bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_REG      = 14  # Register file double bit volatile ECC errors\nNVML_FI_DEV_ECC_SBE_VOL_TEX      = 15  # Texture memory single bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_TEX      = 16  # Texture memory double bit volatile ECC errors\nNVML_FI_DEV_ECC_DBE_VOL_CBU      = 17  # CBU double bit volatile ECC errors\nNVML_FI_DEV_ECC_SBE_AGG_L1       = 18  # L1 cache single bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_L1       = 19  # L1 cache double bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_SBE_AGG_L2       = 20  # L2 cache single bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_L2       = 21  # L2 cache double bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_SBE_AGG_DEV      = 22  # Device memory single bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_DEV      = 23  # Device memory double bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_SBE_AGG_REG      = 24  # Register File single bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_REG      = 25  # Register File double bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_SBE_AGG_TEX      = 26  # Texture memory single bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_TEX      = 27  # Texture memory double bit aggregate (persistent) ECC errors\nNVML_FI_DEV_ECC_DBE_AGG_CBU      = 28  # CBU double bit aggregate ECC errors\n\n# Page Retirement\nNVML_FI_DEV_RETIRED_SBE          = 29  # Number of retired pages because of single bit errors\nNVML_FI_DEV_RETIRED_DBE          = 30  # Number of retired pages because of double bit errors\nNVML_FI_DEV_RETIRED_PENDING      = 31  # If any pages are pending retirement. 1=yes. 0=no.\n\n# NvLink Flit Error Counters\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L0   = 32 # NVLink flow control CRC  Error Counter for Lane 0\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L1   = 33 # NVLink flow control CRC  Error Counter for Lane 1\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L2   = 34 # NVLink flow control CRC  Error Counter for Lane 2\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L3   = 35 # NVLink flow control CRC  Error Counter for Lane 3\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L4   = 36 # NVLink flow control CRC  Error Counter for Lane 4\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L5   = 37 # NVLink flow control CRC  Error Counter for Lane 5\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_TOTAL = 38 # NVLink flow control CRC  Error Counter total for all Lanes\n\n# NvLink CRC Data Error Counters\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L0   = 39 # NVLink data CRC Error Counter for Lane 0\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L1   = 40 # NVLink data CRC Error Counter for Lane 1\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L2   = 41 # NVLink data CRC Error Counter for Lane 2\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L3   = 42 # NVLink data CRC Error Counter for Lane 3\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L4   = 43 # NVLink data CRC Error Counter for Lane 4\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L5   = 44 # NVLink data CRC Error Counter for Lane 5\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_TOTAL = 45 # NvLink data CRC Error Counter total for all Lanes\n\n# NvLink Replay Error Counters\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L0     = 46 # NVLink Replay Error Counter for Lane 0\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L1     = 47 # NVLink Replay Error Counter for Lane 1\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L2     = 48 # NVLink Replay Error Counter for Lane 2\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L3     = 49 # NVLink Replay Error Counter for Lane 3\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L4     = 50 # NVLink Replay Error Counter for Lane 4\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L5     = 51 # NVLink Replay Error Counter for Lane 5\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_TOTAL  = 52 # NVLink Replay Error Counter total for all Lanes\n\n# NvLink Recovery Error Counters\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L0   = 53 # NVLink Recovery Error Counter for Lane 0\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L1   = 54 # NVLink Recovery Error Counter for Lane 1\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L2   = 55 # NVLink Recovery Error Counter for Lane 2\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L3   = 56 # NVLink Recovery Error Counter for Lane 3\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L4   = 57 # NVLink Recovery Error Counter for Lane 4\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L5   = 58 # NVLink Recovery Error Counter for Lane 5\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_TOTAL = 59 # NVLink Recovery Error Counter total for all Lanes\n\n# NvLink Bandwidth Counters\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L0    = 60 # NVLink Bandwidth Counter for Counter Set 0, Lane 0\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L1    = 61 # NVLink Bandwidth Counter for Counter Set 0, Lane 1\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L2    = 62 # NVLink Bandwidth Counter for Counter Set 0, Lane 2\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L3    = 63 # NVLink Bandwidth Counter for Counter Set 0, Lane 3\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L4    = 64 # NVLink Bandwidth Counter for Counter Set 0, Lane 4\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L5    = 65 # NVLink Bandwidth Counter for Counter Set 0, Lane 5\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_TOTAL = 66 # NVLink Bandwidth Counter Total for Counter Set 0, All Lanes\n\n# NvLink Bandwidth Counters\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L0    = 67 # NVLink Bandwidth Counter for Counter Set 1, Lane 0\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L1    = 68 # NVLink Bandwidth Counter for Counter Set 1, Lane 1\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L2    = 69 # NVLink Bandwidth Counter for Counter Set 1, Lane 2\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L3    = 70 # NVLink Bandwidth Counter for Counter Set 1, Lane 3\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L4    = 71 # NVLink Bandwidth Counter for Counter Set 1, Lane 4\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L5    = 72 # NVLink Bandwidth Counter for Counter Set 1, Lane 5\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_TOTAL = 73 # NVLink Bandwidth Counter Total for Counter Set 1, All Lanes\n\n# Perf Policy Counters\nNVML_FI_DEV_PERF_POLICY_POWER             = 74   # Perf Policy Counter for Power Policy\nNVML_FI_DEV_PERF_POLICY_THERMAL           = 75   # Perf Policy Counter for Thermal Policy\nNVML_FI_DEV_PERF_POLICY_SYNC_BOOST        = 76   # Perf Policy Counter for Sync boost Policy\nNVML_FI_DEV_PERF_POLICY_BOARD_LIMIT       = 77   # Perf Policy Counter for Board Limit\nNVML_FI_DEV_PERF_POLICY_LOW_UTILIZATION   = 78   # Perf Policy Counter for Low GPU Utilization Policy\nNVML_FI_DEV_PERF_POLICY_RELIABILITY       = 79   # Perf Policy Counter for Reliability Policy\nNVML_FI_DEV_PERF_POLICY_TOTAL_APP_CLOCKS  = 80   # Perf Policy Counter for Total App Clock Policy\nNVML_FI_DEV_PERF_POLICY_TOTAL_BASE_CLOCKS = 81   # Perf Policy Counter for Total Base Clocks Policy\n\n# Memory temperatures\nNVML_FI_DEV_MEMORY_TEMP  = 82 # Memory temperature for the device\n\n# Energy Counter\nNVML_FI_DEV_TOTAL_ENERGY_CONSUMPTION = 83 # Total energy consumption for the GPU in mJ since the driver was last reloaded\n\n# NVLink Speed\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L0     = 84\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L1     = 85\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L2     = 86\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L3     = 87\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L4     = 88\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L5     = 89\nNVML_FI_DEV_NVLINK_SPEED_MBPS_COMMON = 90\n\n# NVLink Link Count\nNVML_FI_DEV_NVLINK_LINK_COUNT = 91\n\n# Page Retirement pending fields\nNVML_FI_DEV_RETIRED_PENDING_SBE = 92\nNVML_FI_DEV_RETIRED_PENDING_DBE = 93\n\n# PCIe replay and replay rollover counters\nNVML_FI_DEV_PCIE_REPLAY_COUNTER = 94\nNVML_FI_DEV_PCIE_REPLAY_ROLLOVER_COUNTER = 95\n\n# NvLink Flit Error Counters\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L6   = 96 # NVLink flow control CRC  Error Counter for Lane 6\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L7   = 97 # NVLink flow control CRC  Error Counter for Lane 7\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L8   = 98 # NVLink flow control CRC  Error Counter for Lane 8\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L9   = 99 # NVLink flow control CRC  Error Counter for Lane 9\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L10  = 100 # NVLink flow control CRC  Error Counter for Lane 10\nNVML_FI_DEV_NVLINK_CRC_FLIT_ERROR_COUNT_L11  = 101 # NVLink flow control CRC  Error Counter for Lane 11\n\n# NvLink CRC Data Error Counters\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L6   = 102 # NVLink data CRC Error Counter for Lane 6\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L7   = 103 # NVLink data CRC Error Counter for Lane 7\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L8   = 104 # NVLink data CRC Error Counter for Lane 8\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L9   = 105 # NVLink data CRC Error Counter for Lane 9\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L10  = 106 # NVLink data CRC Error Counter for Lane 10\nNVML_FI_DEV_NVLINK_CRC_DATA_ERROR_COUNT_L11  = 107 # NVLink data CRC Error Counter for Lane 11\n\n# NvLink Replay Error Counters\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L6     = 108 # NVLink Replay Error Counter for Lane 6\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L7     = 109 # NVLink Replay Error Counter for Lane 7\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L8     = 110 # NVLink Replay Error Counter for Lane 8\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L9     = 111 # NVLink Replay Error Counter for Lane 9\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L10    = 112 # NVLink Replay Error Counter for Lane 10\nNVML_FI_DEV_NVLINK_REPLAY_ERROR_COUNT_L11    = 113 # NVLink Replay Error Counter for Lane 11\n\n# NvLink Recovery Error Counters\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L6   = 114 # NVLink Recovery Error Counter for Lane 6\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L7   = 115 # NVLink Recovery Error Counter for Lane 7\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L8   = 116 # NVLink Recovery Error Counter for Lane 8\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L9   = 117 # NVLink Recovery Error Counter for Lane 9\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L10  = 118 # NVLink Recovery Error Counter for Lane 10\nNVML_FI_DEV_NVLINK_RECOVERY_ERROR_COUNT_L11  = 119 # NVLink Recovery Error Counter for Lane 11\n\n# NvLink Bandwidth Counters\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L6    = 120 # NVLink Bandwidth Counter for Counter Set 0, Lane 6\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L7    = 121 # NVLink Bandwidth Counter for Counter Set 0, Lane 7\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L8    = 122 # NVLink Bandwidth Counter for Counter Set 0, Lane 8\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L9    = 123 # NVLink Bandwidth Counter for Counter Set 0, Lane 9\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L10   = 124 # NVLink Bandwidth Counter for Counter Set 0, Lane 10\nNVML_FI_DEV_NVLINK_BANDWIDTH_C0_L11   = 125 # NVLink Bandwidth Counter for Counter Set 0, Lane 11\n\n# NvLink Bandwidth Counters\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L6    = 126 # NVLink Bandwidth Counter for Counter Set 1, Lane 6\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L7    = 127 # NVLink Bandwidth Counter for Counter Set 1, Lane 7\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L8    = 128 # NVLink Bandwidth Counter for Counter Set 1, Lane 8\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L9    = 129 # NVLink Bandwidth Counter for Counter Set 1, Lane 9\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L10   = 130 # NVLink Bandwidth Counter for Counter Set 1, Lane 10\nNVML_FI_DEV_NVLINK_BANDWIDTH_C1_L11   = 131 # NVLink Bandwidth Counter for Counter Set 1, Lane 11\n\n# NVLink Speed\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L6     = 132\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L7     = 133\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L8     = 134\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L9     = 135\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L10    = 136\nNVML_FI_DEV_NVLINK_SPEED_MBPS_L11    = 137\n\n# NVLink Throughput Counters\nNVML_FI_DEV_NVLINK_THROUGHPUT_DATA_TX = 138 # NVLink TX Data throughput in KiB\nNVML_FI_DEV_NVLINK_THROUGHPUT_DATA_RX = 139 # NVLink RX Data throughput in KiB\nNVML_FI_DEV_NVLINK_THROUGHPUT_RAW_TX  = 140 # NVLink TX Data + protocol overhead in KiB\nNVML_FI_DEV_NVLINK_THROUGHPUT_RAW_RX  = 141 # NVLink RX Data + protocol overhead in KiB\n\n# Row Remapper\nNVML_FI_DEV_REMAPPED_COR        = 142\nNVML_FI_DEV_REMAPPED_UNC        = 143\nNVML_FI_DEV_REMAPPED_PENDING    = 144\nNVML_FI_DEV_REMAPPED_FAILURE    = 145\n\n#Remote device NVLink ID\nNVML_FI_DEV_NVLINK_REMOTE_NVLINK_ID = 146\n\n# Number of NVLinks connected to NVSwitch\nNVML_FI_DEV_NVSWITCH_CONNECTED_LINK_COUNT = 147\n\n# NvLink ECC Data Error Counters\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L0    = 148 #< NVLink data ECC Error Counter for Link 0\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L1    = 149 #< NVLink data ECC Error Counter for Link 1\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L2    = 150 #< NVLink data ECC Error Counter for Link 2\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L3    = 151 #< NVLink data ECC Error Counter for Link 3\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L4    = 152 #< NVLink data ECC Error Counter for Link 4\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L5    = 153 #< NVLink data ECC Error Counter for Link 5\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L6    = 154 #< NVLink data ECC Error Counter for Link 6\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L7    = 155 #< NVLink data ECC Error Counter for Link 7\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L8    = 156 #< NVLink data ECC Error Counter for Link 8\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L9    = 157 #< NVLink data ECC Error Counter for Link 9\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L10   = 158 #< NVLink data ECC Error Counter for Link 10\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_L11   = 159 #< NVLink data ECC Error Counter for Link 11\nNVML_FI_DEV_NVLINK_ECC_DATA_ERROR_COUNT_TOTAL = 160 #< NvLink data ECC Error Counter total for all Links\n\nNVML_FI_DEV_NVLINK_ERROR_DL_REPLAY            = 161\nNVML_FI_DEV_NVLINK_ERROR_DL_RECOVERY          = 162\nNVML_FI_DEV_NVLINK_ERROR_DL_CRC               = 163\nNVML_FI_DEV_NVLINK_GET_SPEED                  = 164\nNVML_FI_DEV_NVLINK_GET_STATE                  = 165\nNVML_FI_DEV_NVLINK_GET_VERSION                = 166\n\nNVML_FI_DEV_NVLINK_GET_POWER_STATE            = 167\nNVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD        = 168\n\nNVML_FI_DEV_PCIE_L0_TO_RECOVERY_COUNTER       = 169\n\nNVML_FI_DEV_C2C_LINK_COUNT                    = 170\nNVML_FI_DEV_C2C_LINK_GET_STATUS               = 171\nNVML_FI_DEV_C2C_LINK_GET_MAX_BW               = 172\n\nNVML_FI_DEV_PCIE_COUNT_CORRECTABLE_ERRORS     = 173\nNVML_FI_DEV_PCIE_COUNT_NAKS_RECEIVED          = 174\nNVML_FI_DEV_PCIE_COUNT_RECEIVER_ERROR         = 175\nNVML_FI_DEV_PCIE_COUNT_BAD_TLP                = 176\nNVML_FI_DEV_PCIE_COUNT_NAKS_SENT              = 177\nNVML_FI_DEV_PCIE_COUNT_BAD_DLLP               = 178\nNVML_FI_DEV_PCIE_COUNT_NON_FATAL_ERROR        = 179\nNVML_FI_DEV_PCIE_COUNT_FATAL_ERROR            = 180\nNVML_FI_DEV_PCIE_COUNT_UNSUPPORTED_REQ        = 181\nNVML_FI_DEV_PCIE_COUNT_LCRC_ERROR             = 182\nNVML_FI_DEV_PCIE_COUNT_LANE_ERROR             = 183\n\nNVML_FI_DEV_IS_RESETLESS_MIG_SUPPORTED        = 184\n\nNVML_FI_DEV_POWER_AVERAGE                     = 185\nNVML_FI_DEV_POWER_INSTANT                     = 186\nNVML_FI_DEV_POWER_MIN_LIMIT                   = 187\nNVML_FI_DEV_POWER_MAX_LIMIT                   = 188\nNVML_FI_DEV_POWER_DEFAULT_LIMIT               = 189\nNVML_FI_DEV_POWER_CURRENT_LIMIT               = 190\nNVML_FI_DEV_ENERGY                            = 191\nNVML_FI_DEV_POWER_REQUESTED_LIMIT             = 192\n\nNVML_FI_DEV_TEMPERATURE_SHUTDOWN_TLIMIT       = 193\nNVML_FI_DEV_TEMPERATURE_SLOWDOWN_TLIMIT       = 194\nNVML_FI_DEV_TEMPERATURE_MEM_MAX_TLIMIT        = 195\nNVML_FI_DEV_TEMPERATURE_GPU_MAX_TLIMIT        = 196\n\nNVML_FI_DEV_PCIE_COUNT_TX_BYTES               = 197\nNVML_FI_DEV_PCIE_COUNT_RX_BYTES               = 198\n\nNVML_FI_DEV_IS_MIG_MODE_INDEPENDENT_MIG_QUERY_CAPABLE   = 199\n\nNVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD_MAX              = 200\n\nNVML_FI_DEV_NVLINK_COUNT_XMIT_PACKETS                    = 201\nNVML_FI_DEV_NVLINK_COUNT_XMIT_BYTES                      = 202\nNVML_FI_DEV_NVLINK_COUNT_RCV_PACKETS                     = 203\nNVML_FI_DEV_NVLINK_COUNT_RCV_BYTES                       = 204\nNVML_FI_DEV_NVLINK_COUNT_VL15_DROPPED                    = 205 # Deprecated, do not use\nNVML_FI_DEV_NVLINK_COUNT_MALFORMED_PACKET_ERRORS         = 206\nNVML_FI_DEV_NVLINK_COUNT_BUFFER_OVERRUN_ERRORS           = 207\nNVML_FI_DEV_NVLINK_COUNT_RCV_ERRORS                      = 208\nNVML_FI_DEV_NVLINK_COUNT_RCV_REMOTE_ERRORS               = 209\nNVML_FI_DEV_NVLINK_COUNT_RCV_GENERAL_ERRORS              = 210\nNVML_FI_DEV_NVLINK_COUNT_LOCAL_LINK_INTEGRITY_ERRORS     = 211\nNVML_FI_DEV_NVLINK_COUNT_XMIT_DISCARDS                   = 212\n\nNVML_FI_DEV_NVLINK_COUNT_LINK_RECOVERY_SUCCESSFUL_EVENTS = 213\nNVML_FI_DEV_NVLINK_COUNT_LINK_RECOVERY_FAILED_EVENTS     = 214\nNVML_FI_DEV_NVLINK_COUNT_LINK_RECOVERY_EVENTS            = 215\n\nNVML_FI_DEV_NVLINK_COUNT_RAW_BER_LANE0                   = 216  # Deprecated, do not use\nNVML_FI_DEV_NVLINK_COUNT_RAW_BER_LANE1                   = 217  # Deprecated, do not use\nNVML_FI_DEV_NVLINK_COUNT_RAW_BER                         = 218  # Deprecated, do not use\nNVML_FI_DEV_NVLINK_COUNT_EFFECTIVE_ERRORS                = 219\nNVML_FI_DEV_NVLINK_COUNT_EFFECTIVE_BER                   = 220\nNVML_FI_DEV_NVLINK_COUNT_SYMBOL_ERRORS                   = 221\nNVML_FI_DEV_NVLINK_COUNT_SYMBOL_BER                      = 222\n\nNVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD_MIN               = 223\nNVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD_UNITS             = 224 # Values are in the form NVML_NVLINK_LOW_POWER_THRESHOLD_UNIT_*\nNVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD_SUPPORTED         = 225\n\nNVML_FI_DEV_RESET_STATUS                                 = 226 # Deprecated use NVML_FI_DEV_GET_GPU_RECOVERY_ACTION instead \nNVML_FI_DEV_DRAIN_AND_RESET_STATUS                       = 227 # Deprecated use NVML_FI_DEV_GET_GPU_RECOVERY_ACTION instead\nNVML_FI_DEV_PCIE_OUTBOUND_ATOMICS_MASK                   = 228\nNVML_FI_DEV_PCIE_INBOUND_ATOMICS_MASK                    = 229\nNVML_FI_DEV_GET_GPU_RECOVERY_ACTION                      = 230\n\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_0                   = 235\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_1                   = 236\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_2                   = 237\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_3                   = 238\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_4                   = 239\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_5                   = 240\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_6                   = 241\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_7                   = 242\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_8                   = 243\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_9                   = 244\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_10                  = 245\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_11                  = 246\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_12                  = 247\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_13                  = 248\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_14                  = 249\nNVML_FI_DEV_NVLINK_COUNT_FEC_HISTORY_15                  = 250\nNVML_FI_PWR_SMOOTHING_ENABLED                                   = 251 # Enablement (0/DISABLED or 1/ENABLED)\nNVML_FI_PWR_SMOOTHING_PRIV_LVL                                  = 252 # Current privilege level\nNVML_FI_PWR_SMOOTHING_IMM_RAMP_DOWN_ENABLED                     = 253 # Immediate ramp down enablement (0/DISABLED or 1/ENABLED)\nNVML_FI_PWR_SMOOTHING_APPLIED_TMP_CEIL                          = 254 # Applied TMP ceiling value\nNVML_FI_PWR_SMOOTHING_APPLIED_TMP_FLOOR                         = 255 # Applied TMP floor value\nNVML_FI_PWR_SMOOTHING_MAX_PERCENT_TMP_FLOOR_SETTING             = 256 # Max % TMP Floor value\nNVML_FI_PWR_SMOOTHING_MIN_PERCENT_TMP_FLOOR_SETTING             = 257 # Min % TMP Floor value\nNVML_FI_PWR_SMOOTHING_HW_CIRCUITRY_PERCENT_LIFETIME_REMAINING   = 258 # HW Circuitry % lifetime remaining\nNVML_FI_PWR_SMOOTHING_MAX_NUM_PRESET_PROFILES                   = 259 # Max number of preset profiles\nNVML_FI_PWR_SMOOTHING_PROFILE_PERCENT_TMP_FLOOR                 = 260 # % TMP floor for a given profile\nNVML_FI_PWR_SMOOTHING_PROFILE_RAMP_UP_RATE                      = 261 # Ramp up rate in mW/s for a given profile\nNVML_FI_PWR_SMOOTHING_PROFILE_RAMP_DOWN_RATE                    = 262 # Ramp down rate in mW/s for a given profile\nNVML_FI_PWR_SMOOTHING_PROFILE_RAMP_DOWN_HYST_VAL                = 263 # Ramp down hysteresis value in ms for a given profile\nNVML_FI_PWR_SMOOTHING_ACTIVE_PRESET_PROFILE                     = 264 # Active preset profile number\nNVML_FI_PWR_SMOOTHING_ADMIN_OVERRIDE_PERCENT_TMP_FLOOR          = 265 # % TMP floor for a given profile\nNVML_FI_PWR_SMOOTHING_ADMIN_OVERRIDE_RAMP_UP_RATE               = 266 # Ramp up rate in mW/s for a given profile\nNVML_FI_PWR_SMOOTHING_ADMIN_OVERRIDE_RAMP_DOWN_RATE             = 267 # Ramp down rate in mW/s for a given profile\nNVML_FI_PWR_SMOOTHING_ADMIN_OVERRIDE_RAMP_DOWN_HYST_VAL         = 268 # Ramp down hysteresis value in ms for a given profile\n\nNVML_FI_MAX = 269 # One greater than the largest field ID defined above\n\n# NVML_FI_DEV_NVLINK_GET_STATE state enums\nNVML_NVLINK_STATE_INACTIVE = 0x0\nNVML_NVLINK_STATE_ACTIVE   = 0x1\nNVML_NVLINK_STATE_SLEEP    = 0x2\n\nNVML_NVLINK_LOW_POWER_THRESHOLD_UNIT_100US = 0 # NVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD_UNITS\nNVML_NVLINK_LOW_POWER_THRESHOLD_UNIT_50US  = 1 # NVML_FI_DEV_NVLINK_GET_POWER_THRESHOLD_UNITS\n\n## Enums needed for the method nvmlDeviceGetVirtualizationMode and nvmlDeviceSetVirtualizationMode\nNVML_GPU_VIRTUALIZATION_MODE_NONE        = 0  # Represents Bare Metal GPU\nNVML_GPU_VIRTUALIZATION_MODE_PASSTHROUGH = 1  # Device is associated with GPU-Passthorugh\nNVML_GPU_VIRTUALIZATION_MODE_VGPU        = 2  # Device is associated with vGPU inside virtual machine.\nNVML_GPU_VIRTUALIZATION_MODE_HOST_VGPU   = 3  # Device is associated with VGX hypervisor in vGPU mode\nNVML_GPU_VIRTUALIZATION_MODE_HOST_VSGA   = 4  # Device is associated with VGX hypervisor in vSGA mode\n\n## Lib loading ##\nnvmlLib = None\nlibLoadLock = threading.Lock()\n_nvmlLib_refcount = 0 # Incremented on each nvmlInit and decremented on nvmlShutdown\n\n## vGPU Management\n_nvmlVgpuTypeId_t   = c_uint\n_nvmlVgpuInstance_t = c_uint\n\n_nvmlVgpuVmIdType_t = c_uint\nNVML_VGPU_VM_ID_DOMAIN_ID    = 0\nNVML_VGPU_VM_ID_UUID         = 1\n\n_nvmlGridLicenseFeatureCode_t = c_uint\nNVML_GRID_LICENSE_FEATURE_CODE_UNKNOWN      = 0\nNVML_GRID_LICENSE_FEATURE_CODE_VGPU         = 1\nNVML_GRID_LICENSE_FEATURE_CODE_NVIDIA_RTX   = 2\nNVML_GRID_LICENSE_FEATURE_CODE_VWORKSTATION = 2 # deprecated, use NVML_GRID_LICENSE_FEATURE_CODE_NVIDIA_RTX.\nNVML_GRID_LICENSE_FEATURE_CODE_GAMING       = 3\nNVML_GRID_LICENSE_FEATURE_CODE_COMPUTE      = 4\n\n_nvmlGridLicenseExpiryStatus_t = c_uint8\nNVML_GRID_LICENSE_EXPIRY_NOT_AVAILABLE    = 0,   # Expiry information not available\nNVML_GRID_LICENSE_EXPIRY_INVALID          = 1,   # Invalid expiry or error fetching expiry\nNVML_GRID_LICENSE_EXPIRY_VALID            = 2,   # Valid expiry\nNVML_GRID_LICENSE_EXPIRY_NOT_APPLICABLE   = 3,   # Expiry not applicable\nNVML_GRID_LICENSE_EXPIRY_PERMANENT        = 4,   # Permanent expiry\n\n_nvmlVgpuCapability_t = c_uint\nNVML_VGPU_CAP_NVLINK_P2P                    = 0  # vGPU P2P over NVLink is supported\nNVML_VGPU_CAP_GPUDIRECT                     = 1  # GPUDirect capability is supported\nNVML_VGPU_CAP_MULTI_VGPU_EXCLUSIVE          = 2  # vGPU profile cannot be mixed with other vGPU profiles in same VM\nNVML_VGPU_CAP_EXCLUSIVE_TYPE                = 3  # vGPU profile cannot run on a GPU alongside other profiles of different type\nNVML_VGPU_CAP_EXCLUSIVE_SIZE                = 4  # vGPU profile cannot run on a GPU alongside other profiles of different size\nNVML_VGPU_CAP_COUNT                         = 5\n\n_nvmlVgpuDriverCapability_t = c_uint\nNVML_VGPU_DRIVER_CAP_HETEROGENEOUS_MULTI_VGPU   = 0  # Supports mixing of different vGPU profiles within one guest VM\nNVML_VGPU_DRIVER_CAP_WARM_UPDATE                = 1  # Supports FSR and warm update of vGPU host driver without terminating the running guest VM\nNVML_VGPU_DRIVER_CAP_COUNT                      = 2\n\n_nvmlDeviceVgpuCapability_t = c_uint\nNVML_DEVICE_VGPU_CAP_FRACTIONAL_MULTI_VGPU             = 0  # Query whether the fractional vGPU profiles on this GPU can be used in multi-vGPU configurations\nNVML_DEVICE_VGPU_CAP_HETEROGENEOUS_TIMESLICE_PROFILES  = 1  # Query whether the GPU supports concurrent execution of timesliced vGPU profiles of differing types\nNVML_DEVICE_VGPU_CAP_HETEROGENEOUS_TIMESLICE_SIZES     = 2  # Query whether the GPU supports concurrent execution of timesliced vGPU profiles of differing framebuffer sizes\nNVML_DEVICE_VGPU_CAP_READ_DEVICE_BUFFER_BW             = 3  # Query the GPU's read_device_buffer expected bandwidth capacity in megabytes per second\nNVML_DEVICE_VGPU_CAP_WRITE_DEVICE_BUFFER_BW            = 4  # Query the GPU's write_device_buffer expected bandwidth capacity in megabytes per second\nNVML_DEVICE_VGPU_CAP_DEVICE_STREAMING                  = 5  # Query whether the vGPU profiles on the GPU supports migration data streaming\nNVML_DEVICE_VGPU_CAP_MINI_QUARTER_GPU                  = 6  # Set/Get support of mini-quarter vGPU profiles\nNVML_DEVICE_VGPU_CAP_COMPUTE_MEDIA_ENGINE_GPU          = 7  # Set/Get support for compute media engine vGPU profiles\nNVML_DEVICE_VGPU_CAP_WARM_UPDATE                       = 8  # Query whether the GPU supports FSR and warm update\nNVML_DEVICE_VGPU_CAP_HOMOGENEOUS_PLACEMENTS            = 9  # Query whether the GPU supports reporting of placements of timesliced vGPU profiles with identical framebuffer sizes\nNVML_DEVICE_VGPU_CAP_COUNT                             = 10\n\n_nvmlVgpuGuestInfoState_t = c_uint\nNVML_VGPU_INSTANCE_GUEST_INFO_STATE_UNINITIALIZED = 0\nNVML_VGPU_INSTANCE_GUEST_INFO_STATE_INITIALIZED   = 1\n\n_nvmlVgpuVmCompatibility_t = c_uint\nNVML_VGPU_VM_COMPATIBILITY_NONE         = 0x0\nNVML_VGPU_VM_COMPATIBILITY_COLD         = 0x1\nNVML_VGPU_VM_COMPATIBILITY_HIBERNATE    = 0x2\nNVML_VGPU_VM_COMPATIBILITY_SLEEP        = 0x4\nNVML_VGPU_VM_COMPATIBILITY_LIVE         = 0x8\n\n_nvmlVgpuPgpuCompatibilityLimitCode_t = c_uint\nNVML_VGPU_COMPATIBILITY_LIMIT_NONE          = 0x0\nNVML_VGPU_COMPATIBILITY_LIMIT_HOST_DRIVER   = 0x1\nNVML_VGPU_COMPATIBILITY_LIMIT_GUEST_DRIVER  = 0x2\nNVML_VGPU_COMPATIBILITY_LIMIT_GPU           = 0x4\nNVML_VGPU_COMPATIBILITY_LIMIT_OTHER         = 0x80000000\n\n_nvmlHostVgpuMode_t = c_uint\nNVML_HOST_VGPU_MODE_NON_SRIOV   = 0\nNVML_HOST_VGPU_MODE_SRIOV       = 1\n\n_nvmlConfComputeGpusReadyState_t = c_uint\nNVML_CC_ACCEPTING_CLIENT_REQUESTS_FALSE = 0\nNVML_CC_ACCEPTING_CLIENT_REQUESTS_TRUE = 1\n\n_nvmlConfComputeGpuCaps_t = c_uint\nNVML_CC_SYSTEM_GPUS_CC_NOT_CAPABLE = 0\nNVML_CC_SYSTEM_GPUS_CC_CAPABLE = 1\n\n_nvmlConfComputeCpuCaps_t = c_uint\nNVML_CC_SYSTEM_CPU_CAPS_NONE = 0\nNVML_CC_SYSTEM_CPU_CAPS_AMD_SEV = 1\nNVML_CC_SYSTEM_CPU_CAPS_INTEL_TDX = 2\nNVML_CC_SYSTEM_CPU_CAPS_AMD_SEV_SNP = 3\nNVML_CC_SYSTEM_CPU_CAPS_AMD_SNP_VTOM = 4\n\n_nvmlConfComputeDevToolsMode_t = c_uint\nNVML_CC_SYSTEM_DEVTOOLS_MODE_OFF = 0\nNVML_CC_SYSTEM_DEVTOOLS_MODE_ON = 1\n\nNVML_CC_SYSTEM_MULTIGPU_NONE = 0\nNVML_CC_SYSTEM_MULTIGPU_PROTECTED_PCIE = 1\n \nNVML_CC_SYSTEM_ENVIRONMENT_UNAVAILABLE = 0\nNVML_CC_SYSTEM_ENVIRONMENT_SIM = 1\nNVML_CC_SYSTEM_ENVIRONMENT_PROD = 2\n \n_nvmlConfComputeCcFeature_t = c_uint\nNVML_CC_SYSTEM_FEATURE_DISABLED = 0\nNVML_CC_SYSTEM_FEATURE_ENABLED = 1\n\n_nvmlConfComputeCcKeyRotationThreshAttackerAdv_t = c_uint\nNVML_CC_KEY_ROTATION_THRESH_ATTACKER_ADVANTAGE_MIN = 50\nNVML_CC_KEY_ROTATION_THRESH_ATTACKER_ADVANTAGE_MAX = 65\n\n# GSP firmware\nNVML_GSP_FIRMWARE_VERSION_BUF_SIZE = 0x40\n\nclass NVMLLibraryMismatchError(Exception):\n    pass\n\n## Error Checking ##\nclass NVMLError(Exception):\n    _valClassMapping = dict()\n    # List of currently known error codes\n    _errcode_to_string = {\n        NVML_ERROR_UNINITIALIZED:       \"Uninitialized\",\n        NVML_ERROR_INVALID_ARGUMENT:    \"Invalid Argument\",\n        NVML_ERROR_NOT_SUPPORTED:       \"Not Supported\",\n        NVML_ERROR_NO_PERMISSION:       \"Insufficient Permissions\",\n        NVML_ERROR_ALREADY_INITIALIZED: \"Already Initialized\",\n        NVML_ERROR_NOT_FOUND:           \"Not Found\",\n        NVML_ERROR_INSUFFICIENT_SIZE:   \"Insufficient Size\",\n        NVML_ERROR_INSUFFICIENT_POWER:  \"Insufficient External Power\",\n        NVML_ERROR_DRIVER_NOT_LOADED:   \"Driver Not Loaded\",\n        NVML_ERROR_TIMEOUT:             \"Timeout\",\n        NVML_ERROR_IRQ_ISSUE:           \"Interrupt Request Issue\",\n        NVML_ERROR_LIBRARY_NOT_FOUND:   \"NVML Shared Library Not Found\",\n        NVML_ERROR_FUNCTION_NOT_FOUND:  \"Function Not Found\",\n        NVML_ERROR_CORRUPTED_INFOROM:   \"Corrupted infoROM\",\n        NVML_ERROR_GPU_IS_LOST:         \"GPU is lost\",\n        NVML_ERROR_RESET_REQUIRED:      \"GPU requires restart\",\n        NVML_ERROR_OPERATING_SYSTEM:    \"The operating system has blocked the request.\",\n        NVML_ERROR_LIB_RM_VERSION_MISMATCH: \"RM has detected an NVML/RM version mismatch.\",\n        NVML_ERROR_MEMORY:              \"Insufficient Memory\",\n        NVML_ERROR_UNKNOWN:             \"Unknown Error\",\n        }\n    def __new__(typ, value):\n        '''\n        Maps value to a proper subclass of NVMLError.\n        See _extractNVMLErrorsAsClasses function for more details\n        '''\n        if typ == NVMLError:\n            typ = NVMLError._valClassMapping.get(value, typ)\n        obj = Exception.__new__(typ)\n        obj.value = value\n        return obj\n    def __str__(self):\n        try:\n            if self.value not in NVMLError._errcode_to_string:\n                NVMLError._errcode_to_string[self.value] = str(nvmlErrorString(self.value))\n            return NVMLError._errcode_to_string[self.value]\n        except NVMLError:\n            return \"NVML Error with code %d\" % self.value\n    def __eq__(self, other):\n        return self.value == other.value\n\ndef nvmlExceptionClass(nvmlErrorCode):\n    if nvmlErrorCode not in NVMLError._valClassMapping:\n        raise ValueError('nvmlErrorCode %s is not valid' % nvmlErrorCode)\n    return NVMLError._valClassMapping[nvmlErrorCode]\n\ndef _extractNVMLErrorsAsClasses():\n    '''\n    Generates a hierarchy of classes on top of NVMLError class.\n\n    Each NVML Error gets a new NVMLError subclass. This way try,except blocks can filter appropriate\n    exceptions more easily.\n\n    NVMLError is a parent class. Each NVML_ERROR_* gets it's own subclass.\n    e.g. NVML_ERROR_ALREADY_INITIALIZED will be turned into NVMLError_AlreadyInitialized\n    '''\n    this_module = sys.modules[__name__]\n    nvmlErrorsNames = [x for x in dir(this_module) if x.startswith(\"NVML_ERROR_\")]\n    for err_name in nvmlErrorsNames:\n        # e.g. Turn NVML_ERROR_ALREADY_INITIALIZED into NVMLError_AlreadyInitialized\n        class_name = \"NVMLError_\" + string.capwords(err_name.replace(\"NVML_ERROR_\", \"\"), \"_\").replace(\"_\", \"\")\n        err_val = getattr(this_module, err_name)\n        def gen_new(val):\n            def new(typ):\n                obj = NVMLError.__new__(typ, val)\n                return obj\n            return new\n        new_error_class = type(class_name, (NVMLError,), {'__new__': gen_new(err_val)})\n        new_error_class.__module__ = __name__\n        setattr(this_module, class_name, new_error_class)\n        NVMLError._valClassMapping[err_val] = new_error_class\n_extractNVMLErrorsAsClasses()\n\ndef _nvmlCheckReturn(ret):\n    if (ret != NVML_SUCCESS):\n        raise NVMLError(ret)\n    return ret\n\n## Function access ##\n_nvmlGetFunctionPointer_cache = dict() # function pointers are cached to prevent unnecessary libLoadLock locking\ndef _nvmlGetFunctionPointer(name):\n    global nvmlLib\n\n    if name in _nvmlGetFunctionPointer_cache:\n        return _nvmlGetFunctionPointer_cache[name]\n\n    libLoadLock.acquire()\n    try:\n        # ensure library was loaded\n        if (nvmlLib == None):\n            raise NVMLError(NVML_ERROR_UNINITIALIZED)\n        try:\n            _nvmlGetFunctionPointer_cache[name] = getattr(nvmlLib, name)\n            return _nvmlGetFunctionPointer_cache[name]\n        except AttributeError:\n            raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)\n    finally:\n        # lock is always freed\n        libLoadLock.release()\n\n## Alternative object\n# Allows the object to be printed\n# Allows mismatched types to be assigned\n#  - like None when the Structure variant requires c_uint\nclass nvmlFriendlyObject(object):\n    def __init__(self, dictionary):\n        for x in dictionary:\n            setattr(self, x, dictionary[x])\n    def __str__(self):\n        return self.__dict__.__str__()\n\ndef nvmlStructToFriendlyObject(struct):\n    d = {}\n    for x in struct._fields_:\n        key = x[0]\n        value = getattr(struct, key)\n        # only need to convert from bytes if bytes, no need to check python version.\n        d[key] = value.decode() if isinstance(value, bytes) else value\n    obj = nvmlFriendlyObject(d)\n    return obj\n\n# pack the object so it can be passed to the NVML library\ndef nvmlFriendlyObjectToStruct(obj, model):\n    for x in model._fields_:\n        key = x[0]\n        value = obj.__dict__[key]\n        # any c_char_p in python3 needs to be bytes, default encoding works fine.\n        if sys.version_info >= (3,):\n            setattr(model, key, value.encode())\n        else:\n            setattr(model, key, value)\n    return model\n\n## Unit structures\nclass struct_c_nvmlUnit_t(Structure):\n    pass # opaque handle\nc_nvmlUnit_t = POINTER(struct_c_nvmlUnit_t)\n\nclass _PrintableStructure(Structure):\n    \"\"\"\n    Abstract class that produces nicer __str__ output than ctypes.Structure.\n    e.g. instead of:\n      >>> print str(obj)\n      <class_name object at 0x7fdf82fef9e0>\n    this class will print\n      class_name(field_name: formatted_value, field_name: formatted_value)\n\n    _fmt_ dictionary of <str _field_ name> -> <str format>\n    e.g. class that has _field_ 'hex_value', c_uint could be formatted with\n      _fmt_ = {\"hex_value\" : \"%08X\"}\n    to produce nicer output.\n    Default formatting string for all fields can be set with key \"<default>\" like:\n      _fmt_ = {\"<default>\" : \"%d MHz\"} # e.g all values are numbers in MHz.\n    If not set it's assumed to be just \"%s\"\n\n    Exact format of returned str from this class is subject to change in the future.\n    \"\"\"\n    _fmt_ = {}\n    def __str__(self):\n        result = []\n        for x in self._fields_:\n            key = x[0]\n            value = getattr(self, key)\n            fmt = \"%s\"\n            if key in self._fmt_:\n                fmt = self._fmt_[key]\n            elif \"<default>\" in self._fmt_:\n                fmt = self._fmt_[\"<default>\"]\n            result.append((\"%s: \" + fmt) % (key, value))\n        return self.__class__.__name__ + \"(\" +  \", \".join(result) + \")\"\n\n    def __getattribute__(self, name):\n        res = super(_PrintableStructure, self).__getattribute__(name)\n        # need to convert bytes to unicode for python3 don't need to for python2\n        # Python 2 strings are of both str and bytes\n        # Python 3 strings are not of type bytes\n        # ctypes should convert everything to the correct values otherwise\n        if isinstance(res, bytes):\n            if isinstance(res, str):\n                return res\n            return res.decode()\n        return res\n\n    def __setattr__(self, name, value):\n        if isinstance(value, str):\n            # encoding a python2 string returns the same value, since python2 strings are bytes already\n            # bytes passed in python3 will be ignored.\n            value = value.encode()\n        super(_PrintableStructure, self).__setattr__(name, value)\n\nclass c_nvmlUnitInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('name', c_char * 96),\n        ('id', c_char * 96),\n        ('serial', c_char * 96),\n        ('firmwareVersion', c_char * 96),\n    ]\n\nclass c_nvmlC2cModeInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('isC2cEnabled', c_uint)\n    ]\n\nnvmlC2cModeInfo_v1 = 0x1000008;\n\nclass c_nvmlLedState_t(_PrintableStructure):\n    _fields_ = [\n        ('cause', c_char * 256),\n        ('color', _nvmlLedColor_t),\n    ]\n\nclass c_nvmlPSUInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('state', c_char * 256),\n        ('current', c_uint),\n        ('voltage', c_uint),\n        ('power', c_uint),\n    ]\n\nclass c_nvmlUnitFanInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('speed', c_uint),\n        ('state', _nvmlFanState_t),\n    ]\n\nclass c_nvmlUnitFanSpeeds_t(_PrintableStructure):\n    _fields_ = [\n        ('fans', c_nvmlUnitFanInfo_t * 24),\n        ('count', c_uint)\n    ]\n\n## Device structures\nclass struct_c_nvmlDevice_t(Structure):\n    pass # opaque handle\nc_nvmlDevice_t = POINTER(struct_c_nvmlDevice_t)\n\nclass nvmlPciInfoExt_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('domain', c_uint),\n        ('bus', c_uint),\n        ('device', c_uint),\n        ('pciDeviceId', c_uint),\n        ('pciSubSystemId', c_uint),\n        ('baseClass', c_uint),\n        ('subClass', c_uint),\n        ('busId', c_char * NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE),\n    ]\n    _fmt_ = {\n            'version'        : \"0x%04X\",\n            'domain'         : \"0x%04X\",\n            'bus'            : \"0x%02X\",\n            'device'         : \"0x%02X\",\n            'pciDeviceId'    : \"0x%08X\",\n            'pciSubSystemId' : \"0x%08X\",\n            'baseClass'      : \"0x%01X\",\n            'subClass'       : \"0x%01X\",\n            }\n\nnvmlPciInfoExt_v1 = 0x1000040\n\n# Legacy pciInfo used for _v1 and _v2\nclass nvmlPciInfo_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('busId', c_char * NVML_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE),\n        ('domain', c_uint),\n        ('bus', c_uint),\n        ('device', c_uint),\n        ('pciDeviceId', c_uint),\n\n        # Added in 2.285\n        ('pciSubSystemId', c_uint),\n        ('reserved0', c_uint),\n        ('reserved1', c_uint),\n        ('reserved2', c_uint),\n        ('reserved3', c_uint),\n    ]\n    _fmt_ = {\n            'domain'         : \"0x%04X\",\n            'bus'            : \"0x%02X\",\n            'device'         : \"0x%02X\",\n            'pciDeviceId'    : \"0x%08X\",\n            'pciSubSystemId' : \"0x%08X\",\n            }\n\nclass nvmlPciInfo_t(_PrintableStructure):\n    _fields_ = [\n        # Moved to the new busId location below\n        ('busIdLegacy', c_char * NVML_DEVICE_PCI_BUS_ID_BUFFER_V2_SIZE),\n        ('domain', c_uint),\n        ('bus', c_uint),\n        ('device', c_uint),\n        ('pciDeviceId', c_uint),\n\n        # Added in 2.285\n        ('pciSubSystemId', c_uint),\n        # New busId replaced the long deprecated and reserved fields with a\n        # field of the same size in 9.0\n        ('busId', c_char * NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE),\n    ]\n    _fmt_ = {\n            'domain'         : \"0x%08X\",\n            'bus'            : \"0x%02X\",\n            'device'         : \"0x%02X\",\n            'pciDeviceId'    : \"0x%08X\",\n            'pciSubSystemId' : \"0x%08X\",\n            }\n\nclass c_nvmlSystemDriverBranchInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        (\"branch\", c_char * NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),\n    ]\n\nSystemDriverBranchInfo_v1 = 0x1000054\n\nclass c_nvmlExcludedDeviceInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('pci', nvmlPciInfo_t),\n        ('uuid', c_char * NVML_DEVICE_UUID_BUFFER_SIZE)\n    ]\n\nclass nvmlNvLinkUtilizationControl_t(_PrintableStructure):\n    _fields_ = [\n        ('units', _nvmlNvLinkUtilizationCountUnits_t),\n        ('pktfilter', _nvmlNvLinkUtilizationCountPktTypes_t),\n    ]\n\nclass c_nvmlMemory_t(_PrintableStructure):\n    _fields_ = [\n        ('total', c_ulonglong),\n        ('free', c_ulonglong),\n        ('used', c_ulonglong),\n    ]\n    _fmt_ = {'<default>': \"%d B\"}\n\nclass c_nvmlMemory_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('total', c_ulonglong),\n        ('reserved', c_ulonglong),\n        ('free', c_ulonglong),\n        ('used', c_ulonglong),\n    ]\n    _fmt_ = {'<default>': \"%d B\"}\n\nnvmlMemory_v2 = 0x02000028\n\nclass c_nvmlBAR1Memory_t(_PrintableStructure):\n    _fields_ = [\n        ('bar1Total', c_ulonglong),\n        ('bar1Free', c_ulonglong),\n        ('bar1Used', c_ulonglong),\n    ]\n    _fmt_ = {'<default>': \"%d B\"}\n\nclass nvmlClkMonFaultInfo_t(Structure):\n    _fields_ = [(\"clkApiDomain\", c_uint),\n                (\"clkDomainFaultMask\", c_uint)\n    ]\n\nMAX_CLK_DOMAINS = 32\n\nclass nvmlClkMonStatus_t(Structure):\n    _fields_ = [(\"bGlobalStatus\", c_uint),\n                (\"clkMonListSize\", c_uint),\n                (\"clkMonList\", nvmlClkMonFaultInfo_t * MAX_CLK_DOMAINS)\n    ]\n\n# On Windows with the WDDM driver, usedGpuMemory is reported as None\n# Code that processes this structure should check for None, I.E.\n#\n# if (info.usedGpuMemory == None):\n#     # TODO handle the error\n#     pass\n# else:\n#    print(\"Using %d MiB of memory\" % (info.usedGpuMemory / 1024 / 1024))\n# endif\n#\n# See NVML documentation for more information\nclass c_nvmlProcessInfo_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('pid', c_uint),\n        ('usedGpuMemory', c_ulonglong),\n        ('gpuInstanceId', c_uint),\n        ('computeInstanceId', c_uint),\n    ]\n    _fmt_ = {'usedGpuMemory': \"%d B\"}\n\nc_nvmlProcessInfo_v3_t = c_nvmlProcessInfo_v2_t\n\nc_nvmlProcessInfo_t = c_nvmlProcessInfo_v3_t\n\n_nvmlProcessMode_t = c_uint\nNVML_PROCESS_MODE_COMPUTE  = 0\nNVML_PROCESS_MODE_GRAPHICS = 1\nNVML_PROCESS_MODE_MPS      = 2\n\nclass c_nvmlProcessDetail_v1_t(Structure):\n    _fields_ = [\n        ('pid', c_uint),\n        ('usedGpuMemory', c_ulonglong),\n        ('gpuInstanceId', c_uint),\n        ('computeInstanceId', c_uint),\n        ('usedGpuCcProtectedMemory', c_ulonglong),\n    ]\n\nclass c_nvmlProcessDetailList_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('mode', _nvmlProcessMode_t),\n        ('numProcArrayEntries', c_uint),\n        ('procArray', POINTER(c_nvmlProcessDetail_v1_t)),\n    ]\n    _fmt_ = {'numProcArrayEntries': \"%d B\"}\n\nc_nvmlProcessDetailList_t = c_nvmlProcessDetailList_v1_t\n\nnvmlProcessDetailList_v1 = 0x1000018\n\nclass c_nvmlBridgeChipInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('type', _nvmlBridgeChipType_t),\n        ('fwVersion', c_uint),\n    ]\n\nclass c_nvmlBridgeChipHierarchy_t(_PrintableStructure):\n    _fields_ = [\n        ('bridgeCount', c_uint),\n        ('bridgeChipInfo', c_nvmlBridgeChipInfo_t * 128),\n    ]\n\nclass c_nvmlEccErrorCounts_t(_PrintableStructure):\n    _fields_ = [\n        ('l1Cache', c_ulonglong),\n        ('l2Cache', c_ulonglong),\n        ('deviceMemory', c_ulonglong),\n        ('registerFile', c_ulonglong),\n    ]\n\nclass c_nvmlUtilization_t(_PrintableStructure):\n    _fields_ = [\n        ('gpu', c_uint),\n        ('memory', c_uint),\n    ]\n    _fmt_ = {'<default>': \"%d %%\"}\n\n# Added in 2.285\nclass c_nvmlHwbcEntry_t(_PrintableStructure):\n    _fields_ = [\n        ('hwbcId', c_uint),\n        ('firmwareVersion', c_char * 32),\n    ]\n\nclass c_nvmlValue_t(Union):\n    _fields_ = [\n        ('dVal', c_double),\n        ('uiVal', c_uint),\n        ('ulVal', c_ulong),\n        ('ullVal', c_ulonglong),\n        ('sllVal', c_longlong),\n        ('siVal', c_int),\n        ('usVal', c_ushort),\n    ]\n\nclass c_nvmlSample_t(_PrintableStructure):\n    _fields_ = [\n        ('timeStamp', c_ulonglong),\n        ('sampleValue', c_nvmlValue_t),\n    ]\n\nclass c_nvmlViolationTime_t(_PrintableStructure):\n    _fields_ = [\n        ('referenceTime', c_ulonglong),\n        ('violationTime', c_ulonglong),\n    ]\n\nclass c_nvmlFieldValue_t(_PrintableStructure):\n    _fields_ = [\n        ('fieldId', c_uint32),\n        ('scopeId', c_uint32),\n        ('timestamp', c_int64),\n        ('latencyUsec', c_int64),\n        ('valueType', _nvmlValueType_t),\n        ('nvmlReturn', _nvmlReturn_t),\n        ('value', c_nvmlValue_t)\n    ]\n\nNVML_NVLINK_TOTAL_SUPPORTED_BW_MODES = 23\n\nnvmlNvlinkSupportedBwModes_v1 = 0x100001c\nclass c_nvmlNvlinkSupportedBwModes_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('bwModes', c_uint8 * NVML_NVLINK_TOTAL_SUPPORTED_BW_MODES),\n        ('totalBwModes', c_uint8)\n    ]\n\n    def __init__(self):\n        super(c_nvmlNvlinkSupportedBwModes_v1_t, self).__init__(version=nvmlNvlinkSupportedBwModes_v1)\n\nnvmlNvlinkGetBwMode_v1 = 0x100000c\nclass c_nvmlNvlinkGetBwMode_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('bIsBest', c_uint),\n        ('bwMode', c_uint8)\n    ]\n\n    def __init__(self):\n        super(c_nvmlNvlinkGetBwMode_v1_t, self).__init__(version=nvmlNvlinkGetBwMode_v1)\n\nnvmlNvlinkSetBwMode_v1 = 0x100000c\nclass c_nvmlNvlinkSetBwMode_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('bSetBest', c_uint),\n        ('bwMode', c_uint8)\n    ]\n\n    def __init__(self):\n        super(c_nvmlNvlinkSetBwMode_v1_t, self).__init__(version=nvmlNvlinkSetBwMode_v1)\n\nclass c_nvmlVgpuHeterogeneousMode_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('mode', c_uint),\n    ]\n\nVgpuHeterogeneousMode_v1 = 0x1000008\n\nclass c_nvmlVgpuPlacementId_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('placementId', c_uint),\n    ]\n\nVgpuPlacementId_v1 = 0x1000008\n\nclass c_nvmlVgpuPlacementList_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('count', c_uint),\n        ('placementSize', c_uint),\n        ('placementIds', POINTER(c_uint)),\n    ]\n\nVgpuPlacementList_v1 = 0x1000018\n\nNVML_VGPU_PGPU_HETEROGENEOUS_MODE   = 0\nNVML_VGPU_PGPU_HOMOGENEOUS_MODE     = 1\n\nclass c_nvmlVgpuPlacementList_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('placementSize', c_uint),\n        ('count', c_uint),\n        ('placementIds', POINTER(c_uint)),\n        ('mode', c_uint),\n    ]\n\nVgpuPlacementList_v2 = 0x2000020\n\nclass c_nvmlVgpuTypeBar1Info_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('bar1Size', c_ulonglong),\n    ]\n\nVgpuTypeBar1Info_v1 = 0x1000010\n\nclass c_nvmlVgpuInstanceUtilizationSample_t(_PrintableStructure):\n    _fields_ = [\n        ('vgpuInstance', _nvmlVgpuInstance_t),\n        ('timeStamp', c_ulonglong),\n        ('smUtil', c_nvmlValue_t),\n        ('memUtil', c_nvmlValue_t),\n        ('encUtil', c_nvmlValue_t),\n        ('decUtil', c_nvmlValue_t),\n    ]\n\nclass c_nvmlVgpuInstanceUtilizationInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('timeStamp', c_ulonglong),\n        ('vgpuInstance', _nvmlVgpuInstance_t),\n        ('smUtil', c_nvmlValue_t),\n        ('memUtil', c_nvmlValue_t),\n        ('encUtil', c_nvmlValue_t),\n        ('decUtil', c_nvmlValue_t),\n        ('jpgUtil', c_nvmlValue_t),\n        ('ofaUtil', c_nvmlValue_t),\n    ]\n\nclass c_nvmlVgpuInstancesUtilizationInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('sampleValType', _nvmlValueType_t),\n        ('vgpuInstanceCount', c_uint),\n        ('lastSeenTimeStamp', c_ulonglong),\n        ('vgpuUtilArray', POINTER(c_nvmlVgpuInstanceUtilizationInfo_v1_t)),\n    ]\n\nVgpuInstancesUtilizationInfo_v1 = 0x01000020\n\nclass c_nvmlVgpuProcessUtilizationSample_t(_PrintableStructure):\n    _fields_ = [\n        ('vgpuInstance', _nvmlVgpuInstance_t),\n        ('pid', c_uint),\n        ('processName', c_char * NVML_VGPU_NAME_BUFFER_SIZE),\n        ('timeStamp', c_ulonglong),\n        ('smUtil', c_uint),\n        ('memUtil', c_uint),\n        ('encUtil', c_uint),\n        ('decUtil', c_uint),\n    ]\n\nclass c_nvmlVgpuProcessUtilizationInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('processName', c_char * NVML_VGPU_NAME_BUFFER_SIZE),\n        ('timeStamp', c_ulonglong),\n        ('vgpuInstance', _nvmlVgpuInstance_t),\n        ('pid', c_uint),\n        ('smUtil', c_uint),\n        ('memUtil', c_uint),\n        ('encUtil', c_uint),\n        ('decUtil', c_uint),\n        ('jpgUtil', c_uint),\n        ('ofaUtil', c_uint),\n    ]\n\nclass c_nvmlVgpuProcessesUtilizationInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('vgpuProcessCount', c_uint),\n        ('lastSeenTimeStamp', c_ulonglong),\n        ('vgpuProcUtilArray', POINTER(c_nvmlVgpuProcessUtilizationInfo_v1_t)),\n    ]\n\nVgpuProcessesUtilizationInfo_v1 = 0x01000018\n\nclass nvmlVgpuRuntimeState_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('size', c_ulonglong),\n    ]\n\nVgpuRuntimeState_v1 = 0x1000010\n\nclass c_nvmlVgpuLicenseExpiry_t(_PrintableStructure):\n    _fields_ = [\n        ('year',    c_uint32),\n        ('month',   c_uint16),\n        ('day',     c_uint16),\n        ('hour',    c_uint16),\n        ('min',     c_uint16),\n        ('sec',     c_uint16),\n        ('status',  c_uint8),\n    ]\n\nNVML_GRID_LICENSE_STATE_UNKNOWN                 = 0\nNVML_GRID_LICENSE_STATE_UNINITIALIZED           = 1\nNVML_GRID_LICENSE_STATE_UNLICENSED_UNRESTRICTED = 2\nNVML_GRID_LICENSE_STATE_UNLICENSED_RESTRICTED   = 3\nNVML_GRID_LICENSE_STATE_UNLICENSED              = 4\nNVML_GRID_LICENSE_STATE_LICENSED                = 5\n\nclass c_nvmlVgpuLicenseInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('isLicensed',      c_uint8),\n        ('licenseExpiry',   c_nvmlVgpuLicenseExpiry_t),\n        ('currentState',    c_uint),\n    ]\n\nclass c_nvmlEncoderSession_t(_PrintableStructure):\n    _fields_ = [\n        ('sessionId', c_uint),\n        ('pid', c_uint),\n        ('vgpuInstance', _nvmlVgpuInstance_t),\n        ('codecType', c_uint),\n        ('hResolution', c_uint),\n        ('vResolution', c_uint),\n        ('averageFps', c_uint),\n        ('encodeLatency', c_uint),\n    ]\n\nclass c_nvmlProcessUtilizationSample_t(_PrintableStructure):\n    _fields_ = [\n        ('pid', c_uint),\n        ('timeStamp', c_ulonglong),\n        ('smUtil', c_uint),\n        ('memUtil', c_uint),\n        ('encUtil', c_uint),\n        ('decUtil', c_uint),\n    ]\n\nclass c_nvmlProcessUtilizationInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('timeStamp', c_ulonglong),\n        ('pid', c_uint),\n        ('smUtil', c_uint),\n        ('memUtil', c_uint),\n        ('encUtil', c_uint),\n        ('decUtil', c_uint),\n        ('jpgUtil', c_uint),\n        ('ofaUtil', c_uint),\n    ]\n\nclass c_nvmlProcessesUtilizationInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('processSamplesCount', c_uint),\n        ('lastSeenTimeStamp', c_ulonglong),\n        ('procUtilArray', POINTER(c_nvmlProcessUtilizationInfo_v1_t)),\n    ]\n\nProcessesUtilizationInfo_v1 = 0x01000018\n\nclass c_nvmlGridLicenseExpiry_t(_PrintableStructure):\n    _fields_ = [\n        ('year',    c_uint32),\n        ('month',   c_uint16),\n        ('day',     c_uint16),\n        ('hour',    c_uint16),\n        ('min',     c_uint16),\n        ('sec',     c_uint16),\n        ('status',  c_uint8),\n    ]\n\nclass c_nvmlGridLicensableFeature_v4_t(_PrintableStructure):\n    _fields_ = [\n        ('featureCode',    _nvmlGridLicenseFeatureCode_t),\n        ('featureState',   c_uint),\n        ('licenseInfo',    c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n        ('productName',    c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n        ('featureEnabled', c_uint),\n        ('licenseExpiry',  c_nvmlGridLicenseExpiry_t),\n    ]\n\nclass c_nvmlGridLicensableFeatures_v4_t(_PrintableStructure):\n    _fields_ = [\n        ('isGridLicenseSupported',  c_int),\n        ('licensableFeaturesCount', c_uint),\n        ('gridLicensableFeatures',  c_nvmlGridLicensableFeature_v4_t * NVML_GRID_LICENSE_FEATURE_MAX_COUNT),\n    ]\n\nclass c_nvmlGridLicensableFeature_v3_t(_PrintableStructure):\n    _fields_ = [\n        ('featureCode', _nvmlGridLicenseFeatureCode_t),\n        ('featureState', c_uint),\n        ('licenseInfo', c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n        ('productName', c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n        ('featureEnabled', c_uint),\n    ]\n\nclass c_nvmlGridLicensableFeatures_v3_t(_PrintableStructure):\n    _fields_ = [\n        ('isGridLicenseSupported', c_int),\n        ('licensableFeaturesCount', c_uint),\n        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_v3_t * NVML_GRID_LICENSE_FEATURE_MAX_COUNT),\n    ]\n\nclass c_nvmlGridLicensableFeature_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('featureCode', _nvmlGridLicenseFeatureCode_t),\n        ('featureState', c_uint),\n        ('licenseInfo', c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n        ('productName', c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n    ]\n\nclass c_nvmlGridLicensableFeatures_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('isGridLicenseSupported', c_int),\n        ('licensableFeaturesCount', c_uint),\n        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_v2_t * NVML_GRID_LICENSE_FEATURE_MAX_COUNT),\n    ]\n\nclass c_nvmlGridLicensableFeature_t(_PrintableStructure):\n    _fields_ = [\n        ('featureCode', _nvmlGridLicenseFeatureCode_t),\n        ('featureState', c_uint),\n        ('licenseInfo', c_char * NVML_GRID_LICENSE_BUFFER_SIZE),\n    ]\n\nclass c_nvmlGridLicensableFeatures_t(_PrintableStructure):\n    _fields_ = [\n        ('isGridLicenseSupported', c_int),\n        ('licensableFeaturesCount', c_uint),\n        ('gridLicensableFeatures', c_nvmlGridLicensableFeature_t * NVML_GRID_LICENSE_FEATURE_MAX_COUNT),\n    ]\n\nclass c_nvmlMarginTemperature_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('marginTemperature', c_int),\n    ]\n\nnvmlMarginTemperature_v1 = 0x1000008\n\n## Event structures\nclass struct_c_nvmlEventSet_t(Structure):\n    pass # opaque handle\nc_nvmlEventSet_t = POINTER(struct_c_nvmlEventSet_t)\n\nnvmlEventTypeSingleBitEccError      = 0x0000000000000001\nnvmlEventTypeDoubleBitEccError      = 0x0000000000000002\nnvmlEventTypePState                 = 0x0000000000000004\nnvmlEventTypeXidCriticalError       = 0x0000000000000008\nnvmlEventTypeClock                  = 0x0000000000000010\nnvmlEventTypePowerSourceChange      = 0x0000000000000080\nnvmlEventMigConfigChange            = 0x0000000000000100\nnvmlEventTypeSingleBitEccErrorStorm = 0x0000000000000200\nnvmlEventTypeDramRetirementEvent    = 0x0000000000000400\nnvmlEventTypeDramRetirementFailure  = 0x0000000000000800\nnvmlEventTypeNonFatalPoisonError    = 0x0000000000001000\nnvmlEventTypeFatalPoisonError       = 0x0000000000002000\nnvmlEventTypeGpuUnavailableError    = 0x0000000000004000\nnvmlEventTypeGpuRecoveryAction      = 0x0000000000008000\nnvmlEventTypeNone                   = 0x0000000000000000\nnvmlEventTypeAll                    = (\n                                        nvmlEventTypeNone\n                                        | nvmlEventTypeSingleBitEccError\n                                        | nvmlEventTypeDoubleBitEccError\n                                        | nvmlEventTypePState\n                                        | nvmlEventTypeClock\n                                        | nvmlEventTypePowerSourceChange\n                                        | nvmlEventTypeXidCriticalError\n                                        | nvmlEventMigConfigChange\n                                        | nvmlEventTypeSingleBitEccErrorStorm\n                                        | nvmlEventTypeDramRetirementEvent\n                                        | nvmlEventTypeDramRetirementFailure\n                                        | nvmlEventTypeNonFatalPoisonError\n                                        | nvmlEventTypeFatalPoisonError\n                                        | nvmlEventTypeGpuUnavailableError\n                                        | nvmlEventTypeGpuRecoveryAction\n                                        )\n\n## Clock Event Reasons defines\nnvmlClocksEventReasonGpuIdle              = 0x0000000000000001\nnvmlClocksEventReasonApplicationsClocksSetting = 0x0000000000000002\nnvmlClocksEventReasonUserDefinedClocks         = nvmlClocksEventReasonApplicationsClocksSetting # deprecated, use nvmlClocksEventReasonApplicationsClocksSetting\nnvmlClocksEventReasonSwPowerCap           = 0x0000000000000004\nnvmlClocksEventReasonHwSlowdown           = 0x0000000000000008\nnvmlClocksEventReasonSyncBoost            = 0x0000000000000010\nnvmlClocksEventReasonSwThermalSlowdown    = 0x0000000000000020\nnvmlClocksEventReasonHwThermalSlowdown    = 0x0000000000000040\nnvmlClocksEventReasonHwPowerBrakeSlowdown = 0x0000000000000080\nnvmlClocksEventReasonDisplayClockSetting  = 0x0000000000000100\nnvmlClocksEventReasonNone                 = 0x0000000000000000\nnvmlClocksEventReasonAll                  = (\n                                                  nvmlClocksEventReasonNone |\n                                                  nvmlClocksEventReasonGpuIdle |\n                                                  nvmlClocksEventReasonApplicationsClocksSetting |\n                                                  nvmlClocksEventReasonSwPowerCap |\n                                                  nvmlClocksEventReasonHwSlowdown |\n                                                  nvmlClocksEventReasonSyncBoost |\n                                                  nvmlClocksEventReasonSwThermalSlowdown |\n                                                  nvmlClocksEventReasonHwThermalSlowdown |\n                                                  nvmlClocksEventReasonHwPowerBrakeSlowdown |\n                                                  nvmlClocksEventReasonDisplayClockSetting\n                                               )\n\n## Following have been deprecated\nnvmlClocksThrottleReasonGpuIdle              = 0x0000000000000001\nnvmlClocksThrottleReasonApplicationsClocksSetting = 0x0000000000000002\nnvmlClocksThrottleReasonUserDefinedClocks         = nvmlClocksThrottleReasonApplicationsClocksSetting # deprecated, use nvmlClocksThrottleReasonApplicationsClocksSetting\nnvmlClocksThrottleReasonSwPowerCap           = 0x0000000000000004\nnvmlClocksThrottleReasonHwSlowdown           = 0x0000000000000008\nnvmlClocksThrottleReasonSyncBoost            = 0x0000000000000010\nnvmlClocksThrottleReasonSwThermalSlowdown    = 0x0000000000000020\nnvmlClocksThrottleReasonHwThermalSlowdown    = 0x0000000000000040\nnvmlClocksThrottleReasonHwPowerBrakeSlowdown = 0x0000000000000080\nnvmlClocksThrottleReasonDisplayClockSetting  = 0x0000000000000100\nnvmlClocksThrottleReasonNone                 = 0x0000000000000000\nnvmlClocksThrottleReasonAll                  = (\n                                                  nvmlClocksThrottleReasonNone |\n                                                  nvmlClocksThrottleReasonGpuIdle |\n                                                  nvmlClocksThrottleReasonApplicationsClocksSetting |\n                                                  nvmlClocksThrottleReasonSwPowerCap |\n                                                  nvmlClocksThrottleReasonHwSlowdown |\n                                                  nvmlClocksThrottleReasonSyncBoost |\n                                                  nvmlClocksThrottleReasonSwThermalSlowdown |\n                                                  nvmlClocksThrottleReasonHwThermalSlowdown |\n                                                  nvmlClocksThrottleReasonHwPowerBrakeSlowdown |\n                                                  nvmlClocksThrottleReasonDisplayClockSetting\n                                               )\n\nclass c_nvmlEventData_t(_PrintableStructure):\n    _fields_ = [\n        ('device', c_nvmlDevice_t),\n        ('eventType', c_ulonglong),\n        ('eventData', c_ulonglong),\n        ('gpuInstanceId', c_uint),\n        ('computeInstanceId', c_uint)\n    ]\n    _fmt_ = {'eventType': \"0x%08X\"}\n\nclass c_nvmlAccountingStats_t(_PrintableStructure):\n    _fields_ = [\n        ('gpuUtilization', c_uint),\n        ('memoryUtilization', c_uint),\n        ('maxMemoryUsage', c_ulonglong),\n        ('time', c_ulonglong),\n        ('startTime', c_ulonglong),\n        ('isRunning', c_uint),\n        ('reserved', c_uint * 5)\n    ]\n\nclass c_nvmlVgpuVersion_t(Structure):\n    _fields_ = [(\"minVersion\", c_uint),\n                (\"maxVersion\", c_uint)\n               ]\n\nclass c_nvmlVgpuMetadata_t(_PrintableStructure):\n    _fields_ = [(\"version\", c_uint),\n                (\"revision\", c_uint),\n                (\"guestInfoState\", _nvmlVgpuGuestInfoState_t),\n                (\"guestDriverVersion\", c_char * NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),\n                (\"hostDriverVersion\", c_char * NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),\n                (\"reserved\", c_uint * 6),\n                (\"vgpuVirtualizationCaps\", c_uint),\n                (\"guestVgpuVersion\", c_uint),\n                (\"opaqueDataSize\", c_uint),\n                (\"opaqueData\", c_char * NVML_VGPU_METADATA_OPAQUE_DATA_SIZE)\n               ]\n\nclass c_nvmlVgpuPgpuMetadata_t(_PrintableStructure):\n    _fields_ = [(\"version\", c_uint),\n                (\"revision\", c_uint),\n                (\"hostDriverVersion\", c_char * NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE),\n                (\"pgpuVirtualizationCaps\", c_uint),\n                (\"reserved\", c_uint * 5),\n                (\"hostSupportedVgpuRange\", c_nvmlVgpuVersion_t),\n                (\"opaqueDataSize\", c_uint),\n                (\"opaqueData\", c_char * NVML_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE)\n               ]\n\nclass c_nvmlVgpuPgpuCompatibility_t(Structure):\n    _fields_ = [(\"vgpuVmCompatibility\", _nvmlVgpuVmCompatibility_t),\n                (\"compatibilityLimitCode\", _nvmlVgpuPgpuCompatibilityLimitCode_t)\n               ]\n\n## vGPU scheduler policy defines\nNVML_VGPU_SCHEDULER_POLICY_UNKNOWN      = 0\nNVML_VGPU_SCHEDULER_POLICY_BEST_EFFORT  = 1\nNVML_VGPU_SCHEDULER_POLICY_EQUAL_SHARE  = 2\nNVML_VGPU_SCHEDULER_POLICY_FIXED_SHARE  = 3\n\n## Supported vGPU scheduler policy count\nNVML_SUPPORTED_VGPU_SCHEDULER_POLICY_COUNT  = 3\n\nNVML_SCHEDULER_SW_MAX_LOG_ENTRIES           = 200\n\nNVML_VGPU_SCHEDULER_ARR_DEFAULT   = 0\nNVML_VGPU_SCHEDULER_ARR_DISABLE   = 1\nNVML_VGPU_SCHEDULER_ARR_ENABLE    = 2\n\nclass c_nvmlVgpuSchedDataWithARR_t(_PrintableStructure):\n    _fields_ = [\n        ('avgFactor',   c_uint),\n        ('timeslice',   c_uint),\n    ]\n\nclass c_nvmlVgpuSchedData_t(_PrintableStructure):\n    _fields_ = [\n        ('timeslice',   c_uint),\n    ]\n\nclass c_nvmlVgpuSchedulerParams_t(Union):\n    _fields_ = [\n        ('vgpuSchedDataWithARR', c_nvmlVgpuSchedDataWithARR_t),\n        ('vgpuSchedData',        c_nvmlVgpuSchedData_t),\n    ]\n\nclass c_nvmlVgpuSchedulerLogEntry_t(_PrintableStructure):\n    _fields_ = [\n        ('timestamp',                   c_ulonglong),\n        ('timeRunTotal',                c_ulonglong),\n        ('timeRun',                     c_ulonglong),\n        ('swRunlistId',                 c_uint),\n        ('targetTimeSlice',             c_ulonglong),\n        ('cumulativePreemptionTime',    c_ulonglong),\n    ]\n\nclass c_nvmlVgpuSchedulerLog_t(_PrintableStructure):\n    _fields_ = [\n        ('engineId',        c_uint),\n        ('schedulerPolicy', c_uint),\n        ('arrMode',         c_uint),\n        ('schedulerParams', c_nvmlVgpuSchedulerParams_t),\n        ('entriesCount',    c_uint),\n        ('logEntries',      c_nvmlVgpuSchedulerLogEntry_t * NVML_SCHEDULER_SW_MAX_LOG_ENTRIES),\n    ]\n\nclass c_nvmlVgpuSchedulerGetState_t(_PrintableStructure):\n    _fields_ = [\n        ('schedulerPolicy', c_uint),\n        ('arrMode',         c_uint),\n        ('schedulerParams', c_nvmlVgpuSchedulerParams_t),\n    ]\n\nclass c_nvmlVgpuSchedSetDataWithARR_t(_PrintableStructure):\n    _fields_ = [\n        ('avgFactor',   c_uint),\n        ('frequency',   c_uint),\n    ]\n\nclass c_nvmlVgpuSchedSetData_t(_PrintableStructure):\n    _fields_ = [\n        ('timeslice',   c_uint),\n    ]\n\nclass c_nvmlVgpuSchedulerSetParams_t(Union):\n    _fields_ = [\n        ('vgpuSchedDataWithARR', c_nvmlVgpuSchedSetDataWithARR_t),\n        ('vgpuSchedData',        c_nvmlVgpuSchedSetData_t),\n    ]\n\nclass c_nvmlVgpuSchedulerSetState_t(_PrintableStructure):\n    _fields_ = [\n        ('schedulerPolicy', c_uint),\n        ('enableARRMode',   c_uint),\n        ('schedulerParams', c_nvmlVgpuSchedulerSetParams_t),\n    ]\n\nclass c_nvmlVgpuSchedulerCapabilities_t(_PrintableStructure):\n    _fields_ = [\n        ('supportedSchedulers', c_uint * NVML_SUPPORTED_VGPU_SCHEDULER_POLICY_COUNT),\n        ('maxTimeslice',        c_uint),\n        ('minTimeslice',        c_uint),\n        ('isArrModeSupported',  c_uint),\n        ('maxFrequencyForARR',  c_uint),\n        ('minFrequencyForARR',  c_uint),\n        ('maxAvgFactorForARR',  c_uint),\n        ('minAvgFactorForARR',  c_uint),\n    ]\n\nclass c_nvmlFBCStats_t(Structure):\n    _fields_ = [(\"sessionsCount\", c_uint),\n                (\"averageFPS\", c_uint),\n                (\"averageLatency\", c_uint)\n               ]\n\nclass c_nvmlFBCSession_t(_PrintableStructure):\n    _fields_ = [\n        ('sessionId', c_uint),\n        ('pid', c_uint),\n        ('vgpuInstance', _nvmlVgpuInstance_t),\n        ('displayOrdinal', c_uint),\n        ('sessionType', c_uint),\n        ('sessionFlags', c_uint),\n        ('hMaxResolution', c_uint),\n        ('vMaxResolution', c_uint),\n        ('hResolution', c_uint),\n        ('vResolution', c_uint),\n        ('averageFPS', c_uint),\n        ('averageLatency', c_uint),\n    ]\n\nNVML_DEVICE_MIG_DISABLE = 0x0\nNVML_DEVICE_MIG_ENABLE  = 0x1\n\nNVML_GPU_INSTANCE_PROFILE_1_SLICE      = 0x0\nNVML_GPU_INSTANCE_PROFILE_2_SLICE      = 0x1\nNVML_GPU_INSTANCE_PROFILE_3_SLICE      = 0x2\nNVML_GPU_INSTANCE_PROFILE_4_SLICE      = 0x3\nNVML_GPU_INSTANCE_PROFILE_7_SLICE      = 0x4\nNVML_GPU_INSTANCE_PROFILE_8_SLICE      = 0x5\nNVML_GPU_INSTANCE_PROFILE_6_SLICE      = 0x6\nNVML_GPU_INSTANCE_PROFILE_1_SLICE_REV1 = 0x7\nNVML_GPU_INSTANCE_PROFILE_2_SLICE_REV1 = 0x8\nNVML_GPU_INSTANCE_PROFILE_1_SLICE_REV2 = 0x9\nNVML_GPU_INSTANCE_PROFILE_1_SLICE_GFX  = 0xA\nNVML_GPU_INSTANCE_PROFILE_2_SLICE_GFX  = 0xB\nNVML_GPU_INSTANCE_PROFILE_4_SLICE_GFX  = 0xC\nNVML_GPU_INSTANCE_PROFILE_COUNT        = 0xD\n\nclass c_nvmlGpuInstancePlacement_t(Structure):\n    _fields_ = [(\"start\", c_uint),\n                (\"size\", c_uint)\n               ]\n\nclass c_nvmlGpuInstanceProfileInfo_t(Structure):\n    _fields_ = [(\"id\", c_uint),\n                (\"isP2pSupported\", c_uint),\n                (\"sliceCount\", c_uint),\n                (\"instanceCount\", c_uint),\n                (\"multiprocessorCount\", c_uint),\n                (\"copyEngineCount\", c_uint),\n                (\"decoderCount\", c_uint),\n                (\"encoderCount\", c_uint),\n                (\"jpegCount\", c_uint),\n                (\"ofaCount\", c_uint),\n                (\"memorySizeMB\", c_ulonglong),\n               ]\n\nnvmlGpuInstanceProfileInfo_v2 = 0x02000098\n\nclass c_nvmlGpuInstanceProfileInfo_v2_t(_PrintableStructure):\n    _fields_ = [(\"version\", c_uint),\n                (\"id\", c_uint),\n                (\"isP2pSupported\", c_uint),\n                (\"sliceCount\", c_uint),\n                (\"instanceCount\", c_uint),\n                (\"multiprocessorCount\", c_uint),\n                (\"copyEngineCount\", c_uint),\n                (\"decoderCount\", c_uint),\n                (\"encoderCount\", c_uint),\n                (\"jpegCount\", c_uint),\n                (\"ofaCount\", c_uint),\n                (\"memorySizeMB\", c_ulonglong),\n                (\"name\", c_char * NVML_DEVICE_NAME_V2_BUFFER_SIZE)\n               ]\n    \n    def __init__(self):\n        super(c_nvmlGpuInstanceProfileInfo_v2_t, self).__init__(version=nvmlGpuInstanceProfileInfo_v2)\n\nclass c_nvmlGpuInstanceInfo_t(Structure):\n    _fields_ = [(\"device\", c_nvmlDevice_t),\n                (\"id\", c_uint),\n                (\"profileId\", c_uint),\n                (\"placement\", c_nvmlGpuInstancePlacement_t)\n               ]\n\nclass struct_c_nvmlGpuInstance_t(Structure):\n    pass # opaque handle\nc_nvmlGpuInstance_t = POINTER(struct_c_nvmlGpuInstance_t)\n\nNVML_COMPUTE_INSTANCE_PROFILE_1_SLICE      = 0x0\nNVML_COMPUTE_INSTANCE_PROFILE_2_SLICE      = 0x1\nNVML_COMPUTE_INSTANCE_PROFILE_3_SLICE      = 0x2\nNVML_COMPUTE_INSTANCE_PROFILE_4_SLICE      = 0x3\nNVML_COMPUTE_INSTANCE_PROFILE_7_SLICE      = 0x4\nNVML_COMPUTE_INSTANCE_PROFILE_8_SLICE      = 0x5\nNVML_COMPUTE_INSTANCE_PROFILE_6_SLICE      = 0x6\nNVML_COMPUTE_INSTANCE_PROFILE_1_SLICE_REV1 = 0x7\nNVML_COMPUTE_INSTANCE_PROFILE_COUNT        = 0x8\n\nNVML_COMPUTE_INSTANCE_ENGINE_PROFILE_SHARED = 0x0\nNVML_COMPUTE_INSTANCE_ENGINE_PROFILE_COUNT = 0x1\n\nclass c_nvmlComputeInstancePlacement_t(Structure):\n    _fields_ = [(\"start\", c_uint),\n                (\"size\", c_uint)\n               ]\n\nclass c_nvmlComputeInstanceProfileInfo_t(Structure):\n    _fields_ = [(\"id\", c_uint),\n                (\"sliceCount\", c_uint),\n                (\"instanceCount\", c_uint),\n                (\"multiprocessorCount\", c_uint),\n                (\"sharedCopyEngineCount\", c_uint),\n                (\"sharedDecoderCount\", c_uint),\n                (\"sharedEncoderCount\", c_uint),\n                (\"sharedJpegCount\", c_uint),\n                (\"sharedOfaCount\", c_uint)\n               ]\n\nnvmlComputeInstanceProfileInfo_v2 = 0x02000088\n\nclass c_nvmlComputeInstanceProfileInfo_v2_t(_PrintableStructure):\n    _fields_ = [(\"version\", c_uint),\n                (\"id\", c_uint),\n                (\"sliceCount\", c_uint),\n                (\"instanceCount\", c_uint),\n                (\"multiprocessorCount\", c_uint),\n                (\"sharedCopyEngineCount\", c_uint),\n                (\"sharedDecoderCount\", c_uint),\n                (\"sharedEncoderCount\", c_uint),\n                (\"sharedJpegCount\", c_uint),\n                (\"sharedOfaCount\", c_uint),\n                (\"name\", c_char * NVML_DEVICE_NAME_V2_BUFFER_SIZE)\n               ]\n\n    def __init__(self):\n        super(c_nvmlComputeInstanceProfileInfo_v2_t, self).__init__(version=nvmlComputeInstanceProfileInfo_v2)\n\nclass c_nvmlComputeInstanceInfo_t(Structure):\n    _fields_ = [(\"device\", c_nvmlDevice_t),\n                (\"gpuInstance\", c_nvmlGpuInstance_t),\n                (\"id\", c_uint),\n                (\"profileId\", c_uint),\n                (\"placement\", c_nvmlComputeInstancePlacement_t)\n               ]\n\nNVML_MAX_GPU_UTILIZATIONS = 8\nNVML_GPU_UTILIZATION_DOMAIN_GPU    = 0\nNVML_GPU_UTILIZATION_DOMAIN_FB     = 1\nNVML_GPU_UTILIZATION_DOMAIN_VID    = 2\nNVML_GPU_UTILIZATION_DOMAIN_BUS    = 3\nclass c_nvmlGpuDynamicPstatesUtilization_t(Structure):\n    _fields_ = [(\"bIsPresent\", c_uint, 1),\n                (\"percentage\", c_uint),\n                (\"incThreshold\", c_uint),\n                (\"decThreshold\", c_uint)]\nclass c_nvmlGpuDynamicPstatesInfo_t(Structure):\n    _fields_ = [(\"flags\", c_uint),\n                (\"utilization\", c_nvmlGpuDynamicPstatesUtilization_t * NVML_MAX_GPU_UTILIZATIONS)]\n\nNVML_MAX_THERMAL_SENSORS_PER_GPU = 3\n\nNVML_THERMAL_TARGET_NONE          = 0\nNVML_THERMAL_TARGET_GPU           = 1\nNVML_THERMAL_TARGET_MEMORY        = 2\nNVML_THERMAL_TARGET_POWER_SUPPLY  = 4\nNVML_THERMAL_TARGET_BOARD         = 8\nNVML_THERMAL_TARGET_VCD_BOARD     = 9\nNVML_THERMAL_TARGET_VCD_INLET     = 10\nNVML_THERMAL_TARGET_VCD_OUTLET    = 11\nNVML_THERMAL_TARGET_ALL           = 15\nNVML_THERMAL_TARGET_UNKNOWN       = -1\n\nNVML_THERMAL_CONTROLLER_NONE            = 0\nNVML_THERMAL_CONTROLLER_GPU_INTERNAL    = 1\nNVML_THERMAL_CONTROLLER_ADM1032         = 2\nNVML_THERMAL_CONTROLLER_ADT7461         = 3\nNVML_THERMAL_CONTROLLER_MAX6649         = 4\nNVML_THERMAL_CONTROLLER_MAX1617         = 5\nNVML_THERMAL_CONTROLLER_LM99            = 6\nNVML_THERMAL_CONTROLLER_LM89            = 7\nNVML_THERMAL_CONTROLLER_LM64            = 8\nNVML_THERMAL_CONTROLLER_G781            = 9\nNVML_THERMAL_CONTROLLER_ADT7473         = 10\nNVML_THERMAL_CONTROLLER_SBMAX6649       = 11\nNVML_THERMAL_CONTROLLER_VBIOSEVT        = 12\nNVML_THERMAL_CONTROLLER_OS              = 13\nNVML_THERMAL_CONTROLLER_NVSYSCON_CANOAS = 14\nNVML_THERMAL_CONTROLLER_NVSYSCON_E551   = 15\nNVML_THERMAL_CONTROLLER_MAX6649R        = 16\nNVML_THERMAL_CONTROLLER_ADT7473S        = 17\nNVML_THERMAL_CONTROLLER_UNKNOWN         = -1\n\nclass c_nvmlGpuThermalSensor_t(Structure):\n    _fields_ = [(\"controller\", c_int),\n                (\"defaultMinTemp\", c_int),\n                (\"defaultMaxTemp\", c_int),\n                (\"currentTemp\", c_int),\n                (\"target\", c_int)]\nclass c_nvmlGpuThermalSettings_t(Structure):\n    _fields_ = [(\"count\", c_uint),\n                (\"sensor\", c_nvmlGpuThermalSensor_t * NVML_MAX_THERMAL_SENSORS_PER_GPU)]\n\n_nvmlCoolerControl_t = c_uint\nNVML_THERMAL_COOLER_SIGNAL_NONE        = 0\nNVML_THERMAL_COOLER_SIGNAL_TOGGLE      = 1\nNVML_THERMAL_COOLER_SIGNAL_VARIABLE    = 2\nNVML_THERMAL_COOLER_SIGNAL_COUNT       = 3\n\n_nvmlCoolerTarget_t = c_uint\nNVML_THERMAL_COOLER_TARGET_NONE          = (1 << 0)\nNVML_THERMAL_COOLER_TARGET_GPU           = (1 << 1)\nNVML_THERMAL_COOLER_TARGET_MEMORY        = (1 << 2)\nNVML_THERMAL_COOLER_TARGET_POWER_SUPPLY  = (1 << 3)\nNVML_THERMAL_COOLER_TARGET_GPU_RELATED   = (NVML_THERMAL_COOLER_TARGET_GPU | NVML_THERMAL_COOLER_TARGET_MEMORY | NVML_THERMAL_COOLER_TARGET_POWER_SUPPLY)\n\nclass c_nvmlCoolerInfo_t(_PrintableStructure):\n    _fields_ = [(\"version\", c_uint),\n                (\"index\", c_uint),\n                (\"coolerControlType\", _nvmlCoolerControl_t),\n                (\"coolerTarget\", _nvmlCoolerTarget_t)\n               ]\n\nnvmlCoolerInfo_v1 = 0x1000010\n\ndef nvmlDeviceGetCoolerInfo(handle):\n    c_coolerInfo = c_nvmlCoolerInfo_t()\n    c_coolerInfo.version = nvmlCoolerInfo_v1\n    c_coolerInfo.index = 0\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCoolerInfo\")\n    ret = fn(handle, byref(c_coolerInfo))\n    _nvmlCheckReturn(ret)\n    return [c_coolerInfo.coolerControlType, c_coolerInfo.coolerTarget]\n\nclass struct_c_nvmlComputeInstance_t(Structure):\n    pass # opaque handle\nc_nvmlComputeInstance_t = POINTER(struct_c_nvmlComputeInstance_t)\n\nclass c_nvmlDeviceAttributes(Structure):\n    _fields_ = [(\"multiprocessorCount\", c_uint),\n                (\"sharedCopyEngineCount\", c_uint),\n                (\"sharedDecoderCount\", c_uint),\n                (\"sharedEncoderCount\", c_uint),\n                (\"sharedJpegCount\", c_uint),\n                (\"sharedOfaCount\", c_uint),\n                (\"gpuInstanceSliceCount\", c_uint),\n                (\"computeInstanceSliceCount\", c_uint),\n                (\"memorySizeMB\", c_ulonglong),\n               ]\n\nclass c_nvmlRowRemapperHistogramValues(Structure):\n    _fields_ = [(\"max\", c_uint),\n                (\"high\", c_uint),\n                (\"partial\", c_uint),\n                (\"low\", c_uint),\n                (\"none\", c_uint)\n               ]\n\nNVML_GPU_CERT_CHAIN_SIZE                = 0x1000\nNVML_GPU_ATTESTATION_CERT_CHAIN_SIZE    = 0x1400\nNVML_CC_GPU_CEC_NONCE_SIZE              = 0x20\nNVML_CC_GPU_ATTESTATION_REPORT_SIZE     = 0x2000\nNVML_CC_GPU_CEC_ATTESTATION_REPORT_SIZE = 0x1000\nNVML_CC_CEC_ATTESTATION_REPORT_NOT_PRESENT = 0\nNVML_CC_CEC_ATTESTATION_REPORT_PRESENT     = 1\n\nclass c_nvmlConfComputeSystemState_t(Structure):\n    _fields_ = [('environment', c_uint),\n                ('ccFeature', c_uint),\n                ('devToolsMode', c_uint),\n               ]\n\nnvmlSystemConfComputeSettings_v1 = 0x1000014\n\nclass c_nvmlSystemConfComputeSettings_v1_t(Structure):\n    _fields_ = [('version', c_uint),\n                ('environment', c_uint),\n                ('ccFeature', c_uint),\n                ('devToolsMode', c_uint),\n                ('multiGpuMode', c_uint),\n               ]\n    def __init__(self):\n        super(c_nvmlSystemConfComputeSettings_v1_t, self).__init__(version=nvmlSystemConfComputeSettings_v1)\n\nclass c_nvmlConfComputeSystemCaps_t(Structure):\n    _fields_ = [('cpuCaps', c_uint),\n                ('gpusCaps', c_uint),\n               ]\n\nclass c_nvmlConfComputeMemSizeInfo_t(Structure):\n    _fields_ = [('protectedMemSizeKib', c_ulonglong),\n                ('unprotectedMemSizeKib', c_ulonglong),\n               ]\n\nclass c_nvmlConfComputeGpuCertificate_t(Structure):\n    _fields_ = [('certChainSize', c_uint),\n                ('attestationCertChainSize', c_uint),\n                ('certChain', c_uint8 * NVML_GPU_CERT_CHAIN_SIZE),\n                ('attestationCertChain', c_uint8 * NVML_GPU_ATTESTATION_CERT_CHAIN_SIZE),\n               ]\n\nclass c_nvmlConfComputeGpuAttestationReport_t(Structure):\n    _fields_ = [('isCecAttestationReportPresent', c_uint),\n                ('attestationReportSize', c_uint),\n                ('cecAttestationReportSize', c_uint),\n                ('nonce', c_uint8 * NVML_CC_GPU_CEC_NONCE_SIZE),\n                ('attestationReport', c_uint8 * NVML_CC_GPU_ATTESTATION_REPORT_SIZE),\n                ('cecAttestationReport', c_uint8 * NVML_CC_GPU_CEC_ATTESTATION_REPORT_SIZE),\n               ]\n\nclass c_nvmlConfComputeSetKeyRotationThresholdInfo_t(Structure):\n    _fields_ = [('version', c_uint),\n                ('maxAttackerAdvantage', c_ulong),\n               ]\nConfComputeSetKeyRotationThresholdInfo_v1 = 0x1000010\n\nclass c_nvmlConfComputeGetKeyRotationThresholdInfo_t(Structure):\n    _fields_ = [('version', c_uint),\n                ('attackerAdvantage', c_ulong),\n               ]\nConfComputeGetKeyRotationThresholdInfo_v1 = 0x1000010\n\n\n## string/bytes conversion for ease of use\ndef convertStrBytes(func):\n    '''\n    In python 3, strings are unicode instead of bytes, and need to be converted for ctypes\n    Args from caller: (1, 'string', <__main__.c_nvmlDevice_t at 0xFFFFFFFF>)\n    Args passed to function: (1, b'string', <__main__.c_nvmlDevice_t at 0xFFFFFFFF)>\n    ----\n    Returned from function: b'returned string'\n    Returned to caller: 'returned string'\n    '''\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        # encoding a str returns bytes in python 2 and 3\n        args = [arg.encode() if isinstance(arg, str) else arg for arg in args]\n        res = func(*args, **kwargs)\n        # In python 2, str and bytes are the same\n        # In python 3, str is unicode and should be decoded.\n        # Ctypes handles most conversions, this only effects c_char and char arrays.\n        if isinstance(res, bytes):\n            if isinstance(res, str):\n                return res\n            return res.decode()\n        return res\n\n    if sys.version_info >= (3,):\n        return wrapper\n    return func\n\ndef throwOnVersionMismatch(func):\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except NVMLError_FunctionNotFound:\n            raise NVMLLibraryMismatchError(\"Unversioned function called and the \"\n                                           \"pyNVML version does not match the NVML lib version. \"\n                                           \"Either use matching pyNVML and NVML lib versions or \"\n                                           \"use a versioned function such as \" + func.__name__ + \"_v2\")\n    return wrapper\n\n## C function wrappers ##\ndef nvmlInitWithFlags(flags):\n    _LoadNvmlLibrary()\n\n    #\n    # Initialize the library\n    #\n    fn = _nvmlGetFunctionPointer(\"nvmlInitWithFlags\")\n    ret = fn(flags)\n    _nvmlCheckReturn(ret)\n\n    # Atomically update refcount\n    global _nvmlLib_refcount\n    libLoadLock.acquire()\n    _nvmlLib_refcount += 1\n    libLoadLock.release()\n    return None\n\ndef nvmlInit():\n    nvmlInitWithFlags(0)\n    return None\n\ndef _LoadNvmlLibrary():\n    '''\n    Load the library if it isn't loaded already\n    '''\n    global nvmlLib\n\n    if (nvmlLib == None):\n        # lock to ensure only one caller loads the library\n        libLoadLock.acquire()\n\n        try:\n            # ensure the library still isn't loaded\n            if (nvmlLib == None):\n                try:\n                    if (sys.platform[:3] == \"win\"):\n                        # cdecl calling convention\n                        try:\n                            # Check for nvml.dll in System32 first for DCH drivers\n                            nvmlLib = CDLL(os.path.join(os.getenv(\"WINDIR\", \"C:/Windows\"), \"System32/nvml.dll\"))\n                        except OSError as ose:\n                            # If nvml.dll is not found in System32, it should be in ProgramFiles\n                            # load nvml.dll from %ProgramFiles%/NVIDIA Corporation/NVSMI/nvml.dll\n                            nvmlLib = CDLL(os.path.join(os.getenv(\"ProgramFiles\", \"C:/Program Files\"), \"NVIDIA Corporation/NVSMI/nvml.dll\"))\n                    else:\n                        # assume linux\n                        nvmlLib = CDLL(\"libnvidia-ml.so.1\")\n                except OSError as ose:\n                    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n                if (nvmlLib == None):\n                    _nvmlCheckReturn(NVML_ERROR_LIBRARY_NOT_FOUND)\n        finally:\n            # lock is always freed\n            libLoadLock.release()\n\ndef nvmlShutdown():\n    #\n    # Leave the library loaded, but shutdown the interface\n    #\n    fn = _nvmlGetFunctionPointer(\"nvmlShutdown\")\n    ret = fn()\n    _nvmlCheckReturn(ret)\n\n    # Atomically update refcount\n    global _nvmlLib_refcount\n    libLoadLock.acquire()\n    if (0 < _nvmlLib_refcount):\n        _nvmlLib_refcount -= 1\n    libLoadLock.release()\n    return None\n\n# Added in 2.285\n@convertStrBytes\ndef nvmlErrorString(result):\n    fn = _nvmlGetFunctionPointer(\"nvmlErrorString\")\n    fn.restype = c_char_p # otherwise return is an int\n    ret = fn(result)\n    return ret\n\n# Added in 2.285\n@convertStrBytes\ndef nvmlSystemGetNVMLVersion():\n    c_version = create_string_buffer(NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetNVMLVersion\")\n    ret = fn(c_version, c_uint(NVML_SYSTEM_NVML_VERSION_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_version.value\n\ndef nvmlSystemGetCudaDriverVersion():\n    c_cuda_version = c_int()\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetCudaDriverVersion\")\n    ret = fn(byref(c_cuda_version))\n    _nvmlCheckReturn(ret)\n    return c_cuda_version.value\n\ndef nvmlSystemGetCudaDriverVersion_v2():\n    c_cuda_version = c_int()\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetCudaDriverVersion_v2\")\n    ret = fn(byref(c_cuda_version))\n    _nvmlCheckReturn(ret)\n    return c_cuda_version.value\n\n# Added in 2.285\n@convertStrBytes\ndef nvmlSystemGetProcessName(pid):\n    c_name = create_string_buffer(1024)\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetProcessName\")\n    ret = fn(c_uint(pid), c_name, c_uint(1024))\n    _nvmlCheckReturn(ret)\n    return c_name.value\n\n@convertStrBytes\ndef nvmlSystemGetDriverVersion():\n    c_version = create_string_buffer(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetDriverVersion\")\n    ret = fn(c_version, c_uint(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_version.value\n\n# Added in 2.285\ndef nvmlSystemGetHicVersion():\n    c_count = c_uint(0)\n    hics = None\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetHicVersion\")\n\n    # get the count\n    ret = fn(byref(c_count), None)\n\n    # this should only fail with insufficient size\n    if ((ret != NVML_SUCCESS) and\n        (ret != NVML_ERROR_INSUFFICIENT_SIZE)):\n        raise NVMLError(ret)\n\n    # If there are no hics\n    if (c_count.value == 0):\n        return []\n\n    hic_array = c_nvmlHwbcEntry_t * c_count.value\n    hics = hic_array()\n    ret = fn(byref(c_count), hics)\n    _nvmlCheckReturn(ret)\n    return hics\n\ndef nvmlSystemGetDriverBranch():\n    c_branchInfo = c_nvmlSystemDriverBranchInfo_v1_t(0)\n    c_branchInfo.version = SystemDriverBranchInfo_v1\n    fn  = _nvmlGetFunctionPointer(\"nvmlSystemGetDriverBranch\")\n    ret = fn(byref(c_branchInfo), c_uint(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_branchInfo\n\n## Unit get functions\ndef nvmlUnitGetCount():\n    c_count = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetCount\")\n    ret = fn(byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlUnitGetHandleByIndex(index):\n    c_index = c_uint(index)\n    unit = c_nvmlUnit_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetHandleByIndex\")\n    ret = fn(c_index, byref(unit))\n    _nvmlCheckReturn(ret)\n    return unit\n\ndef nvmlUnitGetUnitInfo(unit):\n    c_info = c_nvmlUnitInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetUnitInfo\")\n    ret = fn(unit, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\ndef nvmlUnitGetLedState(unit):\n    c_state =  c_nvmlLedState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetLedState\")\n    ret = fn(unit, byref(c_state))\n    _nvmlCheckReturn(ret)\n    return c_state\n\ndef nvmlUnitGetPsuInfo(unit):\n    c_info = c_nvmlPSUInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetPsuInfo\")\n    ret = fn(unit, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\ndef nvmlUnitGetTemperature(unit, type):\n    c_temp = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetTemperature\")\n    ret = fn(unit, c_uint(type), byref(c_temp))\n    _nvmlCheckReturn(ret)\n    return c_temp.value\n\ndef nvmlUnitGetFanSpeedInfo(unit):\n    c_speeds = c_nvmlUnitFanSpeeds_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetFanSpeedInfo\")\n    ret = fn(unit, byref(c_speeds))\n    _nvmlCheckReturn(ret)\n    return c_speeds\n\n# added to API\ndef nvmlUnitGetDeviceCount(unit):\n    c_count = c_uint(0)\n    # query the unit to determine device count\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetDevices\")\n    ret = fn(unit, byref(c_count), None)\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        ret = NVML_SUCCESS\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlUnitGetDevices(unit):\n    c_count = c_uint(nvmlUnitGetDeviceCount(unit))\n    device_array = c_nvmlDevice_t * c_count.value\n    c_devices = device_array()\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitGetDevices\")\n    ret = fn(unit, byref(c_count), c_devices)\n    _nvmlCheckReturn(ret)\n    return c_devices\n\n## Device get functions\ndef nvmlDeviceGetCount():\n    c_count = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCount_v2\")\n    ret = fn(byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlDeviceGetHandleByIndex(index):\n    c_index = c_uint(index)\n    device = c_nvmlDevice_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleByIndex_v2\")\n    ret = fn(c_index, byref(device))\n    _nvmlCheckReturn(ret)\n    return device\n\n@convertStrBytes\ndef nvmlDeviceGetHandleBySerial(serial):\n    c_serial = c_char_p(serial)\n    device = c_nvmlDevice_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleBySerial\")\n    ret = fn(c_serial, byref(device))\n    _nvmlCheckReturn(ret)\n    return device\n\n@convertStrBytes\ndef nvmlDeviceGetHandleByUUID(uuid):\n    c_uuid = c_char_p(uuid)\n    device = c_nvmlDevice_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleByUUID\")\n    ret = fn(c_uuid, byref(device))\n    _nvmlCheckReturn(ret)\n    return device\n\n@convertStrBytes\ndef nvmlDeviceGetHandleByPciBusId(pciBusId):\n    c_busId = c_char_p(pciBusId)\n    device = c_nvmlDevice_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHandleByPciBusId_v2\")\n    ret = fn(c_busId, byref(device))\n    _nvmlCheckReturn(ret)\n    return device\n\n@convertStrBytes\ndef nvmlDeviceGetName(handle):\n    c_name = create_string_buffer(NVML_DEVICE_NAME_V2_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetName\")\n    ret = fn(handle, c_name, c_uint(NVML_DEVICE_NAME_V2_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_name.value\n\nclass c_nvmlDevicePerfModes_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('str', c_char * NVML_PERF_MODES_BUFFER_SIZE),\n    ]\n\nnvmlDevicePerfModes_v1 = 0x1000804\n\n@convertStrBytes\ndef nvmlDeviceGetPerformanceModes(handle):\n    perfModes = c_nvmlDevicePerfModes_v1_t()\n    perfModes.version = nvmlDevicePerfModes_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPerformanceModes\")\n    ret = fn(handle, byref(perfModes))\n    _nvmlCheckReturn(ret)\n    return perfModes.str\n\nclass c_nvmlDeviceCurrentClockFreqs_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('str', c_char * NVML_PERF_MODES_BUFFER_SIZE),\n    ]\n\nnvmlDeviceCurrentClockFreqs_v1 = 0x1000804\n\n@convertStrBytes\ndef nvmlDeviceGetCurrentClockFreqs(handle):\n    currentClockFreqs = c_nvmlDeviceCurrentClockFreqs_v1_t()\n    currentClockFreqs.version = nvmlDeviceCurrentClockFreqs_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrentClockFreqs\")\n    ret = fn(handle, byref(currentClockFreqs))\n    _nvmlCheckReturn(ret)\n    return currentClockFreqs.str\n\ndef nvmlDeviceGetBoardId(handle):\n    c_id = c_uint();\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBoardId\")\n    ret = fn(handle, byref(c_id))\n    _nvmlCheckReturn(ret)\n    return c_id.value\n\ndef nvmlDeviceGetMultiGpuBoard(handle):\n    c_multiGpu = c_uint();\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMultiGpuBoard\")\n    ret = fn(handle, byref(c_multiGpu))\n    _nvmlCheckReturn(ret)\n    return c_multiGpu.value\n\ndef nvmlDeviceGetBrand(handle):\n    c_type = _nvmlBrandType_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBrand\")\n    ret = fn(handle, byref(c_type))\n    _nvmlCheckReturn(ret)\n    return c_type.value\n\ndef nvmlDeviceGetC2cModeInfoV1(handle):\n    c_info = c_nvmlC2cModeInfo_v1_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetC2cModeInfoV\")\n    ret = fn(handle, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\ndef nvmlDeviceGetC2cModeInfoV(handle):\n    return nvmlDeviceGetC2cModeInfoV1(handle)\n\n@convertStrBytes\ndef nvmlDeviceGetBoardPartNumber(handle):\n    c_part_number = create_string_buffer(NVML_DEVICE_PART_NUMBER_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBoardPartNumber\")\n    ret = fn(handle, c_part_number, c_uint(NVML_DEVICE_PART_NUMBER_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_part_number.value\n\n@convertStrBytes\ndef nvmlDeviceGetSerial(handle):\n    c_serial = create_string_buffer(NVML_DEVICE_SERIAL_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSerial\")\n    ret = fn(handle, c_serial, c_uint(NVML_DEVICE_SERIAL_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_serial.value\n\ndef nvmlDeviceGetModuleId(handle, moduleId=c_uint()):\n    isReference = type(moduleId) is not c_uint\n    moduleIdRef = moduleId if isReference else byref(moduleId)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetModuleId\")\n    ret = fn(handle, moduleIdRef)\n    if isReference:\n        return ret\n    else:\n        _nvmlCheckReturn(ret)\n        return moduleId.value\n\ndef nvmlDeviceGetMemoryAffinity(handle, nodeSetSize, scope):\n    affinity_array = c_ulonglong * nodeSetSize\n    c_affinity = affinity_array()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryAffinity\")\n    ret = fn(handle, nodeSetSize, byref(c_affinity), _nvmlAffinityScope_t(scope))\n    _nvmlCheckReturn(ret)\n    return c_affinity\n\ndef nvmlDeviceGetCpuAffinityWithinScope(handle, cpuSetSize, scope):\n    affinity_array = c_ulonglong * cpuSetSize\n    c_affinity = affinity_array()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCpuAffinityWithinScope\")\n    ret = fn(handle, cpuSetSize, byref(c_affinity), _nvmlAffinityScope_t(scope))\n    _nvmlCheckReturn(ret)\n    return c_affinity\n\ndef nvmlDeviceGetCpuAffinity(handle, cpuSetSize):\n    affinity_array = c_ulonglong * cpuSetSize\n    c_affinity = affinity_array()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCpuAffinity\")\n    ret = fn(handle, cpuSetSize, byref(c_affinity))\n    _nvmlCheckReturn(ret)\n    return c_affinity\n\ndef nvmlDeviceSetCpuAffinity(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetCpuAffinity\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceClearCpuAffinity(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearCpuAffinity\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetNumaNodeId(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNumaNodeId\")\n    node = c_int()\n    ret = fn(handle, byref(node))\n    _nvmlCheckReturn(ret)\n    return node.value\n\ndef nvmlDeviceGetMinorNumber(handle):\n    c_minor_number = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMinorNumber\")\n    ret = fn(handle, byref(c_minor_number))\n    _nvmlCheckReturn(ret)\n    return c_minor_number.value\n\n@convertStrBytes\ndef nvmlDeviceGetUUID(handle):\n    c_uuid = create_string_buffer(NVML_DEVICE_UUID_V2_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetUUID\")\n    ret = fn(handle, c_uuid, c_uint(NVML_DEVICE_UUID_V2_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_uuid.value\n\n@convertStrBytes\ndef nvmlDeviceGetInforomVersion(handle, infoRomObject):\n    c_version = create_string_buffer(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetInforomVersion\")\n    ret = fn(handle, _nvmlInforomObject_t(infoRomObject),\n                 c_version, c_uint(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_version.value\n\n# Added in 4.304\n@convertStrBytes\ndef nvmlDeviceGetInforomImageVersion(handle):\n    c_version = create_string_buffer(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetInforomImageVersion\")\n    ret = fn(handle, c_version, c_uint(NVML_DEVICE_INFOROM_VERSION_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_version.value\n\n# Added in 4.304\ndef nvmlDeviceGetInforomConfigurationChecksum(handle):\n    c_checksum = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetInforomConfigurationChecksum\")\n    ret = fn(handle, byref(c_checksum))\n    _nvmlCheckReturn(ret)\n    return c_checksum.value\n\n# Added in 4.304\ndef nvmlDeviceValidateInforom(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceValidateInforom\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetLastBBXFlushTime(handle):\n    c_timestamp = c_ulonglong()\n    c_durationUs = c_ulong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetLastBBXFlushTime\")\n    ret = fn(handle, byref(c_timestamp), byref(c_durationUs))\n    _nvmlCheckReturn(ret)\n    return [c_timestamp.value, c_durationUs.value]\n\ndef nvmlDeviceGetDisplayMode(handle):\n    c_mode = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDisplayMode\")\n    ret = fn(handle, byref(c_mode))\n    _nvmlCheckReturn(ret)\n    return c_mode.value\n\ndef nvmlDeviceGetDisplayActive(handle):\n    c_mode = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDisplayActive\")\n    ret = fn(handle, byref(c_mode))\n    _nvmlCheckReturn(ret)\n    return c_mode.value\n\n\ndef nvmlDeviceGetPersistenceMode(handle):\n    c_state = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPersistenceMode\")\n    ret = fn(handle, byref(c_state))\n    _nvmlCheckReturn(ret)\n    return c_state.value\n\ndef nvmlDeviceGetPciInfoExt(handle, c_info):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPciInfoExt\")\n    ret = fn(handle, c_info)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetPciInfo_v3(handle):\n    c_info = nvmlPciInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPciInfo_v3\")\n    ret = fn(handle, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\ndef nvmlDeviceGetPciInfo(handle):\n    return nvmlDeviceGetPciInfo_v3(handle)\n\ndef nvmlDeviceGetClockInfo(handle, type):\n    c_clock = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetClockInfo\")\n    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n    _nvmlCheckReturn(ret)\n    return c_clock.value\n\n# Added in 2.285\ndef nvmlDeviceGetMaxClockInfo(handle, type):\n    c_clock = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxClockInfo\")\n    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n    _nvmlCheckReturn(ret)\n    return c_clock.value\n\n# Added in 4.304\ndef nvmlDeviceGetApplicationsClock(handle, type):\n    c_clock = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetApplicationsClock\")\n    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n    _nvmlCheckReturn(ret)\n    return c_clock.value\n\ndef nvmlDeviceGetMaxCustomerBoostClock(handle, type):\n    c_clock = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxCustomerBoostClock\")\n    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n    _nvmlCheckReturn(ret)\n    return c_clock.value\n\ndef nvmlDeviceGetClock(handle, type, id):\n    c_clock = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetClock\")\n    ret = fn(handle, _nvmlClockType_t(type), _nvmlClockId_t(id), byref(c_clock))\n    _nvmlCheckReturn(ret)\n    return c_clock.value\n\n# Added in 5.319\ndef nvmlDeviceGetDefaultApplicationsClock(handle, type):\n    c_clock = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDefaultApplicationsClock\")\n    ret = fn(handle, _nvmlClockType_t(type), byref(c_clock))\n    _nvmlCheckReturn(ret)\n    return c_clock.value\n\n# Added in 4.304\ndef nvmlDeviceGetSupportedMemoryClocks(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedMemoryClocks\")\n    ret = fn(handle, byref(c_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no clocks\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        clocks_array = c_uint * c_count.value\n        c_clocks = clocks_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_count), c_clocks)\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_count.value):\n            procs.append(c_clocks[i])\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\n# Added in 4.304\ndef nvmlDeviceGetSupportedGraphicsClocks(handle, memoryClockMHz):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedGraphicsClocks\")\n    ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no clocks\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        clocks_array = c_uint * c_count.value\n        c_clocks = clocks_array()\n\n        # make the call again\n        ret = fn(handle, c_uint(memoryClockMHz), byref(c_count), c_clocks)\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_count.value):\n            procs.append(c_clocks[i])\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetFanSpeed(handle):\n    c_speed = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFanSpeed\")\n    ret = fn(handle, byref(c_speed))\n    _nvmlCheckReturn(ret)\n    return c_speed.value\n\ndef nvmlDeviceGetFanSpeed_v2(handle, fan):\n    c_speed = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFanSpeed_v2\")\n    ret = fn(handle, fan, byref(c_speed))\n    _nvmlCheckReturn(ret)\n    return c_speed.value\n\nclass c_nvmlFanSpeedInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('fan', c_uint),\n        ('speed', c_uint),\n    ]\n\nnvmlFanSpeedInfo_v1 = 0x100000C\n\ndef nvmlDeviceGetFanSpeedRPM(handle):\n    c_fanSpeed = c_nvmlFanSpeedInfo_t()\n    c_fanSpeed.fan = 0\n    c_fanSpeed.version = nvmlFanSpeedInfo_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFanSpeedRPM\")\n    ret = fn(handle, byref(c_fanSpeed))\n    _nvmlCheckReturn(ret)\n    return c_fanSpeed.speed\n\ndef nvmlDeviceGetTargetFanSpeed(handle, fan):\n    c_speed = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTargetFanSpeed\")\n    ret = fn(handle, fan, byref(c_speed))\n    _nvmlCheckReturn(ret)\n    return c_speed.value\n\ndef nvmlDeviceGetNumFans(device):\n    c_numFans = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNumFans\")\n    ret = fn(device, byref(c_numFans))\n    _nvmlCheckReturn(ret)\n    return c_numFans.value\n\ndef nvmlDeviceSetDefaultFanSpeed_v2(handle, index):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetDefaultFanSpeed_v2\");\n    ret = fn(handle, index)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetMinMaxFanSpeed(handle, minSpeed=c_uint(), maxSpeed=c_uint()):\n    isReference = (type(minSpeed) is not c_uint) or (type(maxSpeed) is not c_uint)\n    minSpeedRef = minSpeed if isReference else byref(minSpeed)\n    maxSpeedRef = maxSpeed if isReference else byref(maxSpeed)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMinMaxFanSpeed\")\n    ret = fn(handle, minSpeedRef, maxSpeedRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else [minSpeed.value, maxSpeed.value]\n\ndef nvmlDeviceGetFanControlPolicy_v2(handle, fan, fanControlPolicy=c_uint()):\n    isReference = type(fanControlPolicy) is not c_uint\n    fanControlPolicyRef = fanControlPolicy if isReference else byref(fanControlPolicy)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFanControlPolicy_v2\")\n    ret = fn(handle, fan, fanControlPolicyRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else fanControlPolicy.value\n\ndef nvmlDeviceSetFanControlPolicy(handle, fan, fanControlPolicy):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetFanControlPolicy\")\n    ret = fn(handle, fan, _nvmlFanControlPolicy_t(fanControlPolicy))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\nclass c_nvmlTemperature_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('sensorType', _nvmlTemperatureSensors_t),\n        ('temperature', c_int),\n    ]\nnvmlTemperature_v1 = 0x100000C\n\ndef nvmlDeviceGetTemperatureV1(handle, sensor):\n    c_temp = c_nvmlTemperature_v1_t()\n    c_temp.version = nvmlTemperature_v1\n    c_temp.sensorType = _nvmlTemperatureSensors_t(sensor) \n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTemperatureV\")\n    ret = fn(handle, byref(c_temp))\n    _nvmlCheckReturn(ret)\n    return c_temp.temperature\n\ndef nvmlDeviceGetTemperatureV(handle, sensor, version=nvmlTemperature_v1):\n    if version == nvmlTemperature_v1:\n        return nvmlDeviceGetTemperatureV1(handle, sensor)\n    else:\n        raise NVMLError(NVML_ERROR_ARGUMENT_VERSION_MISMATCH)\n\n# DEPRECATED use nvmlDeviceGetTemperatureV instead\ndef nvmlDeviceGetTemperature(handle, sensor):\n    c_temp = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTemperature\")\n    ret = fn(handle, _nvmlTemperatureSensors_t(sensor), byref(c_temp))\n    _nvmlCheckReturn(ret)\n    return c_temp.value\n\ndef nvmlDeviceGetTemperatureThreshold(handle, threshold):\n    c_temp = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTemperatureThreshold\")\n    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))\n    _nvmlCheckReturn(ret)\n    return c_temp.value\n\ndef nvmlDeviceSetTemperatureThreshold(handle, threshold, temp):\n    c_temp = c_uint()\n    c_temp.value = temp\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetTemperatureThreshold\")\n    ret = fn(handle, _nvmlTemperatureThresholds_t(threshold), byref(c_temp))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetMarginTemperature(handle):\n    c_marginTempInfo = c_nvmlMarginTemperature_v1_t()\n    c_marginTempInfo.version = nvmlMarginTemperature_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMarginTemperature\")\n    ret = fn(handle, byref(c_marginTempInfo))\n    _nvmlCheckReturn(ret)\n    return c_marginTempInfo.marginTemperature\n\n# DEPRECATED use nvmlDeviceGetPerformanceState\ndef nvmlDeviceGetPowerState(handle):\n    c_pstate = _nvmlPstates_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerState\")\n    ret = fn(handle, byref(c_pstate))\n    _nvmlCheckReturn(ret)\n    return c_pstate.value\n\ndef nvmlDeviceGetPerformanceState(handle):\n    c_pstate = _nvmlPstates_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPerformanceState\")\n    ret = fn(handle, byref(c_pstate))\n    _nvmlCheckReturn(ret)\n    return c_pstate.value\n\ndef nvmlDeviceGetPowerManagementMode(handle):\n    c_pcapMode = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementMode\")\n    ret = fn(handle, byref(c_pcapMode))\n    _nvmlCheckReturn(ret)\n    return c_pcapMode.value\n\ndef nvmlDeviceGetPowerManagementLimit(handle):\n    c_limit = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementLimit\")\n    ret = fn(handle, byref(c_limit))\n    _nvmlCheckReturn(ret)\n    return c_limit.value\n\n# Added in 4.304\ndef nvmlDeviceGetPowerManagementLimitConstraints(handle):\n    c_minLimit = c_uint()\n    c_maxLimit = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementLimitConstraints\")\n    ret = fn(handle, byref(c_minLimit), byref(c_maxLimit))\n    _nvmlCheckReturn(ret)\n    return [c_minLimit.value, c_maxLimit.value]\n\n# Added in 4.304\ndef nvmlDeviceGetPowerManagementDefaultLimit(handle):\n    c_limit = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerManagementDefaultLimit\")\n    ret = fn(handle, byref(c_limit))\n    _nvmlCheckReturn(ret)\n    return c_limit.value\n\n\n# Added in 331\ndef nvmlDeviceGetEnforcedPowerLimit(handle):\n    c_limit = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEnforcedPowerLimit\")\n    ret = fn(handle, byref(c_limit))\n    _nvmlCheckReturn(ret)\n    return c_limit.value\n\ndef nvmlDeviceGetPowerUsage(handle):\n    c_watts = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerUsage\")\n    ret = fn(handle, byref(c_watts))\n    _nvmlCheckReturn(ret)\n    return c_watts.value\n\ndef nvmlDeviceGetTotalEnergyConsumption(handle):\n    c_millijoules = c_uint64()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTotalEnergyConsumption\")\n    ret = fn(handle, byref(c_millijoules))\n    _nvmlCheckReturn(ret)\n    return c_millijoules.value\n\n# Added in 4.304\ndef nvmlDeviceGetGpuOperationMode(handle):\n    c_currState = _nvmlGpuOperationMode_t()\n    c_pendingState = _nvmlGpuOperationMode_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuOperationMode\")\n    ret = fn(handle, byref(c_currState), byref(c_pendingState))\n    _nvmlCheckReturn(ret)\n    return [c_currState.value, c_pendingState.value]\n\n# Added in 4.304\ndef nvmlDeviceGetCurrentGpuOperationMode(handle):\n    return nvmlDeviceGetGpuOperationMode(handle)[0]\n\n# Added in 4.304\ndef nvmlDeviceGetPendingGpuOperationMode(handle):\n    return nvmlDeviceGetGpuOperationMode(handle)[1]\n\ndef nvmlDeviceGetMemoryInfo(handle, version=None):\n    if not version:\n        c_memory = c_nvmlMemory_t()\n        fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryInfo\")\n    else:\n        c_memory = c_nvmlMemory_v2_t()\n        c_memory.version = version\n        fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryInfo_v2\")\n    ret = fn(handle, byref(c_memory))\n    _nvmlCheckReturn(ret)\n    return c_memory\n\ndef nvmlDeviceGetBAR1MemoryInfo(handle):\n    c_bar1_memory = c_nvmlBAR1Memory_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBAR1MemoryInfo\")\n    ret = fn(handle, byref(c_bar1_memory))\n    _nvmlCheckReturn(ret)\n    return c_bar1_memory\n\ndef nvmlDeviceGetComputeMode(handle):\n    c_mode = _nvmlComputeMode_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeMode\")\n    ret = fn(handle, byref(c_mode))\n    _nvmlCheckReturn(ret)\n    return c_mode.value\n\ndef nvmlDeviceGetCudaComputeCapability(handle):\n    c_major = c_int()\n    c_minor = c_int()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCudaComputeCapability\")\n    ret = fn(handle, byref(c_major), byref(c_minor))\n    _nvmlCheckReturn(ret)\n    return (c_major.value, c_minor.value)\n\ndef nvmlDeviceGetEccMode(handle):\n    c_currState = _nvmlEnableState_t()\n    c_pendingState = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEccMode\")\n    ret = fn(handle, byref(c_currState), byref(c_pendingState))\n    _nvmlCheckReturn(ret)\n    return [c_currState.value, c_pendingState.value]\n\n# added to API\ndef nvmlDeviceGetCurrentEccMode(handle):\n    return nvmlDeviceGetEccMode(handle)[0]\n\n# added to API\ndef nvmlDeviceGetPendingEccMode(handle):\n    return nvmlDeviceGetEccMode(handle)[1]\n\ndef nvmlDeviceGetDefaultEccMode(handle):\n    c_defaultState = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDefaultEccMode\")\n    ret = fn(handle, byref(c_defaultState))\n    _nvmlCheckReturn(ret)\n    return [c_defaultState.value]\n\ndef nvmlDeviceGetTotalEccErrors(handle, errorType, counterType):\n    c_count = c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTotalEccErrors\")\n    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),\n                 _nvmlEccCounterType_t(counterType), byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\n# This is deprecated, instead use nvmlDeviceGetMemoryErrorCounter\ndef nvmlDeviceGetDetailedEccErrors(handle, errorType, counterType):\n    c_counts = c_nvmlEccErrorCounts_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDetailedEccErrors\")\n    ret = fn(handle, _nvmlMemoryErrorType_t(errorType),\n                 _nvmlEccCounterType_t(counterType), byref(c_counts))\n    _nvmlCheckReturn(ret)\n    return c_counts\n\n# Added in 4.304\ndef nvmlDeviceGetMemoryErrorCounter(handle, errorType, counterType, locationType):\n    c_count = c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryErrorCounter\")\n    ret = fn(handle,\n             _nvmlMemoryErrorType_t(errorType),\n             _nvmlEccCounterType_t(counterType),\n             _nvmlMemoryLocation_t(locationType),\n             byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlDeviceGetUtilizationRates(handle):\n    c_util = c_nvmlUtilization_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetUtilizationRates\")\n    ret = fn(handle, byref(c_util))\n    _nvmlCheckReturn(ret)\n    return c_util\n\ndef nvmlDeviceGetEncoderUtilization(handle):\n    c_util = c_uint()\n    c_samplingPeriod = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEncoderUtilization\")\n    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))\n    _nvmlCheckReturn(ret)\n    return [c_util.value, c_samplingPeriod.value]\n\ndef nvmlDeviceGetDecoderUtilization(handle):\n    c_util = c_uint()\n    c_samplingPeriod = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDecoderUtilization\")\n    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))\n    _nvmlCheckReturn(ret)\n    return [c_util.value, c_samplingPeriod.value]\n\ndef nvmlDeviceGetJpgUtilization(handle):\n    c_util = c_uint()\n    c_samplingPeriod = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetJpgUtilization\")\n    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))\n    _nvmlCheckReturn(ret)\n    return [c_util.value, c_samplingPeriod.value]\n\ndef nvmlDeviceGetOfaUtilization(handle):\n    c_util = c_uint()\n    c_samplingPeriod = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetOfaUtilization\")\n    ret = fn(handle, byref(c_util), byref(c_samplingPeriod))\n    _nvmlCheckReturn(ret)\n    return [c_util.value, c_samplingPeriod.value]\n\ndef nvmlDeviceGetPcieReplayCounter(handle):\n    c_replay = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPcieReplayCounter\")\n    ret = fn(handle, byref(c_replay))\n    _nvmlCheckReturn(ret)\n    return c_replay.value\n\ndef nvmlDeviceGetDriverModel(handle):\n    c_currModel = _nvmlDriverModel_t()\n    c_pendingModel = _nvmlDriverModel_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDriverModel\")\n    ret = fn(handle, byref(c_currModel), byref(c_pendingModel))\n    _nvmlCheckReturn(ret)\n    return [c_currModel.value, c_pendingModel.value]\n\n# added to API\ndef nvmlDeviceGetCurrentDriverModel(handle):\n    return nvmlDeviceGetDriverModel(handle)[0]\n\n# added to API\ndef nvmlDeviceGetPendingDriverModel(handle):\n    return nvmlDeviceGetDriverModel(handle)[1]\n\n# Added in 2.285\n@convertStrBytes\ndef nvmlDeviceGetVbiosVersion(handle):\n    c_version = create_string_buffer(NVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVbiosVersion\")\n    ret = fn(handle, c_version, c_uint(NVML_DEVICE_VBIOS_VERSION_BUFFER_SIZE))\n    _nvmlCheckReturn(ret)\n    return c_version.value\n\n# Added in 2.285\ndef nvmlDeviceGetComputeRunningProcesses_v2(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeRunningProcesses_v2\")\n    ret = fn(handle, byref(c_count), None)\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        # oversize the array incase more processes are created\n        c_count.value = c_count.value * 2 + 5\n        proc_array = c_nvmlProcessInfo_v2_t * c_count.value\n        c_procs = proc_array()\n        # make the call again\n        ret = fn(handle, byref(c_count), c_procs)\n        _nvmlCheckReturn(ret)\n        procs = []\n        for i in range(c_count.value):\n            # use an alternative struct for this object\n            obj = nvmlStructToFriendlyObject(c_procs[i])\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                # special case for WDDM on Windows, see comment above\n                obj.usedGpuMemory = None\n            procs.append(obj)\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\n# Added in 2.285\ndef nvmlDeviceGetComputeRunningProcesses_v3(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeRunningProcesses_v3\")\n    ret = fn(handle, byref(c_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        # oversize the array incase more processes are created\n        c_count.value = c_count.value * 2 + 5\n        proc_array = c_nvmlProcessInfo_v3_t * c_count.value\n        c_procs = proc_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_count), c_procs)\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_count.value):\n            # use an alternative struct for this object\n            obj = nvmlStructToFriendlyObject(c_procs[i])\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                # special case for WDDM on Windows, see comment above\n                obj.usedGpuMemory = None\n            procs.append(obj)\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\n@throwOnVersionMismatch\ndef nvmlDeviceGetComputeRunningProcesses(handle):\n    return nvmlDeviceGetComputeRunningProcesses_v3(handle)\n\ndef nvmlDeviceGetGraphicsRunningProcesses_v2(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGraphicsRunningProcesses_v2\")\n    ret = fn(handle, byref(c_count), None)\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        # oversize the array incase more processes are created\n        c_count.value = c_count.value * 2 + 5\n        proc_array = c_nvmlProcessInfo_v2_t * c_count.value\n        c_procs = proc_array()\n        # make the call again\n        ret = fn(handle, byref(c_count), c_procs)\n        _nvmlCheckReturn(ret)\n        procs = []\n        for i in range(c_count.value):\n            # use an alternative struct for this object\n            obj = nvmlStructToFriendlyObject(c_procs[i])\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                # special case for WDDM on Windows, see comment above\n                obj.usedGpuMemory = None\n            procs.append(obj)\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetGraphicsRunningProcesses_v3(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGraphicsRunningProcesses_v3\")\n    ret = fn(handle, byref(c_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        # oversize the array incase more processes are created\n        c_count.value = c_count.value * 2 + 5\n        proc_array = c_nvmlProcessInfo_v3_t * c_count.value\n        c_procs = proc_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_count), c_procs)\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_count.value):\n            # use an alternative struct for this object\n            obj = nvmlStructToFriendlyObject(c_procs[i])\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                # special case for WDDM on Windows, see comment above\n                obj.usedGpuMemory = None\n            procs.append(obj)\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\n@throwOnVersionMismatch\ndef nvmlDeviceGetGraphicsRunningProcesses(handle):\n    return nvmlDeviceGetGraphicsRunningProcesses_v3(handle)\n\n@throwOnVersionMismatch\ndef nvmlDeviceGetMPSComputeRunningProcesses(handle):\n    return nvmlDeviceGetMPSComputeRunningProcesses_v3(handle)\n\ndef nvmlDeviceGetMPSComputeRunningProcesses_v2(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMPSComputeRunningProcesses_v2\")\n    ret = fn(handle, byref(c_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        # oversize the array incase more processes are created\n        c_count.value = c_count.value * 2 + 5\n        proc_array = c_nvmlProcessInfo_v2_t * c_count.value\n        c_procs = proc_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_count), c_procs)\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_count.value):\n            # use an alternative struct for this object\n            obj = nvmlStructToFriendlyObject(c_procs[i])\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                # special case for WDDM on Windows, see comment above\n                obj.usedGpuMemory = None\n            procs.append(obj)\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetMPSComputeRunningProcesses_v3(handle):\n    # first call to get the size\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMPSComputeRunningProcesses_v3\")\n    ret = fn(handle, byref(c_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        # oversize the array incase more processes are created\n        c_count.value = c_count.value * 2 + 5\n        proc_array = c_nvmlProcessInfo_v3_t * c_count.value\n        c_procs = proc_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_count), c_procs)\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_count.value):\n            # use an alternative struct for this object\n            obj = nvmlStructToFriendlyObject(c_procs[i])\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                # special case for WDDM on Windows, see comment above\n                obj.usedGpuMemory = None\n            procs.append(obj)\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetRunningProcessDetailList(handle, version, mode):\n    c_processDetailList = c_nvmlProcessDetailList_t()\n    c_processDetailList.version = version\n    c_processDetailList.mode = mode\n\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRunningProcessDetailList\")\n\n    # first call to get the size\n    ret = fn(handle, byref(c_processDetailList))\n    if (ret == NVML_SUCCESS):\n        # special case, no running processes\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        c_procs = c_nvmlProcessDetail_v1_t * c_processDetailList.numProcArrayEntries\n        c_processDetailList.procArray = cast((c_procs)(), POINTER(c_nvmlProcessDetail_v1_t))\n\n        # make the call again\n        ret = fn(handle, byref(c_processDetailList))\n        _nvmlCheckReturn(ret)\n\n        procs = []\n        for i in range(c_processDetailList.numProcArrayEntries):\n            # use an alternative struct for this object\n            obj = c_processDetailList.procArray[i]\n            if (obj.usedGpuMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                obj.usedGpuMemory = None\n            if (obj.usedGpuCcProtectedMemory == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n                obj.usedGpuCcProtectedMemory = None\n            procs.append(obj)\n\n        return procs\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetAutoBoostedClocksEnabled(handle):\n    c_isEnabled = _nvmlEnableState_t()\n    c_defaultIsEnabled = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAutoBoostedClocksEnabled\")\n    ret = fn(handle, byref(c_isEnabled), byref(c_defaultIsEnabled))\n    _nvmlCheckReturn(ret)\n    return [c_isEnabled.value, c_defaultIsEnabled.value]\n    #Throws NVML_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks\n\n## Set functions\ndef nvmlUnitSetLedState(unit, color):\n    fn = _nvmlGetFunctionPointer(\"nvmlUnitSetLedState\")\n    ret = fn(unit, _nvmlLedColor_t(color))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetPersistenceMode(handle, mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetPersistenceMode\")\n    ret = fn(handle, _nvmlEnableState_t(mode))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetComputeMode(handle, mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetComputeMode\")\n    ret = fn(handle, _nvmlComputeMode_t(mode))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetEccMode(handle, mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetEccMode\")\n    ret = fn(handle, _nvmlEnableState_t(mode))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceClearEccErrorCounts(handle, counterType):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearEccErrorCounts\")\n    ret = fn(handle, _nvmlEccCounterType_t(counterType))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetDriverModel(handle, model):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetDriverModel\")\n    ret = fn(handle, _nvmlDriverModel_t(model))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetAutoBoostedClocksEnabled(handle, enabled):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetAutoBoostedClocksEnabled\")\n    ret = fn(handle, _nvmlEnableState_t(enabled))\n    _nvmlCheckReturn(ret)\n    return None\n    #Throws NVML_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks\n\ndef nvmlDeviceSetDefaultAutoBoostedClocksEnabled(handle, enabled, flags):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetDefaultAutoBoostedClocksEnabled\")\n    ret = fn(handle, _nvmlEnableState_t(enabled), c_uint(flags))\n    _nvmlCheckReturn(ret)\n    return None\n    #Throws NVML_ERROR_NOT_SUPPORTED if hardware doesn't support setting auto boosted clocks\n\ndef nvmlDeviceSetGpuLockedClocks(handle, minGpuClockMHz, maxGpuClockMHz):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetGpuLockedClocks\")\n    ret = fn(handle, c_uint(minGpuClockMHz), c_uint(maxGpuClockMHz))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceResetGpuLockedClocks(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceResetGpuLockedClocks\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetMemoryLockedClocks(handle, minMemClockMHz, maxMemClockMHz):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetMemoryLockedClocks\")\n    ret = fn(handle, c_uint(minMemClockMHz), c_uint(maxMemClockMHz))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceResetMemoryLockedClocks(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceResetMemoryLockedClocks\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetClkMonStatus(handle, c_clkMonInfo=nvmlClkMonStatus_t()):\n    isReference = type(c_clkMonInfo) is not nvmlClkMonStatus_t\n    c_clkMonInfoRef = c_clkMonInfo if isReference else byref(c_clkMonInfo)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetClkMonStatus\")\n    ret = fn(handle, c_clkMonInfoRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else c_clkMonInfo\n\n# Added in 4.304\ndef nvmlDeviceSetApplicationsClocks(handle, maxMemClockMHz, maxGraphicsClockMHz):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetApplicationsClocks\")\n    ret = fn(handle, c_uint(maxMemClockMHz), c_uint(maxGraphicsClockMHz))\n    _nvmlCheckReturn(ret)\n    return None\n\n# Added in 4.304\ndef nvmlDeviceResetApplicationsClocks(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceResetApplicationsClocks\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\n# Added in 4.304\ndef nvmlDeviceSetPowerManagementLimit(handle, limit):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetPowerManagementLimit\")\n    ret = fn(handle, c_uint(limit))\n    _nvmlCheckReturn(ret)\n    return None\n\n# Added in 4.304\ndef nvmlDeviceSetGpuOperationMode(handle, mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetGpuOperationMode\")\n    ret = fn(handle, _nvmlGpuOperationMode_t(mode))\n    _nvmlCheckReturn(ret)\n    return None\n\n# Added in 2.285\ndef nvmlEventSetCreate():\n    fn = _nvmlGetFunctionPointer(\"nvmlEventSetCreate\")\n    eventSet = c_nvmlEventSet_t()\n    ret = fn(byref(eventSet))\n    _nvmlCheckReturn(ret)\n    return eventSet\n\n# Added in 2.285\ndef nvmlDeviceRegisterEvents(handle, eventTypes, eventSet):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceRegisterEvents\")\n    ret = fn(handle, c_ulonglong(eventTypes), eventSet)\n    _nvmlCheckReturn(ret)\n    return None\n\n# Added in 2.285\ndef nvmlDeviceGetSupportedEventTypes(handle):\n    c_eventTypes = c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedEventTypes\")\n    ret = fn(handle, byref(c_eventTypes))\n    _nvmlCheckReturn(ret)\n    return c_eventTypes.value\n\n# raises NVML_ERROR_TIMEOUT exception on timeout\ndef nvmlEventSetWait_v2(eventSet, timeoutms):\n    fn = _nvmlGetFunctionPointer(\"nvmlEventSetWait_v2\")\n    data = c_nvmlEventData_t()\n    ret = fn(eventSet, byref(data), c_uint(timeoutms))\n    _nvmlCheckReturn(ret)\n    return data\n\ndef nvmlEventSetWait(eventSet, timeoutms):\n    return nvmlEventSetWait_v2(eventSet, timeoutms)\n\n# Added in 2.285\ndef nvmlEventSetFree(eventSet):\n    fn = _nvmlGetFunctionPointer(\"nvmlEventSetFree\")\n    ret = fn(eventSet)\n    _nvmlCheckReturn(ret)\n    return None\n\n# Added in 3.295\ndef nvmlDeviceOnSameBoard(handle1, handle2):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceOnSameBoard\")\n    onSameBoard = c_int()\n    ret = fn(handle1, handle2, byref(onSameBoard))\n    _nvmlCheckReturn(ret)\n    return (onSameBoard.value != 0)\n\n# Added in 3.295\ndef nvmlDeviceGetCurrPcieLinkGeneration(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrPcieLinkGeneration\")\n    gen = c_uint()\n    ret = fn(handle, byref(gen))\n    _nvmlCheckReturn(ret)\n    return gen.value\n\n# Added in 3.295\ndef nvmlDeviceGetMaxPcieLinkGeneration(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxPcieLinkGeneration\")\n    gen = c_uint()\n    ret = fn(handle, byref(gen))\n    _nvmlCheckReturn(ret)\n    return gen.value\n\n# Added in 3.295\ndef nvmlDeviceGetCurrPcieLinkWidth(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrPcieLinkWidth\")\n    width = c_uint()\n    ret = fn(handle, byref(width))\n    _nvmlCheckReturn(ret)\n    return width.value\n\n# Added in 3.295\ndef nvmlDeviceGetMaxPcieLinkWidth(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxPcieLinkWidth\")\n    width = c_uint()\n    ret = fn(handle, byref(width))\n    _nvmlCheckReturn(ret)\n    return width.value\n\ndef nvmlDeviceGetGpuMaxPcieLinkGeneration(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuMaxPcieLinkGeneration\")\n    gen = c_uint()\n    ret = fn(handle, byref(gen))\n    _nvmlCheckReturn(ret)\n    return gen.value\n\n# Added in 4.304\ndef nvmlDeviceGetSupportedClocksThrottleReasons(handle):\n    c_reasons= c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedClocksThrottleReasons\")\n    ret = fn(handle, byref(c_reasons))\n    _nvmlCheckReturn(ret)\n    return c_reasons.value\n\ndef nvmlDeviceGetSupportedClocksEventReasons(handle):\n    c_reasons= c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedClocksEventReasons\")\n    ret = fn(handle, byref(c_reasons))\n    _nvmlCheckReturn(ret)\n    return c_reasons.value\n\n# Added in 4.304\ndef nvmlDeviceGetCurrentClocksThrottleReasons(handle):\n    c_reasons= c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrentClocksThrottleReasons\")\n    ret = fn(handle, byref(c_reasons))\n    _nvmlCheckReturn(ret)\n    return c_reasons.value\n\ndef nvmlDeviceGetCurrentClocksEventReasons(handle):\n    c_reasons= c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCurrentClocksEventReasons\")\n    ret = fn(handle, byref(c_reasons))\n    _nvmlCheckReturn(ret)\n    return c_reasons.value\n\n# Added in 5.319\ndef nvmlDeviceGetIndex(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetIndex\")\n    c_index = c_uint()\n    ret = fn(handle, byref(c_index))\n    _nvmlCheckReturn(ret)\n    return c_index.value\n\n# Added in 5.319\ndef nvmlDeviceGetAccountingMode(handle):\n    c_mode = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingMode\")\n    ret = fn(handle, byref(c_mode))\n    _nvmlCheckReturn(ret)\n    return c_mode.value\n\ndef nvmlDeviceSetAccountingMode(handle, mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetAccountingMode\")\n    ret = fn(handle, _nvmlEnableState_t(mode))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceClearAccountingPids(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearAccountingPids\")\n    ret = fn(handle)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetAccountingStats(handle, pid):\n    stats = c_nvmlAccountingStats_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingStats\")\n    ret = fn(handle, c_uint(pid), byref(stats))\n    _nvmlCheckReturn(ret)\n    if (stats.maxMemoryUsage == NVML_VALUE_NOT_AVAILABLE_ulonglong.value):\n        # special case for WDDM on Windows, see comment above\n        stats.maxMemoryUsage = None\n    return stats\n\ndef nvmlDeviceGetAccountingPids(handle):\n    count = c_uint(nvmlDeviceGetAccountingBufferSize(handle))\n    pids = (c_uint * count.value)()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingPids\")\n    ret = fn(handle, byref(count), pids)\n    _nvmlCheckReturn(ret)\n    return list(map(int, pids[0:count.value]))\n\ndef nvmlDeviceGetAccountingBufferSize(handle):\n    bufferSize = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAccountingBufferSize\")\n    ret = fn(handle, byref(bufferSize))\n    _nvmlCheckReturn(ret)\n    return int(bufferSize.value)\n\ndef nvmlDeviceGetRetiredPages(device, sourceFilter):\n    c_source = _nvmlPageRetirementCause_t(sourceFilter)\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRetiredPages\")\n\n    # First call will get the size\n    ret = fn(device, c_source, byref(c_count), None)\n\n    # this should only fail with insufficient size\n    if ((ret != NVML_SUCCESS) and\n        (ret != NVML_ERROR_INSUFFICIENT_SIZE)):\n        raise NVMLError(ret)\n\n    # call again with a buffer\n    # oversize the array for the rare cases where additional pages\n    # are retired between NVML calls\n    c_count.value = c_count.value * 2 + 5\n    page_array = c_ulonglong * c_count.value\n    c_pages = page_array()\n    ret = fn(device, c_source, byref(c_count), c_pages)\n    _nvmlCheckReturn(ret)\n    return list(map(int, c_pages[0:c_count.value]))\n\ndef nvmlDeviceGetRetiredPages_v2(device, sourceFilter):\n    c_source = _nvmlPageRetirementCause_t(sourceFilter)\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRetiredPages_v2\")\n\n    # First call will get the size\n    ret = fn(device, c_source, byref(c_count), None)\n\n    # this should only fail with insufficient size\n    if ((ret != NVML_SUCCESS) and\n        (ret != NVML_ERROR_INSUFFICIENT_SIZE)):\n        raise NVMLError(ret)\n\n    # call again with a buffer\n    # oversize the array for the rare cases where additional pages\n    # are retired between NVML calls\n    c_count.value = c_count.value * 2 + 5\n    page_array = c_ulonglong * c_count.value\n    c_pages = page_array()\n    times_array = c_ulonglong * c_count.value\n    c_times = times_array()\n    ret = fn(device, c_source, byref(c_count), c_pages, c_times)\n    _nvmlCheckReturn(ret)\n    return [ { 'address': int(c_pages[i]), 'timestamp': int(c_times[i]) } for i in range(c_count.value) ];\n\ndef nvmlDeviceGetRetiredPagesPendingStatus(device):\n    c_pending = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRetiredPagesPendingStatus\")\n    ret = fn(device, byref(c_pending))\n    _nvmlCheckReturn(ret)\n    return int(c_pending.value)\n\ndef nvmlDeviceGetAPIRestriction(device, apiType):\n    c_permission = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAPIRestriction\")\n    ret = fn(device, _nvmlRestrictedAPI_t(apiType), byref(c_permission))\n    _nvmlCheckReturn(ret)\n    return int(c_permission.value)\n\ndef nvmlDeviceSetAPIRestriction(handle, apiType, isRestricted):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetAPIRestriction\")\n    ret = fn(handle, _nvmlRestrictedAPI_t(apiType), _nvmlEnableState_t(isRestricted))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetBridgeChipInfo(handle):\n    bridgeHierarchy = c_nvmlBridgeChipHierarchy_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBridgeChipInfo\")\n    ret = fn(handle, byref(bridgeHierarchy))\n    _nvmlCheckReturn(ret)\n    return bridgeHierarchy\n\ndef nvmlDeviceGetSamples(device, sampling_type, timeStamp):\n    c_sampling_type = _nvmlSamplingType_t(sampling_type)\n    c_time_stamp = c_ulonglong(timeStamp)\n    c_sample_count = c_uint(0)\n    c_sample_value_type = _nvmlValueType_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSamples\")\n\n    ## First Call gets the size\n    ret = fn(device, c_sampling_type, c_time_stamp, byref(c_sample_value_type), byref(c_sample_count), None)\n\n    # Stop if this fails\n    if (ret != NVML_SUCCESS):\n        raise NVMLError(ret)\n\n    sampleArray = c_sample_count.value * c_nvmlSample_t\n    c_samples = sampleArray()\n    ret = fn(device, c_sampling_type, c_time_stamp,  byref(c_sample_value_type), byref(c_sample_count), c_samples)\n    _nvmlCheckReturn(ret)\n    return (c_sample_value_type.value, c_samples[0:c_sample_count.value])\n\ndef nvmlDeviceGetViolationStatus(device, perfPolicyType):\n    c_perfPolicy_type = _nvmlPerfPolicyType_t(perfPolicyType)\n    c_violTime = c_nvmlViolationTime_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetViolationStatus\")\n\n    ## Invoke the method to get violation time\n    ret = fn(device, c_perfPolicy_type, byref(c_violTime))\n    _nvmlCheckReturn(ret)\n    return c_violTime\n\ndef nvmlDeviceGetPcieThroughput(device, counter):\n    c_util = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPcieThroughput\")\n    ret = fn(device, _nvmlPcieUtilCounter_t(counter), byref(c_util))\n    _nvmlCheckReturn(ret)\n    return c_util.value\n\ndef nvmlSystemGetTopologyGpuSet(cpuNumber):\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetTopologyGpuSet\")\n\n    # First call will get the size\n    ret = fn(cpuNumber, byref(c_count), None)\n\n    if ret != NVML_SUCCESS:\n        raise NVMLError(ret)\n    # call again with a buffer\n    device_array = c_nvmlDevice_t * c_count.value\n    c_devices = device_array()\n    ret = fn(cpuNumber, byref(c_count), c_devices)\n    _nvmlCheckReturn(ret)\n    return list(c_devices[0:c_count.value])\n\ndef nvmlDeviceGetTopologyNearestGpus(device, level):\n    c_count = c_uint(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTopologyNearestGpus\")\n\n    # First call will get the size\n    ret = fn(device, level, byref(c_count), None)\n\n    if ret != NVML_SUCCESS:\n        raise NVMLError(ret)\n\n    # call again with a buffer\n    device_array = c_nvmlDevice_t * c_count.value\n    c_devices = device_array()\n    ret = fn(device, level, byref(c_count), c_devices)\n    _nvmlCheckReturn(ret)\n    return list(c_devices[0:c_count.value])\n\ndef nvmlDeviceGetTopologyCommonAncestor(device1, device2):\n    c_level = _nvmlGpuTopologyLevel_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetTopologyCommonAncestor\")\n    ret = fn(device1, device2, byref(c_level))\n    _nvmlCheckReturn(ret)\n    return c_level.value\n\ndef nvmlDeviceGetNvLinkUtilizationCounter(device, link, counter):\n    c_rxcounter = c_ulonglong()\n    c_txcounter = c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkUtilizationCounter\")\n    ret = fn(device, link, counter, byref(c_rxcounter), byref(c_txcounter))\n    _nvmlCheckReturn(ret)\n    return (c_rxcounter.value, c_txcounter.value)\n\ndef nvmlDeviceFreezeNvLinkUtilizationCounter(device, link, counter, freeze):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceFreezeNvLinkUtilizationCounter\")\n    ret = fn(device, link, counter, freeze)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceResetNvLinkUtilizationCounter(device, link, counter):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceResetNvLinkUtilizationCounter\")\n    ret = fn(device, link, counter)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceSetNvLinkUtilizationControl(device, link, counter, control, reset):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetNvLinkUtilizationControl\")\n    ret = fn(device, link, counter, byref(control), reset)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetNvLinkUtilizationControl(device, link, counter):\n    c_control = nvmlNvLinkUtilizationControl_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkUtilizationControl\")\n    ret = fn(device, link, counter, byref(c_control))\n    _nvmlCheckReturn(ret)\n    return c_control\n\ndef nvmlDeviceGetNvLinkCapability(device, link, capability):\n    c_capResult = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkCapability\")\n    ret = fn(device, link, capability, byref(c_capResult))\n    _nvmlCheckReturn(ret)\n    return c_capResult.value\n\ndef nvmlDeviceGetNvLinkErrorCounter(device, link, counter):\n    c_result = c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkErrorCounter\")\n    ret = fn(device, link, counter, byref(c_result))\n    _nvmlCheckReturn(ret)\n    return c_result.value\n\ndef nvmlDeviceResetNvLinkErrorCounters(device, link):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceResetNvLinkErrorCounters\")\n    ret = fn(device, link)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetNvLinkRemotePciInfo(device, link):\n    c_pci = nvmlPciInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkRemotePciInfo_v2\")\n    ret = fn(device, link, byref(c_pci))\n    _nvmlCheckReturn(ret)\n    return c_pci\n\ndef nvmlDeviceGetNvLinkRemoteDeviceType(handle, link):\n    c_type = _nvmlNvLinkDeviceType_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkRemoteDeviceType\")\n    ret = fn(handle, link, byref(c_type))\n    _nvmlCheckReturn(ret)\n    return c_type.value\n\ndef nvmlDeviceGetNvLinkState(device, link):\n    c_isActive = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkState\")\n    ret = fn(device, link, byref(c_isActive))\n    _nvmlCheckReturn(ret)\n    return c_isActive.value\n\ndef nvmlDeviceGetNvLinkVersion(device, link):\n    c_version = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvLinkVersion\")\n    ret = fn(device, link, byref(c_version))\n    _nvmlCheckReturn(ret)\n    return c_version.value\n\ndef nvmlDeviceModifyDrainState(pciInfo, newState):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceModifyDrainState\")\n    ret = fn(pointer(pciInfo), newState)\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceQueryDrainState(pciInfo):\n    c_newState = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceQueryDrainState\")\n    ret = fn(pointer(pciInfo), byref(c_newState))\n    _nvmlCheckReturn(ret)\n    return c_newState.value\n\ndef nvmlDeviceRemoveGpu(pciInfo):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceRemoveGpu\")\n    ret = fn(pointer(pciInfo))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceDiscoverGpus(pciInfo):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceDiscoverGpus\")\n    ret = fn(pointer(pciInfo))\n    _nvmlCheckReturn(ret)\n    return None\n\ndef nvmlDeviceGetFieldValues(handle, fieldIds):\n    values_arr = c_nvmlFieldValue_t * len(fieldIds)\n    values = values_arr()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFieldValues\")\n\n    for i, fieldId in enumerate(fieldIds):\n        try:\n            (values[i].fieldId, values[i].scopeId) = fieldId\n        except TypeError:\n            values[i].fieldId = fieldId\n\n    ret = fn(handle, c_int32(len(fieldIds)), byref(values))\n    _nvmlCheckReturn(ret)\n    return values\n\ndef nvmlDeviceClearFieldValues(handle, fieldIds):\n    values_arr = c_nvmlFieldValue_t * len(fieldIds)\n    values = values_arr()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceClearFieldValues\")\n\n    for i, fieldId in enumerate(fieldIds):\n        try:\n            (values[i].fieldId, values[i].scopeId) = fieldId\n        except TypeError:\n            values[i].fieldId = fieldId\n\n    ret = fn(handle, c_int32(len(fieldIds)), byref(values))\n    _nvmlCheckReturn(ret)\n    return values\n\ndef nvmlDeviceGetVirtualizationMode(handle):\n    c_virtualization_mode = c_ulonglong()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVirtualizationMode\")\n    ret = fn(handle, byref(c_virtualization_mode))\n    _nvmlCheckReturn(ret)\n    return c_virtualization_mode.value\n\ndef nvmlDeviceSetVirtualizationMode(handle, virtualization_mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetVirtualizationMode\")\n    return fn(handle, virtualization_mode)\n\ndef nvmlDeviceGetVgpuHeterogeneousMode(handle):\n    c_vgpuHeterogeneousMode = c_nvmlVgpuHeterogeneousMode_v1_t(0)\n    c_vgpuHeterogeneousMode.version = VgpuHeterogeneousMode_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuHeterogeneousMode\")\n    ret = fn(handle, byref(c_vgpuHeterogeneousMode))\n    _nvmlCheckReturn(ret)\n    return c_vgpuHeterogeneousMode.mode\n\ndef nvmlDeviceSetVgpuHeterogeneousMode(handle, heterogeneous_mode):\n    c_vgpuHeterogeneousMode = c_nvmlVgpuHeterogeneousMode_v1_t(0)\n    c_vgpuHeterogeneousMode.version = VgpuHeterogeneousMode_v1\n    c_vgpuHeterogeneousMode.mode = heterogeneous_mode\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetVgpuHeterogeneousMode\")\n    ret = fn(handle, byref(c_vgpuHeterogeneousMode))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlVgpuInstanceGetPlacementId(vgpuInstance):\n    c_placement = c_nvmlVgpuPlacementId_v1_t(0)\n    c_placement.version = VgpuPlacementId_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetPlacementId\")\n    ret = fn(vgpuInstance, byref(c_placement))\n    _nvmlCheckReturn(ret)\n    return c_placement.placementId\n\ndef nvmlDeviceGetVgpuTypeSupportedPlacements(handle, vgpuTypeId, mode=0, version=1):\n    c_max_instances = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetMaxInstances\")\n    ret = fn(handle, vgpuTypeId, byref(c_max_instances))\n    _nvmlCheckReturn(ret)\n\n    if version == 2:\n        c_vgpu_placements = c_nvmlVgpuPlacementList_v2_t()\n        c_vgpu_placements.version = VgpuPlacementList_v2\n        c_vgpu_placements.count = c_max_instances.value\n        c_vgpu_placements.mode = mode\n    elif version == 1:\n        c_vgpu_placements = c_nvmlVgpuPlacementList_v1_t()\n        c_vgpu_placements.version = VgpuPlacementList_v1\n    else:\n        raise NVMLError(NVML_ERROR_ARGUMENT_VERSION_MISMATCH)\n\n    c_placements = c_uint * c_max_instances.value\n    c_vgpu_placements.placementIds = c_placements()\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuTypeSupportedPlacements\")\n    ret = fn(handle, vgpuTypeId, byref(c_vgpu_placements))\n    _nvmlCheckReturn(ret)\n    return c_vgpu_placements\n\ndef nvmlDeviceGetVgpuTypeCreatablePlacements(handle, vgpuTypeId, version=1):\n    c_max_instances = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetMaxInstances\")\n    ret = fn(handle, vgpuTypeId, byref(c_max_instances))\n    _nvmlCheckReturn(ret)\n\n    if version == 2:\n        c_vgpu_placements = c_nvmlVgpuPlacementList_v2_t()\n        c_vgpu_placements.version = VgpuPlacementList_v2\n        c_vgpu_placements.count = c_max_instances.value\n    elif version == 1:\n        c_vgpu_placements = c_nvmlVgpuPlacementList_v1_t()\n        c_vgpu_placements.version = VgpuPlacementList_v1\n\n    c_placements = c_uint * c_max_instances.value\n    c_vgpu_placements.placementIds = c_placements()\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuTypeCreatablePlacements\")\n    ret = fn(handle, vgpuTypeId, byref(c_vgpu_placements))\n    _nvmlCheckReturn(ret)\n    return c_vgpu_placements\n\ndef nvmlGetVgpuDriverCapabilities(capability):\n    c_capResult = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlGetVgpuDriverCapabilities\")\n    ret = fn(_nvmlVgpuDriverCapability_t(capability), byref(c_capResult))\n    _nvmlCheckReturn(ret)\n    return c_capResult.value\n\ndef nvmlDeviceGetVgpuCapabilities(handle, capability):\n    c_capResult = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuCapabilities\")\n    ret = fn(handle, _nvmlDeviceVgpuCapability_t(capability), byref(c_capResult))\n    _nvmlCheckReturn(ret)\n    return c_capResult.value\n\ndef nvmlDeviceSetVgpuCapabilities(handle, capability, state):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetVgpuCapabilities\")\n    ret = fn(handle, _nvmlDeviceVgpuCapability_t(capability), state)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetSupportedVgpus(handle):\n    # first call to get the size\n    c_vgpu_count = c_uint(0)\n\n    fn =  _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedVgpus\")\n    ret = fn(handle, byref(c_vgpu_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no supported vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        vgpu_type_ids_array = _nvmlVgpuTypeId_t * c_vgpu_count.value\n        c_vgpu_type_ids = vgpu_type_ids_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_vgpu_count), c_vgpu_type_ids)\n        _nvmlCheckReturn(ret)\n        vgpus = []\n        for i in range(c_vgpu_count.value):\n            vgpus.append(c_vgpu_type_ids[i])\n        return vgpus\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetCreatableVgpus(handle):\n    # first call to get the size\n    c_vgpu_count = c_uint(0)\n\n    fn =  _nvmlGetFunctionPointer(\"nvmlDeviceGetCreatableVgpus\")\n    ret = fn(handle, byref(c_vgpu_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no supported vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        vgpu_type_ids_array = _nvmlVgpuTypeId_t * c_vgpu_count.value\n        c_vgpu_type_ids = vgpu_type_ids_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_vgpu_count), c_vgpu_type_ids)\n        _nvmlCheckReturn(ret)\n        vgpus = []\n        for i in range(c_vgpu_count.value):\n            vgpus.append(c_vgpu_type_ids[i])\n        return vgpus\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlVgpuTypeGetGpuInstanceProfileId(vgpuTypeId):\n    c_profile_id = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetGpuInstanceProfileId\")\n    ret = fn(vgpuTypeId, byref(c_profile_id))\n    _nvmlCheckReturn(ret)\n    return (c_profile_id.value)\n\n@convertStrBytes\ndef nvmlVgpuTypeGetClass(vgpuTypeId):\n    c_class = create_string_buffer(NVML_DEVICE_NAME_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_DEVICE_NAME_BUFFER_SIZE)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetClass\")\n    ret = fn(vgpuTypeId, c_class, byref(c_buffer_size))\n    _nvmlCheckReturn(ret)\n    return c_class.value\n\n@convertStrBytes\ndef nvmlVgpuTypeGetName(vgpuTypeId):\n    c_name = create_string_buffer(NVML_DEVICE_NAME_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_DEVICE_NAME_BUFFER_SIZE)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetName\")\n    ret = fn(vgpuTypeId, c_name, byref(c_buffer_size))\n    _nvmlCheckReturn(ret)\n    return c_name.value\n\ndef nvmlVgpuTypeGetDeviceID(vgpuTypeId):\n    c_device_id    = c_ulonglong(0)\n    c_subsystem_id = c_ulonglong(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetDeviceID\")\n    ret = fn(vgpuTypeId, byref(c_device_id), byref(c_subsystem_id))\n    _nvmlCheckReturn(ret)\n    return (c_device_id.value, c_subsystem_id.value)\n\ndef nvmlVgpuTypeGetFramebufferSize(vgpuTypeId):\n    c_fb_size = c_ulonglong(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetFramebufferSize\")\n    ret = fn(vgpuTypeId, byref(c_fb_size))\n    _nvmlCheckReturn(ret)\n    return c_fb_size.value\n\ndef nvmlVgpuTypeGetNumDisplayHeads(vgpuTypeId):\n    c_num_heads = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetNumDisplayHeads\")\n    ret = fn(vgpuTypeId, byref(c_num_heads))\n    _nvmlCheckReturn(ret)\n    return c_num_heads.value\n\ndef nvmlVgpuTypeGetResolution(vgpuTypeId):\n    c_xdim = c_uint(0)\n    c_ydim = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetResolution\")\n    ret = fn(vgpuTypeId, 0, byref(c_xdim), byref(c_ydim))\n    _nvmlCheckReturn(ret)\n    return (c_xdim.value, c_ydim.value)\n\n@convertStrBytes\ndef nvmlVgpuTypeGetLicense(vgpuTypeId):\n    c_license = create_string_buffer(NVML_GRID_LICENSE_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_GRID_LICENSE_BUFFER_SIZE)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetLicense\")\n    ret = fn(vgpuTypeId, c_license, c_buffer_size)\n    _nvmlCheckReturn(ret)\n    return c_license.value\n\ndef nvmlVgpuTypeGetFrameRateLimit(vgpuTypeId):\n    c_frl_config = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetFrameRateLimit\")\n    ret = fn(vgpuTypeId, byref(c_frl_config))\n    _nvmlCheckReturn(ret)\n    return c_frl_config.value\n\ndef nvmlVgpuTypeGetGspHeapSize(vgpuTypeId):\n    c_gsp_heap = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetGspHeapSize\")\n    ret = fn(vgpuTypeId, byref(c_gsp_heap))\n    _nvmlCheckReturn(ret)\n    return c_gsp_heap.value\n\ndef nvmlVgpuTypeGetFbReservation(vgpuTypeId):\n    c_fb_reservation = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetFbReservation\")\n    ret = fn(vgpuTypeId, byref(c_fb_reservation))\n    _nvmlCheckReturn(ret)\n    return c_fb_reservation.value\n\ndef nvmlVgpuInstanceGetRuntimeStateSize(vgpuInstance):\n    c_runtime_state = nvmlVgpuRuntimeState_v1_t()\n    c_runtime_state.version = VgpuRuntimeState_v1\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetRuntimeStateSize\")\n    ret = fn(vgpuInstance, byref(c_runtime_state))\n    _nvmlCheckReturn(ret)\n    return c_runtime_state\n\ndef nvmlVgpuTypeGetMaxInstances(handle, vgpuTypeId):\n    c_max_instances = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetMaxInstances\")\n    ret = fn(handle, vgpuTypeId, byref(c_max_instances))\n    _nvmlCheckReturn(ret)\n    return c_max_instances.value\n\ndef nvmlVgpuTypeGetMaxInstancesPerVm(vgpuTypeId):\n    c_max_instances_per_vm = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetMaxInstancesPerVm\")\n    ret = fn(vgpuTypeId, byref(c_max_instances_per_vm))\n    _nvmlCheckReturn(ret)\n    return c_max_instances_per_vm.value\n\ndef nvmlVgpuTypeGetBAR1Info(vgpuTypeId):\n    c_bar1Info = c_nvmlVgpuTypeBar1Info_v1_t(0)\n    c_bar1Info.version = VgpuTypeBar1Info_v1\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetBAR1Info\")\n    ret = fn(vgpuTypeId, byref(c_bar1Info))\n    _nvmlCheckReturn(ret)\n    return c_bar1Info\n\ndef nvmlDeviceGetActiveVgpus(handle):\n    # first call to get the size\n    c_vgpu_count = c_uint(0)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetActiveVgpus\")\n    ret = fn(handle, byref(c_vgpu_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no active vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        vgpu_instance_array = _nvmlVgpuInstance_t * c_vgpu_count.value\n        c_vgpu_instances = vgpu_instance_array()\n\n        # make the call again\n        ret = fn(handle, byref(c_vgpu_count), c_vgpu_instances)\n        _nvmlCheckReturn(ret)\n        vgpus = []\n        for i in range(c_vgpu_count.value):\n            vgpus.append(c_vgpu_instances[i])\n        return vgpus\n    else:\n        # error case\n        raise NVMLError(ret)\n\n@convertStrBytes\ndef nvmlVgpuInstanceGetVmID(vgpuInstance):\n    c_vm_id = create_string_buffer(NVML_DEVICE_UUID_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_GRID_LICENSE_BUFFER_SIZE)\n    c_vm_id_type  = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetVmID\")\n    ret = fn(vgpuInstance, byref(c_vm_id), c_buffer_size, byref(c_vm_id_type))\n    _nvmlCheckReturn(ret)\n    return (c_vm_id.value, c_vm_id_type.value)\n\n@convertStrBytes\ndef nvmlVgpuInstanceGetUUID(vgpuInstance):\n    c_uuid = create_string_buffer(NVML_DEVICE_UUID_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_DEVICE_UUID_BUFFER_SIZE)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetUUID\")\n    ret = fn(vgpuInstance, byref(c_uuid), c_buffer_size)\n    _nvmlCheckReturn(ret)\n    return c_uuid.value\n\n@convertStrBytes\ndef nvmlVgpuInstanceGetMdevUUID(vgpuInstance):\n    c_uuid = create_string_buffer(NVML_DEVICE_UUID_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_DEVICE_UUID_BUFFER_SIZE)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetMdevUUID\")\n    ret = fn(vgpuInstance, byref(c_uuid), c_buffer_size)\n    _nvmlCheckReturn(ret)\n    return c_uuid.value\n\n@convertStrBytes\ndef nvmlVgpuInstanceGetVmDriverVersion(vgpuInstance):\n    c_driver_version = create_string_buffer(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)\n    c_buffer_size = c_uint(NVML_SYSTEM_DRIVER_VERSION_BUFFER_SIZE)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetVmDriverVersion\")\n    ret = fn(vgpuInstance, byref(c_driver_version), c_buffer_size)\n    _nvmlCheckReturn(ret)\n    return c_driver_version.value\n\ndef nvmlVgpuInstanceGetLicenseStatus(vgpuInstance):\n    c_license_status = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetLicenseStatus\")\n    ret = fn(vgpuInstance, byref(c_license_status))\n    _nvmlCheckReturn(ret)\n    return c_license_status.value\n\ndef nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance):\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetLicenseInfo_v2\")\n    c_license_info = c_nvmlVgpuLicenseInfo_t()\n    ret = fn(vgpuInstance, byref(c_license_info))\n    _nvmlCheckReturn(ret)\n    return c_license_info\n\ndef nvmlVgpuInstanceGetLicenseInfo(vgpuInstance):\n    return nvmlVgpuInstanceGetLicenseInfo_v2(vgpuInstance)\n\ndef nvmlVgpuInstanceGetFrameRateLimit(vgpuInstance):\n    c_frl = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetFrameRateLimit\")\n    ret = fn(vgpuInstance, byref(c_frl))\n    _nvmlCheckReturn(ret)\n    return c_frl.value\n\ndef nvmlVgpuInstanceGetEccMode(vgpuInstance):\n    c_mode = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetEccMode\")\n    ret = fn(vgpuInstance, byref(c_mode))\n    _nvmlCheckReturn(ret)\n    return c_mode.value\n\ndef nvmlVgpuInstanceGetType(vgpuInstance):\n    c_vgpu_type = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetType\")\n    ret = fn(vgpuInstance, byref(c_vgpu_type))\n    _nvmlCheckReturn(ret)\n    return c_vgpu_type.value\n\ndef nvmlVgpuInstanceGetEncoderCapacity(vgpuInstance):\n    c_encoder_capacity = c_ulonglong(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetEncoderCapacity\")\n    ret = fn(vgpuInstance, byref(c_encoder_capacity))\n    _nvmlCheckReturn(ret)\n    return c_encoder_capacity.value\n\ndef nvmlVgpuInstanceSetEncoderCapacity(vgpuInstance, encoder_capacity):\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceSetEncoderCapacity\")\n    return fn(vgpuInstance, encoder_capacity)\n\ndef nvmlVgpuInstanceGetFbUsage(vgpuInstance):\n    c_fb_usage = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetFbUsage\")\n    ret = fn(vgpuInstance, byref(c_fb_usage))\n    _nvmlCheckReturn(ret)\n    return c_fb_usage.value\n\ndef nvmlVgpuTypeGetCapabilities(vgpuTypeId, capability):\n    c_cap_result = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuTypeGetCapabilities\")\n    ret = fn(vgpuTypeId, _nvmlVgpuCapability_t(capability), byref(c_cap_result))\n    _nvmlCheckReturn(ret)\n    return (c_cap_result.value)\n\ndef nvmlVgpuInstanceGetGpuInstanceId(vgpuInstance):\n    c_id = c_uint(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetGpuInstanceId\")\n    ret = fn(vgpuInstance, byref(c_id))\n    _nvmlCheckReturn(ret)\n    return (c_id.value)\n\n@convertStrBytes\ndef nvmlVgpuInstanceGetGpuPciId(vgpuInstance):\n    c_vgpuPciId = create_string_buffer(NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE)\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetGpuPciId\")\n    ret = fn(vgpuInstance, c_vgpuPciId, byref(c_uint(NVML_DEVICE_PCI_BUS_ID_BUFFER_SIZE)))\n    _nvmlCheckReturn(ret)\n    return c_vgpuPciId.value\n\ndef nvmlDeviceGetVgpuUtilization(handle, timeStamp):\n    # first call to get the size\n    c_vgpu_count = c_uint(0)\n    c_time_stamp = c_ulonglong(timeStamp)\n    c_sample_value_type = _nvmlValueType_t()\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuUtilization\")\n    ret = fn(handle, c_time_stamp, byref(c_sample_value_type), byref(c_vgpu_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no active vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        sampleArray = c_vgpu_count.value * c_nvmlVgpuInstanceUtilizationSample_t\n        c_samples = sampleArray()\n\n        # make the call again\n        ret = fn(handle, c_time_stamp, byref(c_sample_value_type), byref(c_vgpu_count), c_samples)\n        _nvmlCheckReturn(ret)\n\n        return c_samples[0:c_vgpu_count.value]\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetVgpuInstancesUtilizationInfo(handle, timeStamp):\n    # first call to get the size\n    c_time_stamp = c_ulonglong(timeStamp)\n    c_vgpuUtilInfo = c_nvmlVgpuInstancesUtilizationInfo_v1_t(0)\n    c_vgpuUtilInfo.version = VgpuInstancesUtilizationInfo_v1\n    c_vgpuUtilInfo.sampleValType = _nvmlValueType_t()\n    c_vgpuUtilInfo.vgpuInstanceCount = c_uint(0)\n    c_vgpuUtilInfo.lastSeenTimeStamp = c_time_stamp\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuInstancesUtilizationInfo\")\n    ret = fn(handle, byref(c_vgpuUtilInfo))\n\n    if (ret == NVML_SUCCESS):\n        # special case, no active vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        sampleArray = c_vgpuUtilInfo.vgpuInstanceCount * c_nvmlVgpuInstanceUtilizationInfo_v1_t\n        c_samples = sampleArray()\n        c_vgpuUtilInfo.vgpuUtilArray = c_samples\n\n        # make the call again\n        ret = fn(handle, byref(c_vgpuUtilInfo))\n        _nvmlCheckReturn(ret)\n\n        return c_samples[0:c_vgpuUtilInfo.vgpuInstanceCount]\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetP2PStatus(device1, device2, p2pIndex):\n    c_p2pstatus = _nvmlGpuP2PStatus_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetP2PStatus\")\n    ret = fn(device1, device2,p2pIndex, byref(c_p2pstatus))\n    _nvmlCheckReturn(ret)\n    return c_p2pstatus.value\n\ndef nvmlDeviceGetGridLicensableFeatures_v4(handle):\n    c_get_grid_licensable_features = c_nvmlGridLicensableFeatures_v4_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGridLicensableFeatures_v4\")\n    ret = fn(handle, byref(c_get_grid_licensable_features))\n    _nvmlCheckReturn(ret)\n\n    return (c_get_grid_licensable_features)\n\ndef nvmlDeviceGetGridLicensableFeatures(handle):\n    return nvmlDeviceGetGridLicensableFeatures_v4(handle)\n\ndef nvmlDeviceGetGspFirmwareVersion(handle, version=None):\n    isUserDefined = version is not None\n    if not isUserDefined:\n        version = (c_char * NVML_GSP_FIRMWARE_VERSION_BUF_SIZE)()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGspFirmwareVersion\")\n    ret = fn(handle, version)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isUserDefined else version.value\n\ndef nvmlDeviceGetGspFirmwareMode(handle, isEnabled=c_uint(), defaultMode=c_uint()):\n    isReference = type(isEnabled) is not c_uint\n    isEnabledRef = isEnabled if isReference else byref(isEnabled)\n    defaultModeRef = defaultMode if isReference else byref(defaultMode)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGspFirmwareMode\")\n    ret = fn(handle, isEnabledRef, defaultModeRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else [isEnabled.value, defaultMode.value]\n\ndef nvmlDeviceGetEncoderCapacity(handle, encoderQueryType):\n    c_encoder_capacity = c_ulonglong(0)\n    c_encoderQuery_type = _nvmlEncoderQueryType_t(encoderQueryType)\n\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEncoderCapacity\")\n    ret = fn(handle, c_encoderQuery_type, byref(c_encoder_capacity))\n    _nvmlCheckReturn(ret)\n    return c_encoder_capacity.value\n\ndef nvmlDeviceGetVgpuProcessUtilization(handle, timeStamp):\n    # first call to get the size\n    c_vgpu_count = c_uint(0)\n    c_time_stamp = c_ulonglong(timeStamp)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuProcessUtilization\")\n    ret = fn(handle, c_time_stamp, byref(c_vgpu_count), None)\n\n    if (ret == NVML_SUCCESS):\n        # special case, no active vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        sampleArray = c_vgpu_count.value * c_nvmlVgpuProcessUtilizationSample_t\n        c_samples = sampleArray()\n\n        # make the call again\n        ret = fn(handle, c_time_stamp, byref(c_vgpu_count), c_samples)\n        _nvmlCheckReturn(ret)\n\n        return c_samples[0:c_vgpu_count.value]\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetVgpuProcessesUtilizationInfo(handle, timeStamp):\n    # first call to get the size\n    c_time_stamp = c_ulonglong(timeStamp)\n    c_vgpuProcUtilInfo = c_nvmlVgpuProcessesUtilizationInfo_v1_t(0)\n    c_vgpuProcUtilInfo.version = VgpuProcessesUtilizationInfo_v1\n    c_vgpuProcUtilInfo.vgpuProcessCount = c_uint(0)\n    c_vgpuProcUtilInfo.lastSeenTimeStamp = c_time_stamp\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuProcessesUtilizationInfo\")\n    ret = fn(handle, byref(c_vgpuProcUtilInfo))\n\n    if (ret == NVML_SUCCESS):\n        # special case, no active vGPUs\n        return []\n    elif (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        sampleArray = c_vgpuProcUtilInfo.vgpuProcessCount * c_nvmlVgpuProcessUtilizationInfo_v1_t\n        c_samples = sampleArray()\n        c_vgpuProcUtilInfo.vgpuProcUtilArray = c_samples\n\n        # make the call again\n        ret = fn(handle, byref(c_vgpuProcUtilInfo))\n        _nvmlCheckReturn(ret)\n\n        return c_samples[0:c_vgpuProcUtilInfo.vgpuProcessCount]\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetEncoderStats(handle):\n    c_encoderCount = c_ulonglong(0)\n    c_encodeFps = c_ulonglong(0)\n    c_encoderLatency = c_ulonglong(0)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetEncoderStats\")\n    ret = fn(handle, byref(c_encoderCount), byref(c_encodeFps), byref(c_encoderLatency))\n    _nvmlCheckReturn(ret)\n    return (c_encoderCount.value, c_encodeFps.value, c_encoderLatency.value)\n\ndef nvmlDeviceGetEncoderSessions(handle):\n    # first call to get the size\n    c_session_count = c_uint(0)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetEncoderSessions\")\n    ret = fn(handle, byref(c_session_count), None)\n\n    if (ret == NVML_SUCCESS):\n        if (c_session_count.value != 0):\n            # typical case\n            session_array = c_nvmlEncoderSession_t * c_session_count.value\n            c_sessions = session_array()\n\n            # make the call again\n            ret = fn(handle, byref(c_session_count), c_sessions)\n            _nvmlCheckReturn(ret)\n            sessions = []\n            for i in range(c_session_count.value):\n                sessions.append(c_sessions[i])\n            return sessions\n        else:\n            return []  # no active sessions\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetFBCStats(handle):\n    c_fbcStats = c_nvmlFBCStats_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetFBCStats\")\n    ret = fn(handle, byref(c_fbcStats))\n    _nvmlCheckReturn(ret)\n    return c_fbcStats\n\ndef nvmlDeviceGetFBCSessions(handle):\n    # first call to get the size\n    c_session_count = c_uint(0)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetFBCSessions\")\n    ret = fn(handle, byref(c_session_count), None)\n\n    if (ret == NVML_SUCCESS):\n        if (c_session_count.value != 0):\n            # typical case\n            session_array = c_nvmlFBCSession_t * c_session_count.value\n            c_sessions = session_array()\n\n            # make the call again\n            ret = fn(handle, byref(c_session_count), c_sessions)\n            _nvmlCheckReturn(ret)\n            sessions = []\n            for i in range(c_session_count.value):\n                sessions.append(c_sessions[i])\n            return sessions\n        else:\n            return []  # no active sessions\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlVgpuInstanceGetEncoderStats(vgpuInstance):\n    c_encoderCount    = c_ulonglong(0)\n    c_encodeFps       = c_ulonglong(0)\n    c_encoderLatency  = c_ulonglong(0)\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetEncoderStats\")\n    ret = fn(vgpuInstance, byref(c_encoderCount), byref(c_encodeFps), byref(c_encoderLatency))\n    _nvmlCheckReturn(ret)\n    return (c_encoderCount.value, c_encodeFps.value, c_encoderLatency.value)\n\ndef nvmlVgpuInstanceGetEncoderSessions(vgpuInstance):\n    # first call to get the size\n    c_session_count = c_uint(0)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetEncoderSessions\")\n    ret = fn(vgpuInstance, byref(c_session_count), None)\n\n    if (ret == NVML_SUCCESS):\n        if (c_session_count.value != 0):\n            # typical case\n            session_array = c_nvmlEncoderSession_t * c_session_count.value\n            c_sessions = session_array()\n\n            # make the call again\n            ret = fn(vgpuInstance, byref(c_session_count), c_sessions)\n            _nvmlCheckReturn(ret)\n            sessions = []\n            for i in range(c_session_count.value):\n                sessions.append(c_sessions[i])\n            return sessions\n        else:\n            return []  # no active sessions\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlVgpuInstanceGetFBCStats(vgpuInstance):\n    c_fbcStats = c_nvmlFBCStats_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetFBCStats\")\n    ret = fn(vgpuInstance, byref(c_fbcStats))\n    _nvmlCheckReturn(ret)\n    return c_fbcStats\n\ndef nvmlVgpuInstanceGetFBCSessions(vgpuInstance):\n    # first call to get the size\n    c_session_count = c_uint(0)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetFBCSessions\")\n    ret = fn(vgpuInstance, byref(c_session_count), None)\n\n    if (ret == NVML_SUCCESS):\n        if (c_session_count.value != 0):\n            # typical case\n            session_array = c_nvmlFBCSession_t * c_session_count.value\n            c_sessions = session_array()\n\n            # make the call again\n            ret = fn(vgpuInstance, byref(c_session_count), c_sessions)\n            _nvmlCheckReturn(ret)\n            sessions = []\n            for i in range(c_session_count.value):\n                sessions.append(c_sessions[i])\n            return sessions\n        else:\n            return []  # no active sessions\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetProcessUtilization(handle, timeStamp):\n    # first call to get the size\n    c_count = c_uint(0)\n    c_time_stamp = c_ulonglong(timeStamp)\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetProcessUtilization\")\n    ret = fn(handle, None, byref(c_count), c_time_stamp)\n\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        sampleArray = c_count.value * c_nvmlProcessUtilizationSample_t\n        c_samples = sampleArray()\n\n        # make the call again\n        ret = fn(handle, c_samples, byref(c_count), c_time_stamp)\n        _nvmlCheckReturn(ret)\n\n        return c_samples[0:c_count.value]\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlDeviceGetProcessesUtilizationInfo(handle, timeStamp):\n    # first call to get the size\n    c_time_stamp = c_ulonglong(timeStamp)\n    c_processesUtilInfo = c_nvmlProcessesUtilizationInfo_v1_t(0)\n    c_processesUtilInfo.version = ProcessesUtilizationInfo_v1\n    c_processesUtilInfo.processSamplesCount = c_uint(0)\n    c_processesUtilInfo.lastSeenTimeStamp = c_time_stamp\n\n    fn  = _nvmlGetFunctionPointer(\"nvmlDeviceGetProcessesUtilizationInfo\")\n    ret = fn(handle, byref(c_processesUtilInfo))\n\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        # typical case\n        sampleArray = c_processesUtilInfo.processSamplesCount * c_nvmlProcessUtilizationInfo_v1_t\n        c_samples = sampleArray()\n        c_processesUtilInfo.procUtilArray = c_samples\n\n        # make the call again\n        ret = fn(handle, byref(c_processesUtilInfo))\n        _nvmlCheckReturn(ret)\n\n        return c_samples[0:c_processesUtilInfo.processSamplesCount]\n    else:\n        # error case\n        raise NVMLError(ret)\n\ndef nvmlVgpuInstanceGetMetadata(vgpuInstance):\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetMetadata\")\n    c_vgpuMetadata = c_nvmlVgpuMetadata_t()\n    c_bufferSize = c_uint(0)\n    # Make the first NVML API call to get the c_bufferSize value.\n    # We have already allocated required buffer above.\n    ret = fn(vgpuInstance, byref(c_vgpuMetadata), byref(c_bufferSize))\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        ret = fn(vgpuInstance, byref(c_vgpuMetadata), byref(c_bufferSize))\n        _nvmlCheckReturn(ret)\n    else:\n        raise NVMLError(ret)\n    return c_vgpuMetadata\n\ndef nvmlDeviceGetVgpuMetadata(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuMetadata\")\n    c_vgpuPgpuMetadata = c_nvmlVgpuPgpuMetadata_t()\n    c_bufferSize = c_uint(0)\n    # Make the first NVML API call to get the c_bufferSize value.\n    # We have already allocated required buffer above.\n    ret = fn(handle, byref(c_vgpuPgpuMetadata), byref(c_bufferSize))\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        ret = fn(handle, byref(c_vgpuPgpuMetadata), byref(c_bufferSize))\n        _nvmlCheckReturn(ret)\n    else:\n        raise NVMLError(ret)\n    return c_vgpuPgpuMetadata\n\ndef nvmlGetVgpuCompatibility(vgpuMetadata, pgpuMetadata):\n    fn = _nvmlGetFunctionPointer(\"nvmlGetVgpuCompatibility\")\n    c_vgpuPgpuCompatibility = c_nvmlVgpuPgpuCompatibility_t()\n    ret = fn(byref(vgpuMetadata), byref(pgpuMetadata), byref(c_vgpuPgpuCompatibility))\n    _nvmlCheckReturn(ret)\n    return c_vgpuPgpuCompatibility\n\n@convertStrBytes\ndef nvmlDeviceGetPgpuMetadataString(handle):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPgpuMetadataString\")\n    c_pgpuMetadata = create_string_buffer(NVML_VGPU_PGPU_METADATA_OPAQUE_DATA_SIZE)\n    c_bufferSize = c_uint(0)\n    # Make the first NVML API call to get the c_bufferSize value.\n    # We have already allocated required buffer above.\n    ret = fn(handle, byref(c_pgpuMetadata), byref(c_bufferSize))\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        ret = fn(handle, byref(c_pgpuMetadata), byref(c_bufferSize))\n        _nvmlCheckReturn(ret)\n    else:\n        raise NVMLError(ret)\n    return (c_pgpuMetadata.value, c_bufferSize.value)\n\ndef nvmlDeviceGetVgpuSchedulerLog(handle):\n    c_vgpu_sched_log = c_nvmlVgpuSchedulerLog_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuSchedulerLog\")\n    ret = fn(handle, byref(c_vgpu_sched_log))\n    _nvmlCheckReturn(ret)\n    return c_vgpu_sched_log\n\ndef nvmlDeviceGetVgpuSchedulerState(handle):\n    c_vgpu_sched_state = c_nvmlVgpuSchedulerGetState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuSchedulerState\")\n    ret = fn(handle, byref(c_vgpu_sched_state))\n    _nvmlCheckReturn(ret)\n    return c_vgpu_sched_state\n\ndef nvmlDeviceGetVgpuSchedulerCapabilities(handle):\n    c_vgpu_sched_caps = c_nvmlVgpuSchedulerCapabilities_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetVgpuSchedulerCapabilities\")\n    ret = fn(handle, byref(c_vgpu_sched_caps))\n    _nvmlCheckReturn(ret)\n    return c_vgpu_sched_caps\n\ndef nvmlDeviceSetVgpuSchedulerState(handle, sched_state):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetVgpuSchedulerState\")\n    ret = fn(handle, byref(sched_state))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlSetVgpuVersion(vgpuVersion):\n    fn = _nvmlGetFunctionPointer(\"nvmlSetVgpuVersion\")\n    ret = fn(byref(vgpuVersion))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlGetVgpuVersion(supported=None, current=None):\n    isUserDefined = (supported is not None) or (current is not None)\n    if not isUserDefined:\n        supported = c_nvmlVgpuVersion_t()\n        current = c_nvmlVgpuVersion_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGetVgpuVersion\")\n    ret = fn(byref(supported), byref(current))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isUserDefined else [(supported.minVersion,\n                                                supported.maxVersion),\n                                               (current.minVersion,\n                                                current.maxVersion)]\n\ndef nvmlVgpuInstanceGetAccountingMode(vgpuInstance):\n    c_mode = _nvmlEnableState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetAccountingMode\")\n    ret = fn(vgpuInstance, byref(c_mode))\n    _nvmlCheckReturn(ret)\n    return c_mode.value\n\ndef nvmlVgpuInstanceGetAccountingPids(vgpuInstance):\n    c_pidCount = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetAccountingPids\")\n    ret = fn(vgpuInstance, byref(c_pidCount), None)\n    if (ret == NVML_ERROR_INSUFFICIENT_SIZE):\n        sampleArray = c_pidCount.value * c_uint\n        c_pidArray = sampleArray()\n        ret = fn(vgpuInstance, byref(c_pidCount), byref(c_pidArray))\n        _nvmlCheckReturn(ret)\n    else:\n        raise NVMLError(ret)\n    return (c_pidCount, c_pidArray)\n\ndef nvmlVgpuInstanceGetAccountingStats(vgpuInstance, pid):\n    c_accountingStats = c_nvmlAccountingStats_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceGetAccountingStats\")\n    ret = fn(vgpuInstance, pid, byref(c_accountingStats))\n    _nvmlCheckReturn(ret)\n    return c_accountingStats\n\ndef nvmlVgpuInstanceClearAccountingPids(vgpuInstance):\n    fn = _nvmlGetFunctionPointer(\"nvmlVgpuInstanceClearAccountingPids\")\n    ret = fn(vgpuInstance)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlGetExcludedDeviceCount():\n    c_count = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlGetExcludedDeviceCount\")\n    ret = fn(byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlGetExcludedDeviceInfoByIndex(index):\n    c_index = c_uint(index)\n    info = c_nvmlExcludedDeviceInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGetExcludedDeviceInfoByIndex\")\n    ret = fn(c_index, byref(info))\n    _nvmlCheckReturn(ret)\n    return info\n\ndef nvmlDeviceGetHostVgpuMode(handle):\n    c_host_vgpu_mode = _nvmlHostVgpuMode_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetHostVgpuMode\")\n    ret = fn(handle, byref(c_host_vgpu_mode))\n    _nvmlCheckReturn(ret)\n    return c_host_vgpu_mode.value\n\ndef nvmlDeviceSetMigMode(device, mode):\n    c_activationStatus = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetMigMode\")\n    ret = fn(device, mode, byref(c_activationStatus))\n    _nvmlCheckReturn(ret)\n    return c_activationStatus.value\n\ndef nvmlDeviceGetMigMode(device):\n    c_currentMode = c_uint()\n    c_pendingMode = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMigMode\")\n    ret = fn(device, byref(c_currentMode), byref(c_pendingMode))\n    _nvmlCheckReturn(ret)\n    return [c_currentMode.value, c_pendingMode.value]\n\ndef nvmlDeviceGetGpuInstanceProfileInfo(device, profile, version=2):\n    if version == 2:\n        c_info = c_nvmlGpuInstanceProfileInfo_v2_t()\n        fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstanceProfileInfoV\")\n    elif version == 1:\n        c_info = c_nvmlGpuInstanceProfileInfo_t()\n        fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstanceProfileInfo\")\n    else:\n        raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND)\n    ret = fn(device, profile, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\n# Define function alias for the API exposed by NVML\nnvmlDeviceGetGpuInstanceProfileInfoV = nvmlDeviceGetGpuInstanceProfileInfo\n\ndef nvmlDeviceGetGpuInstanceRemainingCapacity(device, profileId):\n    c_count = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstanceRemainingCapacity\")\n    ret = fn(device, profileId, byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlDeviceGetGpuInstancePossiblePlacements(device, profileId, placementsRef, countRef):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstancePossiblePlacements_v2\")\n    ret = fn(device, profileId, placementsRef, countRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceCreateGpuInstance(device, profileId):\n    c_instance = c_nvmlGpuInstance_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceCreateGpuInstance\")\n    ret = fn(device, profileId, byref(c_instance))\n    _nvmlCheckReturn(ret)\n    return c_instance\n\ndef nvmlDeviceCreateGpuInstanceWithPlacement(device, profileId, placement):\n    c_instance = c_nvmlGpuInstance_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceCreateGpuInstanceWithPlacement\")\n    ret = fn(device, profileId, placement, byref(c_instance))\n    _nvmlCheckReturn(ret)\n    return c_instance\n\ndef nvmlGpuInstanceDestroy(gpuInstance):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceDestroy\")\n    ret = fn(gpuInstance)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetGpuInstances(device, profileId, gpuInstancesRef, countRef):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstances\")\n    ret = fn(device, profileId, gpuInstancesRef, countRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetGpuInstanceById(device, gpuInstanceId):\n    c_instance = c_nvmlGpuInstance_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstanceById\")\n    ret = fn(device, gpuInstanceId, byref(c_instance))\n    _nvmlCheckReturn(ret)\n    return c_instance\n\ndef nvmlGpuInstanceGetInfo(gpuInstance):\n    c_info = c_nvmlGpuInstanceInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetInfo\")\n    ret = fn(gpuInstance, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\ndef nvmlGpuInstanceGetComputeInstanceProfileInfo(device, profile, engProfile, version=2):\n    if version == 2:\n        c_info = c_nvmlComputeInstanceProfileInfo_v2_t()\n        fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetComputeInstanceProfileInfoV\")\n    elif version == 1:\n        c_info = c_nvmlComputeInstanceProfileInfo_t()\n        fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetComputeInstanceProfileInfo\")\n    else:\n        raise NVMLError(NVML_ERROR_FUNCTION_NOT_FOUND) \n    ret = fn(device, profile, engProfile, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\n# Define function alias for the API exposed by NVML\nnvmlGpuInstanceGetComputeInstanceProfileInfoV = nvmlGpuInstanceGetComputeInstanceProfileInfo\n\ndef nvmlGpuInstanceGetComputeInstanceRemainingCapacity(gpuInstance, profileId):\n    c_count = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetComputeInstanceRemainingCapacity\")\n    ret = fn(gpuInstance, profileId, byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlGpuInstanceGetComputeInstancePossiblePlacements(gpuInstance, profileId, placementsRef, countRef):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetComputeInstancePossiblePlacements\")\n    ret = fn(gpuInstance, profileId, placementsRef, countRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlGpuInstanceCreateComputeInstance(gpuInstance, profileId):\n    c_instance = c_nvmlComputeInstance_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceCreateComputeInstance\")\n    ret = fn(gpuInstance, profileId, byref(c_instance))\n    _nvmlCheckReturn(ret)\n    return c_instance\n\ndef nvmlGpuInstanceCreateComputeInstanceWithPlacement(gpuInstance, profileId, placement):\n    c_instance = c_nvmlComputeInstance_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceCreateComputeInstanceWithPlacement\")\n    ret = fn(gpuInstance, profileId, placement, byref(c_instance))\n    _nvmlCheckReturn(ret)\n    return c_instance\n\ndef nvmlComputeInstanceDestroy(computeInstance):\n    fn = _nvmlGetFunctionPointer(\"nvmlComputeInstanceDestroy\")\n    ret = fn(computeInstance)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlGpuInstanceGetComputeInstances(gpuInstance, profileId, computeInstancesRef, countRef):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetComputeInstances\")\n    ret = fn(gpuInstance, profileId, computeInstancesRef, countRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlGpuInstanceGetComputeInstanceById(gpuInstance, computeInstanceId):\n    c_instance = c_nvmlComputeInstance_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpuInstanceGetComputeInstanceById\")\n    ret = fn(gpuInstance, computeInstanceId, byref(c_instance))\n    _nvmlCheckReturn(ret)\n    return c_instance\n\ndef nvmlComputeInstanceGetInfo_v2(computeInstance):\n    c_info = c_nvmlComputeInstanceInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlComputeInstanceGetInfo_v2\")\n    ret = fn(computeInstance, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return c_info\n\ndef nvmlComputeInstanceGetInfo(computeInstance):\n    return nvmlComputeInstanceGetInfo_v2(computeInstance)\n\ndef nvmlDeviceIsMigDeviceHandle(device):\n    c_isMigDevice = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceIsMigDeviceHandle\")\n    ret = fn(device, byref(c_isMigDevice))\n    _nvmlCheckReturn(ret)\n    return c_isMigDevice\n\ndef nvmlDeviceGetGpuInstanceId(device):\n    c_gpuInstanceId = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuInstanceId\")\n    ret = fn(device, byref(c_gpuInstanceId))\n    _nvmlCheckReturn(ret)\n    return c_gpuInstanceId.value\n\ndef nvmlDeviceGetComputeInstanceId(device):\n    c_computeInstanceId = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetComputeInstanceId\")\n    ret = fn(device, byref(c_computeInstanceId))\n    _nvmlCheckReturn(ret)\n    return c_computeInstanceId.value\n\ndef nvmlDeviceGetMaxMigDeviceCount(device):\n    c_count = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMaxMigDeviceCount\")\n    ret = fn(device, byref(c_count))\n    _nvmlCheckReturn(ret)\n    return c_count.value\n\ndef nvmlDeviceGetMigDeviceHandleByIndex(device, index):\n    c_index = c_uint(index)\n    migDevice = c_nvmlDevice_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMigDeviceHandleByIndex\")\n    ret = fn(device, c_index, byref(migDevice))\n    _nvmlCheckReturn(ret)\n    return migDevice\n\ndef nvmlDeviceGetDeviceHandleFromMigDeviceHandle(migDevice):\n    device = c_nvmlDevice_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDeviceHandleFromMigDeviceHandle\")\n    ret = fn(migDevice, byref(device))\n    _nvmlCheckReturn(ret)\n    return device\n\ndef nvmlDeviceGetAttributes_v2(device):\n    c_attrs = c_nvmlDeviceAttributes()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAttributes_v2\")\n    ret = fn(device, byref(c_attrs))\n    _nvmlCheckReturn(ret)\n    return c_attrs\n\ndef nvmlDeviceGetAttributes(device):\n    return nvmlDeviceGetAttributes_v2(device)\n\ndef nvmlDeviceGetRemappedRows(device):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRemappedRows\")\n    c_corr = c_uint()\n    c_unc = c_uint()\n    c_bpending = c_uint()\n    c_bfailure = c_uint()\n    ret = fn(device, byref(c_corr), byref(c_unc), byref(c_bpending), byref(c_bfailure))\n    _nvmlCheckReturn(ret)\n    return (c_corr.value, c_unc.value, c_bpending.value, c_bfailure.value)\n\ndef nvmlDeviceGetRowRemapperHistogram(device):\n    c_vals = c_nvmlRowRemapperHistogramValues()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetRowRemapperHistogram\")\n    ret = fn(device, byref(c_vals))\n    _nvmlCheckReturn(ret)\n    return c_vals\n\ndef nvmlDeviceGetArchitecture(device):\n    arch = _nvmlDeviceArchitecture_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetArchitecture\")\n    ret = fn(device, byref(arch))\n    _nvmlCheckReturn(ret)\n    return arch.value\n\ndef nvmlDeviceGetBusType(device):\n    c_busType = _nvmlBusType_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetBusType\")\n    ret = fn(device, byref(c_busType))\n    _nvmlCheckReturn(ret)\n    return c_busType.value\n\ndef nvmlDeviceGetIrqNum(device):\n    c_irqNum = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetIrqNum\")\n    ret = fn(device, byref(c_irqNum))\n    _nvmlCheckReturn(ret)\n    return c_irqNum.value\n\ndef nvmlDeviceGetNumGpuCores(device):\n    c_numCores = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNumGpuCores\")\n    ret = fn(device, byref(c_numCores))\n    _nvmlCheckReturn(ret)\n    return c_numCores.value\n\ndef nvmlDeviceGetPowerSource(device):\n    c_powerSource = _nvmlPowerSource_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPowerSource\")\n    ret = fn(device, byref(c_powerSource))\n    _nvmlCheckReturn(ret)\n    return c_powerSource.value\n\ndef nvmlDeviceGetMemoryBusWidth(device):\n    c_memBusWidth = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemoryBusWidth\")\n    ret = fn(device, byref(c_memBusWidth))\n    _nvmlCheckReturn(ret)\n    return c_memBusWidth.value\n\ndef nvmlDeviceGetPcieLinkMaxSpeed(device):\n    c_speed = _nvmlPcieLinkMaxSpeed_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPcieLinkMaxSpeed\")\n    ret = fn(device, byref(c_speed))\n    _nvmlCheckReturn(ret)\n    return c_speed.value\n\ndef nvmlDeviceGetAdaptiveClockInfoStatus(device):\n    c_adaptiveClockInfoStatus = _nvmlAdaptiveClockInfoStatus_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetAdaptiveClockInfoStatus\")\n    ret = fn(device, byref(c_adaptiveClockInfoStatus))\n    _nvmlCheckReturn(ret)\n    return c_adaptiveClockInfoStatus.value\n\ndef nvmlDeviceGetPcieSpeed(device):\n    c_speed = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPcieSpeed\")\n    ret = fn(device, byref(c_speed))\n    _nvmlCheckReturn(ret)\n    return c_speed.value\n\ndef nvmlDeviceGetDynamicPstatesInfo(device, c_dynamicpstatesinfo=c_nvmlGpuDynamicPstatesInfo_t()):\n    isReference = type(c_dynamicpstatesinfo) is not c_nvmlGpuDynamicPstatesInfo_t\n    dynamicpstatesinfoRef = c_dynamicpstatesinfo if isReference else byref(c_dynamicpstatesinfo)\n\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDynamicPstatesInfo\");\n    ret = fn(device, dynamicpstatesinfoRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else c_dynamicpstatesinfo\n\ndef nvmlDeviceSetFanSpeed_v2(handle, index, speed):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetFanSpeed_v2\");\n    ret = fn(handle, index, speed)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetThermalSettings(device, sensorindex, c_thermalsettings=c_nvmlGpuThermalSettings_t()):\n    isReference = type(c_thermalsettings) is not c_nvmlGpuThermalSettings_t\n    thermalsettingsRef = c_thermalsettings if isReference else byref(c_thermalsettings)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetThermalSettings\");\n    ret = fn(device, sensorindex, thermalsettingsRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else c_thermalsettings.sensor[:]\n\ndef nvmlDeviceGetMinMaxClockOfPState(device, clockType, pstate, minClockMHz=c_uint(), maxClockMHz=c_uint()):\n    isReference = (type(minClockMHz) is not c_uint) or (type(maxClockMHz) is not c_uint)\n    minClockMHzRef = minClockMHz if isReference else byref(minClockMHz)\n    maxClockMHzRef = maxClockMHz if isReference else byref(maxClockMHz)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMinMaxClockOfPState\");\n    ret = fn(device, _nvmlClockType_t(clockType), _nvmlClockType_t(pstate), minClockMHzRef, maxClockMHzRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else (minClockMHz.value, maxClockMHz.value)\n\nclass c_nvmlClockOffset_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('type', _nvmlClockType_t),\n        ('pstate', _nvmlPstates_t),\n        ('clockOffsetMHz', c_int),\n        ('minClockOffsetMHz', c_int),\n        ('maxClockOffsetMHz', c_int),\n    ]\n\nnvmlClockOffset_v1 = 0x1000018\n\ndef nvmlDeviceGetClockOffsets(device, info):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetClockOffsets\");\n    ret = fn(device, info)\n    return NVML_SUCCESS\n\ndef nvmlDeviceSetClockOffsets(device, info):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetClockOffsets\");\n    ret = fn(device, info)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetSupportedPerformanceStates(device):\n    pstates = []\n    c_count = c_uint(NVML_MAX_GPU_PERF_PSTATES)\n    c_size = sizeof(c_uint)*c_count.value\n\n    # NOTE: use 'c_uint' to represent the size of the nvmlPstate_t enumeration.\n    pstates_array = _nvmlPstates_t * c_count.value\n    c_pstates = pstates_array()\n\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSupportedPerformanceStates\")\n    ret = fn(device, c_pstates, c_size)\n    _nvmlCheckReturn(ret)\n\n    for value in c_pstates:\n        if value != NVML_PSTATE_UNKNOWN:\n            pstates.append(value)\n\n    return pstates\n\ndef nvmlDeviceGetGpcClkVfOffset(device):\n    offset = c_int32()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpcClkVfOffset\")\n    ret = fn(device, byref(offset))\n    _nvmlCheckReturn(ret)\n    return offset.value\n\ndef nvmlDeviceSetGpcClkVfOffset(device, offset):\n    c_offset = c_int32(offset)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetGpcClkVfOffset\")\n    ret = fn(device, c_offset)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetGpcClkMinMaxVfOffset(device, minOffset=c_int(), maxOffset=c_int()):\n    isReference = (type(minOffset) is not c_int) or (type(maxOffset) is not c_int)\n    minOffsetRef = minOffset if isReference else byref(minOffset)\n    maxOffsetRef = maxOffset if isReference else byref(maxOffset)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpcClkMinMaxVfOffset\")\n    ret = fn(device, minOffsetRef, maxOffsetRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else (minOffset.value, maxOffset.value)\n\ndef nvmlDeviceGetMemClkVfOffset(device):\n    offset = c_int32()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemClkVfOffset\")\n    ret = fn(device, byref(offset))\n    _nvmlCheckReturn(ret)\n    return offset.value\n\ndef nvmlDeviceSetMemClkVfOffset(device, offset):\n    c_offset = c_int32(offset)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetMemClkVfOffset\")\n    ret = fn(device, c_offset)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetMemClkMinMaxVfOffset(device, minOffset=c_int(), maxOffset=c_int()):\n    isReference = (type(minOffset) is not c_int) or (type(maxOffset) is not c_int)\n    minOffsetRef = minOffset if isReference else byref(minOffset)\n    maxOffsetRef = maxOffset if isReference else byref(maxOffset)\n\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetMemClkMinMaxVfOffset\")\n    ret = fn(device, minOffsetRef, maxOffsetRef)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS if isReference else (minOffset.value, maxOffset.value)\n\ndef nvmlSystemSetConfComputeGpusReadyState(state):\n    c_state = c_uint(state)\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemSetConfComputeGpusReadyState\")\n    ret = fn(c_state)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlSystemGetConfComputeGpusReadyState():\n    c_state = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetConfComputeGpusReadyState\")\n    ret = fn(byref(c_state))\n    _nvmlCheckReturn(ret)\n    return c_state.value\n\ndef nvmlSystemGetConfComputeCapabilities():\n    c_ccSysCaps = c_nvmlConfComputeSystemCaps_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetConfComputeCapabilities\")\n    ret = fn(byref(c_ccSysCaps))\n    _nvmlCheckReturn(ret)\n    return c_ccSysCaps\n\ndef nvmlSystemGetConfComputeState():\n    c_state = c_nvmlConfComputeSystemState_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetConfComputeState\")\n    ret = fn(byref(c_state))\n    _nvmlCheckReturn(ret)\n    return c_state\n\ndef nvmlSystemGetConfComputeSettings(settings):\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetConfComputeSettings\")\n    return fn(settings)\n\ndef nvmlDeviceSetConfComputeUnprotectedMemSize(device, c_ccMemSize):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetConfComputeUnprotectedMemSize\")\n    ret = fn(device, c_ccMemSize)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetConfComputeMemSizeInfo(device):\n    c_ccMemSize = c_nvmlConfComputeMemSizeInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetConfComputeMemSizeInfo\")\n    ret = fn(device, byref(c_ccMemSize))\n    _nvmlCheckReturn(ret)\n    return c_ccMemSize\n\ndef nvmlDeviceGetConfComputeProtectedMemoryUsage(device):\n    c_memory = c_nvmlMemory_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetConfComputeProtectedMemoryUsage\")\n    ret = fn(device, byref(c_memory))\n    _nvmlCheckReturn(ret)\n    return c_memory\n\ndef nvmlDeviceGetConfComputeGpuCertificate(device):\n    c_cert = c_nvmlConfComputeGpuCertificate_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetConfComputeGpuCertificate\")\n    ret = fn(device, byref(c_cert))\n    _nvmlCheckReturn(ret)\n    return c_cert\n\ndef nvmlDeviceGetConfComputeGpuAttestationReport(device, c_nonce):\n    c_attestReport = c_nvmlConfComputeGpuAttestationReport_t()\n    c_nonce_arr = (c_uint8 * len(c_nonce))(*(c_nonce))\n    setattr(c_attestReport, 'nonce', c_nonce_arr)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetConfComputeGpuAttestationReport\")\n    ret = fn(device, byref(c_attestReport))\n    _nvmlCheckReturn(ret)\n    return c_attestReport\n\ndef nvmlSystemSetConfComputeKeyRotationThresholdInfo(max_atk_adv):\n    c_keyRotationThrInfo = c_nvmlConfComputeSetKeyRotationThresholdInfo_t(0)\n    c_keyRotationThrInfo.version = ConfComputeSetKeyRotationThresholdInfo_v1\n    c_keyRotationThrInfo.maxAttackerAdvantage = max_atk_adv\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemSetConfComputeKeyRotationThresholdInfo\")\n    ret = fn(byref(c_keyRotationThrInfo))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlSystemGetConfComputeKeyRotationThresholdInfo():\n    c_keyRotationThrInfo = c_nvmlConfComputeGetKeyRotationThresholdInfo_t(0)\n    c_keyRotationThrInfo.version = ConfComputeGetKeyRotationThresholdInfo_v1\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetConfComputeKeyRotationThresholdInfo\")\n    ret = fn(byref(c_keyRotationThrInfo))\n    _nvmlCheckReturn(ret)\n    return c_keyRotationThrInfo\n\n## GPM ##\n#########\n\n## Enums/defines\n\n#### GPM Metric Identifiers\nNVML_GPM_METRIC_GRAPHICS_UTIL               = 1 # Percentage of time any compute/graphics app was active on the GPU. 0.0 - 100.0\nNVML_GPM_METRIC_SM_UTIL                     = 2 # Percentage of SMs that were busy. 0.0 - 100.0\nNVML_GPM_METRIC_SM_OCCUPANCY                = 3 # Percentage of warps that were active vs theoretical maximum. 0.0 - 100.0\nNVML_GPM_METRIC_INTEGER_UTIL                = 4 # Percentage of time the GPU's SMs were doing integer operations. 0.0 - 100.0\nNVML_GPM_METRIC_ANY_TENSOR_UTIL             = 5 # Percentage of time the GPU's SMs were doing ANY tensor operations. 0.0 - 100.0\nNVML_GPM_METRIC_DFMA_TENSOR_UTIL            = 6 # Percentage of time the GPU's SMs were doing DFMA tensor operations. 0.0 - 100.0\nNVML_GPM_METRIC_HMMA_TENSOR_UTIL            = 7 # Percentage of time the GPU's SMs were doing HMMA tensor operations. 0.0 - 100.0\nNVML_GPM_METRIC_IMMA_TENSOR_UTIL            = 9 # Percentage of time the GPU's SMs were doing IMMA tensor operations. 0.0 - 100.0\nNVML_GPM_METRIC_DRAM_BW_UTIL                = 10 # Percentage of DRAM bw used vs theoretical maximum. 0.0 - 100.0\nNVML_GPM_METRIC_FP64_UTIL                   = 11 # Percentage of time the GPU's SMs were doing non-tensor FP64 math. 0.0 - 100.0\nNVML_GPM_METRIC_FP32_UTIL                   = 12 # Percentage of time the GPU's SMs were doing non-tensor FP32 math. 0.0 - 100.0\nNVML_GPM_METRIC_FP16_UTIL                   = 13 # Percentage of time the GPU's SMs were doing non-tensor FP16 math. 0.0 - 100.0\nNVML_GPM_METRIC_PCIE_TX_PER_SEC             = 20 # PCIe traffic from this GPU in MiB/sec\nNVML_GPM_METRIC_PCIE_RX_PER_SEC             = 21 # PCIe traffic to this GPU in MiB/sec\nNVML_GPM_METRIC_NVDEC_0_UTIL                = 30 # Percent utilization of NVDEC 0. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_1_UTIL                = 31 # Percent utilization of NVDEC 1. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_2_UTIL                = 32 # Percent utilization of NVDEC 2. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_3_UTIL                = 33 # Percent utilization of NVDEC 3. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_4_UTIL                = 34 # Percent utilization of NVDEC 4. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_5_UTIL                = 35 # Percent utilization of NVDEC 5. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_6_UTIL                = 36 # Percent utilization of NVDEC 6. 0.0 - 100.0\nNVML_GPM_METRIC_NVDEC_7_UTIL                = 37 # Percent utilization of NVDEC 7. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_0_UTIL                = 40 # Percent utilization of NVJPG 0. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_1_UTIL                = 41 # Percent utilization of NVJPG 1. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_2_UTIL                = 42 # Percent utilization of NVJPG 2. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_3_UTIL                = 43 # Percent utilization of NVJPG 3. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_4_UTIL                = 44 # Percent utilization of NVJPG 4. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_5_UTIL                = 45 # Percent utilization of NVJPG 5. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_6_UTIL                = 46 # Percent utilization of NVJPG 6. 0.0 - 100.0\nNVML_GPM_METRIC_NVJPG_7_UTIL                = 47 # Percent utilization of NVJPG 7. 0.0 - 100.0\nNVML_GPM_METRIC_NVOFA_0_UTIL                = 50 # Percent utilization of NVOFA 0. 0.0 - 100.0\nNVML_GPM_METRIC_NVOFA_1_UTIL                = 51 # Percent utilization of NVOFA 1. 0.0 - 100.0\nNVML_GPM_METRIC_NVLINK_TOTAL_RX_PER_SEC     = 60 # NvLink read bandwidth for all links in MiB/sec\nNVML_GPM_METRIC_NVLINK_TOTAL_TX_PER_SEC     = 61 # NvLink write bandwidth for all links in MiB/sec\nNVML_GPM_METRIC_NVLINK_L0_RX_PER_SEC        = 62 # NvLink read bandwidth for link 0 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L0_TX_PER_SEC        = 63 # NvLink write bandwidth for link 0 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L1_RX_PER_SEC        = 64 # NvLink read bandwidth for link 1 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L1_TX_PER_SEC        = 65 # NvLink write bandwidth for link 1 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L2_RX_PER_SEC        = 66 # NvLink read bandwidth for link 2 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L2_TX_PER_SEC        = 67 # NvLink write bandwidth for link 2 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L3_RX_PER_SEC        = 68 # NvLink read bandwidth for link 3 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L3_TX_PER_SEC        = 69 # NvLink write bandwidth for link 3 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L4_RX_PER_SEC        = 70 # NvLink read bandwidth for link 4 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L4_TX_PER_SEC        = 71 # NvLink write bandwidth for link 4 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L5_RX_PER_SEC        = 72 # NvLink read bandwidth for link 5 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L5_TX_PER_SEC        = 73 # NvLink write bandwidth for link 5 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L6_RX_PER_SEC        = 74 # NvLink read bandwidth for link 6 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L6_TX_PER_SEC        = 75 # NvLink write bandwidth for link 6 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L7_RX_PER_SEC        = 76 # NvLink read bandwidth for link 7 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L7_TX_PER_SEC        = 77 # NvLink write bandwidth for link 7 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L8_RX_PER_SEC        = 78 # NvLink read bandwidth for link 8 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L8_TX_PER_SEC        = 79 # NvLink write bandwidth for link 8 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L9_RX_PER_SEC        = 80 # NvLink read bandwidth for link 9 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L9_TX_PER_SEC        = 81 # NvLink write bandwidth for link 9 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L10_RX_PER_SEC       = 82 # NvLink read bandwidth for link 10 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L10_TX_PER_SEC       = 83 # NvLink write bandwidth for link 10 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L11_RX_PER_SEC       = 84 # NvLink read bandwidth for link 11 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L11_TX_PER_SEC       = 85 # NvLink write bandwidth for link 11 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L12_RX_PER_SEC       = 86 # NvLink read bandwidth for link 12 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L12_TX_PER_SEC       = 87 # NvLink write bandwidth for link 12 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L13_RX_PER_SEC       = 88 # NvLink read bandwidth for link 13 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L13_TX_PER_SEC       = 89 # NvLink write bandwidth for link 13 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L14_RX_PER_SEC       = 90 # NvLink read bandwidth for link 14 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L14_TX_PER_SEC       = 91 # NvLink write bandwidth for link 14 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L15_RX_PER_SEC       = 92 # NvLink read bandwidth for link 15 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L15_TX_PER_SEC       = 93 # NvLink write bandwidth for link 15 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L16_RX_PER_SEC       = 94 # NvLink read bandwidth for link 16 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L16_TX_PER_SEC       = 95 # NvLink write bandwidth for link 16 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L17_RX_PER_SEC       = 96 # NvLink read bandwidth for link 17 in MiB/sec\nNVML_GPM_METRIC_NVLINK_L17_TX_PER_SEC       = 97 # NvLink write bandwidth for link 17 in MiB/sec\nNVML_GPM_METRIC_MAX                         = 98\n\n## Structs\n\nclass c_nvmlUnitInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('name', c_char * 96),\n        ('id', c_char * 96),\n        ('serial', c_char * 96),\n        ('firmwareVersion', c_char * 96),\n    ]\n\nclass struct_c_nvmlGpmSample_t(Structure):\n    pass # opaque handle\nc_nvmlGpmSample_t = POINTER(struct_c_nvmlGpmSample_t)\n\nclass c_metricInfo_t(Structure):\n    _fields_ = [\n        (\"shortName\", c_char_p),\n        (\"longName\", c_char_p),\n        (\"unit\", c_char_p),\n    ]\n\nclass c_nvmlGpmMetric_t(_PrintableStructure):\n    _fields_ = [\n        ('metricId', c_uint),\n        ('nvmlReturn', _nvmlReturn_t),\n        ('value', c_double),\n        ('metricInfo', c_metricInfo_t)\n    ]\n\nclass c_nvmlGpmMetricsGet_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('numMetrics', c_uint),\n        ('sample1', c_nvmlGpmSample_t),\n        ('sample2', c_nvmlGpmSample_t),\n        ('metrics', c_nvmlGpmMetric_t * NVML_GPM_METRIC_MAX)\n    ]\n\nNVML_GPM_METRICS_GET_VERSION = 1\n\nclass c_nvmlGpmSupport_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('isSupportedDevice', c_uint),\n    ]\n\nNVML_GPM_SUPPORT_VERSION = 1\n\n## Functions\n\ndef nvmlGpmMetricsGet(metricsGet):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmMetricsGet\")\n    ret = fn(byref(metricsGet))\n    _nvmlCheckReturn(ret)\n    return metricsGet\n\ndef nvmlGpmSampleFree(gpmSample):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmSampleFree\")\n    ret = fn(gpmSample)\n    _nvmlCheckReturn(ret)\n    return\n\ndef nvmlGpmSampleAlloc():\n    gpmSample = c_nvmlGpmSample_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmSampleAlloc\")\n    ret = fn(byref(gpmSample))\n    _nvmlCheckReturn(ret)\n    return gpmSample\n\ndef nvmlGpmSampleGet(device, gpmSample):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmSampleGet\")\n    ret = fn(device, gpmSample)\n    _nvmlCheckReturn(ret)\n    return gpmSample\n\ndef nvmlGpmMigSampleGet(device, gpuInstanceId, gpmSample):\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmMigSampleGet\")\n    ret = fn(device, gpuInstanceId, gpmSample)\n    _nvmlCheckReturn(ret)\n    return gpmSample\n\ndef nvmlGpmQueryDeviceSupport(device):\n    gpmSupport = c_nvmlGpmSupport_t()\n    gpmSupport.version = NVML_GPM_SUPPORT_VERSION\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmQueryDeviceSupport\")\n    ret = fn(device, byref(gpmSupport))\n    _nvmlCheckReturn(ret)\n    return gpmSupport\n\ndef nvmlGpmSetStreamingEnabled(device, state):\n    c_state = c_uint(state)\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmSetStreamingEnabled\")\n    ret = fn(device, c_state)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlGpmQueryIfStreamingEnabled(device):\n    c_state = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlGpmQueryIfStreamingEnabled\")\n    ret = fn(device, byref(c_state))\n    _nvmlCheckReturn(ret)\n    return c_state.value\n\n# Low Power Structure and Function\n\nNVML_NVLINK_POWER_STATE_HIGH_SPEED    = 0x0\nNVML_NVLINK_POWER_STATE_LOW           = 0x1\n\nNVML_NVLINK_LOW_POWER_THRESHOLD_MIN     = 0x1\nNVML_NVLINK_LOW_POWER_THRESHOLD_MAX     = 0x1FFF\nNVML_NVLINK_LOW_POWER_THRESHOLD_RESET   = 0xFFFFFFFF\nNVML_NVLINK_LOW_POWER_THRESHOLD_DEFAULT = NVML_NVLINK_LOW_POWER_THRESHOLD_RESET\n\nclass c_nvmlNvLinkPowerThres_t(Structure):\n    _fields_ = [\n        (\"lowPwrThreshold\", c_uint),\n    ]\n\ndef nvmlDeviceSetNvLinkDeviceLowPowerThreshold(device, l1threshold):\n    c_info = c_nvmlNvLinkPowerThres_t()\n    c_info.lowPwrThreshold = l1threshold\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetNvLinkDeviceLowPowerThreshold\")\n    ret = fn(device, byref(c_info))\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\nNVML_GPU_FABRIC_UUID_LEN = 16\n\n_nvmlGpuFabricState_t = c_uint\nNVML_GPU_FABRIC_STATE_NOT_SUPPORTED = 0\nNVML_GPU_FABRIC_STATE_NOT_STARTED   = 1\nNVML_GPU_FABRIC_STATE_IN_PROGRESS   = 2\nNVML_GPU_FABRIC_STATE_COMPLETED     = 3\n\nclass c_nvmlGpuFabricInfo_t(_PrintableStructure):\n    _fields_ = [\n        (\"clusterUuid\", c_char * NVML_DEVICE_UUID_BUFFER_SIZE),\n        (\"status\", _nvmlReturn_t),\n        (\"cliqueId\", c_uint32),\n        (\"state\", _nvmlGpuFabricState_t)\n    ]\n\nNVML_GPU_FABRIC_HEALTH_MASK_DEGRADED_BW_NOT_SUPPORTED = 0\nNVML_GPU_FABRIC_HEALTH_MASK_DEGRADED_BW_TRUE          = 1\nNVML_GPU_FABRIC_HEALTH_MASK_DEGRADED_BW_FALSE         = 2\nNVML_GPU_FABRIC_HEALTH_MASK_SHIFT_DEGRADED_BW         = 0\nNVML_GPU_FABRIC_HEALTH_MASK_WIDTH_DEGRADED_BW         = 0x11\n\nNVML_GPU_FABRIC_HEALTH_MASK_ROUTE_RECOVERY_NOT_SUPPORTED   = 0\nNVML_GPU_FABRIC_HEALTH_MASK_ROUTE_RECOVERY_TRUE            = 1\nNVML_GPU_FABRIC_HEALTH_MASK_ROUTE_RECOVERY_FALSE           = 2\nNVML_GPU_FABRIC_HEALTH_MASK_SHIFT_ROUTE_RECOVERY           = 2\nNVML_GPU_FABRIC_HEALTH_MASK_WIDTH_ROUTE_RECOVERY           = 0x11\n\nNVML_GPU_FABRIC_HEALTH_MASK_ROUTE_UNHEALTHY_NOT_SUPPORTED  = 0\nNVML_GPU_FABRIC_HEALTH_MASK_ROUTE_UNHEALTHY_TRUE           = 1\nNVML_GPU_FABRIC_HEALTH_MASK_ROUTE_UNHEALTHY_FALSE          = 2\nNVML_GPU_FABRIC_HEALTH_MASK_SHIFT_ROUTE_UNHEALTHY          = 4\nNVML_GPU_FABRIC_HEALTH_MASK_WIDTH_ROUTE_UNHEALTHY          = 0x11\n\nNVML_GPU_FABRIC_HEALTH_MASK_ACCESS_TIMEOUT_RECOVERY_NOT_SUPPORTED = 0\nNVML_GPU_FABRIC_HEALTH_MASK_ACCESS_TIMEOUT_RECOVERY_TRUE          = 1\nNVML_GPU_FABRIC_HEALTH_MASK_ACCESS_TIMEOUT_RECOVERY_FALSE         = 2\nNVML_GPU_FABRIC_HEALTH_MASK_SHIFT_ACCESS_TIMEOUT_RECOVERY         = 6\nNVML_GPU_FABRIC_HEALTH_MASK_WIDTH_ACCESS_TIMEOUT_RECOVERY         = 0x11\n\nnvmlGpuFabricInfo_v2 = 0x02000024\n\nclass c_nvmlGpuFabricInfoV_t(_PrintableStructure):\n    _fields_ = [\n        (\"version\", c_uint),\n        (\"clusterUuid\", c_char * NVML_GPU_FABRIC_UUID_LEN),\n        (\"status\", _nvmlReturn_t),\n        (\"cliqueId\", c_uint32),\n        (\"state\", _nvmlGpuFabricState_t),\n        (\"healthMask\", c_uint32)\n    ]\n\n    def __init__(self):\n        super(c_nvmlGpuFabricInfoV_t, self).__init__(version=nvmlGpuFabricInfo_v2)\n\ndef nvmlDeviceGetGpuFabricInfo(device, gpuFabricInfo):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuFabricInfo\");\n    ret = fn(device, gpuFabricInfo)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetGpuFabricInfoV(device, gpuFabricInfo):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetGpuFabricInfoV\");\n    ret = fn(device, gpuFabricInfo)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\n######################\n## Enums/defines\n#### NVML GPU NVLINK BW MODE\nNVML_GPU_NVLINK_BW_MODE_FULL      = 0x0\nNVML_GPU_NVLINK_BW_MODE_OFF       = 0x1\nNVML_GPU_NVLINK_BW_MODE_MIN       = 0x2\nNVML_GPU_NVLINK_BW_MODE_HALF      = 0x3\nNVML_GPU_NVLINK_BW_MODE_3QUARTER  = 0x4\nNVML_GPU_NVLINK_BW_MODE_COUNT     = 0x5\n\ndef nvmlSystemSetNvlinkBwMode(mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemSetNvlinkBwMode\")\n    ret = fn(mode)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlSystemGetNvlinkBwMode():\n    mode = c_uint()\n    fn = _nvmlGetFunctionPointer(\"nvmlSystemGetNvlinkBwMode\")\n    ret = fn(byref(mode))\n    _nvmlCheckReturn(ret)\n    return mode.value\n\n_nvmlPowerScopeType_t = c_uint\nNVML_POWER_SCOPE_GPU     = 0\nNVML_POWER_SCOPE_MODULE  = 1\nNVML_POWER_SCOPE_MEMORY  = 2\n\nclass c_nvmlPowerValue_v2_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('powerScope', _nvmlPowerScopeType_t),\n        ('powerValueMw', c_uint),\n    ]\n    _fmt_ = {'<default>': \"%d B\"}\n\nnvmlPowerValue_v2 = 0x0200000C\n\ndef nvmlDeviceSetPowerManagementLimit_v2(device, powerScope, powerLimit, version=nvmlPowerValue_v2):\n    c_powerScope = _nvmlPowerScopeType_t(powerScope)\n    c_powerValue = c_nvmlPowerValue_v2_t()\n    c_powerValue.version = c_uint(version)\n    c_powerValue.powerScope = c_powerScope\n    c_powerValue.powerValueMw = c_uint(powerLimit)\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetPowerManagementLimit_v2\")\n    ret = fn(device, byref(c_powerValue))\n    return NVML_SUCCESS\n\nclass c_nvmlEccSramErrorStatus_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('aggregateUncParity', c_ulonglong),\n        ('aggregateUncSecDed', c_ulonglong),\n        ('aggregateCor', c_ulonglong),\n        ('volatileUncParity', c_ulonglong),\n        ('volatileUncSecDed', c_ulonglong),\n        ('volatileCor', c_ulonglong),\n        ('aggregateUncBucketL2', c_ulonglong),\n        ('aggregateUncBucketSm', c_ulonglong),\n        ('aggregateUncBucketPcie', c_ulonglong),\n        ('aggregateUncBucketMcu', c_ulonglong),\n        ('aggregateUncBucketOther', c_ulonglong),\n        ('bThresholdExceeded', c_uint)\n    ]\n\n    def __init__(self):\n        super(c_nvmlEccSramErrorStatus_v1_t, self).__init__(version=nvmlEccSramErrorStatus_v1)\n\nnvmlEccSramErrorStatus_v1 = 0x1000068\ndef nvmlDeviceGetSramEccErrorStatus(device, status):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetSramEccErrorStatus\")\n    ret = fn(device, status)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\nNVML_DEV_CAP_EGM = (1 << 0)\nnvmlDeviceCapabilities_v1 = 0x1000008\n\nclass c_nvmlDeviceCapabilities_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('capMask', c_uint),\n    ]\n\n    def __init__(self):\n        super(c_nvmlDeviceCapabilities_v1_t, self).__init__(version=nvmlDeviceCapabilities_v1)\n\n\ndef nvmlDeviceGetCapabilities(device, caps):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetCapabilities\")\n    return fn(device, caps)\n\nclass c_nvmlPlatformInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('ibGuid', c_char * 16),\n        ('rackGuid', c_char * 16),\n        ('chassisPhysicalSlotNumber', c_char),\n        ('computeSlotIndex', c_char),\n        ('nodeIndex', c_char),\n        ('peerType', c_char),\n        ('moduleId', c_char)\n    ]\n\n    def __init__(self):\n        super(c_nvmlPlatformInfo_v1_t, self).__init__(version=nvmlPlatformInfo_v1)\n\nnvmlPlatformInfo_v1 = 0x100002c\ndef nvmlDeviceGetPlatformInfo(device, platformInfo):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetPlatformInfo\")\n    ret = fn(device, platformInfo)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\nclass c_nvmlMask255_t(_PrintableStructure):\n    _fields_ = [\n        ('mask', c_uint * 8),\n    ]\n\nNVML_WORKLOAD_POWER_MAX_PROFILES    = 255\nNVML_POWER_PROFILE_MAX_P            = 0\nNVML_POWER_PROFILE_MAX_Q            = 1\nNVML_POWER_PROFILE_COMPUTE          = 2\nNVML_POWER_PROFILE_MEMORY_BOUND     = 3\nNVML_POWER_PROFILE_NETWORK          = 4\nNVML_POWER_PROFILE_BALANCED         = 5\nNVML_POWER_PROFILE_LLM_INFERENCE    = 6\nNVML_POWER_PROFILE_LLM_TRAINING     = 7\nNVML_POWER_PROFILE_RBM              = 8\nNVML_POWER_PROFILE_DCPCIE           = 9\nNVML_POWER_PROFILE_HMMA_SPARSE      = 10\nNVML_POWER_PROFILE_HMMA_DENSE       = 11\nNVML_POWER_PROFILE_SYNC_BALANCED    = 12\nNVML_POWER_PROFILE_HPC              = 13\nNVML_POWER_PROFILE_MIG              = 14\nNVML_POWER_PROFILE_MAX              = 15\n\nnvmlWorkloadPowerProfileInfo_v1 = 0x100002c\nclass c_nvmlWorkloadPowerProfileInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('profileId', c_uint),\n        ('priority', c_uint),\n        ('conflictingmask', c_nvmlMask255_t)\n    ]\n\n    def __init__(self):\n        super(c_nvmlWorkloadPowerProfileInfo_v1_t, self).__init__(version=nvmlWorkloadPowerProfileInfo_v1)\n\nnvmlWorkloadPowerProfileProfilesInfo_v1 = 0x1002bf8\nclass c_nvmlWorkloadPowerProfileProfilesInfo_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('perfProfilesMask', c_nvmlMask255_t),\n        ('perfProfile', c_nvmlWorkloadPowerProfileInfo_v1_t * NVML_WORKLOAD_POWER_MAX_PROFILES)\n    ]\n\n    def __init__(self):\n        super(c_nvmlWorkloadPowerProfileProfilesInfo_v1_t, self).__init__(version=nvmlWorkloadPowerProfileProfilesInfo_v1)\n\nnvmlWorkloadPowerProfileCurrentProfiles_v1 = 0x1000064\nclass c_nvmlWorkloadPowerProfileCurrentProfiles_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('perfProfilesMask', c_nvmlMask255_t),\n        ('requestedProfilesMask', c_nvmlMask255_t),\n        ('enforcedProfilesMask', c_nvmlMask255_t)\n    ]\n\n    def __init__(self):\n        super(c_nvmlWorkloadPowerProfileCurrentProfiles_v1_t, self).__init__(version=nvmlWorkloadPowerProfileCurrentProfiles_v1)\n\nnvmlWorkloadPowerProfileRequestedProfiles_v1 = 0x1000024\nclass c_nvmlWorkloadPowerProfileRequestedProfiles_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('requestedProfilesMask', c_nvmlMask255_t),\n    ]\n\n    def __init__(self):\n        super(c_nvmlWorkloadPowerProfileRequestedProfiles_v1_t, self).__init__(version=nvmlWorkloadPowerProfileRequestedProfiles_v1)\n\ndef nvmlDeviceWorkloadPowerProfileGetProfilesInfo(device, profilesInfo):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceWorkloadPowerProfileGetProfilesInfo\")\n    ret = fn(device, profilesInfo)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceWorkloadPowerProfileGetCurrentProfiles(device, currentProfiles):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceWorkloadPowerProfileGetCurrentProfiles\")\n    ret = fn(device, currentProfiles)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceWorkloadPowerProfileSetRequestedProfiles(device, requestedProfiles):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceWorkloadPowerProfileSetRequestedProfiles\")\n    ret = fn(device, requestedProfiles)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceWorkloadPowerProfileClearRequestedProfiles(device, requestedProfiles):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceWorkloadPowerProfileClearRequestedProfiles\")\n    ret = fn(device, requestedProfiles)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetNvlinkSupportedBwModes(device, supportedBwModes):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvlinkSupportedBwModes\")\n    ret = fn(device, supportedBwModes)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceGetNvlinkBwMode(device, getBwMode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetNvlinkBwMode\")\n    ret = fn(device, getBwMode)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\ndef nvmlDeviceSetNvlinkBwMode(device, setBwMode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetNvlinkBwMode\")\n    ret = fn(device, setBwMode)\n    _nvmlCheckReturn(ret)\n    return NVML_SUCCESS\n\nnvmlDramEncryptionInfo_v1 = 0x01000008\n\nclass c_nvmlDramEncryptionInfo_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('encryptionState',  _nvmlEnableState_t),\n    ]\n\n    def __init__(self):\n        super(c_nvmlDramEncryptionInfo_t, self).__init__(version=nvmlDramEncryptionInfo_v1)\n\ndef nvmlDeviceGetDramEncryptionMode(handle):\n    c_currState = c_nvmlDramEncryptionInfo_t()\n    c_pendingState = c_nvmlDramEncryptionInfo_t()\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceGetDramEncryptionMode\")\n    ret = fn(handle, byref(c_currState), byref(c_pendingState))\n    _nvmlCheckReturn(ret)\n    return [c_currState.encryptionState, c_pendingState.encryptionState]\n\n# added to API\ndef nvmlDeviceGetCurrentDramEncryptionMode(handle):\n    return nvmlDeviceGetDramEncryptionMode(handle)[0]\n\n# added to API\ndef nvmlDeviceGetPendingDramEncryptionMode(handle):\n    return nvmlDeviceGetDramEncryptionMode(handle)[1]\n\ndef nvmlDeviceSetDramEncryptionMode(handle, mode):\n    fn = _nvmlGetFunctionPointer(\"nvmlDeviceSetDramEncryptionMode\")\n    c_dramEncryptionMode = c_nvmlDramEncryptionInfo_t()\n    c_dramEncryptionMode.encryptionState = mode;\n    ret = fn(handle, byref(c_dramEncryptionMode))\n    _nvmlCheckReturn(ret)\n    return None\n\n# Power Smoothing defines\nNVML_POWER_SMOOTHING_MAX_NUM_PROFILES                   = 5\nNVML_POWER_SMOOTHING_ADMIN_OVERRIDE_NOT_SET             = 0xFFFFFFFF\nNVML_POWER_SMOOTHING_PROFILE_PARAM_PERCENT_TMP_FLOOR    = 0\nNVML_POWER_SMOOTHING_PROFILE_PARAM_RAMP_UP_RATE         = 1\nNVML_POWER_SMOOTHING_PROFILE_PARAM_RAMP_DOWN_RATE       = 2\nNVML_POWER_SMOOTHING_PROFILE_PARAM_RAMP_DOWN_HYSTERESIS = 3\n\nnvmlPowerSmoothingState_v1=0x1000008\nclass c_nvmlPowerSmoothingState_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('state', c_uint),\n    ]\n\n    def __init__(self):\n        super(c_nvmlPowerSmoothingState_v1_t, self).__init__(version=nvmlPowerSmoothingState_v1)\n\nnvmlPowerSmoothingProfile_v1=0x1000018\nclass c_nvmlPowerSmoothingProfile_v1_t(_PrintableStructure):\n    _fields_ = [\n        ('version', c_uint),\n        ('profileId', c_uint),\n        ('paramId', c_uint),\n        ('value', c_double),\n    ]\n\n    def __init__(self):\n        super(c_nvmlPowerSmoothingProfile_v1_t, self).__init__(version=nvmlPowerSmoothingProfile_v1)\n\ndef nvmlDevicePowerSmoothingActivatePresetProfile(device, profile):\n    fn = _nvmlGetFunctionPointer(\"nvmlDevicePowerSmoothingActivatePresetProfile\")\n    ret = fn(device, profile)\n    _nvmlCheckReturn(ret)\n\ndef nvmlDevicePowerSmoothingUpdatePresetProfileParam(device, profile):\n    fn = _nvmlGetFunctionPointer(\"nvmlDevicePowerSmoothingUpdatePresetProfileParam\")\n    ret = fn(device, profile)\n    _nvmlCheckReturn(ret)\n\ndef nvmlDevicePowerSmoothingSetState(device, state):\n    fn = _nvmlGetFunctionPointer(\"nvmlDevicePowerSmoothingSetState\")\n    ret = fn(device, state)\n    _nvmlCheckReturn(ret)\n\n", 6139], "/home/jeromeku/vllm/vllm/platforms/cuda.py": ["# SPDX-License-Identifier: Apache-2.0\n\"\"\"Code inside this file can safely assume cuda platform, e.g. importing\npynvml. However, it should not initialize cuda context.\n\"\"\"\n\nimport os\nfrom functools import wraps\nfrom typing import TYPE_CHECKING, Callable, Optional, TypeVar, Union\n\nimport torch\nfrom typing_extensions import ParamSpec\n\n# import custom ops, trigger op registration\nimport vllm._C  # noqa\nimport vllm.envs as envs\nfrom vllm.logger import init_logger\nfrom vllm.utils import import_pynvml\n\nfrom .interface import DeviceCapability, Platform, PlatformEnum, _Backend\n\nif TYPE_CHECKING:\n    from vllm.config import ModelConfig, VllmConfig\n\nlogger = init_logger(__name__)\n\n_P = ParamSpec(\"_P\")\n_R = TypeVar(\"_R\")\n\npynvml = import_pynvml()\n\n# pytorch 2.5 uses cudnn sdpa by default, which will cause crash on some models\n# see https://github.com/huggingface/diffusers/issues/9704 for details\ntorch.backends.cuda.enable_cudnn_sdp(False)\n\n\ndef with_nvml_context(fn: Callable[_P, _R]) -> Callable[_P, _R]:\n\n    @wraps(fn)\n    def wrapper(*args: _P.args, **kwargs: _P.kwargs) -> _R:\n        pynvml.nvmlInit()\n        try:\n            return fn(*args, **kwargs)\n        finally:\n            pynvml.nvmlShutdown()\n\n    return wrapper\n\n\nclass CudaPlatformBase(Platform):\n    _enum = PlatformEnum.CUDA\n    device_name: str = \"cuda\"\n    device_type: str = \"cuda\"\n    dispatch_key: str = \"CUDA\"\n    ray_device_key: str = \"GPU\"\n    device_control_env_var: str = \"CUDA_VISIBLE_DEVICES\"\n\n    @property\n    def supported_dtypes(self) -> list[torch.dtype]:\n        if self.has_device_capability(80):\n            # Ampere and Hopper or later NVIDIA GPUs.\n            return [torch.bfloat16, torch.float16, torch.float32]\n        elif (not self.has_device_capability(80)\n              ) and self.has_device_capability(60):\n            # Pascal, Volta and Turing NVIDIA GPUs, BF16 is not supported\n            return [torch.float16, torch.float32]\n        # Kepler and Maxwell NVIDIA GPUs, only FP32 is supported,\n        # though vLLM doesn't support these GPUs.\n        return [torch.float32]\n\n    @classmethod\n    def get_device_capability(cls,\n                              device_id: int = 0\n                              ) -> Optional[DeviceCapability]:\n        raise NotImplementedError\n\n    @classmethod\n    def get_device_name(cls, device_id: int = 0) -> str:\n        raise NotImplementedError\n\n    @classmethod\n    def get_device_total_memory(cls, device_id: int = 0) -> int:\n        raise NotImplementedError\n\n    @classmethod\n    def is_async_output_supported(cls, enforce_eager: Optional[bool]) -> bool:\n        if enforce_eager:\n            logger.warning(\n                \"To see benefits of async output processing, enable CUDA \"\n                \"graph. Since, enforce-eager is enabled, async output \"\n                \"processor cannot be used\")\n            return False\n        return True\n\n    @classmethod\n    def is_fully_connected(cls, device_ids: list[int]) -> bool:\n        raise NotImplementedError\n\n    @classmethod\n    def log_warnings(cls):\n        pass\n\n    @classmethod\n    def check_and_update_config(cls, vllm_config: \"VllmConfig\") -> None:\n        parallel_config = vllm_config.parallel_config\n        scheduler_config = vllm_config.scheduler_config\n        compilation_config = vllm_config.compilation_config\n        model_config = vllm_config.model_config\n\n        if parallel_config.worker_cls == \"auto\":\n            if scheduler_config.is_multi_step:\n                if envs.VLLM_USE_V1:\n                    raise NotImplementedError(\n                        \"Multi-step scheduling is not supported (and not \"\n                        \"needed) on vLLM V1. Please launch without \"\n                        \"--num-scheduler-steps.\")\n                else:\n                    parallel_config.worker_cls = \\\n                        \"vllm.worker.multi_step_worker.MultiStepWorker\"\n            elif vllm_config.speculative_config:\n                if envs.VLLM_USE_V1:\n                    parallel_config.worker_cls = \\\n                            \"vllm.v1.worker.gpu_worker.Worker\"\n                else:\n                    parallel_config.worker_cls = \\\n                        \"vllm.spec_decode.spec_decode_worker.create_spec_worker\"\n                    parallel_config.sd_worker_cls = \\\n                        \"vllm.worker.worker.Worker\"\n            else:\n                if envs.VLLM_USE_V1:\n                    parallel_config.worker_cls = \\\n                            \"vllm.v1.worker.gpu_worker.Worker\"\n                else:\n                    parallel_config.worker_cls = \"vllm.worker.worker.Worker\"\n\n        cache_config = vllm_config.cache_config\n        if cache_config and cache_config.block_size is None:\n            cache_config.block_size = 16\n\n        # TODO(lucas): handle this more gracefully\n        # Note: model_config may be None during testing\n        if model_config is not None and model_config.use_mla:\n            # if `VLLM_ATTENTION_BACKEND` is not set and we are using MLA, then\n            # we default to FlashMLA backend, so we need to force the blocksize\n            # here\n            use_flashmla = (envs.VLLM_ATTENTION_BACKEND is None \\\n                or envs.VLLM_ATTENTION_BACKEND == \"FLASHMLA\")\n            from vllm.attention.ops.flashmla import is_flashmla_supported\n            if use_flashmla and is_flashmla_supported()[0] \\\n                and cache_config.block_size != 64:\n                cache_config.block_size = 64\n                logger.info(\n                    \"Forcing kv cache block size to 64 for FlashMLA backend.\")\n\n        if (parallel_config.data_parallel_size > 1\n                and compilation_config.use_cudagraph):\n            logger.info(\n                \"Data Parallel: Forcing enforce eager to be True since DP is \"\n                \"currently not supported with CUDA Graphs.\")\n            vllm_config.model_config.enforce_eager = True\n            compilation_config.use_cudagraph = False\n            compilation_config.use_inductor = False\n\n    @classmethod\n    def get_current_memory_usage(cls,\n                                 device: Optional[torch.types.Device] = None\n                                 ) -> float:\n        torch.cuda.reset_peak_memory_stats(device)\n        return torch.cuda.max_memory_allocated(device)\n\n    @classmethod\n    def get_attn_backend_cls(cls, selected_backend, head_size, dtype,\n                             kv_cache_dtype, block_size, use_v1,\n                             use_mla) -> str:\n        if use_mla:\n            # TODO(lucas): refactor to  be more concise\n            #  we should probably consider factoring out V1 here\n            if selected_backend == _Backend.TRITON_MLA or block_size != 64:\n                if use_v1:\n                    logger.info_once(\"Using Triton MLA backend on V1 engine.\")\n                    return (\"vllm.v1.attention.backends.mla.\"\n                            \"triton_mla.TritonMLABackend\")\n                else:\n                    logger.info(\"Using Triton MLA backend.\")\n                    return \"vllm.attention.backends.triton_mla.TritonMLABackend\"\n            else:\n                from vllm.attention.backends.flashmla import (\n                    is_flashmla_supported)\n                if not is_flashmla_supported()[0]:\n                    logger.warning(\n                        \"FlashMLA backend is not supported due to %s\",\n                        is_flashmla_supported()[1])\n                elif block_size != 64:\n                    logger.warning(\n                        \"FlashMLA backend is not supported for block size %d\"\n                        \" (currently only supports block size 64).\",\n                        block_size)\n                else:\n                    if use_v1:\n                        logger.info_once(\n                            \"Using FlashMLA backend on V1 engine.\")\n                        return (\"vllm.v1.attention.backends.mla.\"\n                                \"flashmla.FlashMLABackend\")\n                    else:\n                        logger.info(\"Using FlashMLA backend.\")\n                        return (\"vllm.attention.backends.\"\n                                \"flashmla.FlashMLABackend\")\n        if use_v1:\n            if selected_backend == _Backend.FLASHINFER:\n                logger.info_once(\"Using FlashInfer backend on V1 engine.\")\n                return \"vllm.v1.attention.backends.flashinfer.FlashInferBackend\"\n            if selected_backend == _Backend.TRITON_ATTN_VLLM_V1:\n                logger.info_once(\"Using Triton backend on V1 engine.\")\n                return (\"vllm.v1.attention.backends.\"\n                        \"triton_attn.TritonAttentionBackend\")\n            if cls.has_device_capability(80):\n                logger.info_once(\"Using Flash Attention backend on V1 engine.\")\n                return (\"vllm.v1.attention.backends.\"\n                        \"flash_attn.FlashAttentionBackend\")\n        if selected_backend == _Backend.FLASHINFER:\n            logger.info(\"Using FlashInfer backend.\")\n            return \"vllm.attention.backends.flashinfer.FlashInferBackend\"\n        elif selected_backend == _Backend.XFORMERS:\n            logger.info(\"Using XFormers backend.\")\n            return \"vllm.attention.backends.xformers.XFormersBackend\"\n        elif selected_backend == _Backend.DUAL_CHUNK_FLASH_ATTN:\n            logger.info(\"Using DualChunkFlashAttention backend.\")\n            return (\"vllm.attention.backends.dual_chunk_flash_attn.\"\n                    \"DualChunkFlashAttentionBackend\")\n        elif selected_backend == _Backend.FLASH_ATTN:\n            pass\n        elif selected_backend:\n            raise ValueError(\n                f\"Invalid attention backend for {cls.device_name}, \"\n                f\"with use_v1: {use_v1} use_mla: {use_mla}\")\n\n        target_backend = _Backend.FLASH_ATTN\n        if not cls.has_device_capability(80):\n            # Volta and Turing NVIDIA GPUs.\n            logger.info(\n                \"Cannot use FlashAttention-2 backend for Volta and Turing \"\n                \"GPUs.\")\n            target_backend = _Backend.XFORMERS\n        elif dtype not in (torch.float16, torch.bfloat16):\n            logger.info(\n                \"Cannot use FlashAttention-2 backend for dtype other than \"\n                \"torch.float16 or torch.bfloat16.\")\n            target_backend = _Backend.XFORMERS\n        elif block_size % 16 != 0:\n            logger.info(\n                \"Cannot use FlashAttention-2 backend for block size not \"\n                \"divisible by 16.\")\n            target_backend = _Backend.XFORMERS\n\n        # FlashAttn is valid for the model, checking if the package is\n        # installed.\n        if target_backend == _Backend.FLASH_ATTN:\n            try:\n                import vllm.vllm_flash_attn  # noqa: F401\n                from vllm.attention.backends.flash_attn import (  # noqa: F401\n                    FlashAttentionBackend, flash_attn_supports_fp8)\n\n                supported_sizes = \\\n                    FlashAttentionBackend.get_supported_head_sizes()\n                if head_size not in supported_sizes:\n                    logger.info(\n                        \"Cannot use FlashAttention-2 backend for head size %d.\",\n                        head_size)\n                    target_backend = _Backend.XFORMERS\n                fp8_kv_cache = (kv_cache_dtype is not None\n                                and kv_cache_dtype.startswith(\"fp8\"))\n                if (fp8_kv_cache and not flash_attn_supports_fp8()):\n                    logger.info(\n                        \"Cannot use FlashAttention backend for FP8 KV cache.\")\n                    logger.warning(\n                        \"Please use FlashInfer backend with FP8 KV Cache for \"\n                        \"better performance by setting environment variable \"\n                        \"VLLM_ATTENTION_BACKEND=FLASHINFER\")\n                    target_backend = _Backend.XFORMERS\n            except ImportError:\n                logger.info(\n                    \"Cannot use FlashAttention-2 backend because the \"\n                    \"vllm.vllm_flash_attn package is not found. \"\n                    \"Make sure that vllm_flash_attn was built and installed \"\n                    \"(on by default).\")\n                target_backend = _Backend.XFORMERS\n\n        if target_backend == _Backend.XFORMERS:\n            logger.info(\"Using XFormers backend.\")\n            return \"vllm.attention.backends.xformers.XFormersBackend\"\n\n        logger.info(\"Using Flash Attention backend.\")\n        return \"vllm.attention.backends.flash_attn.FlashAttentionBackend\"\n\n    @classmethod\n    def get_punica_wrapper(cls) -> str:\n        return \"vllm.lora.punica_wrapper.punica_gpu.PunicaWrapperGPU\"\n\n    @classmethod\n    def get_device_communicator_cls(cls) -> str:\n        return \"vllm.distributed.device_communicators.cuda_communicator.CudaCommunicator\"  # noqa\n\n    @classmethod\n    def supports_fp8(cls) -> bool:\n        return cls.has_device_capability(89)\n\n    @classmethod\n    def supports_v1(cls, model_config: \"ModelConfig\") -> bool:\n        return True\n\n    @classmethod\n    def use_custom_allreduce(cls) -> bool:\n        return True\n\n\n# NVML utils\n# Note that NVML is not affected by `CUDA_VISIBLE_DEVICES`,\n# all the related functions work on real physical device ids.\n# the major benefit of using NVML is that it will not initialize CUDA\nclass NvmlCudaPlatform(CudaPlatformBase):\n\n    @classmethod\n    @with_nvml_context\n    def get_device_capability(cls,\n                              device_id: int = 0\n                              ) -> Optional[DeviceCapability]:\n        try:\n            physical_device_id = cls.device_id_to_physical_device_id(device_id)\n            handle = pynvml.nvmlDeviceGetHandleByIndex(physical_device_id)\n            major, minor = pynvml.nvmlDeviceGetCudaComputeCapability(handle)\n            return DeviceCapability(major=major, minor=minor)\n        except RuntimeError:\n            return None\n\n    @classmethod\n    @with_nvml_context\n    def has_device_capability(\n        cls,\n        capability: Union[tuple[int, int], int],\n        device_id: int = 0,\n    ) -> bool:\n        try:\n            return super().has_device_capability(capability, device_id)\n        except RuntimeError:\n            return False\n\n    @classmethod\n    @with_nvml_context\n    def get_device_name(cls, device_id: int = 0) -> str:\n        physical_device_id = cls.device_id_to_physical_device_id(device_id)\n        return cls._get_physical_device_name(physical_device_id)\n\n    @classmethod\n    @with_nvml_context\n    def get_device_uuid(cls, device_id: int = 0) -> str:\n        physical_device_id = cls.device_id_to_physical_device_id(device_id)\n        handle = pynvml.nvmlDeviceGetHandleByIndex(physical_device_id)\n        return pynvml.nvmlDeviceGetUUID(handle)\n\n    @classmethod\n    @with_nvml_context\n    def get_device_total_memory(cls, device_id: int = 0) -> int:\n        physical_device_id = cls.device_id_to_physical_device_id(device_id)\n        handle = pynvml.nvmlDeviceGetHandleByIndex(physical_device_id)\n        return int(pynvml.nvmlDeviceGetMemoryInfo(handle).total)\n\n    @classmethod\n    @with_nvml_context\n    def is_fully_connected(cls, physical_device_ids: list[int]) -> bool:\n        \"\"\"\n        query if the set of gpus are fully connected by nvlink (1 hop)\n        \"\"\"\n        handles = [\n            pynvml.nvmlDeviceGetHandleByIndex(i) for i in physical_device_ids\n        ]\n        for i, handle in enumerate(handles):\n            for j, peer_handle in enumerate(handles):\n                if i < j:\n                    try:\n                        p2p_status = pynvml.nvmlDeviceGetP2PStatus(\n                            handle,\n                            peer_handle,\n                            pynvml.NVML_P2P_CAPS_INDEX_NVLINK,\n                        )\n                        if p2p_status != pynvml.NVML_P2P_STATUS_OK:\n                            return False\n                    except pynvml.NVMLError:\n                        logger.exception(\n                            \"NVLink detection failed. This is normal if\"\n                            \" your machine has no NVLink equipped.\")\n                        return False\n        return True\n\n    @classmethod\n    def _get_physical_device_name(cls, device_id: int = 0) -> str:\n        handle = pynvml.nvmlDeviceGetHandleByIndex(device_id)\n        return pynvml.nvmlDeviceGetName(handle)\n\n    @classmethod\n    @with_nvml_context\n    def log_warnings(cls):\n        device_ids: int = pynvml.nvmlDeviceGetCount()\n        if device_ids > 1:\n            device_names = [\n                cls._get_physical_device_name(i) for i in range(device_ids)\n            ]\n            if (len(set(device_names)) > 1\n                    and os.environ.get(\"CUDA_DEVICE_ORDER\") != \"PCI_BUS_ID\"):\n                logger.warning(\n                    \"Detected different devices in the system: %s. Please\"\n                    \" make sure to set `CUDA_DEVICE_ORDER=PCI_BUS_ID` to \"\n                    \"avoid unexpected behavior.\",\n                    \", \".join(device_names),\n                )\n\n\nclass NonNvmlCudaPlatform(CudaPlatformBase):\n\n    @classmethod\n    def get_device_capability(cls, device_id: int = 0) -> DeviceCapability:\n        major, minor = torch.cuda.get_device_capability(device_id)\n        return DeviceCapability(major=major, minor=minor)\n\n    @classmethod\n    def get_device_name(cls, device_id: int = 0) -> str:\n        return torch.cuda.get_device_name(device_id)\n\n    @classmethod\n    def get_device_total_memory(cls, device_id: int = 0) -> int:\n        device_props = torch.cuda.get_device_properties(device_id)\n        return device_props.total_memory\n\n    @classmethod\n    def is_fully_connected(cls, physical_device_ids: list[int]) -> bool:\n        logger.exception(\n            \"NVLink detection not possible, as context support was\"\n            \" not found. Assuming no NVLink available.\")\n        return False\n\n\n# Autodetect either NVML-enabled or non-NVML platform\n# based on whether NVML is available.\nnvml_available = False\ntry:\n    try:\n        pynvml.nvmlInit()\n        nvml_available = True\n    except Exception:\n        # On Jetson, NVML is not supported.\n        nvml_available = False\nfinally:\n    if nvml_available:\n        pynvml.nvmlShutdown()\n\nCudaPlatform = NvmlCudaPlatform if nvml_available else NonNvmlCudaPlatform\n\nCudaPlatform.log_warnings()\n", 456], "/home/jeromeku/vllm/vllm/model_executor/models/registry.py": ["# SPDX-License-Identifier: Apache-2.0\n\"\"\"\nWhenever you add an architecture to this page, please also update\n`tests/models/registry.py` with example HuggingFace models for it.\n\"\"\"\nimport importlib\nimport os\nimport pickle\nimport subprocess\nimport sys\nimport tempfile\nfrom abc import ABC, abstractmethod\nfrom collections.abc import Set\nfrom dataclasses import dataclass, field\nfrom functools import lru_cache\nfrom typing import Callable, Optional, TypeVar, Union\n\nimport cloudpickle\nimport torch.nn as nn\n\nfrom vllm.logger import init_logger\n\nfrom .interfaces import (has_inner_state, has_noops, is_attention_free,\n                         is_hybrid, supports_cross_encoding,\n                         supports_multimodal, supports_pp,\n                         supports_transcription, supports_v0_only)\nfrom .interfaces_base import is_text_generation_model\n\nlogger = init_logger(__name__)\n\n# yapf: disable\n_TEXT_GENERATION_MODELS = {\n    # [Decoder-only]\n    \"AquilaModel\": (\"llama\", \"LlamaForCausalLM\"),\n    \"AquilaForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),  # AquilaChat2\n    \"ArcticForCausalLM\": (\"arctic\", \"ArcticForCausalLM\"),\n    \"MiniMaxText01ForCausalLM\": (\"minimax_text_01\", \"MiniMaxText01ForCausalLM\"),\n    # baichuan-7b, upper case 'C' in the class name\n    \"BaiChuanForCausalLM\": (\"baichuan\", \"BaiChuanForCausalLM\"),\n    # baichuan-13b, lower case 'c' in the class name\n    \"BaichuanForCausalLM\": (\"baichuan\", \"BaichuanForCausalLM\"),\n    \"BambaForCausalLM\": (\"bamba\", \"BambaForCausalLM\"),\n    \"BloomForCausalLM\": (\"bloom\", \"BloomForCausalLM\"),\n    \"ChatGLMModel\": (\"chatglm\", \"ChatGLMForCausalLM\"),\n    \"ChatGLMForConditionalGeneration\": (\"chatglm\", \"ChatGLMForCausalLM\"),\n    \"CohereForCausalLM\": (\"commandr\", \"CohereForCausalLM\"),\n    \"Cohere2ForCausalLM\": (\"commandr\", \"CohereForCausalLM\"),\n    \"DbrxForCausalLM\": (\"dbrx\", \"DbrxForCausalLM\"),\n    \"DeciLMForCausalLM\": (\"nemotron_nas\", \"DeciLMForCausalLM\"),\n    \"DeepseekForCausalLM\": (\"deepseek\", \"DeepseekForCausalLM\"),\n    \"DeepseekV2ForCausalLM\": (\"deepseek_v2\", \"DeepseekV2ForCausalLM\"),\n    \"DeepseekV3ForCausalLM\": (\"deepseek_v2\", \"DeepseekV3ForCausalLM\"),\n    \"ExaoneForCausalLM\": (\"exaone\", \"ExaoneForCausalLM\"),\n    \"FalconForCausalLM\": (\"falcon\", \"FalconForCausalLM\"),\n    \"Fairseq2LlamaForCausalLM\": (\"fairseq2_llama\", \"Fairseq2LlamaForCausalLM\"),\n    \"GemmaForCausalLM\": (\"gemma\", \"GemmaForCausalLM\"),\n    \"Gemma2ForCausalLM\": (\"gemma2\", \"Gemma2ForCausalLM\"),\n    \"Gemma3ForCausalLM\": (\"gemma3\", \"Gemma3ForCausalLM\"),\n    \"GlmForCausalLM\": (\"glm\", \"GlmForCausalLM\"),\n    \"Glm4ForCausalLM\": (\"glm4\", \"Glm4ForCausalLM\"),\n    \"GPT2LMHeadModel\": (\"gpt2\", \"GPT2LMHeadModel\"),\n    \"GPTBigCodeForCausalLM\": (\"gpt_bigcode\", \"GPTBigCodeForCausalLM\"),\n    \"GPTJForCausalLM\": (\"gpt_j\", \"GPTJForCausalLM\"),\n    \"GPTNeoXForCausalLM\": (\"gpt_neox\", \"GPTNeoXForCausalLM\"),\n    \"GraniteForCausalLM\": (\"granite\", \"GraniteForCausalLM\"),\n    \"GraniteMoeForCausalLM\": (\"granitemoe\", \"GraniteMoeForCausalLM\"),\n    \"GraniteMoeHybridForCausalLM\": (\"granitemoehybrid\", \"GraniteMoeHybridForCausalLM\"),   # noqa: E501\n    \"GraniteMoeSharedForCausalLM\": (\"granitemoeshared\", \"GraniteMoeSharedForCausalLM\"),   # noqa: E501\n    \"GritLM\": (\"gritlm\", \"GritLM\"),\n    \"Grok1ModelForCausalLM\": (\"grok1\", \"Grok1ForCausalLM\"),\n    \"InternLMForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"InternLM2ForCausalLM\": (\"internlm2\", \"InternLM2ForCausalLM\"),\n    \"InternLM2VEForCausalLM\": (\"internlm2_ve\", \"InternLM2VEForCausalLM\"),\n    \"InternLM3ForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"JAISLMHeadModel\": (\"jais\", \"JAISLMHeadModel\"),\n    \"JambaForCausalLM\": (\"jamba\", \"JambaForCausalLM\"),\n    \"LlamaForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    # For decapoda-research/llama-*\n    \"LLaMAForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"MambaForCausalLM\": (\"mamba\", \"MambaForCausalLM\"),\n    \"FalconMambaForCausalLM\": (\"mamba\", \"MambaForCausalLM\"),\n    \"Mamba2ForCausalLM\": (\"mamba2\", \"Mamba2ForCausalLM\"),\n    \"MiniCPMForCausalLM\": (\"minicpm\", \"MiniCPMForCausalLM\"),\n    \"MiniCPM3ForCausalLM\": (\"minicpm3\", \"MiniCPM3ForCausalLM\"),\n    \"MistralForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"MixtralForCausalLM\": (\"mixtral\", \"MixtralForCausalLM\"),\n    \"QuantMixtralForCausalLM\": (\"mixtral_quant\", \"MixtralForCausalLM\"),\n    # transformers's mpt class has lower case\n    \"MptForCausalLM\": (\"mpt\", \"MPTForCausalLM\"),\n    \"MPTForCausalLM\": (\"mpt\", \"MPTForCausalLM\"),\n    \"MiMoForCausalLM\": (\"mimo\", \"MiMoForCausalLM\"),\n    \"NemotronForCausalLM\": (\"nemotron\", \"NemotronForCausalLM\"),\n    \"OlmoForCausalLM\": (\"olmo\", \"OlmoForCausalLM\"),\n    \"Olmo2ForCausalLM\": (\"olmo2\", \"Olmo2ForCausalLM\"),\n    \"OlmoeForCausalLM\": (\"olmoe\", \"OlmoeForCausalLM\"),\n    \"OPTForCausalLM\": (\"opt\", \"OPTForCausalLM\"),\n    \"OrionForCausalLM\": (\"orion\", \"OrionForCausalLM\"),\n    \"PersimmonForCausalLM\": (\"persimmon\", \"PersimmonForCausalLM\"),\n    \"PhiForCausalLM\": (\"phi\", \"PhiForCausalLM\"),\n    \"Phi3ForCausalLM\": (\"phi3\", \"Phi3ForCausalLM\"),\n    \"Phi3SmallForCausalLM\": (\"phi3_small\", \"Phi3SmallForCausalLM\"),\n    \"PhiMoEForCausalLM\": (\"phimoe\", \"PhiMoEForCausalLM\"),\n    \"Plamo2ForCausalLM\": (\"plamo2\", \"Plamo2ForCausalLM\"),\n    \"QWenLMHeadModel\": (\"qwen\", \"QWenLMHeadModel\"),\n    \"Qwen2ForCausalLM\": (\"qwen2\", \"Qwen2ForCausalLM\"),\n    \"Qwen2MoeForCausalLM\": (\"qwen2_moe\", \"Qwen2MoeForCausalLM\"),\n    \"Qwen3ForCausalLM\": (\"qwen3\", \"Qwen3ForCausalLM\"),\n    \"Qwen3MoeForCausalLM\": (\"qwen3_moe\", \"Qwen3MoeForCausalLM\"),\n    \"RWForCausalLM\": (\"falcon\", \"FalconForCausalLM\"),\n    \"StableLMEpochForCausalLM\": (\"stablelm\", \"StablelmForCausalLM\"),\n    \"StableLmForCausalLM\": (\"stablelm\", \"StablelmForCausalLM\"),\n    \"Starcoder2ForCausalLM\": (\"starcoder2\", \"Starcoder2ForCausalLM\"),\n    \"SolarForCausalLM\": (\"solar\", \"SolarForCausalLM\"),\n    \"TeleChat2ForCausalLM\": (\"telechat2\", \"TeleChat2ForCausalLM\"),\n    \"TeleFLMForCausalLM\": (\"teleflm\", \"TeleFLMForCausalLM\"),\n    \"XverseForCausalLM\": (\"llama\", \"LlamaForCausalLM\"),\n    \"Zamba2ForCausalLM\": (\"zamba2\", \"Zamba2ForCausalLM\"),\n    # [Encoder-decoder]\n    \"BartModel\": (\"bart\", \"BartForConditionalGeneration\"),\n    \"BartForConditionalGeneration\": (\"bart\", \"BartForConditionalGeneration\"),\n}\n\n_EMBEDDING_MODELS = {\n    # [Text-only]\n    \"BertModel\": (\"bert\", \"BertEmbeddingModel\"),\n    \"DeciLMForCausalLM\": (\"nemotron_nas\", \"DeciLMForCausalLM\"),\n    \"Gemma2Model\": (\"gemma2\", \"Gemma2ForCausalLM\"),\n    \"GlmForCausalLM\": (\"glm\", \"GlmForCausalLM\"),\n    \"GritLM\": (\"gritlm\", \"GritLM\"),\n    \"GteModel\": (\"bert_with_rope\", \"SnowflakeGteNewModel\"),\n    \"GteNewModel\": (\"bert_with_rope\", \"GteNewModel\"),\n    \"InternLM2ForRewardModel\": (\"internlm2\", \"InternLM2ForRewardModel\"),\n    \"JambaForSequenceClassification\": (\"jamba\", \"JambaForSequenceClassification\"),  # noqa: E501\n    \"LlamaModel\": (\"llama\", \"LlamaForCausalLM\"),\n    **{\n        # Multiple models share the same architecture, so we include them all\n        k: (mod, arch) for k, (mod, arch) in _TEXT_GENERATION_MODELS.items()\n        if arch == \"LlamaForCausalLM\"\n    },\n    \"MistralModel\": (\"llama\", \"LlamaForCausalLM\"),\n    \"ModernBertModel\": (\"modernbert\", \"ModernBertModel\"),\n    \"NomicBertModel\": (\"bert_with_rope\", \"NomicBertModel\"),\n    \"Phi3ForCausalLM\": (\"phi3\", \"Phi3ForCausalLM\"),\n    \"Qwen2Model\": (\"qwen2\", \"Qwen2EmbeddingModel\"),\n    \"Qwen2ForCausalLM\": (\"qwen2\", \"Qwen2ForCausalLM\"),\n    \"Qwen2ForRewardModel\": (\"qwen2_rm\", \"Qwen2ForRewardModel\"),\n    \"Qwen2ForProcessRewardModel\": (\"qwen2_rm\", \"Qwen2ForProcessRewardModel\"),\n    \"RobertaForMaskedLM\": (\"roberta\", \"RobertaEmbeddingModel\"),\n    \"RobertaModel\": (\"roberta\", \"RobertaEmbeddingModel\"),\n    \"TeleChat2ForCausalLM\": (\"telechat2\", \"TeleChat2ForCausalLM\"),\n    \"XLMRobertaModel\": (\"roberta\", \"RobertaEmbeddingModel\"),\n    # [Multimodal]\n    \"LlavaNextForConditionalGeneration\": (\"llava_next\", \"LlavaNextForConditionalGeneration\"),  # noqa: E501\n    \"Phi3VForCausalLM\": (\"phi3v\", \"Phi3VForCausalLM\"),\n    \"Qwen2VLForConditionalGeneration\": (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),  # noqa: E501\n    # [Auto-converted (see adapters.py)]\n    \"Qwen2ForSequenceClassification\": (\"qwen2\", \"Qwen2ForCausalLM\"),\n    # Technically PrithviGeoSpatialMAE is a model that works on images, both in\n    # input and output. I am adding it here because it piggy-backs on embedding\n    # models for the time being.\n    \"PrithviGeoSpatialMAE\": (\"prithvi_geospatial_mae\", \"PrithviGeoSpatialMAE\"),\n}\n\n_CROSS_ENCODER_MODELS = {\n    \"BertForSequenceClassification\": (\"bert\", \"BertForSequenceClassification\"),\n    \"RobertaForSequenceClassification\": (\"roberta\",\n                                         \"RobertaForSequenceClassification\"),\n    \"XLMRobertaForSequenceClassification\": (\"roberta\",\n                                            \"RobertaForSequenceClassification\"),\n    \"ModernBertForSequenceClassification\": (\"modernbert\",\n                                            \"ModernBertForSequenceClassification\"),\n}\n\n_MULTIMODAL_MODELS = {\n    # [Decoder-only]\n    \"AriaForConditionalGeneration\": (\"aria\", \"AriaForConditionalGeneration\"),\n    \"AyaVisionForConditionalGeneration\": (\"aya_vision\", \"AyaVisionForConditionalGeneration\"),  # noqa: E501\n    \"Blip2ForConditionalGeneration\": (\"blip2\", \"Blip2ForConditionalGeneration\"),\n    \"ChameleonForConditionalGeneration\": (\"chameleon\", \"ChameleonForConditionalGeneration\"),  # noqa: E501\n    \"DeepseekVLV2ForCausalLM\": (\"deepseek_vl2\", \"DeepseekVLV2ForCausalLM\"),\n    \"FuyuForCausalLM\": (\"fuyu\", \"FuyuForCausalLM\"),\n    \"Gemma3ForConditionalGeneration\": (\"gemma3_mm\", \"Gemma3ForConditionalGeneration\"),  # noqa: E501\n    \"GLM4VForCausalLM\": (\"glm4v\", \"GLM4VForCausalLM\"),\n    \"GraniteSpeechForConditionalGeneration\": (\"granite_speech\", \"GraniteSpeechForConditionalGeneration\"),  # noqa: E501\n    \"H2OVLChatModel\": (\"h2ovl\", \"H2OVLChatModel\"),\n    \"InternVLChatModel\": (\"internvl\", \"InternVLChatModel\"),\n    \"Idefics3ForConditionalGeneration\":(\"idefics3\",\"Idefics3ForConditionalGeneration\"),\n    \"SmolVLMForConditionalGeneration\": (\"smolvlm\",\"SmolVLMForConditionalGeneration\"),  # noqa: E501\n    \"KimiVLForConditionalGeneration\": (\"kimi_vl\", \"KimiVLForConditionalGeneration\"),  # noqa: E501\n    \"LlavaForConditionalGeneration\": (\"llava\", \"LlavaForConditionalGeneration\"),\n    \"LlavaNextForConditionalGeneration\": (\"llava_next\", \"LlavaNextForConditionalGeneration\"),  # noqa: E501\n    \"LlavaNextVideoForConditionalGeneration\": (\"llava_next_video\", \"LlavaNextVideoForConditionalGeneration\"),  # noqa: E501\n    \"LlavaOnevisionForConditionalGeneration\": (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),  # noqa: E501\n    \"MantisForConditionalGeneration\": (\"llava\", \"MantisForConditionalGeneration\"),  # noqa: E501\n    \"MiniMaxVL01ForConditionalGeneration\": (\"minimax_vl_01\", \"MiniMaxVL01ForConditionalGeneration\"),  # noqa: E501\n    \"MiniCPMO\": (\"minicpmo\", \"MiniCPMO\"),\n    \"MiniCPMV\": (\"minicpmv\", \"MiniCPMV\"),\n    \"Mistral3ForConditionalGeneration\": (\"mistral3\", \"Mistral3ForConditionalGeneration\"),  # noqa: E501\n    \"MolmoForCausalLM\": (\"molmo\", \"MolmoForCausalLM\"),\n    \"NVLM_D\": (\"nvlm_d\", \"NVLM_D_Model\"),\n    \"Ovis\": (\"ovis\", \"Ovis\"),\n    \"PaliGemmaForConditionalGeneration\": (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),  # noqa: E501\n    \"Phi3VForCausalLM\": (\"phi3v\", \"Phi3VForCausalLM\"),\n    \"PixtralForConditionalGeneration\": (\"pixtral\", \"PixtralForConditionalGeneration\"),  # noqa: E501\n    \"QwenVLForConditionalGeneration\": (\"qwen_vl\", \"QwenVLForConditionalGeneration\"),  # noqa: E501\n    \"Qwen2VLForConditionalGeneration\": (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),  # noqa: E501\n    \"Qwen2_5_VLForConditionalGeneration\": (\"qwen2_5_vl\", \"Qwen2_5_VLForConditionalGeneration\"),  # noqa: E501\n    \"Qwen2AudioForConditionalGeneration\": (\"qwen2_audio\", \"Qwen2AudioForConditionalGeneration\"),  # noqa: E501\n    \"Qwen2_5OmniModel\": (\"qwen2_5_omni_thinker\", \"Qwen2_5OmniThinkerForConditionalGeneration\"),  # noqa: E501\n    \"UltravoxModel\": (\"ultravox\", \"UltravoxModel\"),\n    \"Phi4MMForCausalLM\": (\"phi4mm\", \"Phi4MMForCausalLM\"),\n    # [Encoder-decoder]\n    \"Florence2ForConditionalGeneration\": (\"florence2\", \"Florence2ForConditionalGeneration\"),  # noqa: E501\n    \"MllamaForConditionalGeneration\": (\"mllama\", \"MllamaForConditionalGeneration\"),  # noqa: E501\n    \"Llama4ForConditionalGeneration\": (\"mllama4\", \"Llama4ForConditionalGeneration\"),  # noqa: E501\n    \"SkyworkR1VChatModel\": (\"skyworkr1v\", \"SkyworkR1VChatModel\"),\n    \"WhisperForConditionalGeneration\": (\"whisper\", \"WhisperForConditionalGeneration\"),  # noqa: E501\n}\n\n_SPECULATIVE_DECODING_MODELS = {\n    \"MiMoMTPModel\": (\"mimo_mtp\", \"MiMoMTP\"),\n    \"EAGLEModel\": (\"eagle\", \"EAGLE\"),\n    \"EagleLlamaForCausalLM\": (\"llama_eagle\", \"EagleLlamaForCausalLM\"),\n    \"Eagle3LlamaForCausalLM\": (\"llama_eagle3\", \"Eagle3LlamaForCausalLM\"),\n    \"DeepSeekMTPModel\": (\"deepseek_mtp\", \"DeepSeekMTP\"),\n    \"MedusaModel\": (\"medusa\", \"Medusa\"),\n    \"MLPSpeculatorPreTrainedModel\": (\"mlp_speculator\", \"MLPSpeculator\"),\n}\n\n_TRANSFORMERS_MODELS = {\n    \"TransformersForCausalLM\": (\"transformers\", \"TransformersForCausalLM\"),\n}\n# yapf: enable\n\n_VLLM_MODELS = {\n    **_TEXT_GENERATION_MODELS,\n    **_EMBEDDING_MODELS,\n    **_CROSS_ENCODER_MODELS,\n    **_MULTIMODAL_MODELS,\n    **_SPECULATIVE_DECODING_MODELS,\n    **_TRANSFORMERS_MODELS,\n}\n\n# This variable is used as the args for subprocess.run(). We\n# can modify  this variable to alter the args if needed. e.g.\n# when we use par format to pack things together, sys.executable\n# might not be the target we want to run.\n_SUBPROCESS_COMMAND = [\n    sys.executable, \"-m\", \"vllm.model_executor.models.registry\"\n]\n\n\n@dataclass(frozen=True)\nclass _ModelInfo:\n    architecture: str\n    is_text_generation_model: bool\n    is_pooling_model: bool\n    supports_cross_encoding: bool\n    supports_multimodal: bool\n    supports_pp: bool\n    has_inner_state: bool\n    is_attention_free: bool\n    is_hybrid: bool\n    has_noops: bool\n    supports_transcription: bool\n    supports_v0_only: bool\n\n    @staticmethod\n    def from_model_cls(model: type[nn.Module]) -> \"_ModelInfo\":\n        return _ModelInfo(\n            architecture=model.__name__,\n            is_text_generation_model=is_text_generation_model(model),\n            is_pooling_model=True,  # Can convert any model into a pooling model\n            supports_cross_encoding=supports_cross_encoding(model),\n            supports_multimodal=supports_multimodal(model),\n            supports_pp=supports_pp(model),\n            has_inner_state=has_inner_state(model),\n            is_attention_free=is_attention_free(model),\n            is_hybrid=is_hybrid(model),\n            supports_transcription=supports_transcription(model),\n            supports_v0_only=supports_v0_only(model),\n            has_noops=has_noops(model),\n        )\n\n\nclass _BaseRegisteredModel(ABC):\n\n    @abstractmethod\n    def inspect_model_cls(self) -> _ModelInfo:\n        raise NotImplementedError\n\n    @abstractmethod\n    def load_model_cls(self) -> type[nn.Module]:\n        raise NotImplementedError\n\n\n@dataclass(frozen=True)\nclass _RegisteredModel(_BaseRegisteredModel):\n    \"\"\"\n    Represents a model that has already been imported in the main process.\n    \"\"\"\n\n    interfaces: _ModelInfo\n    model_cls: type[nn.Module]\n\n    @staticmethod\n    def from_model_cls(model_cls: type[nn.Module]):\n        return _RegisteredModel(\n            interfaces=_ModelInfo.from_model_cls(model_cls),\n            model_cls=model_cls,\n        )\n\n    def inspect_model_cls(self) -> _ModelInfo:\n        return self.interfaces\n\n    def load_model_cls(self) -> type[nn.Module]:\n        return self.model_cls\n\n\n@dataclass(frozen=True)\nclass _LazyRegisteredModel(_BaseRegisteredModel):\n    \"\"\"\n    Represents a model that has not been imported in the main process.\n    \"\"\"\n    module_name: str\n    class_name: str\n\n    # Performed in another process to avoid initializing CUDA\n    def inspect_model_cls(self) -> _ModelInfo:\n        return _run_in_subprocess(\n            lambda: _ModelInfo.from_model_cls(self.load_model_cls()))\n\n    def load_model_cls(self) -> type[nn.Module]:\n        mod = importlib.import_module(self.module_name)\n        return getattr(mod, self.class_name)\n\n\n@lru_cache(maxsize=128)\ndef _try_load_model_cls(\n    model_arch: str,\n    model: _BaseRegisteredModel,\n) -> Optional[type[nn.Module]]:\n    from vllm.platforms import current_platform\n    current_platform.verify_model_arch(model_arch)\n    try:\n        return model.load_model_cls()\n    except Exception:\n        logger.exception(\"Error in loading model architecture '%s'\",\n                         model_arch)\n        return None\n\n\n@lru_cache(maxsize=128)\ndef _try_inspect_model_cls(\n    model_arch: str,\n    model: _BaseRegisteredModel,\n) -> Optional[_ModelInfo]:\n    try:\n        return model.inspect_model_cls()\n    except Exception:\n        logger.exception(\"Error in inspecting model architecture '%s'\",\n                         model_arch)\n        return None\n\n\n@dataclass\nclass _ModelRegistry:\n    # Keyed by model_arch\n    models: dict[str, _BaseRegisteredModel] = field(default_factory=dict)\n\n    def get_supported_archs(self) -> Set[str]:\n        return self.models.keys()\n\n    def register_model(\n        self,\n        model_arch: str,\n        model_cls: Union[type[nn.Module], str],\n    ) -> None:\n        \"\"\"\n        Register an external model to be used in vLLM.\n\n        `model_cls` can be either:\n\n        - A {class}`torch.nn.Module` class directly referencing the model.\n        - A string in the format `<module>:<class>` which can be used to\n          lazily import the model. This is useful to avoid initializing CUDA\n          when importing the model and thus the related error\n          `RuntimeError: Cannot re-initialize CUDA in forked subprocess`.\n        \"\"\"\n        if not isinstance(model_arch, str):\n            msg = f\"`model_arch` should be a string, not a {type(model_arch)}\"\n            raise TypeError(msg)\n\n        if model_arch in self.models:\n            logger.warning(\n                \"Model architecture %s is already registered, and will be \"\n                \"overwritten by the new model class %s.\", model_arch,\n                model_cls)\n\n        if isinstance(model_cls, str):\n            split_str = model_cls.split(\":\")\n            if len(split_str) != 2:\n                msg = \"Expected a string in the format `<module>:<class>`\"\n                raise ValueError(msg)\n\n            model = _LazyRegisteredModel(*split_str)\n        elif isinstance(model_cls, type) and issubclass(model_cls, nn.Module):\n            model = _RegisteredModel.from_model_cls(model_cls)\n        else:\n            msg = (\"`model_cls` should be a string or PyTorch model class, \"\n                   f\"not a {type(model_arch)}\")\n            raise TypeError(msg)\n\n        self.models[model_arch] = model\n\n    def _raise_for_unsupported(self, architectures: list[str]):\n        all_supported_archs = self.get_supported_archs()\n\n        if any(arch in all_supported_archs for arch in architectures):\n            raise ValueError(\n                f\"Model architectures {architectures} failed \"\n                \"to be inspected. Please check the logs for more details.\")\n\n        raise ValueError(\n            f\"Model architectures {architectures} are not supported for now. \"\n            f\"Supported architectures: {all_supported_archs}\")\n\n    def _try_load_model_cls(self,\n                            model_arch: str) -> Optional[type[nn.Module]]:\n        if model_arch not in self.models:\n            return None\n\n        return _try_load_model_cls(model_arch, self.models[model_arch])\n\n    def _try_inspect_model_cls(self, model_arch: str) -> Optional[_ModelInfo]:\n        if model_arch not in self.models:\n            return None\n\n        return _try_inspect_model_cls(model_arch, self.models[model_arch])\n\n    def _normalize_archs(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> list[str]:\n        if isinstance(architectures, str):\n            architectures = [architectures]\n        if not architectures:\n            logger.warning(\"No model architectures are specified\")\n\n        # filter out support architectures\n        normalized_arch = list(\n            filter(lambda model: model in self.models, architectures))\n\n        # make sure Transformers backend is put at the last as a fallback\n        if len(normalized_arch) != len(architectures):\n            normalized_arch.append(\"TransformersForCausalLM\")\n        return normalized_arch\n\n    def inspect_model_cls(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> tuple[_ModelInfo, str]:\n        architectures = self._normalize_archs(architectures)\n\n        for arch in architectures:\n            model_info = self._try_inspect_model_cls(arch)\n            if model_info is not None:\n                return (model_info, arch)\n\n        return self._raise_for_unsupported(architectures)\n\n    def resolve_model_cls(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> tuple[type[nn.Module], str]:\n        architectures = self._normalize_archs(architectures)\n\n        for arch in architectures:\n            model_cls = self._try_load_model_cls(arch)\n            if model_cls is not None:\n                return (model_cls, arch)\n\n        return self._raise_for_unsupported(architectures)\n\n    def is_text_generation_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.is_text_generation_model\n\n    def is_pooling_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.is_pooling_model\n\n    def is_cross_encoder_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.supports_cross_encoding\n\n    def is_multimodal_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.supports_multimodal\n\n    def is_pp_supported_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.supports_pp\n\n    def model_has_inner_state(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.has_inner_state\n\n    def is_attention_free_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.is_attention_free\n\n    def is_hybrid_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.is_hybrid\n\n    def is_noops_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.has_noops\n\n    def is_transcription_model(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return model_cls.supports_transcription\n\n    def is_v1_compatible(\n        self,\n        architectures: Union[str, list[str]],\n    ) -> bool:\n        model_cls, _ = self.inspect_model_cls(architectures)\n        return not model_cls.supports_v0_only\n\n\nModelRegistry = _ModelRegistry({\n    model_arch:\n    _LazyRegisteredModel(\n        module_name=f\"vllm.model_executor.models.{mod_relname}\",\n        class_name=cls_name,\n    )\n    for model_arch, (mod_relname, cls_name) in _VLLM_MODELS.items()\n})\n\n_T = TypeVar(\"_T\")\n\n\ndef _run_in_subprocess(fn: Callable[[], _T]) -> _T:\n    # NOTE: We use a temporary directory instead of a temporary file to avoid\n    # issues like https://stackoverflow.com/questions/23212435/permission-denied-to-write-to-my-temporary-file\n    with tempfile.TemporaryDirectory() as tempdir:\n        output_filepath = os.path.join(tempdir, \"registry_output.tmp\")\n\n        # `cloudpickle` allows pickling lambda functions directly\n        input_bytes = cloudpickle.dumps((fn, output_filepath))\n\n        # cannot use `sys.executable __file__` here because the script\n        # contains relative imports\n        returned = subprocess.run(_SUBPROCESS_COMMAND,\n                                  input=input_bytes,\n                                  capture_output=True)\n\n        # check if the subprocess is successful\n        try:\n            returned.check_returncode()\n        except Exception as e:\n            # wrap raised exception to provide more information\n            raise RuntimeError(f\"Error raised in subprocess:\\n\"\n                               f\"{returned.stderr.decode()}\") from e\n\n        with open(output_filepath, \"rb\") as f:\n            return pickle.load(f)\n\n\ndef _run() -> None:\n    # Setup plugins\n    from vllm.plugins import load_general_plugins\n    load_general_plugins()\n\n    fn, output_file = pickle.loads(sys.stdin.buffer.read())\n\n    result = fn()\n\n    with open(output_file, \"wb\") as f:\n        f.write(pickle.dumps(result))\n\n\nif __name__ == \"__main__\":\n    _run()\n", 616], "/home/jeromeku/vllm/vllm/config.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport ast\nimport copy\nimport enum\nimport hashlib\nimport inspect\nimport json\nimport re\nimport textwrap\nimport uuid\nimport warnings\nfrom collections import Counter\nfrom contextlib import contextmanager\nfrom dataclasses import (MISSING, Field, asdict, dataclass, field, fields,\n                         is_dataclass, replace)\nfrom functools import cached_property\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Literal, Optional,\n                    Protocol, TypeVar, Union, cast, get_args, get_origin)\n\nimport torch\nfrom torch.distributed import ProcessGroup, ReduceOp\nfrom transformers import PretrainedConfig\nfrom typing_extensions import deprecated\n\nimport vllm.envs as envs\nfrom vllm import version\nfrom vllm.compilation.inductor_pass import CallableInductorPass, InductorPass\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n                                                     QuantizationMethods,\n                                                     get_quantization_config)\nfrom vllm.model_executor.models import ModelRegistry\nfrom vllm.platforms import current_platform\nfrom vllm.tracing import is_otel_available, otel_import_error_traceback\nfrom vllm.transformers_utils.config import (\n    ConfigFormat, get_config, get_hf_image_processor_config,\n    get_hf_text_config, get_pooling_config,\n    get_sentence_transformer_tokenizer_config, is_encoder_decoder,\n    try_get_generation_config, uses_mrope)\nfrom vllm.transformers_utils.s3_utils import S3Model\nfrom vllm.transformers_utils.utils import is_s3, maybe_model_redirect\nfrom vllm.utils import (GiB_bytes, LayerBlockType, cuda_device_count_stateless,\n                        get_cpu_memory, get_open_port, is_torch_equal_or_newer,\n                        random_uuid, resolve_obj_by_qualname)\n\nif TYPE_CHECKING:\n    from _typeshed import DataclassInstance\n    from ray.util.placement_group import PlacementGroup\n\n    from vllm.executor.executor_base import ExecutorBase\n    from vllm.model_executor.layers.quantization.base_config import (\n        QuantizationConfig)\n    from vllm.model_executor.model_loader import BaseModelLoader\n\n    ConfigType = type[DataclassInstance]\nelse:\n    QuantizationConfig = Any\n    ConfigType = type\n\nlogger = init_logger(__name__)\n\nConfigT = TypeVar(\"ConfigT\", bound=ConfigType)\n\n# This value is chosen to have a balance between ITL and TTFT. Note it is\n# not optimized for throughput.\n_DEFAULT_MAX_NUM_BATCHED_TOKENS = 2048\n_POOLING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768\n_MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS = 5120\n\nTaskOption = Literal[\"auto\", \"generate\", \"embedding\", \"embed\", \"classify\",\n                     \"score\", \"reward\", \"transcription\"]\n\n_ResolvedTask = Literal[\"generate\", \"embed\", \"classify\", \"score\", \"reward\",\n                        \"draft\", \"transcription\"]\n\nRunnerType = Literal[\"generate\", \"pooling\", \"draft\", \"transcription\"]\n\n_RUNNER_TASKS: dict[RunnerType, list[_ResolvedTask]] = {\n    \"generate\": [\"generate\"],\n    \"pooling\": [\"embed\", \"classify\", \"score\", \"reward\"],\n    \"draft\": [\"draft\"],\n    \"transcription\": [\"transcription\"],\n}\n\n_TASK_RUNNER: dict[_ResolvedTask, RunnerType] = {\n    task: runner\n    for runner, tasks in _RUNNER_TASKS.items()\n    for task in tasks\n}\n\nHfOverrides = Union[dict[str, Any], Callable[[PretrainedConfig],\n                                             PretrainedConfig]]\n\n\nclass SupportsHash(Protocol):\n\n    def compute_hash(self) -> str:\n        ...\n\n\nclass SupportsMetricsInfo(Protocol):\n\n    def metrics_info(self) -> dict[str, str]:\n        ...\n\n\nclass ModelImpl(str, enum.Enum):\n    AUTO = \"auto\"\n    VLLM = \"vllm\"\n    TRANSFORMERS = \"transformers\"\n\n\ndef get_attr_docs(cls: type[Any]) -> dict[str, str]:\n    \"\"\"\n    Get any docstrings placed after attribute assignments in a class body.\n\n    https://davidism.com/mit-license/\n    \"\"\"\n\n    def pairwise(iterable):\n        \"\"\"\n        Manually implement https://docs.python.org/3/library/itertools.html#itertools.pairwise\n\n        Can be removed when Python 3.9 support is dropped.\n        \"\"\"\n        iterator = iter(iterable)\n        a = next(iterator, None)\n\n        for b in iterator:\n            yield a, b\n            a = b\n\n    cls_node = ast.parse(textwrap.dedent(inspect.getsource(cls))).body[0]\n\n    if not isinstance(cls_node, ast.ClassDef):\n        raise TypeError(\"Given object was not a class.\")\n\n    out = {}\n\n    # Consider each pair of nodes.\n    for a, b in pairwise(cls_node.body):\n        # Must be an assignment then a constant string.\n        if (not isinstance(a, (ast.Assign, ast.AnnAssign))\n                or not isinstance(b, ast.Expr)\n                or not isinstance(b.value, ast.Constant)\n                or not isinstance(b.value.value, str)):\n            continue\n\n        doc = inspect.cleandoc(b.value.value)\n\n        # An assignment can have multiple targets (a = b = v), but an\n        # annotated assignment only has one target.\n        targets = a.targets if isinstance(a, ast.Assign) else [a.target]\n\n        for target in targets:\n            # Must be assigning to a plain name.\n            if not isinstance(target, ast.Name):\n                continue\n\n            out[target.id] = doc\n\n    return out\n\n\ndef config(cls: ConfigT) -> ConfigT:\n    \"\"\"\n    A decorator that ensures all fields in a dataclass have default values\n    and that each field has a docstring.\n\n    If a `ConfigT` is used as a CLI argument itself, the default value provided\n    by `get_kwargs` will be the result parsing a JSON string as the kwargs\n    (i.e. `ConfigT(**json.loads(cli_arg))`). However, if a particular `ConfigT`\n    requires custom construction from CLI (i.e. `CompilationConfig`), it can\n    have a `from_cli` method, which will be called instead.\n    \"\"\"\n    if not is_dataclass(cls):\n        raise TypeError(\"The decorated class must be a dataclass.\")\n    attr_docs = get_attr_docs(cls)\n    for f in fields(cls):\n        if f.init and f.default is MISSING and f.default_factory is MISSING:\n            raise ValueError(\n                f\"Field '{f.name}' in {cls.__name__} must have a default value.\"\n            )\n\n        if f.name not in attr_docs:\n            raise ValueError(\n                f\"Field '{f.name}' in {cls.__name__} must have a docstring.\")\n\n        if get_origin(f.type) is Union:\n            args = get_args(f.type)\n            literal_args = [arg for arg in args if get_origin(arg) is Literal]\n            if len(literal_args) > 1:\n                raise ValueError(\n                    f\"Field '{f.name}' in {cls.__name__} must use a single \"\n                    \"Literal type. Please use 'Literal[Literal1, Literal2]' \"\n                    \"instead of 'Union[Literal1, Literal2]'.\")\n    return cls\n\n\ndef get_field(cls: ConfigType, name: str) -> Field:\n    \"\"\"Get the default factory field of a dataclass by name. Used for getting\n    default factory fields in `EngineArgs`.\"\"\"\n    if not is_dataclass(cls):\n        raise TypeError(\"The given class is not a dataclass.\")\n    cls_fields = {f.name: f for f in fields(cls)}\n    if name not in cls_fields:\n        raise ValueError(f\"Field '{name}' not found in {cls.__name__}.\")\n    named_field: Field = cls_fields[name]\n    if (default_factory := named_field.default_factory) is not MISSING:\n        return field(default_factory=default_factory)\n    if (default := named_field.default) is not MISSING:\n        return field(default=default)\n    raise ValueError(\n        f\"{cls.__name__}.{name} must have a default value or default factory.\")\n\n\ndef is_init_field(cls: ConfigType, name: str) -> bool:\n    return next(f for f in fields(cls) if f.name == name).init\n\n\nTokenizerMode = Literal[\"auto\", \"slow\", \"mistral\", \"custom\"]\nModelDType = Literal[\"auto\", \"half\", \"float16\", \"bfloat16\", \"float\", \"float32\"]\n\n\n@config\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for the model.\"\"\"\n\n    model: str = \"facebook/opt-125m\"\n    \"\"\"Name or path of the Hugging Face model to use. It is also used as the\n    content for `model_name` tag in metrics output when `served_model_name` is\n    not specified.\"\"\"\n    task: Literal[TaskOption, Literal[\"draft\"]] = \"auto\"\n    \"\"\"The task to use the model for. Each vLLM instance only supports one\n    task, even if the same model can be used for multiple tasks. When the model\n    only supports one task, \"auto\" can be used to select it; otherwise, you\n    must specify explicitly which task to use.\"\"\"\n    tokenizer: str = None  # type: ignore\n    \"\"\"Name or path of the Hugging Face tokenizer to use. If unspecified, model\n    name or path will be used.\"\"\"\n    tokenizer_mode: TokenizerMode = \"auto\"\n    \"\"\"Tokenizer mode:\\n\n    - \"auto\" will use the fast tokenizer if available.\\n\n    - \"slow\" will always use the slow tokenizer.\\n\n    - \"mistral\" will always use the tokenizer from `mistral_common`.\\n\n    - \"custom\" will use --tokenizer to select the preregistered tokenizer.\"\"\"\n    trust_remote_code: bool = False\n    \"\"\"Trust remote code (e.g., from HuggingFace) when downloading the model\n    and tokenizer.\"\"\"\n    dtype: Union[ModelDType, torch.dtype] = \"auto\"\n    \"\"\"Data type for model weights and activations:\\n\n    - \"auto\" will use FP16 precision for FP32 and FP16 models, and BF16\n    precision for BF16 models.\\n\n    - \"half\" for FP16. Recommended for AWQ quantization.\\n\n    - \"float16\" is the same as \"half\".\\n\n    - \"bfloat16\" for a balance between precision and range.\\n\n    - \"float\" is shorthand for FP32 precision.\\n\n    - \"float32\" for FP32 precision.\"\"\"\n    seed: Optional[int] = None\n    \"\"\"Random seed for reproducibility. Initialized to None in V0, but\n    initialized to 0 in V1.\"\"\"\n    hf_config_path: Optional[str] = None\n    \"\"\"Name or path of the Hugging Face config to use. If unspecified, model\n    name or path will be used.\"\"\"\n    allowed_local_media_path: str = \"\"\n    \"\"\"Allowing API requests to read local images or videos from directories\n    specified by the server file system. This is a security risk. Should only\n    be enabled in trusted environments.\"\"\"\n    revision: Optional[str] = None\n    \"\"\"The specific model version to use. It can be a branch name, a tag name,\n    or a commit id. If unspecified, will use the default version.\"\"\"\n    code_revision: Optional[str] = None\n    \"\"\"The specific revision to use for the model code on the Hugging Face Hub.\n    It can be a branch name, a tag name, or a commit id. If unspecified, will\n    use the default version.\"\"\"\n    rope_scaling: dict[str, Any] = field(default_factory=dict)\n    \"\"\"RoPE scaling configuration. For example,\n    `{\"rope_type\":\"dynamic\",\"factor\":2.0}`.\"\"\"\n    rope_theta: Optional[float] = None\n    \"\"\"RoPE theta. Use with `rope_scaling`. In some cases, changing the RoPE\n    theta improves the performance of the scaled model.\"\"\"\n    tokenizer_revision: Optional[str] = None\n    \"\"\"The specific revision to use for the tokenizer on the Hugging Face Hub.\n    It can be a branch name, a tag name, or a commit id. If unspecified, will\n    use the default version.\"\"\"\n    max_model_len: int = None  # type: ignore\n    \"\"\"Model context length (prompt and output). If unspecified, will be\n    automatically derived from the model config.\n\n    When passing via `--max-model-len`, supports k/m/g/K/M/G in human-readable\n    format. Examples:\\n\n    - 1k -> 1000\\n\n    - 1K -> 1024\\n\n    - 25.6k -> 25,600\"\"\"\n    spec_target_max_model_len: Optional[int] = None\n    \"\"\"Specify the maximum length for spec decoding draft models.\"\"\"\n    quantization: Optional[QuantizationMethods] = None\n    \"\"\"Method used to quantize the weights. If `None`, we first check the\n    `quantization_config` attribute in the model config file. If that is\n    `None`, we assume the model weights are not quantized and use `dtype` to\n    determine the data type of the weights.\"\"\"\n    enforce_eager: bool = False\n    \"\"\"Whether to always use eager-mode PyTorch. If True, we will disable CUDA\n    graph and always execute the model in eager mode. If False, we will use\n    CUDA graph and eager execution in hybrid for maximal performance and\n    flexibility.\"\"\"\n    max_seq_len_to_capture: int = 8192\n    \"\"\"Maximum sequence len covered by CUDA graphs. When a sequence has context\n    length larger than this, we fall back to eager mode. Additionally for\n    encoder-decoder models, if the sequence length of the encoder input is\n    larger than this, we fall back to the eager mode.\"\"\"\n    max_logprobs: int = 20\n    \"\"\"Maximum number of log probabilities to return when `logprobs` is\n    specified in `SamplingParams`. The default value comes the default for the\n    OpenAI Chat Completions API.\"\"\"\n    disable_sliding_window: bool = False\n    \"\"\"Whether to disable sliding window. If True, we will disable the sliding\n    window functionality of the model, capping to sliding window size. If the\n    model does not support sliding window, this argument is ignored.\"\"\"\n    disable_cascade_attn: bool = False\n    \"\"\"Disable cascade attention for V1. While cascade attention does not\n    change the mathematical correctness, disabling it could be useful for\n    preventing potential numerical issues. Note that even if this is set to\n    False, cascade attention will be only used when the heuristic tells that\n    it's beneficial.\"\"\"\n    skip_tokenizer_init: bool = False\n    \"\"\"Skip initialization of tokenizer and detokenizer. Expects valid\n    `prompt_token_ids` and `None` for prompt from the input. The generated\n    output will contain token ids.\"\"\"\n    enable_prompt_embeds: bool = False\n    \"\"\"If `True`, enables passing text embeddings as inputs via the\n    `prompt_embeds` key. Note that enabling this will double the time required\n    for graph compilation.\"\"\"\n    served_model_name: Optional[Union[str, list[str]]] = None\n    \"\"\"The model name(s) used in the API. If multiple names are provided, the\n    server will respond to any of the provided names. The model name in the\n    model field of a response will be the first name in this list. If not\n    specified, the model name will be the same as the `--model` argument. Noted\n    that this name(s) will also be used in `model_name` tag content of\n    prometheus metrics, if multiple names provided, metrics tag will take the\n    first one.\"\"\"\n    limit_mm_per_prompt: dict[str, int] = field(default_factory=dict)\n    \"\"\"Maximum number of data items per modality per prompt. Only applicable\n    for multimodal models.\"\"\"\n    use_async_output_proc: bool = True\n    \"\"\"Whether to use async output processor.\"\"\"\n    config_format: Union[str, ConfigFormat] = ConfigFormat.AUTO.value\n    \"\"\"The format of the model config to load:\\n\n    - \"auto\" will try to load the config in hf format if available else it\n    will try to load in mistral format.\\n\n    - \"hf\" will load the config in hf format.\\n\n    - \"mistral\" will load the config in mistral format.\"\"\"\n    hf_token: Optional[Union[bool, str]] = None\n    \"\"\"The token to use as HTTP bearer authorization for remote files . If\n    `True`, will use the token generated when running `huggingface-cli login`\n    (stored in `~/.huggingface`).\"\"\"\n    hf_overrides: HfOverrides = field(default_factory=dict)\n    \"\"\"If a dictionary, contains arguments to be forwarded to the Hugging Face\n    config. If a callable, it is called to update the HuggingFace config.\"\"\"\n    mm_processor_kwargs: Optional[dict[str, Any]] = None\n    \"\"\"Arguments to be forwarded to the model's processor for multi-modal data,\n    e.g., image processor. Overrides for the multi-modal processor obtained\n    from `AutoProcessor.from_pretrained`. The available overrides depend on the\n    model that is being run. For example, for Phi-3-Vision: `{\"num_crops\": 4}`.\n    \"\"\"\n    disable_mm_preprocessor_cache: bool = False\n    \"\"\"If `True`, disable caching of the multi-modal preprocessor/mapper (not\n    recommended).\"\"\"\n    override_neuron_config: dict[str, Any] = field(default_factory=dict)\n    \"\"\"Initialize non-default neuron config or override default neuron config\n    that are specific to Neuron devices, this argument will be used to\n    configure the neuron config that can not be gathered from the vllm\n    arguments. e.g. `{\"cast_logits_dtype\": \"bloat16\"}`.\"\"\"\n    pooler_config: Optional[\"PoolerConfig\"] = field(init=False)\n    \"\"\"Pooler config which controls the behaviour of output pooling in pooling\n    models.\"\"\"\n    override_pooler_config: Optional[Union[dict, \"PoolerConfig\"]] = None\n    \"\"\"Initialize non-default pooling config or override default pooling config\n    for the pooling model. e.g. `{\"pooling_type\": \"mean\", \"normalize\": false}`.\n    \"\"\"\n    logits_processor_pattern: Optional[str] = None\n    \"\"\"Optional regex pattern specifying valid logits processor qualified names\n    that can be passed with the `logits_processors` extra completion argument.\n    Defaults to `None`, which allows no processors.\"\"\"\n    generation_config: str = \"auto\"\n    \"\"\"The folder path to the generation config. Defaults to `\"auto\"`, the\n    generation config will be loaded from model path. If set to `\"vllm\"`, no\n    generation config is loaded, vLLM defaults will be used. If set to a folder\n    path, the generation config will be loaded from the specified folder path.\n    If `max_new_tokens` is specified in generation config, then it sets a\n    server-wide limit on the number of output tokens for all requests.\"\"\"\n    override_generation_config: dict[str, Any] = field(default_factory=dict)\n    \"\"\"Overrides or sets generation config. e.g. `{\"temperature\": 0.5}`. If\n    used with `--generation-config auto`, the override parameters will be\n    merged with the default config from the model. If used with\n    `--generation-config vllm`, only the override parameters are used.\"\"\"\n    enable_sleep_mode: bool = False\n    \"\"\"Enable sleep mode for the engine (only cuda platform is supported).\"\"\"\n    model_impl: Union[str, ModelImpl] = ModelImpl.AUTO.value\n    \"\"\"Which implementation of the model to use:\\n\n    - \"auto\" will try to use the vLLM implementation, if it exists, and fall\n    back to the Transformers implementation if no vLLM implementation is\n    available.\\n\n    - \"vllm\" will use the vLLM model implementation.\\n\n    - \"transformers\" will use the Transformers model implementation.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.model)\n        factors.append(self.dtype)\n        factors.append(self.quantization)\n        factors.append(self.revision)\n        factors.append(self.code_revision)\n        factors.append(self.max_model_len)\n        factors.append(self.max_logprobs)\n        factors.append(self.disable_sliding_window)\n        factors.append(self.trust_remote_code)\n        factors.append(self.generation_config)\n        factors.append(self.model_impl)\n        factors.append(self.override_generation_config)\n        factors.append(self.rope_scaling)\n        factors.append(self.rope_theta)\n        # hf_config can control how the model looks!\n        factors.append(self.hf_config.to_json_string())\n        str_factors = str(factors)\n        assert_hashable(str_factors)\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __post_init__(self) -> None:\n        # Set the default seed to 0 in V1.\n        # NOTE(woosuk): In V0, we set the default seed to None because the\n        # driver worker shares the same process as the user process, and thus\n        # setting a seed affects the user process as well.\n        # In V1, we use separate processes for workers (unless\n        # VLLM_ENABLE_V1_MULTIPROCESSING=0), so setting a seed here\n        # doesn't affect the user process. However, without a consistent seed,\n        # different tensor parallel workers would sample different tokens,\n        # leading to inconsistent results.\n        if envs.VLLM_USE_V1 and self.seed is None:\n            self.seed = 0\n            if not envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n                logger.warning(\n                    \"The global random seed is set to %d. Since \"\n                    \"VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may \"\n                    \"affect the random state of the Python process that \"\n                    \"launched vLLM.\", self.seed)\n\n        self.model = maybe_model_redirect(self.model)\n        # The tokenizer is consistent with the model by default.\n        if self.tokenizer is None:\n            self.tokenizer = self.model\n        if self.tokenizer_revision is None:\n            self.tokenizer_revision = self.revision\n        self.tokenizer = maybe_model_redirect(self.tokenizer)\n\n        if isinstance(self.hf_config_path, str):\n            self.hf_config_path = maybe_model_redirect(self.hf_config_path)\n\n        if callable(self.hf_overrides):\n            hf_overrides_kw = {}\n            hf_overrides_fn = self.hf_overrides\n        else:\n            hf_overrides_kw = self.hf_overrides\n            hf_overrides_fn = None\n\n        if self.rope_scaling:\n            hf_override: dict[str, Any] = {\"rope_scaling\": self.rope_scaling}\n            hf_overrides_kw.update(hf_override)\n            hf_overrides_str = json.dumps(hf_overrides_kw)\n            msg = (\n                \"`--rope-scaling` will be removed in a future release. \"\n                f\"'Please instead use `--hf-overrides '{hf_overrides_str}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n        if self.rope_theta is not None:\n            hf_override = {\"rope_theta\": self.rope_theta}\n            hf_overrides_kw.update(hf_override)\n            hf_overrides_str = json.dumps(hf_overrides_kw)\n            msg = (\n                \"`--rope-theta` will be removed in a future release. \"\n                f\"'Please instead use `--hf-overrides '{hf_overrides_str}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n\n        self.maybe_pull_model_tokenizer_for_s3(self.model, self.tokenizer)\n\n        if (backend := envs.VLLM_ATTENTION_BACKEND\n            ) and backend == \"FLASHINFER\" and find_spec(\"flashinfer\") is None:\n            raise ValueError(\n                \"VLLM_ATTENTION_BACKEND is set to FLASHINFER, but flashinfer \"\n                \"module was not found. See \"\n                \"https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile \"  # noqa: E501\n                \"for instructions on how to install it.\")\n\n        from vllm.platforms import current_platform\n\n        if (self.enable_sleep_mode\n                and not current_platform.is_sleep_mode_available()):\n            raise ValueError(\n                \"Sleep mode is not supported on current platform.\")\n\n        if isinstance(self.config_format, str):\n            self.config_format = ConfigFormat(self.config_format)\n\n        hf_config = get_config(self.hf_config_path or self.model,\n                               self.trust_remote_code, self.revision,\n                               self.code_revision, self.config_format)\n\n        if hf_overrides_kw:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_kw)\n            hf_config.update(hf_overrides_kw)\n        if hf_overrides_fn:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_fn)\n            hf_config = hf_overrides_fn(hf_config)\n\n        self.hf_config = hf_config\n\n        self.hf_text_config = get_hf_text_config(self.hf_config)\n        self.attention_chunk_size = getattr(self.hf_text_config,\n                                            \"attention_chunk_size\", None)\n        self.encoder_config = self._get_encoder_config()\n        self.hf_image_processor_config = get_hf_image_processor_config(\n            self.model, hf_token=self.hf_token, revision=self.revision)\n        self.dtype = _get_and_verify_dtype(self.hf_config, self.dtype)\n\n        interleaved_attn_models = [\"gemma2\", \"gemma3_text\", \"cohere2\"]\n        sliding_window = getattr(self.hf_text_config, \"sliding_window\", None)\n        has_interleaved_attention = (sliding_window is not None) and (\n            isinstance(sliding_window, list) or\n            (self.hf_text_config.model_type in interleaved_attn_models))\n\n        if (not self.disable_sliding_window and has_interleaved_attention):\n            if (backend :=\n                    envs.VLLM_ATTENTION_BACKEND) in (\"XFORMERS\", \"FLASHINFER\"):\n                sliding_window_len_min = get_min_sliding_window(\n                    self.hf_text_config.sliding_window)\n\n                logger.warning_once(\n                    \"%s has interleaved attention, which is currently not supported by the %s backend. Disabling sliding window and capping the max length to the sliding window size (%d).\",  # noqa: E501\n                    self.hf_text_config.model_type,\n                    backend,\n                    sliding_window_len_min,\n                )\n                self.disable_sliding_window = True\n            else:\n                # for a model with interleaved attention,\n                # the scheduler and the model treat it as full attention\n                # (i.e., not dropping any tokens outside the window).\n                # only the attention layer itself is aware of the sliding\n                # window, and use the window size to compute the attention.\n                self.hf_text_config.interleaved_sliding_window = sliding_window\n                delattr(self.hf_text_config, \"sliding_window\")\n                sliding_window = None\n\n        self.max_model_len = _get_and_verify_max_len(\n            hf_config=self.hf_text_config,\n            max_model_len=self.max_model_len,\n            disable_sliding_window=self.disable_sliding_window,\n            sliding_window_len=self.get_hf_config_sliding_window(),\n            spec_target_max_model_len=self.spec_target_max_model_len,\n            encoder_config=self.encoder_config)\n        self.served_model_name = get_served_model_name(self.model,\n                                                       self.served_model_name)\n        self.multimodal_config = self._init_multimodal_config()\n        if not self.skip_tokenizer_init:\n            self._verify_tokenizer_mode()\n\n        self.is_attention_free = self._init_attention_free()\n        self.is_hybrid = self._init_is_hybrid()\n        self.has_noops = self._init_has_noops()\n        self.has_inner_state = self._init_has_inner_state()\n\n        if (not current_platform.is_neuron() and self.override_neuron_config):\n            raise ValueError(\n                \"`override_neuron_config` is only supported on Neuron.\")\n\n        supported_tasks, task = self._resolve_task(self.task)\n        self.supported_tasks = supported_tasks\n        self.task = task\n        if self.task in (\"draft\", \"generate\"):\n            self.truncation_side = \"left\"\n        else:\n            self.truncation_side = \"right\"\n\n        self.pooler_config = self._init_pooler_config()\n\n        self._verify_quantization()\n        self._verify_cuda_graph()\n        self._verify_bnb_config()\n\n    @property\n    def registry(self):\n        return ModelRegistry\n\n    @property\n    def architectures(self) -> list[str]:\n        return getattr(self.hf_config, \"architectures\", [])\n\n    def maybe_pull_model_tokenizer_for_s3(self, model: str,\n                                          tokenizer: str) -> None:\n        \"\"\"Pull model/tokenizer from S3 to temporary directory when needed.\n        \n        Args:\n            model: Model name or path\n            tokenizer: Tokenizer name or path\n        \"\"\"\n        if not (is_s3(model) or is_s3(tokenizer)):\n            return\n\n        if is_s3(model):\n            s3_model = S3Model()\n            s3_model.pull_files(model,\n                                allow_pattern=[\"*.model\", \"*.py\", \"*.json\"])\n            self.model_weights = model\n            self.model = s3_model.dir\n\n            # If tokenizer is same as model, download to same directory\n            if model == tokenizer:\n                s3_model.pull_files(\n                    model, ignore_pattern=[\"*.pt\", \"*.safetensors\", \"*.bin\"])\n                self.tokenizer = s3_model.dir\n                return\n\n        # Only download tokenizer if needed and not already handled\n        if is_s3(tokenizer):\n            s3_tokenizer = S3Model()\n            s3_tokenizer.pull_files(\n                model, ignore_pattern=[\"*.pt\", \"*.safetensors\", \"*.bin\"])\n            self.tokenizer = s3_tokenizer.dir\n\n    def _init_multimodal_config(self) -> Optional[\"MultiModalConfig\"]:\n        if self.registry.is_multimodal_model(self.architectures):\n            return MultiModalConfig(\n                limit_per_prompt=self.limit_mm_per_prompt,\n                mm_processor_kwargs=self.mm_processor_kwargs,\n                disable_mm_preprocessor_cache=self.\n                disable_mm_preprocessor_cache)\n\n        if self.limit_mm_per_prompt:\n            raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\n                             \"multimodal models.\")\n        if self.mm_processor_kwargs:\n            raise ValueError(\"`mm_processor_kwargs` is only supported for \"\n                             \"multimodal models.\")\n        if self.disable_mm_preprocessor_cache:\n            raise ValueError(\"`disable_mm_preprocessor_cache` is only \"\n                             \"supported for multimodal models.\")\n\n        return None\n\n    def _get_encoder_config(self):\n        return get_sentence_transformer_tokenizer_config(\n            self.model, self.revision)\n\n    def _init_pooler_config(self) -> Optional[\"PoolerConfig\"]:\n\n        if self.runner_type == \"pooling\":\n            if isinstance(self.override_pooler_config, dict):\n                self.override_pooler_config = PoolerConfig(\n                    **self.override_pooler_config)\n\n            pooler_config = self.override_pooler_config or PoolerConfig()\n\n            base_config = get_pooling_config(self.model, self.revision)\n            if base_config is not None:\n                # Only set values that are not overridden by the user\n                for k, v in base_config.items():\n                    if getattr(pooler_config, k) is None:\n                        setattr(pooler_config, k, v)\n\n            if self.is_matryoshka:\n                if pooler_config.normalize is None:\n                    pooler_config.normalize = True\n                elif not pooler_config.normalize:\n                    raise ValueError(\n                        \"`normalize` must be enabled (set to True) \"\n                        \"for models that are compatible with \"\n                        \"Matryoshka Representation.\")\n\n            return pooler_config\n\n        return None\n\n    def _init_attention_free(self) -> bool:\n        return self.registry.is_attention_free_model(self.architectures)\n\n    def _init_is_hybrid(self) -> bool:\n        return self.registry.is_hybrid_model(self.architectures)\n\n    def _init_has_noops(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return self.registry.is_noops_model(architectures)\n\n    def _init_has_inner_state(self) -> bool:\n        return self.registry.model_has_inner_state(self.architectures)\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = cast(TokenizerMode, self.tokenizer_mode.lower())\n        if tokenizer_mode not in get_args(TokenizerMode):\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                f\"one of {get_args(TokenizerMode)}.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _get_preferred_task(\n        self,\n        architectures: list[str],\n        supported_tasks: set[_ResolvedTask],\n    ) -> Optional[_ResolvedTask]:\n        model_id = self.model\n        if get_pooling_config(model_id, self.revision):\n            return \"embed\"\n        if self.registry.is_cross_encoder_model(architectures):\n            return \"score\"\n        if self.registry.is_transcription_model(architectures):\n            return \"transcription\"\n\n        suffix_to_preferred_task: list[tuple[str, _ResolvedTask]] = [\n            # Other models follow this pattern\n            (\"ForCausalLM\", \"generate\"),\n            (\"ForConditionalGeneration\", \"generate\"),\n            (\"ForSequenceClassification\", \"classify\"),\n            (\"ChatModel\", \"generate\"),\n            (\"LMHeadModel\", \"generate\"),\n            (\"EmbeddingModel\", \"embed\"),\n            (\"RewardModel\", \"reward\"),\n        ]\n        _, arch = self.registry.inspect_model_cls(architectures)\n\n        for suffix, pref_task in suffix_to_preferred_task:\n            if arch.endswith(suffix) and pref_task in supported_tasks:\n                return pref_task\n\n        return None\n\n    def _resolve_task(\n        self,\n        task_option: Literal[TaskOption, Literal[\"draft\"]],\n    ) -> tuple[set[_ResolvedTask], _ResolvedTask]:\n        if task_option == \"draft\":\n            return {\"draft\"}, \"draft\"\n\n        registry = self.registry\n        architectures = self.architectures\n\n        runner_support: dict[RunnerType, bool] = {\n            # NOTE: Listed from highest to lowest priority,\n            # in case the model supports multiple of them\n            \"transcription\": registry.is_transcription_model(architectures),\n            \"generate\": registry.is_text_generation_model(architectures),\n            \"pooling\": registry.is_pooling_model(architectures),\n        }\n        supported_runner_types_lst: list[RunnerType] = [\n            runner_type\n            for runner_type, is_supported in runner_support.items()\n            if is_supported\n        ]\n\n        supported_tasks_lst: list[_ResolvedTask] = [\n            task for runner_type in supported_runner_types_lst\n            for task in _RUNNER_TASKS[runner_type]\n        ]\n        supported_tasks = set(supported_tasks_lst)\n\n        if task_option == \"auto\":\n            selected_task = next(iter(supported_tasks_lst))\n\n            if len(supported_tasks_lst) > 1:\n                preferred_task = self._get_preferred_task(\n                    architectures, supported_tasks)\n                if preferred_task is not None:\n                    selected_task = preferred_task\n\n                logger.info(\n                    \"This model supports multiple tasks: %s. \"\n                    \"Defaulting to '%s'.\", supported_tasks, selected_task)\n        else:\n            # Aliases\n            if task_option == \"embedding\":\n                preferred_task = self._get_preferred_task(\n                    architectures, supported_tasks)\n                if preferred_task != \"embed\":\n                    msg = (\"The 'embedding' task will be restricted to \"\n                           \"embedding models in a future release. Please \"\n                           \"pass `--task classify`, `--task score`, or \"\n                           \"`--task reward` explicitly for other pooling \"\n                           \"models.\")\n                    warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n                task_option = preferred_task or \"embed\"\n\n            if task_option not in supported_tasks:\n                msg = (\n                    f\"This model does not support the '{task_option}' task. \"\n                    f\"Supported tasks: {supported_tasks}\")\n                raise ValueError(msg)\n\n            selected_task = task_option\n\n        return supported_tasks, selected_task\n\n    def _parse_quant_hf_config(self):\n        quant_cfg = getattr(self.hf_config, \"quantization_config\", None)\n        if quant_cfg is None:\n            # compressed-tensors uses a \"compression_config\" key\n            quant_cfg = getattr(self.hf_config, \"compression_config\", None)\n        return quant_cfg\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = QUANTIZATION_METHODS\n        optimized_quantization_methods = [\n            \"fp8\", \"marlin\", \"modelopt\", \"gptq_marlin_24\", \"gptq_marlin\",\n            \"awq_marlin\", \"fbgemm_fp8\", \"compressed-tensors\", \"experts_int8\",\n            \"quark\", \"nvfp4\", \"bitblas\", \"gptq_bitblas\"\n        ]\n        if self.quantization is not None:\n            self.quantization = cast(QuantizationMethods,\n                                     self.quantization.lower())\n\n        # Parse quantization method from the HF model config, if available.\n        quant_cfg = self._parse_quant_hf_config()\n\n        if quant_cfg is not None:\n            quant_method = quant_cfg.get(\"quant_method\", \"\").lower()\n            quant_method = quant_method.replace(\"compressed_tensors\",\n                                                \"compressed-tensors\")\n            quant_cfg[\"quant_method\"] = quant_method\n\n            # Quantization methods which are overrides (i.e. they have a\n            # `override_quantization_method` method) must be checked in order\n            # of preference (this is particularly important for GPTQ).\n            overrides = [\n                \"marlin\",\n                \"bitblas\",\n                \"gptq_marlin_24\",\n                \"gptq_marlin\",\n                \"gptq_bitblas\",\n                \"awq_marlin\",\n                \"ipex\",\n                \"moe_wna16\",\n            ]\n            quantization_methods = [\n                q for q in supported_quantization if q not in overrides\n            ]\n            # Any custom overrides will be in quantization_methods so we place\n            # them at the start of the list so custom overrides have preference\n            # over the built in ones.\n            quantization_methods = quantization_methods + overrides\n\n            # Detect which checkpoint is it\n            for name in quantization_methods:\n                method = get_quantization_config(name)\n                quantization_override = method.override_quantization_method(\n                    quant_cfg, self.quantization)\n                if quantization_override is not None:\n                    # Raise error if the override is not custom (custom would\n                    # be in QUANTIZATION_METHODS but not QuantizationMethods)\n                    # and hasn't been added to the overrides list.\n                    if (name in get_args(QuantizationMethods)\n                            and name not in overrides):\n                        raise ValueError(\n                            f\"Quantization method {name} is an override but \"\n                            \"is has not been added to the `overrides` list \"\n                            \"above. This is necessary to ensure that the \"\n                            \"overrides are checked in order of preference.\")\n                    quant_method = quantization_override\n                    self.quantization = quantization_override\n                    break\n\n            # Verify quantization configurations.\n            if self.quantization is None:\n                self.quantization = quant_method\n            elif self.quantization != quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            from vllm.platforms import current_platform\n            current_platform.verify_quantization(self.quantization)\n            if self.quantization not in optimized_quantization_methods:\n                logger.warning(\n                    \"%s quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\", self.quantization)\n\n    def _verify_cuda_graph(self) -> None:\n        self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,\n                                          self.max_model_len)\n        # CUDAGraph capture not supported for enc-dec models and mllama on ROCm\n        ROCM_UNSUPPORTED_MODELS = ['mllama']\n        unsupported_rocm = (self.hf_config.model_type\n                            in ROCM_UNSUPPORTED_MODELS\n                            or self.is_encoder_decoder)\n\n        if (unsupported_rocm and not self.enforce_eager\n                and current_platform.is_rocm()):\n            logger.warning(\n                \"CUDA graph is not supported for %s on ROCm yet, fallback \"\n                \"to eager mode.\", self.hf_config.model_type)\n            self.enforce_eager = True\n\n    def _verify_bnb_config(self) -> None:\n        \"\"\"\n        The current version of bitsandbytes (0.45.3) with 8-bit models does not\n        yet support CUDA graph.\n        # TODO Remove this when bitsandbytes supports.\n        \"\"\"\n        is_bitsandbytes = self.quantization == \"bitsandbytes\"\n        has_quantization_config = (getattr(self.hf_config,\n                                           \"quantization_config\", None)\n                                   is not None)\n        is_8bit = (self.hf_config.quantization_config.get(\n            \"load_in_8bit\", False) if has_quantization_config else False)\n        if all([\n                is_bitsandbytes,\n                has_quantization_config,\n                is_8bit,\n                not self.enforce_eager,\n        ]):\n            logger.warning(\n                \"CUDA graph is not supported on BitsAndBytes 8bit yet, \"\n                \"fallback to the eager mode.\")\n\n            self.enforce_eager = True\n\n    def _verify_with_expert_parallelism(self) -> None:\n        num_expert_names = [\n            \"moe_num_experts\",  # Dbrx\n            \"num_experts\",  # Jamba\n            \"n_routed_experts\",  # DeepSeek\n            \"num_local_experts\",  # Mixtral\n        ]\n        num_experts = 0\n        for name in num_expert_names:\n            num_experts = getattr(self.hf_text_config, name, 0)\n            if num_experts > 0:\n                break\n        if num_experts < 1:\n            raise ValueError(\n                \"Number of experts in the model must be greater than 0 \"\n                \"when expert parallelism is enabled.\")\n\n    def verify_dual_chunk_attention_config(\n        self,\n        load_config: \"LoadConfig\",\n    ) -> None:\n        if hasattr(self.hf_config, \"dual_chunk_attention_config\"):\n            # Try loading the sparse attention config\n            from vllm.model_executor.model_loader.weight_utils import (\n                get_sparse_attention_config)\n            sparse_attn_config = get_sparse_attention_config(self, load_config)\n            if sparse_attn_config:\n                self.hf_config.dual_chunk_attention_config[\n                    \"sparse_attention_config\"] = sparse_attn_config\n                if \"sparse_attention_enabled\" not in \\\n                        self.hf_config.dual_chunk_attention_config:\n                    self.hf_config.dual_chunk_attention_config[\n                        \"sparse_attention_enabled\"] = True\n\n    def verify_async_output_proc(self, parallel_config, speculative_config,\n                                 device_config) -> None:\n        if not self.use_async_output_proc:\n            # Nothing to check\n            return\n\n        if parallel_config.pipeline_parallel_size > 1:\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        from vllm.platforms import current_platform\n        if not current_platform.is_async_output_supported(self.enforce_eager):\n            self.use_async_output_proc = False\n            return\n\n        if envs.VLLM_USE_RAY_SPMD_WORKER:\n            self.use_async_output_proc = False\n            return\n\n        # Async postprocessor is not necessary for pooling models\n        # since there is no token generation\n        if self.runner_type == \"pooling\":\n            self.use_async_output_proc = False\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        if speculative_config:\n            self.use_async_output_proc = False\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n\n        if parallel_config.distributed_executor_backend == \"external_launcher\":\n            assert self.seed is not None, (\n                \"Seed must be set when using external launcher backend to \"\n                \"make sure sampling results are the same across workers.\")\n\n        total_num_attention_heads = getattr(self.hf_text_config,\n                                            \"num_attention_heads\", 0)\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        if parallel_config.enable_expert_parallel:\n            self._verify_with_expert_parallelism()\n\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if pipeline_parallel_size > 1:\n            if not self.registry.is_pp_supported_model(self.architectures):\n                raise NotImplementedError(\n                    \"Pipeline parallelism is not supported for this model. \"\n                    \"Supported models implement the `SupportsPP` interface.\")\n\n            if self.use_async_output_proc:\n                self.use_async_output_proc = False\n\n    def get_hf_config_sliding_window(\n            self) -> Union[Optional[int], list[Optional[int]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_text_config, \"use_sliding_window\")\n                and not self.hf_text_config.use_sliding_window):\n            return None\n        return getattr(self.hf_text_config, \"sliding_window\", None)\n\n    def get_sliding_window(self) -> Optional[Union[int, list[Optional[int]]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\n        # If user disables sliding window, return None.\n        if self.disable_sliding_window:\n            return None\n        # Otherwise get the value from the hf config.\n        return self.get_hf_config_sliding_window()\n\n    def get_vocab_size(self) -> int:\n        return self.hf_text_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_text_config.hidden_size\n\n    @property\n    def is_deepseek_mla(self) -> bool:\n        if not hasattr(self.hf_text_config, \"model_type\"):\n            return False\n        elif self.hf_text_config.model_type in \\\n            ('deepseek_v2', 'deepseek_v3', 'deepseek_mtp'):\n            return self.hf_text_config.kv_lora_rank is not None\n        elif self.hf_text_config.model_type == 'eagle':\n            # if the model is an EAGLE module, check for the\n            # underlying architecture\n            return self.hf_text_config.model.model_type in \\\n                    ('deepseek_v2', 'deepseek_v3') \\\n                and self.hf_text_config.kv_lora_rank is not None\n        return False\n\n    def get_head_size(self) -> int:\n        # TODO remove hard code\n        if self.is_deepseek_mla:\n            qk_rope_head_dim = getattr(self.hf_text_config, \"qk_rope_head_dim\",\n                                       0)\n            if self.use_mla:\n                return self.hf_text_config.kv_lora_rank + qk_rope_head_dim\n            else:\n                qk_nope_head_dim = getattr(self.hf_text_config,\n                                           \"qk_nope_head_dim\", 0)\n                if qk_rope_head_dim and qk_nope_head_dim:\n                    return qk_rope_head_dim + qk_nope_head_dim\n\n        if hasattr(self.hf_text_config,\n                   \"model_type\") and (self.hf_text_config.model_type\n                                      == \"zamba2\"):\n            return self.hf_text_config.attention_head_dim\n\n        if self.is_attention_free:\n            return 0\n\n        # NOTE: Some configs may set head_dim=None in the config\n        if getattr(self.hf_text_config, \"head_dim\", None) is not None:\n            return self.hf_text_config.head_dim\n\n        # FIXME(woosuk): This may not be true for all models.\n        return (self.hf_text_config.hidden_size //\n                self.hf_text_config.num_attention_heads)\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_text_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        # For DBRX and MPT\n        if self.hf_config.model_type == \"mpt\":\n            if \"kv_n_heads\" in self.hf_config.attn_config:\n                return self.hf_config.attn_config[\"kv_n_heads\"]\n            return self.hf_config.num_attention_heads\n        if self.hf_config.model_type == \"dbrx\":\n            return getattr(self.hf_config.attn_config, \"kv_n_heads\",\n                           self.hf_config.num_attention_heads)\n\n        if self.hf_config.model_type == \"nemotron-nas\":\n            for block in self.hf_config.block_configs:\n                if not block.attention.no_op:\n                    return self.hf_config.num_attention_heads \\\n                        // block.attention.n_heads_in_group\n\n            raise RuntimeError(\"Couldn't determine number of kv heads\")\n\n        if self.is_attention_free:\n            return 0\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_text_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_text_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        if self.use_mla:\n            # When using MLA during decode it becomes MQA\n            return 1\n\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_attention_heads(self,\n                                parallel_config: \"ParallelConfig\") -> int:\n        num_heads = getattr(self.hf_text_config, \"num_attention_heads\", 0)\n        return num_heads // parallel_config.tensor_parallel_size\n\n    def get_layers_start_end_indices(\n            self, parallel_config: \"ParallelConfig\") -> tuple[int, int]:\n        from vllm.distributed.utils import get_pp_indices\n        if (self.hf_text_config.model_type == \"deepseek_mtp\"\n                or self.hf_config.model_type == \"mimo_mtp\"):\n            total_num_hidden_layers = getattr(self.hf_text_config,\n                                              \"num_nextn_predict_layers\", 0)\n        else:\n            total_num_hidden_layers = getattr(self.hf_text_config,\n                                              \"num_hidden_layers\", 0)\n        # the layout order is: DP x PP x TP\n        pp_rank = (parallel_config.rank // parallel_config.tensor_parallel_size\n                   ) % parallel_config.pipeline_parallel_size\n        pp_size = parallel_config.pipeline_parallel_size\n        start, end = get_pp_indices(total_num_hidden_layers, pp_rank, pp_size)\n        return start, end\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        start, end = self.get_layers_start_end_indices(parallel_config)\n        return end - start\n\n    def get_num_layers_by_block_type(\n        self,\n        parallel_config: \"ParallelConfig\",\n        block_type: LayerBlockType = LayerBlockType.attention,\n    ) -> int:\n        # This function relies on 'layers_block_type' in hf_config,\n        # for w/o this attribute, we will need to have workarounds like so\n        attn_block_type = block_type == LayerBlockType.attention\n        is_transformer = not self.is_hybrid and \\\n                            not self.has_noops and \\\n                            not self.is_attention_free\n        start, end = self.get_layers_start_end_indices(parallel_config)\n\n        if is_transformer:\n            # Handle the basic case first\n            return end - start if attn_block_type else 0\n        elif self.is_attention_free:\n            # Attention free\n            # Note that this code assumes there\n            # is only one type of attention-free block type.\n            return 0 if attn_block_type else end - start\n        elif self.has_noops:\n            block_configs = self.hf_config.block_configs\n            return sum(not bc.attention.no_op\n                       for bc in block_configs[start:end])\n        else:\n            # Hybrid model Jamba\n            layers_block_type_value = getattr(self.hf_config,\n                                              \"layers_block_type\", None)\n            if layers_block_type_value is not None:\n                if hasattr(self.hf_text_config,\n                           \"model_type\") and (self.hf_text_config.model_type\n                                              == \"zamba2\"):\n                    if attn_block_type:\n                        return sum(t == \"hybrid\"\n                                   for t in layers_block_type_value[start:end])\n                    else:\n                        return self.get_num_layers(parallel_config)\n                return sum(t == block_type.value\n                           for t in layers_block_type_value[start:end])\n\n            # Hybrid model Minimax\n            attn_type_list = getattr(self.hf_config, \"attn_type_list\", None)\n            if attn_type_list:\n                return sum(t == 1 for t in attn_type_list[start:end])\n\n            if layers_block_type_value is None and attn_type_list is None:\n                raise ValueError(\n                    \"The model is an hybrid without a\"\n                    \"layers_block_type or an attn_type_list in the hf_config,\"\n                    \"cannot determine the num of \"\n                    f\"{block_type.value} layers\")\n\n            return sum(t == 1 for t in attn_type_list[start:end])\n\n    def get_multimodal_config(self) -> \"MultiModalConfig\":\n        \"\"\"\n        Get the multimodal configuration of the model.\n\n        Raises:\n            ValueError: If the model is not multimodal.\n        \"\"\"\n        if self.multimodal_config is None:\n            raise ValueError(\"The model is not multimodal.\")\n\n        return self.multimodal_config\n\n    def try_get_generation_config(self) -> dict[str, Any]:\n        if self.generation_config in (\"auto\", \"vllm\"):\n            config = try_get_generation_config(\n                self.hf_config_path or self.model,\n                trust_remote_code=self.trust_remote_code,\n                revision=self.revision,\n            )\n        else:\n            config = try_get_generation_config(\n                self.generation_config,\n                trust_remote_code=self.trust_remote_code,\n            )\n\n        if config is None:\n            return {}\n\n        return config.to_diff_dict()\n\n    def get_diff_sampling_param(self) -> dict[str, Any]:\n        \"\"\"\n        This method returns a dictionary containing the parameters\n        that differ from the default sampling parameters. If\n        `generation_config` is `\"vllm\"`, an empty dictionary is returned.\n\n        Returns:\n            dict[str, Any]: A dictionary with the differing sampling\n            parameters, if `generation_config` is `\"vllm\"` an empty dictionary.\n        \"\"\"\n        if self.generation_config == \"vllm\":\n            config = {}\n        else:\n            config = self.try_get_generation_config()\n\n        # Overriding with given generation config\n        config.update(self.override_generation_config)\n\n        available_params = [\n            \"repetition_penalty\",\n            \"temperature\",\n            \"top_k\",\n            \"top_p\",\n            \"min_p\",\n            \"max_new_tokens\",\n        ]\n        if any(p in config for p in available_params):\n            diff_sampling_param = {\n                p: config.get(p)\n                for p in available_params if config.get(p) is not None\n            }\n            # Huggingface definition of max_new_tokens is equivalent\n            # to vLLM's max_tokens\n            if \"max_new_tokens\" in diff_sampling_param:\n                diff_sampling_param[\"max_tokens\"] = diff_sampling_param.pop(\n                    \"max_new_tokens\")\n        else:\n            diff_sampling_param = {}\n\n        if diff_sampling_param:\n            logger.warning_once(\n                \"Default sampling parameters have been overridden by the \"\n                \"model's Hugging Face generation config recommended from the \"\n                \"model creator. If this is not intended, please relaunch \"\n                \"vLLM instance with `--generation-config vllm`.\")\n        return diff_sampling_param\n\n    @property\n    def is_encoder_decoder(self) -> bool:\n        \"\"\"Extract the HF encoder/decoder model flag.\"\"\"\n        return is_encoder_decoder(self.hf_config)\n\n    @property\n    def uses_mrope(self) -> bool:\n        return uses_mrope(self.hf_config)\n\n    @property\n    def is_multimodal_model(self) -> bool:\n        return self.multimodal_config is not None\n\n    @property\n    def is_cross_encoder(self) -> bool:\n        return self.registry.is_cross_encoder_model(self.architectures)\n\n    @property\n    def use_mla(self) -> bool:\n        return self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE\n\n    @property\n    def supported_runner_types(self) -> set[RunnerType]:\n        return {_TASK_RUNNER[task] for task in self.supported_tasks}\n\n    @property\n    def runner_type(self) -> RunnerType:\n        return _TASK_RUNNER[cast(_ResolvedTask, self.task)]\n\n    @property\n    def is_v1_compatible(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_v1_compatible(architectures)\n\n    @property\n    def is_matryoshka(self) -> bool:\n        return (hasattr(self.hf_config, \"matryoshka_dimensions\")\n                or getattr(self.hf_config, \"is_matryoshka\", False))\n\n    @property\n    def matryoshka_dimensions(self):\n        return getattr(self.hf_config, \"matryoshka_dimensions\", None)\n\n\nBlockSize = Literal[1, 8, 16, 32, 64, 128]\nCacheDType = Literal[\"auto\", \"fp8\", \"fp8_e4m3\", \"fp8_e5m2\"]\nPrefixCachingHashAlgo = Literal[\"builtin\", \"sha256\"]\n\n\n@config\n@dataclass\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\"\"\"\n\n    block_size: BlockSize = None  # type: ignore\n    \"\"\"Size of a contiguous cache block in number of tokens. This is ignored on\n    neuron devices and set to `--max-model-len`. On CUDA devices, only block\n    sizes up to 32 are supported. On HPU devices, block size defaults to 128.\n\n    This config has no static default. If left unspecified by the user, it will\n    be set in `Platform.check_and_update_configs()` based on the current\n    platform.\"\"\"\n    gpu_memory_utilization: float = 0.9\n    \"\"\"The fraction of GPU memory to be used for the model executor, which can\n    range from 0 to 1. For example, a value of 0.5 would imply 50% GPU memory\n    utilization. If unspecified, will use the default value of 0.9. This is a\n    per-instance limit, and only applies to the current vLLM instance. It does\n    not matter if you have another vLLM instance running on the same GPU. For\n    example, if you have two vLLM instances running on the same GPU, you can\n    set the GPU memory utilization to 0.5 for each instance.\"\"\"\n    swap_space: float = 4\n    \"\"\"Size of the CPU swap space per GPU (in GiB).\"\"\"\n    cache_dtype: CacheDType = \"auto\"\n    \"\"\"Data type for kv cache storage. If \"auto\", will use model data type.\n    CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports\n    fp8 (=fp8_e4m3).\"\"\"\n    is_attention_free: bool = False\n    \"\"\"Whether the model is attention-free. This is primarily set in\n    `ModelConfig` and that value should be manually duplicated here.\"\"\"\n    num_gpu_blocks_override: Optional[int] = None\n    \"\"\"Number of GPU blocks to use. This overrides the profiled `num_gpu_blocks`\n    if specified. Does nothing if `None`. Used for testing preemption.\"\"\"\n    sliding_window: Optional[int] = None\n    \"\"\"Sliding window size for the KV cache. This is primarily set in\n    `ModelConfig` and that value should be manually duplicated here.\"\"\"\n    enable_prefix_caching: Optional[bool] = None\n    \"\"\"Whether to enable prefix caching. Disabled by default for V0. Enabled by\n    default for V1.\"\"\"\n    prefix_caching_hash_algo: PrefixCachingHashAlgo = \"builtin\"\n    \"\"\"Set the hash algorithm for prefix caching:\\n\n    - \"builtin\" is Python's built-in hash.\\n\n    - \"sha256\" is collision resistant but with certain overheads.\"\"\"\n    cpu_offload_gb: float = 0\n    \"\"\"The space in GiB to offload to CPU, per GPU. Default is 0, which means\n    no offloading. Intuitively, this argument can be seen as a virtual way to\n    increase the GPU memory size. For example, if you have one 24 GB GPU and\n    set this to 10, virtually you can think of it as a 34 GB GPU. Then you can\n    load a 13B model with BF16 weight, which requires at least 26GB GPU memory.\n    Note that this requires fast CPU-GPU interconnect, as part of the model is\n    loaded from CPU memory to GPU memory on the fly in each model forward pass.\n    \"\"\"\n    calculate_kv_scales: bool = False\n    \"\"\"This enables dynamic calculation of `k_scale` and `v_scale` when\n    kv_cache_dtype is fp8. If `False`, the scales will be loaded from the model\n    checkpoint if available. Otherwise, the scales will default to 1.0.\"\"\"\n\n    # Will be set after profiling.\n    num_gpu_blocks: Optional[int] = field(default=None, init=False)\n    \"\"\"The number of blocks to allocate for GPU memory.\"\"\"\n    num_cpu_blocks: Optional[int] = field(default=None, init=False)\n    \"\"\"The number of blocks to allocate for CPU memory.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.cache_dtype)\n        # `cpu_offload_gb` does not use `torch.compile` yet.\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        self.swap_space_bytes = self.swap_space * GiB_bytes\n\n        self._verify_args()\n        self._verify_cache_dtype()\n        self._verify_prefix_caching()\n\n    def metrics_info(self):\n        # convert cache_config to dict(key: str, value: str) for prometheus\n        # metrics info\n        return {key: str(value) for key, value in self.__dict__.items()}\n\n    def _verify_args(self) -> None:\n        if self.cpu_offload_gb < 0:\n            raise ValueError(\"CPU offload space must be non-negative\"\n                             f\", but got {self.cpu_offload_gb}\")\n\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def _verify_cache_dtype(self) -> None:\n        if self.cache_dtype == \"auto\":\n            pass\n        elif self.cache_dtype in get_args(CacheDType):\n            logger.info(\n                \"Using fp8 data type to store kv cache. It reduces the GPU \"\n                \"memory footprint and boosts the performance. \"\n                \"Meanwhile, it may cause accuracy drop without a proper \"\n                \"scaling factor\")\n        else:\n            raise ValueError(f\"Unknown kv cache dtype: {self.cache_dtype}\")\n\n    def _verify_prefix_caching(self) -> None:\n        if not self.enable_prefix_caching:\n            return\n\n        if self.sliding_window is not None and not envs.VLLM_USE_V1:\n            raise NotImplementedError(\n                \"Prefix caching is not supported with sliding window. \"\n                \"Run with --disable-sliding-window to use prefix caching.\")\n\n        if (self.enable_prefix_caching and self.prefix_caching_hash_algo\n                not in get_args(PrefixCachingHashAlgo)):\n            raise ValueError(\n                \"Unknown prefix caching hash algorithm: \"\n                f\"{self.prefix_caching_hash_algo}. Must be one of \"\n                f\"{get_args(PrefixCachingHashAlgo)}.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / GiB_bytes:.2f} GiB out of the \"\n               f\"{total_cpu_memory / GiB_bytes:.2f} GiB total CPU memory \"\n               \"is allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. %s\", msg)\n\n\n@config\n@dataclass\nclass TokenizerPoolConfig:\n    \"\"\"This config is deprecated and will be removed in a future release.\n\n    Passing these parameters will have no effect. Please remove them from your\n    configurations.\n    \"\"\"\n\n    pool_size: int = 0\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Passing this parameter will have no effect. Please remove it from your\n    configurations.\"\"\"\n    pool_type: str = \"ray\"\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Passing this parameter will have no effect. Please remove it from your\n    configurations.\"\"\"\n    extra_config: dict = field(default_factory=dict)\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Passing this parameter will have no effect. Please remove it from your\n    configurations.\"\"\"\n\n    def __post_init__(self) -> None:\n        logger.warning_once(\n            \"TokenizerPoolConfig is deprecated and will be removed in a \"\n            \"future release. Passing this parameter will have no effect. \"\n            \"Please remove it from your configurations.\")\n\n\nclass LoadFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    PT = \"pt\"\n    SAFETENSORS = \"safetensors\"\n    NPCACHE = \"npcache\"\n    DUMMY = \"dummy\"\n    TENSORIZER = \"tensorizer\"\n    SHARDED_STATE = \"sharded_state\"\n    GGUF = \"gguf\"\n    BITSANDBYTES = \"bitsandbytes\"\n    MISTRAL = \"mistral\"\n    RUNAI_STREAMER = \"runai_streamer\"\n    RUNAI_STREAMER_SHARDED = \"runai_streamer_sharded\"\n    FASTSAFETENSORS = \"fastsafetensors\"\n\n\n@config\n@dataclass\nclass LoadConfig:\n    \"\"\"Configuration for loading the model weights.\"\"\"\n\n    load_format: Union[str, LoadFormat,\n                       \"BaseModelLoader\"] = LoadFormat.AUTO.value\n    \"\"\"The format of the model weights to load:\\n\n    - \"auto\" will try to load the weights in the safetensors format and fall\n    back to the pytorch bin format if safetensors format is not available.\\n\n    - \"pt\" will load the weights in the pytorch bin format.\\n\n    - \"safetensors\" will load the weights in the safetensors format.\\n\n    - \"npcache\" will load the weights in pytorch format and store a numpy cache\n    to speed up the loading.\\n\n    - \"dummy\" will initialize the weights with random values, which is mainly\n    for profiling.\\n\n    - \"tensorizer\" will use CoreWeave's tensorizer library for fast weight\n    loading. See the Tensorize vLLM Model script in the Examples section for\n    more information.\\n\n    - \"runai_streamer\" will load the Safetensors weights using Run:ai Model\n    Streamer.\\n\n    - \"bitsandbytes\" will load the weights using bitsandbytes quantization.\\n\n    - \"sharded_state\" will load weights from pre-sharded checkpoint files,\n    supporting efficient loading of tensor-parallel models.\\n\n    - \"gguf\" will load weights from GGUF format files (details specified in\n    https://github.com/ggml-org/ggml/blob/master/docs/gguf.md).\\n\n    - \"mistral\" will load weights from consolidated safetensors files used by\n    Mistral models.\"\"\"\n    download_dir: Optional[str] = None\n    \"\"\"Directory to download and load the weights, default to the default\n    cache directory of Hugging Face.\"\"\"\n    model_loader_extra_config: dict = field(default_factory=dict)\n    \"\"\"Extra config for model loader. This will be passed to the model loader\n    corresponding to the chosen load_format.\"\"\"\n    ignore_patterns: Optional[Union[list[str], str]] = None\n    \"\"\"The list of patterns to ignore when loading the model. Default to\n    \"original/**/*\" to avoid repeated loading of llama's checkpoints.\"\"\"\n    use_tqdm_on_load: bool = True\n    \"\"\"Whether to enable tqdm for showing progress bar when loading model\n    weights.\"\"\"\n    pt_load_map_location: Union[str, dict[str, str]] = \"cpu\"\n    \"\"\"\n    pt_load_map_location: the map location for loading pytorch checkpoint, to\n    support loading checkpoints can only be loaded on certain devices like\n    \"cuda\", this is equivalent to {\"\": \"cuda\"}. Another supported format is\n    mapping from different devices like from GPU 1 to GPU 0:\n    {\"cuda:1\": \"cuda:0\"}. Note that when passed from command line, the strings\n    in dictionary needs to be double quoted for json parsing. For more details,\n    see original doc for `map_location` in https://pytorch.org/docs/stable/generated/torch.load.html\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if isinstance(self.load_format, str):\n            load_format = self.load_format.lower()\n            self.load_format = LoadFormat(load_format)\n\n        if self.ignore_patterns is not None and len(self.ignore_patterns) > 0:\n            logger.info(\n                \"Ignoring the following patterns when downloading weights: %s\",\n                self.ignore_patterns)\n        else:\n            self.ignore_patterns = [\"original/**/*\"]\n\n\nDistributedExecutorBackend = Literal[\"ray\", \"mp\", \"uni\", \"external_launcher\"]\n\n\n@config\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\"\"\"\n\n    pipeline_parallel_size: int = 1\n    \"\"\"Number of pipeline parallel groups.\"\"\"\n    tensor_parallel_size: int = 1\n    \"\"\"Number of tensor parallel groups.\"\"\"\n    data_parallel_size: int = 1\n    \"\"\"Number of data parallel groups. MoE layers will be sharded according to\n    the product of the tensor parallel size and data parallel size.\"\"\"\n    data_parallel_size_local: int = 1\n    \"\"\"Number of local data parallel groups.\"\"\"\n    data_parallel_rank: int = 0\n    \"\"\"Rank of the data parallel group.\"\"\"\n    data_parallel_rank_local: Optional[int] = None\n    \"\"\"Local rank of the data parallel group,\n    set only in SPMD mode.\"\"\"\n    data_parallel_master_ip: str = \"127.0.0.1\"\n    \"\"\"IP of the data parallel master.\"\"\"\n    data_parallel_rpc_port: int = 29550\n    \"\"\"Port for data parallel messaging.\"\"\"\n    data_parallel_master_port: int = 29500\n    \"\"\"Port of the data parallel master.\"\"\"\n    enable_expert_parallel: bool = False\n    \"\"\"Use expert parallelism instead of tensor parallelism for MoE layers.\"\"\"\n    max_parallel_loading_workers: Optional[int] = None\n    \"\"\"Maximum number of parallel loading workers when loading model\n    sequentially in multiple batches. To avoid RAM OOM when using tensor\n    parallel and large models.\"\"\"\n\n    disable_custom_all_reduce: bool = False\n    \"\"\"Disable the custom all-reduce kernel and fall back to NCCL.\"\"\"\n\n    tokenizer_pool_config: Optional[TokenizerPoolConfig] = None\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Please remove it from your configs\"\"\"\n\n    ray_workers_use_nsight: bool = False\n    \"\"\"Whether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\"\"\"\n\n    placement_group: Optional[\"PlacementGroup\"] = None\n    \"\"\"ray distributed model workers placement group.\"\"\"\n\n    distributed_executor_backend: Optional[Union[DistributedExecutorBackend,\n                                                 type[\"ExecutorBase\"]]] = None\n    \"\"\"Backend to use for distributed model\n    workers, either \"ray\" or \"mp\" (multiprocessing). If the product\n    of pipeline_parallel_size and tensor_parallel_size is less than\n    or equal to the number of GPUs available, \"mp\" will be used to\n    keep processing on a single host. Otherwise, this will default\n    to \"ray\" if Ray is installed and fail otherwise. Note that tpu\n    and hpu only support Ray for distributed inference.\"\"\"\n\n    worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use. If \"auto\", the worker class\n    will be determined based on the platform.\"\"\"\n    sd_worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use for speculative decofing.\n    If \"auto\", the worker class will be determined based on the platform.\"\"\"\n    worker_extension_cls: str = \"\"\n    \"\"\"The full name of the worker extension class to use. The worker extension\n    class is dynamically inherited by the worker class. This is used to inject\n    new attributes and methods to the worker class for use in collective_rpc\n    calls.\"\"\"\n\n    world_size: int = field(init=False)\n    \"\"\"world_size is TPxPP, it affects the number of workers we create.\"\"\"\n\n    rank: int = 0\n    \"\"\"Global rank in distributed setup.\"\"\"\n\n    @property\n    def world_size_across_dp(self) -> int:\n        \"\"\"world_size_across_dp is TPxPPxDP, it is the size of the world\n        including data parallelism.\"\"\"\n        return self.world_size * self.data_parallel_size\n\n    def get_next_dp_init_port(self) -> int:\n        \"\"\"\n        We might need to initialize process groups in multiple\n        processes that is related to data parallelism,\n        e.g. both in the worker and in the engine, which\n        can live in different processes. To avoid port conflicts, we\n        increment the port number each time we need to initialize a\n        new process group related to data parallelism.\n        \"\"\"\n        answer = self.data_parallel_master_port\n        self.data_parallel_master_port += 1\n        return answer\n\n    def stateless_init_dp_group(self) -> \"ProcessGroup\":\n        from vllm.distributed.utils import (\n            stateless_init_torch_distributed_process_group)\n\n        # use gloo since the engine process might not have cuda device\n        dp_group = stateless_init_torch_distributed_process_group(\n            self.data_parallel_master_ip,\n            self.get_next_dp_init_port(),\n            self.data_parallel_rank,\n            self.data_parallel_size,\n            backend=\"gloo\")\n\n        return dp_group\n\n    @staticmethod\n    def has_unfinished_dp(dp_group: \"ProcessGroup\",\n                          has_unfinished: bool) -> bool:\n        tensor = torch.tensor([has_unfinished],\n                              dtype=torch.int32,\n                              device=\"cpu\")\n        # dp rank 0: has_unfinished_seqs=True\n        # dp rank 1: has_unfinished_seqs=False\n        # aggregated: has_unfinished_seqs=True\n        # so this is an OR operation, i.e. MAX in integers\n        torch.distributed.all_reduce(tensor, op=ReduceOp.MAX, group=dp_group)\n        aggregated_has_unfinished = bool(tensor.item())\n        return aggregated_has_unfinished\n\n    def compute_hash(self):\n        \"\"\"\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.pipeline_parallel_size)\n        factors.append(self.tensor_parallel_size)\n        factors.append(self.enable_expert_parallel)\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __post_init__(self) -> None:\n        self.world_size = self.pipeline_parallel_size * \\\n            self.tensor_parallel_size\n\n        if self.data_parallel_size_local > self.data_parallel_size:\n            raise ValueError(\n                f\"data_parallel_size_local ({self.data_parallel_size_local}) \"\n                f\"must be <= data_parallel_size ({self.data_parallel_size})\")\n\n        if self.data_parallel_size > 1 or self.data_parallel_size_local == 0:\n            # Data parallel was specified in the engine args.\n            self.data_parallel_master_port = get_open_port()\n        else:\n            # Otherwise fall back to env vars (e.g. for offline SPMD case).\n            self.data_parallel_size = envs.VLLM_DP_SIZE\n            self.data_parallel_rank = envs.VLLM_DP_RANK\n            self.data_parallel_rank_local = envs.VLLM_DP_RANK_LOCAL\n            self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP\n            self.data_parallel_master_port = envs.VLLM_DP_MASTER_PORT\n\n        if self.distributed_executor_backend == \"external_launcher\":\n            import os\n            os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n            logger.info(\"Disabling V1 multiprocessing for external launcher.\")\n\n        ray_only_devices: list[str] = []\n        from vllm.platforms import current_platform\n        if (current_platform.device_type in ray_only_devices\n                and self.world_size > 1):\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            if self.distributed_executor_backend != \"ray\":\n                raise ValueError(\n                    f\"{current_platform.device_type.upper()} backend only \"\n                    \"supports Ray for distributed inference.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.executor import ray_utils\n            backend: DistributedExecutorBackend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if current_platform.is_neuron():\n                # neuron uses single process to control multiple devices\n                backend = \"uni\"\n            elif (current_platform.is_cuda()\n                  and cuda_device_count_stateless() < self.world_size):\n                if not ray_found:\n                    raise ValueError(\"Unable to load Ray which is \"\n                                     \"required for multi-node inference, \"\n                                     \"please install Ray with `pip install \"\n                                     \"ray`.\") from ray_utils.ray_import_err\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.info(\"Defaulting to use %s for distributed inference\",\n                        backend)\n\n        if self.distributed_executor_backend is None and self.world_size == 1:\n            self.distributed_executor_backend = \"uni\"\n\n        self._verify_args()\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and self.distributed_executor_backend.uses_ray)\n\n    def _verify_args(self) -> None:\n        # Lazy import to avoid circular import\n        from vllm.executor.executor_base import ExecutorBase\n        from vllm.platforms import current_platform\n        if self.distributed_executor_backend not in (\n                \"ray\", \"mp\", \"uni\",\n                \"external_launcher\", None) and not (isinstance(\n                    self.distributed_executor_backend, type) and issubclass(\n                        self.distributed_executor_backend, ExecutorBase)):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' 'uni', 'external_launcher' or\"\n                \" custom ExecutorBase subclass.\")\n        if self.use_ray:\n            from vllm.executor import ray_utils\n            ray_utils.assert_ray_available()\n\n        if not current_platform.use_custom_allreduce():\n            self.disable_custom_all_reduce = True\n            logger.info(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on current platform.\")\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\"Unable to use nsight profiling unless workers \"\n                             \"run with Ray.\")\n\n        assert isinstance(self.worker_extension_cls, str), (\n            \"worker_extension_cls must be a string (qualified class name).\")\n\n\nPreemptionMode = Literal[\"swap\", \"recompute\"]\nSchedulerPolicy = Literal[\"fcfs\", \"priority\"]\n\n\n@config\n@dataclass\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\"\"\"\n\n    runner_type: RunnerType = \"generate\"\n    \"\"\"The runner type to launch for the model.\"\"\"\n\n    max_num_batched_tokens: int = None  # type: ignore\n    \"\"\"Maximum number of tokens to be processed in a single iteration.\n\n    This config has no static default. If left unspecified by the user, it will\n    be set in `EngineArgs.create_engine_config` based on the usage context.\"\"\"\n\n    max_num_seqs: int = None  # type: ignore\n    \"\"\"Maximum number of sequences to be processed in a single iteration.\n\n    This config has no static default. If left unspecified by the user, it will\n    be set in `EngineArgs.create_engine_config` based on the usage context.\"\"\"\n\n    max_model_len: int = None  # type: ignore\n    \"\"\"Maximum length of a sequence (including prompt and generated text). This\n    is primarily set in `ModelConfig` and that value should be manually\n    duplicated here.\"\"\"\n\n    max_num_partial_prefills: int = 1\n    \"\"\"For chunked prefill, the maximum number of sequences that can be\n    partially prefilled concurrently.\"\"\"\n\n    max_long_partial_prefills: int = 1\n    \"\"\"For chunked prefill, the maximum number of prompts longer than\n    long_prefill_token_threshold that will be prefilled concurrently. Setting\n    this less than max_num_partial_prefills will allow shorter prompts to jump\n    the queue in front of longer prompts in some cases, improving latency.\"\"\"\n\n    long_prefill_token_threshold: int = 0\n    \"\"\"For chunked prefill, a request is considered long if the prompt is\n    longer than this number of tokens.\"\"\"\n\n    num_lookahead_slots: int = 0\n    \"\"\"The number of slots to allocate per sequence per\n    step, beyond the known token ids. This is used in speculative\n    decoding to store KV activations of tokens which may or may not be\n    accepted.\n\n    NOTE: This will be replaced by speculative config in the future; it is\n    present to enable correctness tests until then.\"\"\"\n\n    cuda_graph_sizes: list[int] = field(default_factory=lambda: [512])\n    \"\"\"Cuda graph capture sizes, default is 512.\n    1. if one value is provided, then the capture list would follow the\n    pattern: [1, 2, 4] + [i for i in range(8, cuda_graph_sizes + 1, 8)]\n    2. more than one value (e.g. 1 2 128) is provided, then the capture list\n    will follow the provided list.\"\"\"\n\n    delay_factor: float = 0.0\n    \"\"\"Apply a delay (of delay factor multiplied by previous\n    prompt latency) before scheduling next prompt.\"\"\"\n\n    enable_chunked_prefill: bool = None  # type: ignore\n    \"\"\"If True, prefill requests can be chunked based\n    on the remaining max_num_batched_tokens.\"\"\"\n\n    is_multimodal_model: bool = False\n    \"\"\"True if the model is multimodal.\"\"\"\n\n    # TODO (ywang96): Make this configurable.\n    max_num_encoder_input_tokens: int = field(init=False)\n    \"\"\"Multimodal encoder compute budget, only used in V1.\n\n    NOTE: This is not currently configurable. It will be overridden by\n    max_num_batched_tokens in case max multimodal embedding size is larger.\"\"\"\n\n    # TODO (ywang96): Make this configurable.\n    encoder_cache_size: int = field(init=False)\n    \"\"\"Multimodal encoder cache size, only used in V1.\n\n    NOTE: This is not currently configurable. It will be overridden by\n    max_num_batched_tokens in case max multimodal embedding size is larger.\"\"\"\n\n    preemption_mode: Optional[PreemptionMode] = None\n    \"\"\"Whether to perform preemption by swapping or\n    recomputation. If not specified, we determine the mode as follows:\n    We use recomputation by default since it incurs lower overhead than\n    swapping. However, when the sequence group has multiple sequences\n    (e.g., beam search), recomputation is not currently supported. In\n    such a case, we use swapping instead.\"\"\"\n\n    num_scheduler_steps: int = 1\n    \"\"\"Maximum number of forward steps per scheduler call.\"\"\"\n\n    multi_step_stream_outputs: bool = True\n    \"\"\"If False, then multi-step will stream outputs at the end of all steps\"\"\"\n\n    send_delta_data: bool = False\n    \"\"\"Private API. If used, scheduler sends delta data to\n    workers instead of an entire data. It should be enabled only\n    when SPMD worker architecture is enabled. I.e.,\n    VLLM_USE_RAY_SPMD_WORKER=1\"\"\"\n\n    policy: SchedulerPolicy = \"fcfs\"\n    \"\"\"The scheduling policy to use:\\n\n    - \"fcfs\" means first come first served, i.e. requests are handled in order\n    of arrival.\\n\n    - \"priority\" means requests are handled based on given priority (lower\n    value means earlier handling) and time of arrival deciding any ties).\"\"\"\n\n    chunked_prefill_enabled: bool = field(init=False)\n    \"\"\"True if chunked prefill is enabled.\"\"\"\n\n    disable_chunked_mm_input: bool = False\n    \"\"\"If set to true and chunked prefill is enabled, we do not want to\n    partially schedule a multimodal item. Only used in V1\n    This ensures that if a request has a mixed prompt\n    (like text tokens TTTT followed by image tokens IIIIIIIIII) where only\n    some image tokens can be scheduled (like TTTTIIIII, leaving IIIII),\n    it will be scheduled as TTTT in one step and IIIIIIIIII in the next.\"\"\"\n\n    # scheduler class or path. \"vllm.core.scheduler.Scheduler\" (default)\n    # or \"mod.custom_class\".\n    scheduler_cls: Union[str, type[object]] = \"vllm.core.scheduler.Scheduler\"\n    \"\"\"The scheduler class to use. \"vllm.core.scheduler.Scheduler\" is the\n    default scheduler. Can be a class directly or the path to a class of form\n    \"mod.custom_class\".\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        if self.max_model_len is None:\n            self.max_model_len = 8192\n\n        if self.max_num_seqs is None:\n            self.max_num_seqs = 128\n\n        if self.max_num_batched_tokens is None:\n            if self.enable_chunked_prefill:\n                if self.num_scheduler_steps > 1:\n                    # Multi-step Chunked-Prefill doesn't allow prompt-chunking\n                    # for now. Have max_num_batched_tokens set to max_model_len\n                    # so we don't reject sequences on account of a short\n                    # max_num_batched_tokens.\n                    self.max_num_batched_tokens = max(\n                        self.max_model_len, _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n                else:\n                    self.max_num_batched_tokens = (\n                        _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n            else:\n                # If max_model_len is too short, use\n                # _DEFAULT_MAX_NUM_BATCHED_TOKENS as the default value\n                # for higher throughput.\n                self.max_num_batched_tokens = max(\n                    self.max_model_len, _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n\n            if self.runner_type == \"pooling\":\n                # Choose specific value for higher throughput\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _POOLING_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n            if self.is_multimodal_model:\n                # The value needs to be at least the number of multimodal tokens\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n\n            # When using default settings,\n            # Ensure max_num_batched_tokens does not exceed model limit.\n            # Some models (e.g., Whisper) have embeddings tied to max length.\n            self.max_num_batched_tokens = min(\n                self.max_num_seqs * self.max_model_len,\n                self.max_num_batched_tokens)\n\n        self.max_num_encoder_input_tokens = self.max_num_batched_tokens\n        self.encoder_cache_size = self.max_num_batched_tokens\n\n        if self.enable_chunked_prefill:\n            logger.info(\n                \"Chunked prefill is enabled with max_num_batched_tokens=%d.\",\n                self.max_num_batched_tokens)\n\n        self.chunked_prefill_enabled = self.enable_chunked_prefill\n        if self.max_num_partial_prefills > 1:\n            if self.long_prefill_token_threshold == 0:\n                self.long_prefill_token_threshold = int(self.max_model_len *\n                                                        0.04)\n\n            logger.info(\n                \"Concurrent partial prefills enabled with \"\n                \"max_num_partial_prefills=%d, max_long_partial_prefills=%d, \"\n                \"long_prefill_token_threshold=%d\",\n                self.max_num_partial_prefills, self.max_long_partial_prefills,\n                self.long_prefill_token_threshold)\n\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if (self.max_num_batched_tokens < self.max_model_len\n                and not self.chunked_prefill_enabled):\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n        if self.max_num_batched_tokens > self.max_num_seqs * self.max_model_len:\n            logger.warning(\n                \"max_num_batched_tokens (%d) exceeds max_num_seqs\"\n                \"* max_model_len (%d). This may lead to unexpected behavior.\",\n                self.max_num_batched_tokens,\n                self.max_num_seqs * self.max_model_len)\n\n        if self.num_lookahead_slots < 0:\n            raise ValueError(\n                \"num_lookahead_slots \"\n                f\"({self.num_lookahead_slots}) must be greater than or \"\n                \"equal to 0.\")\n\n        if self.num_scheduler_steps < 1:\n            raise ValueError(\n                \"num_scheduler_steps \"\n                f\"({self.num_scheduler_steps}) must be greater than or \"\n                \"equal to 1.\")\n\n        if self.max_num_partial_prefills < 1:\n            raise ValueError(\n                f\"max_num_partial_prefills ({self.max_num_partial_prefills}) \"\n                \"must be greater than or equal to 1.\")\n        elif self.max_num_partial_prefills > 1:\n            if not self.chunked_prefill_enabled:\n                raise ValueError(\"Chunked prefill must be enabled to set \"\n                                 \"max_num_partial_prefills > 1.\")\n\n            if self.long_prefill_token_threshold > self.max_model_len:\n                raise ValueError(\n                    \"long_prefill_token_threshold \"\n                    f\"({self.long_prefill_token_threshold}) cannot be greater \"\n                    f\"than the max_model_len ({self.max_model_len}).\")\n\n        if (self.max_long_partial_prefills\n                < 1) or (self.max_long_partial_prefills\n                         > self.max_num_partial_prefills):\n            raise ValueError(\n                f\"max_long_partial_prefills ({self.max_long_partial_prefills}) \"\n                \"must be greater than or equal to 1 and less than or equal to \"\n                f\"max_num_partial_prefills ({self.max_num_partial_prefills}).\")\n\n    @property\n    def is_multi_step(self) -> bool:\n        return self.num_scheduler_steps > 1\n\n\nDevice = Literal[\"auto\", \"cuda\", \"neuron\", \"cpu\", \"tpu\", \"xpu\", \"hpu\"]\n\n\n@config\n@dataclass\nclass DeviceConfig:\n    \"\"\"Configuration for the device to use for vLLM execution.\"\"\"\n\n    device: Union[Device, torch.device] = \"auto\"\n    \"\"\"Device type for vLLM execution.\"\"\"\n    device_type: str = field(init=False)\n    \"\"\"Device type from the current platform. This is set in\n    `__post_init__`.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # the device/platform information will be summarized\n        # by torch/vllm automatically.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if self.device == \"auto\":\n            # Automated device type detection\n            from vllm.platforms import current_platform\n            self.device_type = current_platform.device_type\n            if not self.device_type:\n                raise RuntimeError(\n                    \"Failed to infer device type, please set \"\n                    \"the environment variable `VLLM_LOGGING_LEVEL=DEBUG` \"\n                    \"to turn on verbose logging to help debug the issue.\")\n        else:\n            # Device type is assigned explicitly\n            self.device_type = self.device\n\n        # Some device types require processing inputs on CPU\n        if self.device_type in [\"neuron\"]:\n            self.device = torch.device(\"cpu\")\n        elif self.device_type in [\"tpu\"]:\n            self.device = None\n        else:\n            # Set device with device type\n            self.device = torch.device(self.device_type)\n\n\nSpeculativeMethod = Literal[\"ngram\", \"eagle\", \"medusa\", \"mlp_speculator\",\n                            \"draft_model\"]\nSpeculativeAcceptanceMethod = Literal[\"rejection_sampler\",\n                                      \"typical_acceptance_sampler\"]\n\n\n@config\n@dataclass\nclass SpeculativeConfig:\n    \"\"\"Configuration for speculative decoding.\"\"\"\n\n    # General speculative decoding control\n    num_speculative_tokens: int = field(default=None,\n                                        init=True)  # type: ignore\n    \"\"\"The number of speculative tokens, if provided. It will default to the\n    number in the draft model config if present, otherwise, it is required.\"\"\"\n    model: Optional[str] = None\n    \"\"\"The name of the draft model, eagle head, or additional weights, if\n    provided.\"\"\"\n    method: Optional[SpeculativeMethod] = None\n    \"\"\"The name of the speculative method to use. If users provide and set the\n    `model` param, the speculative method type will be detected automatically\n    if possible, if `model` param is not provided, the method name must be\n    provided.\n\n    If using `ngram` method, the related configuration `prompt_lookup_max` and\n    `prompt_lookup_min` should be considered.\"\"\"\n    acceptance_method: SpeculativeAcceptanceMethod = \"rejection_sampler\"\n    \"\"\"The method to use for accepting draft tokens:\\n\n    - \"rejection_sampler\" maps to `RejectionSampler`.\\n\n    - \"typical_acceptance_sampler\" maps to `TypicalAcceptanceSampler`.\n\n    If using `typical_acceptance_sampler`, the related configuration\n    `posterior_threshold` and `posterior_alpha` should be considered.\"\"\"\n    draft_tensor_parallel_size: Optional[int] = None\n    \"\"\"The degree of the tensor parallelism for the draft model. Can only be 1\n    or the same as the target model's tensor parallel size.\"\"\"\n    disable_logprobs: bool = True\n    \"\"\"If set to True, token log probabilities are not returned during\n    speculative decoding. If set to False, token log probabilities are returned\n    according to the log probability settings in SamplingParams.\"\"\"\n\n    # Draft model configuration\n    quantization: Optional[QuantizationMethods] = None\n    \"\"\"Quantization method that was used to quantize the draft model weights.\n    If `None`, we assume the model weights are not quantized. Note that it only\n    takes effect when using the draft model-based speculative method.\"\"\"\n    max_model_len: Optional[int] = None\n    \"\"\"The maximum model length of the draft model. Used when testing the\n    ability to skip speculation for some sequences.\"\"\"\n    revision: Optional[str] = None\n    \"\"\"The specific model version to use for the draft model. It can be a\n    branch name, a tag name, or a commit id. If unspecified, will use the\n    default version.\"\"\"\n    code_revision: Optional[str] = None\n    \"\"\"The specific revision to use for the draft model code on Hugging Face\n    Hub. It can be a branch name, a tag name, or a commit id. If unspecified,\n    will use the default version.\"\"\"\n\n    # Advanced control\n    disable_mqa_scorer: bool = False\n    \"\"\"Disable the MQA scorer and fall back to batch expansion for scoring\n    proposals.\"\"\"\n    disable_by_batch_size: Optional[int] = None\n    \"\"\"Disable speculative decoding for new incoming requests when the number\n    of enqueued requests is larger than this value, if provided.\"\"\"\n\n    # Ngram proposer configuration\n    prompt_lookup_max: Optional[int] = None\n    \"\"\"Maximum size of ngram token window when using Ngram proposer, required\n    when method is set to ngram.\"\"\"\n    prompt_lookup_min: Optional[int] = None\n    \"\"\"Minimum size of ngram token window when using Ngram proposer, if\n    provided. Defaults to 1.\"\"\"\n\n    # Typical acceptance sampler configuration\n    posterior_threshold: Optional[float] = None\n    \"\"\"A threshold value that sets a lower bound on the posterior probability\n    of a token in the target model for it to be accepted. This threshold is\n    used only when we use the `TypicalAcceptanceSampler` for token acceptance.\n    \"\"\"\n    posterior_alpha: Optional[float] = None\n    \"\"\"Scaling factor for entropy-based threshold, applied when using\n    `TypicalAcceptanceSampler`.\"\"\"\n\n    speculative_token_tree: Optional[str] = None\n    \"\"\"Specifies the tree structure for speculative token generation.\n    \"\"\"\n    # required configuration params passed from engine\n    target_model_config: ModelConfig = field(default=None,\n                                             init=True)  # type: ignore\n    \"\"\"The configuration of the target model.\"\"\"\n    target_parallel_config: ParallelConfig = field(default=None,\n                                                   init=True)  # type: ignore\n    \"\"\"The parallel configuration for the target model.\"\"\"\n    enable_chunked_prefill: bool = field(default=None,\n                                         init=True)  # type: ignore\n    \"\"\"Whether vLLM is configured to use chunked prefill or not. Used for\n    raising an error since it's not yet compatible with speculative decode.\"\"\"\n    disable_log_stats: bool = field(default=None, init=True)  # type: ignore\n    \"\"\"Whether to disable the periodic printing of stage times in speculative\n    decoding.\"\"\"\n\n    # params generated in the post-init stage\n    draft_model_config: ModelConfig = field(default=None,\n                                            init=True)  # type: ignore\n    \"\"\"The configuration of the draft model initialized internal.\"\"\"\n    draft_parallel_config: ParallelConfig = field(default=None,\n                                                  init=True)  # type: ignore\n    \"\"\"The parallel configuration for the draft model initialized internal.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        # Eagle3 affects the computation graph because it returns intermediate\n        # hidden states in addition to the final hidden state.\n        factors.append(self.method == \"eagle3\")\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    @classmethod\n    def from_dict(cls, dict_value: dict) -> \"SpeculativeConfig\":\n        \"\"\"Parse the CLI value for the speculative config.\"\"\"\n        return cls(**dict_value)\n\n    @staticmethod\n    def hf_config_override(hf_config: PretrainedConfig) -> PretrainedConfig:\n        if hf_config.model_type == \"deepseek_v3\":\n            hf_config.model_type = \"deepseek_mtp\"\n        if hf_config.model_type == \"deepseek_mtp\":\n            n_predict = getattr(hf_config, \"num_nextn_predict_layers\", None)\n            hf_config.update({\n                \"n_predict\": n_predict,\n                \"architectures\": [\"DeepSeekMTPModel\"]\n            })\n\n        if hf_config.architectures[0] == \"MiMoForCausalLM\":\n            hf_config.model_type = \"mimo_mtp\"\n            n_predict = getattr(hf_config, \"num_nextn_predict_layers\", None)\n            hf_config.update({\n                \"num_hidden_layers\": 0,\n                \"n_predict\": n_predict,\n                \"architectures\": [\"MiMoMTPModel\"]\n            })\n            return hf_config\n\n        return hf_config\n\n    def __post_init__(self):\n\n        # Note: \"method\" is a new parameter that helps to extend the\n        # configuration of non-model-based proposers, and the \"model\" parameter\n        # will be used to set the draft model, eagle head, or additional weight\n        # when needed. If users do not specify \"method\", the speculative method\n        # will be detected automatically if possible. If the speculative method\n        # can not be detected, it will be considered as the \"draft_model\" by\n        # default.\n\n        if self.model is None and self.num_speculative_tokens is not None:\n            # TODO(Shangming): Refactor mtp configuration logic when supporting\n            # mtp acceleration for more models besides deepseek_v3\n            if self.target_model_config and \\\n                (self.target_model_config.hf_text_config.model_type \\\n                        == \"deepseek_v3\" or\n                    self.target_model_config.hf_text_config.model_type \\\n                        == \"mimo\"):\n                # use the draft model from the same model:\n                self.model = self.target_model_config.model\n            elif self.method in (\"ngram\", \"[ngram]\"):\n                self.model = \"ngram\"\n            else:\n                raise ValueError(\"num_speculative_tokens was provided without \"\n                                 \"speculative model.\")\n\n        # Automatically configure the method for ngram when \"model\" is used\n        # instead of \"method\"\n        if self.method is None and (self.model is not None\n                                    and self.model in (\"ngram\", \"[ngram]\")):\n            self.method = \"ngram\"\n\n        if self.method in (\"ngram\", \"[ngram]\"):\n            # Unified to \"ngram\" internally\n            self.method = \"ngram\"\n            # Set default values if not provided\n            if (self.prompt_lookup_min is None\n                    and self.prompt_lookup_max is None):\n                # TODO(woosuk): Tune these values. They are arbitrarily chosen.\n                self.prompt_lookup_min = 5\n                self.prompt_lookup_max = 5\n            elif self.prompt_lookup_min is None:\n                assert self.prompt_lookup_max is not None\n                self.prompt_lookup_min = self.prompt_lookup_max\n            elif self.prompt_lookup_max is None:\n                assert self.prompt_lookup_min is not None\n                self.prompt_lookup_max = self.prompt_lookup_min\n\n            # Validate values\n            if self.prompt_lookup_min < 1:\n                raise ValueError(\n                    f\"prompt_lookup_min={self.prompt_lookup_min} must be > 0\")\n            if self.prompt_lookup_max < 1:\n                raise ValueError(\n                    f\"prompt_lookup_max={self.prompt_lookup_max} must be > 0\")\n            if self.prompt_lookup_min > self.prompt_lookup_max:\n                raise ValueError(\n                    f\"prompt_lookup_min={self.prompt_lookup_min} must \"\n                    f\"be <= prompt_lookup_max={self.prompt_lookup_max}\")\n\n            # TODO: current we still need extract vocab_size from target model\n            # config, in future, we may try refactor it out, and set\n            # draft related config as None here.\n            self.draft_model_config = self.target_model_config\n            self.draft_parallel_config = self.target_parallel_config\n        else:\n            self.prompt_lookup_max = 0\n            self.prompt_lookup_min = 0\n\n            if self.model is not None:\n                self.draft_model_config = ModelConfig(\n                    model=self.model,\n                    task=\"draft\",\n                    tokenizer=self.target_model_config.tokenizer,\n                    tokenizer_mode=self.target_model_config.tokenizer_mode,\n                    trust_remote_code=self.target_model_config.\n                    trust_remote_code,\n                    allowed_local_media_path=self.target_model_config.\n                    allowed_local_media_path,\n                    dtype=self.target_model_config.dtype,\n                    seed=self.target_model_config.seed,\n                    revision=self.revision,\n                    code_revision=self.code_revision,\n                    tokenizer_revision=self.target_model_config.\n                    tokenizer_revision,\n                    spec_target_max_model_len=self.target_model_config.\n                    max_model_len,\n                    quantization=self.quantization,\n                    enforce_eager=self.target_model_config.enforce_eager,\n                    max_seq_len_to_capture=self.target_model_config.\n                    max_seq_len_to_capture,\n                    max_logprobs=self.target_model_config.max_logprobs,\n                    hf_overrides=SpeculativeConfig.hf_config_override,\n                )\n\n                # Automatically detect the method\n                if self.method in ('eagle', 'eagle3'):\n                    pass\n                elif \"eagle-\" in self.draft_model_config.model.lower() or \\\n                        \"eagle3-\" in self.draft_model_config.model.lower():\n                    self.method = \"eagle\"\n                elif self.draft_model_config.hf_config.model_type == \"medusa\":\n                    self.method = \"medusa\"\n                elif (self.draft_model_config.hf_config.model_type ==\n                      \"mlp_speculator\"):\n                    self.method = \"mlp_speculator\"\n                else:\n                    self.method = \"draft_model\"\n\n                # Replace hf_config for EAGLE draft_model\n                if self.method in (\"eagle\", \"eagle3\"):\n                    if self.enable_chunked_prefill and not envs.VLLM_USE_V1:\n                        raise ValueError(\n                            \"Chunked prefill and EAGLE are not compatible \"\n                            \"when using V0.\")\n\n                    from vllm.platforms import current_platform\n                    from vllm.transformers_utils.configs.eagle import (\n                        EAGLEConfig)\n                    if isinstance(self.draft_model_config.hf_config,\n                                  EAGLEConfig) or current_platform.is_neuron():\n                        pass\n                    else:\n                        eagle_config = EAGLEConfig(\n                            self.draft_model_config.hf_config,\n                            method=self.method)\n                        self.draft_model_config.hf_config = eagle_config\n\n                if (self.num_speculative_tokens is not None\n                        and hasattr(self.draft_model_config.hf_config,\n                                    \"num_lookahead_tokens\")):\n                    self.draft_model_config.hf_config.num_lookahead_tokens = \\\n                    self.num_speculative_tokens\n\n                n_predict = getattr(self.draft_model_config.hf_config,\n                                    \"n_predict\", None)\n                if n_predict is not None:\n                    if self.num_speculative_tokens is None:\n                        # Default to max value defined in draft model config.\n                        self.num_speculative_tokens = n_predict\n                    elif self.num_speculative_tokens > n_predict and \\\n                            self.num_speculative_tokens % n_predict != 0:\n                        # Ensure divisibility for MTP module reuse.\n                        raise ValueError(\n                            f\"num_speculative_tokens:{self.num_speculative_tokens}\"\n                            f\" must be divisible by {n_predict=}\")\n\n                self.draft_tensor_parallel_size = \\\n                    SpeculativeConfig._verify_and_get_draft_tp(\n                        self.target_parallel_config,\n                        self.draft_tensor_parallel_size,\n                        self.draft_model_config.hf_config\n                )\n\n                self.draft_model_config.max_model_len = (\n                    SpeculativeConfig._maybe_override_draft_max_model_len(\n                        self.max_model_len,\n                        self.draft_model_config.max_model_len,\n                        self.target_model_config.max_model_len,\n                    ))\n\n                self.draft_parallel_config = (\n                    SpeculativeConfig.create_draft_parallel_config(\n                        self.target_parallel_config,\n                        self.draft_tensor_parallel_size))\n\n        if self.acceptance_method == \"typical_acceptance_sampler\":\n            if self.posterior_threshold is None:\n                self.posterior_threshold = 0.09\n            if self.posterior_alpha is None:\n                self.posterior_alpha = 0.3\n\n        self._verify_args()\n\n    @staticmethod\n    def _maybe_override_draft_max_model_len(\n        speculative_max_model_len: Optional[int],\n        draft_max_model_len: int,\n        target_max_model_len: int,\n    ) -> int:\n        \"\"\"Determine the max sequence len for the draft model. This is usually\n        the draft_max_model_len, but may be the target_max_model_len if it is\n        less than the draft_max_model_len, or may be speculative_max_model_len\n        if it is specified.\n\n        This is necessary so that sequences do not exceed the capacity of the\n        draft model or the target model.\n\n        speculative_max_model_len is mainly used for testing that sequences can\n        skip speculation.\n        \"\"\"\n\n        if speculative_max_model_len is not None:\n\n            if speculative_max_model_len > draft_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {draft_max_model_len=}\")\n\n            if speculative_max_model_len > target_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {target_max_model_len=}\")\n\n            return speculative_max_model_len\n\n        return min(\n            draft_max_model_len,\n            target_max_model_len,\n        )\n\n    @staticmethod\n    def _verify_and_get_draft_tp(\n            target_parallel_config: ParallelConfig,\n            speculative_draft_tensor_parallel_size: Optional[int],\n            draft_hf_config: PretrainedConfig) -> int:\n        \"\"\"\n        Verifies and adjusts the tensor parallel size for a draft model\n        specified using speculative_draft_tensor_parallel_size.\n        \"\"\"\n        # If speculative_draft_tensor_parallel_size is unset then set it\n        # appropriately else verify that it is set correctly.\n        if speculative_draft_tensor_parallel_size is None:\n            if draft_hf_config.model_type == \"mlp_speculator\":\n                speculative_draft_tensor_parallel_size = 1\n                if target_parallel_config.tensor_parallel_size > 1:\n                    logger.warning(\n                        \"%s cannot currently be run with tp>1; \"\n                        \"setting speculative_draft_tensor_parallel_size=1\",\n                        draft_hf_config.model_type)\n            else:\n                speculative_draft_tensor_parallel_size = \\\n                    target_parallel_config.tensor_parallel_size\n        elif speculative_draft_tensor_parallel_size not in (\n                1, target_parallel_config.tensor_parallel_size):\n            raise ValueError(\n                f\"{speculative_draft_tensor_parallel_size=} cannot be \"\n                f\"other value than 1 or target model tensor_parallel_size\")\n        return speculative_draft_tensor_parallel_size\n\n    @staticmethod\n    def create_draft_parallel_config(\n        target_parallel_config: ParallelConfig,\n        speculative_draft_tensor_parallel_size: int,\n    ) -> ParallelConfig:\n        \"\"\"Create a parallel config for use by the draft worker.\n\n        This is mostly a copy of the target parallel config, except the tp_size.\n        \"\"\"\n        draft_parallel_config = ParallelConfig(\n            pipeline_parallel_size=target_parallel_config.\n            pipeline_parallel_size,\n            tensor_parallel_size=speculative_draft_tensor_parallel_size,\n            distributed_executor_backend=target_parallel_config.\n            distributed_executor_backend,\n            max_parallel_loading_workers=target_parallel_config.\n            max_parallel_loading_workers,\n            disable_custom_all_reduce=target_parallel_config.\n            disable_custom_all_reduce,\n            ray_workers_use_nsight=target_parallel_config.\n            ray_workers_use_nsight,\n            placement_group=target_parallel_config.placement_group,\n        )\n\n        return draft_parallel_config\n\n    def _verify_args(self) -> None:\n        if self.num_speculative_tokens is None:\n            raise ValueError(\n                \"num_speculative_tokens must be provided with \"\n                \"speculative model unless the draft model config contains an \"\n                \"n_predict parameter.\")\n\n        if self.num_speculative_tokens <= 0:\n            raise ValueError(\"Expected num_speculative_tokens to be greater \"\n                             f\"than zero ({self.num_speculative_tokens}).\")\n\n        if self.draft_model_config:\n            self.draft_model_config.verify_with_parallel_config(\n                self.draft_parallel_config)\n            # Validate and set draft token acceptance related settings.\n\n        if self.acceptance_method is None:\n            raise ValueError(\"acceptance_method is not set. \"\n                             \"Expected values are rejection_sampler or \"\n                             \"typical_acceptance_sampler.\")\n\n        if (self.acceptance_method != 'rejection_sampler'\n                and self.acceptance_method != 'typical_acceptance_sampler'):\n            raise ValueError(\n                \"Expected acceptance_method to be either \"\n                \"rejection_sampler or typical_acceptance_sampler. Instead it \"\n                f\"is {self.acceptance_method}\")\n\n        if self.acceptance_method == \"typical_acceptance_sampler\" and (\n            (self.posterior_threshold is not None\n             and self.posterior_threshold < 0) or\n            (self.posterior_alpha is not None and self.posterior_alpha < 0)):\n            raise ValueError(\n                \"Expected the posterior_threshold and posterior_alpha of \"\n                \"typical_acceptance_sampler to be > 0. \"\n                \"Instead found posterior_threshold = \"\n                f\"{self.posterior_threshold} and posterior_alpha = \"\n                f\"{self.posterior_alpha}\")\n\n        if (self.disable_by_batch_size is not None\n                and self.disable_by_batch_size < 2):\n            raise ValueError(\"Expect the batch size threshold of disabling \"\n                             \"speculative decoding is > 1, but got \"\n                             f\"{self.disable_by_batch_size=}\")\n\n        if self.method == \"eagle3\" and self.target_model_config and \\\n            \"llama\" not in self.target_model_config.hf_text_config.model_type:\n            raise ValueError(\n                \"Eagle3 is only supported for Llama models. \"\n                f\"Got {self.target_model_config.hf_text_config.model_type=}\")\n\n    @property\n    def num_lookahead_slots(self) -> int:\n        \"\"\"The number of additional slots the scheduler should allocate per\n        step, in addition to the slots allocated for each known token.\n\n        This is equal to the number of speculative tokens, as each speculative\n        token must be scored.\n        \"\"\"\n        return self.num_speculative_tokens\n\n    def use_eagle(self) -> bool:\n        return self.method in (\"eagle\", \"eagle3\")\n\n    def __repr__(self) -> str:\n        method = self.method\n        model = None if method == \"ngram\" else self.draft_model_config.model\n        num_spec_tokens = self.num_speculative_tokens\n        return f\"SpeculativeConfig({method=}, {model=}, {num_spec_tokens=})\"\n\n\nLoRADType = Literal[\"auto\", \"float16\", \"bfloat16\"]\n\n\n@config\n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for LoRA.\"\"\"\n\n    max_lora_rank: int = 16\n    \"\"\"Max LoRA rank.\"\"\"\n    max_loras: int = 1\n    \"\"\"Max number of LoRAs in a single batch.\"\"\"\n    fully_sharded_loras: bool = False\n    \"\"\"By default, only half of the LoRA computation is sharded with tensor\n    parallelism. Enabling this will use the fully sharded layers. At high\n    sequence length, max rank or tensor parallel size, this is likely faster.\n    \"\"\"\n    max_cpu_loras: Optional[int] = None\n    \"\"\"Maximum number of LoRAs to store in CPU memory. Must be >= than\n    `max_loras`.\"\"\"\n    lora_dtype: Union[torch.dtype, LoRADType] = \"auto\"\n    \"\"\"Data type for LoRA. If auto, will default to base model dtype.\"\"\"\n    lora_extra_vocab_size: int = 256\n    \"\"\"Maximum size of extra vocabulary that can be present in a LoRA adapter\n    (added to the base model vocabulary).\"\"\"\n    lora_vocab_padding_size: ClassVar[int] = current_platform\\\n        .get_lora_vocab_padding_size()\n    long_lora_scaling_factors: Optional[tuple[float, ...]] = None\n    \"\"\"Specify multiple scaling factors (which can be different from base model\n    scaling factor - see eg. Long LoRA) to allow for multiple LoRA adapters\n    trained with those scaling factors to be used at the same time. If not\n    specified, only adapters trained with the base model scaling factor are\n    allowed.\"\"\"\n    bias_enabled: bool = False\n    \"\"\"Enable bias for LoRA adapters.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.max_lora_rank)\n        factors.append(self.max_loras)\n        factors.append(self.fully_sharded_loras)\n        factors.append(self.lora_dtype)\n        factors.append(self.lora_extra_vocab_size)\n        factors.append(self.lora_vocab_padding_size)\n        factors.append(self.long_lora_scaling_factors)\n        factors.append(self.bias_enabled)\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        # Setting the maximum rank to 512 should be able to satisfy the vast\n        # majority of applications.\n        possible_max_ranks = (8, 16, 32, 64, 128, 256, 320, 512)\n        possible_lora_extra_vocab_size = (256, 512)\n        if self.max_lora_rank not in possible_max_ranks:\n            raise ValueError(\n                f\"max_lora_rank ({self.max_lora_rank}) must be one of \"\n                f\"{possible_max_ranks}.\")\n        if self.lora_extra_vocab_size not in possible_lora_extra_vocab_size:\n            raise ValueError(\n                f\"lora_extra_vocab_size ({self.lora_extra_vocab_size}) \"\n                f\"must be one of {possible_lora_extra_vocab_size}.\")\n        if self.max_loras < 1:\n            raise ValueError(f\"max_loras ({self.max_loras}) must be >= 1.\")\n        if self.max_cpu_loras is None:\n            self.max_cpu_loras = self.max_loras\n        elif self.max_cpu_loras < self.max_loras:\n            raise ValueError(\n                f\"max_cpu_loras ({self.max_cpu_loras}) must be >= \"\n                f\"max_loras ({self.max_loras})\")\n\n    def verify_with_cache_config(self, cache_config: CacheConfig):\n        if cache_config.cpu_offload_gb > 0 and not envs.VLLM_USE_V1:\n            raise ValueError(\n                \"V0 LoRA does not support CPU offload, please use V1.\")\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.lora_dtype in (None, \"auto\"):\n            self.lora_dtype = model_config.dtype\n        elif isinstance(self.lora_dtype, str):\n            self.lora_dtype = getattr(torch, self.lora_dtype)\n\n    def verify_lora_support(self):\n        if self.long_lora_scaling_factors is not None and envs.VLLM_USE_V1:\n            raise ValueError(\n                \"V1 LoRA does not support long LoRA, please use V0.\")\n\n\n@config\n@dataclass\nclass PromptAdapterConfig:\n    \"\"\"Configuration for PromptAdapters.\"\"\"\n\n    max_prompt_adapters: int = 1\n    \"\"\"Max number of PromptAdapters in a batch.\"\"\"\n    max_prompt_adapter_token: int = 0\n    \"\"\"Max number of PromptAdapters tokens.\"\"\"\n    max_cpu_prompt_adapters: Optional[int] = None\n    \"\"\"Maximum number of PromptAdapters to store in CPU memory. Must be >= than\n    `max_prompt_adapters`.\"\"\"\n    prompt_adapter_dtype: Union[torch.dtype, str] = \"auto\"\n    \"\"\"Data type for PromptAdapter. If auto, will default to base model dtype.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n\n        if self.max_prompt_adapters < 1:\n            raise ValueError(f\"max_prompt_adapters \"\n                             f\"({self.max_prompt_adapters}) must be >= 1.\")\n        if self.max_prompt_adapter_token == 0:\n            raise ValueError(\"max_prompt_adapter_token must be set.\")\n        if self.max_cpu_prompt_adapters is None:\n            self.max_cpu_prompt_adapters = self.max_prompt_adapters\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.prompt_adapter_dtype == \"auto\":\n            self.prompt_adapter_dtype = model_config.dtype\n        elif isinstance(self.prompt_adapter_dtype, str):\n            self.prompt_adapter_dtype = getattr(torch,\n                                                self.prompt_adapter_dtype)\n\n\n@config\n@dataclass\nclass MultiModalConfig:\n    \"\"\"Controls the behavior of multimodal models.\"\"\"\n\n    limit_per_prompt: dict[str, int] = \\\n        cast(dict[str, int], get_field(ModelConfig, \"limit_mm_per_prompt\"))\n    \"\"\"\n    The maximum number of input items allowed per prompt for each modality.\n    Defaults to 1 (V0) or 999 (V1) for each modality.\n\n    For example, to allow up to 16 images and 2 videos per prompt:\n    `{\"images\": 16, \"videos\": 2}`\n    \"\"\"\n\n    mm_processor_kwargs: Optional[dict[str, object]] = None\n    \"\"\"\n    Overrides for the multi-modal processor obtained from\n    `transformers.AutoProcessor.from_pretrained`.\n\n    The available overrides depend on the model that is being run.\n\n    For example, for Phi-3-Vision:\n    `{\"num_crops\": 4}`.\n    \"\"\"\n\n    disable_mm_preprocessor_cache: bool = False\n    \"\"\"\n    If `True`, disable caching of the processed multi-modal inputs.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def get_limit_per_prompt(self, modality: str) -> int:\n        \"\"\"\n        Get the maximum number of input items allowed per prompt\n        for the given modality.\n        \"\"\"\n        return self.limit_per_prompt.get(\n            modality,\n            999 if envs.VLLM_USE_V1 else 1,\n        )\n\n    # TODO: Add configs to init vision tower or not.\n\n\n@config\n@dataclass\nclass PoolerConfig:\n    \"\"\"Controls the behavior of output pooling in pooling models.\"\"\"\n\n    pooling_type: Optional[str] = None\n    \"\"\"\n    The pooling method of the pooling model. This should be a key in\n    {class}`vllm.model_executor.layers.pooler.PoolingType`.\n    \"\"\"\n\n    normalize: Optional[bool] = None\n    \"\"\"\n    Whether to normalize the pooled outputs. Usually, this should be set to\n    ``True`` for embedding outputs.\n    \"\"\"\n\n    softmax: Optional[bool] = None\n    \"\"\"\n    Whether to apply softmax to the pooled outputs. Usually, this should be set\n    to ``True`` for classification outputs.\n    \"\"\"\n\n    step_tag_id: Optional[int] = None\n    \"\"\"\n    If set, only the score corresponding to the ``step_tag_id`` in the\n    generated sentence should be returned. Otherwise, the scores for all tokens\n    are returned.\n    \"\"\"\n\n    returned_token_ids: Optional[list[int]] = None\n    \"\"\"\n    A list of indices for the vocabulary dimensions to be extracted,\n    such as the token IDs of ``good_token`` and ``bad_token`` in the\n    ``math-shepherd-mistral-7b-prm`` model.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE: list[str] = []  #\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n\n    # Fallbacks for multi-modal models if the root config\n    # does not define torch_dtype\n    if config_dtype is None:\n        config_dtype = getattr(config.get_text_config(), \"torch_dtype\", None)\n    if config_dtype is None and hasattr(config, \"vision_config\"):\n        config_dtype = getattr(config.vision_config, \"torch_dtype\", None)\n\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            # Set default dtype from model config\n            if config_dtype == torch.float32:\n                # Following common practice, we use float16 for float32 models\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n\n            if config.model_type == \"plamo2\":\n                logger.warning(\n                    \"For PLaMo2, we cast models to bfloat16 instead of using \"\n                    \"float16 by default. This is because float16 does not work.\"\n                )\n                torch_dtype = torch.bfloat16\n\n            # Deal with torch dtype fallback for device compatibility.\n            from vllm.platforms import current_platform\n            if torch_dtype not in current_platform.supported_dtypes:\n                device_name = current_platform.get_device_name()\n\n                if ((capability := current_platform.get_device_capability())\n                        is None):\n                    compute_str = \"\"\n                else:\n                    version_str = capability.as_version_str()\n                    compute_str = f\" (with compute capability {version_str})\"\n                fallback_dtype = current_platform.supported_dtypes[0]\n                logger.warning(\n                    \"Your %s device%s doesn't support %s. \" \\\n                    \"Falling back to %s for compatibility.\",\n                    device_name, compute_str, torch_dtype, fallback_dtype\n                    )\n                torch_dtype = fallback_dtype\n\n            if current_platform.is_hpu() and torch_dtype == torch.float16:\n                logger.warning(\n                    \"For HPU, we cast models to bfloat16 instead of \"\n                    \"using float16 by default. Please specify `dtype` if you \"\n                    \"want to use float16.\")\n                torch_dtype = torch.bfloat16\n        elif dtype == \"float16\" and config.model_type == \"plamo2\":\n            logger.warning(\n                \"For PLaMo2, using float16 is unstable and might cause \"\n                \"unexpected behavior. Please use bfloat16 or float32 instead.\")\n            torch_dtype = torch.float16\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            logger.info(\"Upcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            logger.info(\"Downcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(\"Casting %s to %s.\", config_dtype, torch_dtype)\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n    disable_sliding_window: bool,\n    sliding_window_len: Optional[Union[int, list[Optional[int]]]],\n    spec_target_max_model_len: Optional[int] = None,\n    encoder_config: Optional[Any] = None,\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Command-R\n        \"model_max_length\",\n        # Whisper\n        \"max_target_positions\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    # Choose the smallest \"max_length\" from the possible keys.\n    max_len_key = None\n    for key in possible_keys:\n        max_len = getattr(hf_config, key, None)\n        if max_len is not None:\n            max_len_key = key if max_len < derived_max_model_len \\\n                else max_len_key\n            derived_max_model_len = min(derived_max_model_len, max_len)\n    # For Command-R / Cohere, Cohere2 / Aya Vision models\n    if tmp_max_len := getattr(hf_config, \"model_max_length\", None):\n        max_len_key = \"model_max_length\"\n        derived_max_model_len = tmp_max_len\n\n    # If sliding window is manually disabled, max_length should be less\n    # than the sliding window length in the model config.\n    if disable_sliding_window and sliding_window_len is not None:\n\n        sliding_window_len_min = get_min_sliding_window(sliding_window_len)\n        max_len_key = \"sliding_window\" \\\n            if sliding_window_len_min < derived_max_model_len else max_len_key\n        derived_max_model_len = min(derived_max_model_len,\n                                    sliding_window_len_min)\n\n    # If none of the keys were found in the config, use a default and\n    # log a warning.\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        if spec_target_max_model_len is not None:\n            # If this is a speculative draft model, we use the max model len\n            # from the target model.\n            return spec_target_max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            \"%s. Assuming the model's maximum length is %d.\", possible_keys,\n            default_max_len)\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    # NOTE(woosuk): Gemma3's max_model_len (128K) is already scaled by RoPE\n    # scaling, so we skip applying the scaling factor again.\n    if rope_scaling is not None and \"gemma3\" not in hf_config.model_type:\n        # No need to consider \"type\" key because of patch_rope_scaling when\n        # loading HF config\n        rope_type = rope_scaling[\"rope_type\"]\n\n        if rope_type not in (\"su\", \"longrope\", \"llama3\"):\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that supports rope_scaling\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"with rope_scaling. Please raise an issue so we can \"\n                    \"investigate.\")\n\n            # NOTE: rope_type == \"default\" does not define factor\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/modeling_rope_utils.py\n            scaling_factor = rope_scaling.get(\"factor\", 1.0)\n\n            if rope_type == \"yarn\":\n                derived_max_model_len = rope_scaling[\n                    \"original_max_position_embeddings\"]\n            derived_max_model_len *= scaling_factor\n\n    if encoder_config and \"max_seq_length\" in encoder_config:\n        derived_max_model_len = encoder_config[\"max_seq_length\"]\n\n    # If the user specified a max length, make sure it is smaller than the\n    # derived length from the HF model config.\n    if max_model_len is None:\n        max_model_len = int(derived_max_model_len)\n        if current_platform.is_tpu():\n            logger.warning(\n                \"--max-model-len is not specified, \"\n                \"it's currently using model's default length %s, \"\n                \"which might be too large.\"\n                \"Please input with --max-model-len based on your \"\n                \"request input length and output length, to avoid \"\n                \"unnecessary degradation.\", max_model_len)\n    elif max_model_len > derived_max_model_len:\n        # Some models might have a separate key for specifying model_max_length\n        # that will be bigger than derived_max_model_len. We compare user input\n        # with model_max_length and allow this override when it's smaller.\n        model_max_length = getattr(hf_config, \"model_max_length\", None)\n        if model_max_length is not None and max_model_len <= model_max_length:\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that has model_max_length\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"model_max_length in the config. Please raise an issue \"\n                    \"so we can investigate.\")\n        else:\n            msg = (\n                f\"User-specified max_model_len ({max_model_len}) is greater \"\n                f\"than the derived max_model_len ({max_len_key}=\"\n                f\"{derived_max_model_len} or model_max_length=\"\n                f\"{model_max_length} in model's config.json). This may lead \"\n                \"to incorrect model outputs or CUDA errors.\")\n            if envs.VLLM_ALLOW_LONG_MAX_MODEL_LEN:\n                logger.warning(\n                    \"%s Make sure the value is correct and within the \"\n                    \"model context size.\", msg)\n            else:\n                raise ValueError(\n                    f\"{msg} To allow overriding this maximum, set \"\n                    \"the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\")\n    return int(max_model_len)\n\n\ndef get_min_sliding_window(\n        sliding_window: Union[int, list[Optional[int]]]) -> int:\n    if isinstance(sliding_window, list):\n        return min(s for s in sliding_window if s is not None)\n\n    return sliding_window\n\n\ndef get_served_model_name(model: str,\n                          served_model_name: Optional[Union[str, list[str]]]):\n    \"\"\"\n    If the input is a non-empty list, the first model_name in\n    `served_model_name` is taken.\n    If the input is a non-empty string, it is used directly.\n    For cases where the input is either an empty string or an\n    empty list, the fallback is to use `self.model`.\n    \"\"\"\n    if not served_model_name:\n        return model\n    if isinstance(served_model_name, list):\n        return served_model_name[0]\n    return served_model_name\n\n\nGuidedDecodingBackendV0 = Literal[\"auto\", \"outlines\", \"lm-format-enforcer\",\n                                  \"xgrammar\", \"guidance\"]\nGuidedDecodingBackendV1 = Literal[\"auto\", \"xgrammar\", \"guidance\"]\nGuidedDecodingBackend = Literal[GuidedDecodingBackendV0,\n                                GuidedDecodingBackendV1]\n\n\n@config\n@dataclass\nclass DecodingConfig:\n    \"\"\"Dataclass which contains the decoding strategy of the engine.\"\"\"\n\n    @property\n    @deprecated(\n        \"`guided_decoding_backend` is deprecated and has been renamed to \"\n        \"`backend`. This will be removed in v0.10.0. Please use the \"\n        \"`backend` argument instead.\")\n    def guided_decoding_backend(self) -> GuidedDecodingBackend:\n        return self.backend\n\n    @guided_decoding_backend.setter\n    def guided_decoding_backend(self, value: GuidedDecodingBackend):\n        self.backend = value\n\n    backend: GuidedDecodingBackend = \"auto\" if envs.VLLM_USE_V1 else \"xgrammar\"\n    \"\"\"Which engine will be used for guided decoding (JSON schema / regex etc)\n    by default. With \"auto\", we will make opinionated choices based on request\n    contents and what the backend libraries currently support, so the behavior\n    is subject to change in each release.\"\"\"\n\n    disable_fallback: bool = False\n    \"\"\"If `True`, vLLM will not fallback to a different backend on error.\"\"\"\n\n    disable_any_whitespace: bool = False\n    \"\"\"If `True`, the model will not generate any whitespace during guided\n    decoding. This is only supported for xgrammar and guidance backends.\"\"\"\n\n    disable_additional_properties: bool = False\n    \"\"\"If `True`, the `guidance` backend will not use `additionalProperties`\n    in the JSON schema. This is only supported for the `guidance` backend and\n    is used to better align its behaviour with `outlines` and `xgrammar`.\"\"\"\n\n    reasoning_backend: str = \"\"\n    \"\"\"Select the reasoning parser depending on the model that you're using.\n    This is used to parse the reasoning content into OpenAI API format.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if \":\" in self.backend:\n            self._extract_backend_options()\n\n        if envs.VLLM_USE_V1:\n            valid_guided_backends = get_args(GuidedDecodingBackendV1)\n        else:\n            valid_guided_backends = get_args(GuidedDecodingBackendV0)\n        if self.backend not in valid_guided_backends:\n            raise ValueError(f\"Invalid backend '{self.backend}',\"\n                             f\" must be one of {valid_guided_backends}\")\n        if (self.disable_any_whitespace\n                and self.backend not in (\"xgrammar\", \"guidance\")):\n            raise ValueError(\"disable_any_whitespace is only supported for \"\n                             \"xgrammar and guidance backends.\")\n        if (self.disable_additional_properties and self.backend != \"guidance\"):\n            raise ValueError(\"disable_additional_properties is only supported \"\n                             \"for the guidance backend.\")\n\n    @deprecated(\n        \"Passing guided decoding backend options inside backend in the format \"\n        \"'backend:...' is deprecated. This will be removed in v0.10.0. Please \"\n        \"use the dedicated arguments '--disable-fallback', \"\n        \"'--disable-any-whitespace' and '--disable-additional-properties' \"\n        \"instead.\")\n    def _extract_backend_options(self):\n        \"\"\"Extract backend options from the backend string.\"\"\"\n        backend, options = self.backend.split(\":\")\n        self.backend = cast(GuidedDecodingBackend, backend)\n        options_set = set(options.strip().split(\",\"))\n        if \"no-fallback\" in options_set:\n            self.disable_fallback = True\n        if \"disable-any-whitespace\" in options_set:\n            self.disable_any_whitespace = True\n        if \"no-additional-properties\" in options_set:\n            self.disable_additional_properties = True\n\n\nDetailedTraceModules = Literal[\"model\", \"worker\", \"all\"]\n\n\n@config\n@dataclass\nclass ObservabilityConfig:\n    \"\"\"Configuration for observability - metrics and tracing.\"\"\"\n\n    show_hidden_metrics_for_version: Optional[str] = None\n    \"\"\"Enable deprecated Prometheus metrics that have been hidden since the\n    specified version. For example, if a previously deprecated metric has been\n    hidden since the v0.7.0 release, you use\n    `--show-hidden-metrics-for-version=0.7` as a temporary escape hatch while\n    you migrate to new metrics. The metric is likely to be removed completely\n    in an upcoming release.\"\"\"\n\n    @cached_property\n    def show_hidden_metrics(self) -> bool:\n        \"\"\"Check if the hidden metrics should be shown.\"\"\"\n        if self.show_hidden_metrics_for_version is None:\n            return False\n        return version._prev_minor_version_was(\n            self.show_hidden_metrics_for_version)\n\n    otlp_traces_endpoint: Optional[str] = None\n    \"\"\"Target URL to which OpenTelemetry traces will be sent.\"\"\"\n\n    collect_detailed_traces: Optional[list[DetailedTraceModules]] = None\n    \"\"\"It makes sense to set this only if `--otlp-traces-endpoint` is set. If\n    set, it will collect detailed traces for the specified modules. This\n    involves use of possibly costly and or blocking operations and hence might\n    have a performance impact.\n\n    Note that collecting detailed timing information for each request can be\n    expensive.\"\"\"\n\n    @cached_property\n    def collect_model_forward_time(self) -> bool:\n        \"\"\"Whether to collect model forward time for the request.\"\"\"\n        return (self.collect_detailed_traces is not None\n                and (\"model\" in self.collect_detailed_traces\n                     or \"all\" in self.collect_detailed_traces))\n\n    @cached_property\n    def collect_model_execute_time(self) -> bool:\n        \"\"\"Whether to collect model execute time for the request.\"\"\"\n        return (self.collect_detailed_traces is not None\n                and (\"worker\" in self.collect_detailed_traces\n                     or \"all\" in self.collect_detailed_traces))\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if (self.collect_detailed_traces is not None\n                and len(self.collect_detailed_traces) == 1\n                and \",\" in self.collect_detailed_traces[0]):\n            self._parse_collect_detailed_traces()\n\n        if not is_otel_available() and self.otlp_traces_endpoint is not None:\n            raise ValueError(\n                \"OpenTelemetry is not available. Unable to configure \"\n                \"'otlp_traces_endpoint'. Ensure OpenTelemetry packages are \"\n                f\"installed. Original error:\\n{otel_import_error_traceback}\")\n\n    def _parse_collect_detailed_traces(self):\n        assert isinstance(self.collect_detailed_traces, list)\n        self.collect_detailed_traces = cast(\n            list[DetailedTraceModules],\n            self.collect_detailed_traces[0].split(\",\"))\n\n\nKVProducer = Literal[\"kv_producer\", \"kv_both\"]\nKVConsumer = Literal[\"kv_consumer\", \"kv_both\"]\nKVRole = Literal[KVProducer, KVConsumer]\n\n\n@config\n@dataclass\nclass KVTransferConfig:\n    \"\"\"Configuration for distributed KV cache transfer.\"\"\"\n\n    kv_connector: Optional[str] = None\n    \"\"\"The KV connector for vLLM to transmit KV caches between vLLM instances.\n    \"\"\"\n\n    engine_id: str = str(uuid.uuid4())\n    \"\"\"The engine id for KV transfers.\"\"\"\n\n    kv_buffer_device: Optional[str] = \"cuda\"\n    \"\"\"The device used by kv connector to buffer the KV cache.\n    Currently only support 'cuda'.\"\"\"\n\n    kv_buffer_size: float = 1e9\n    \"\"\"The buffer size for TorchDistributedConnector. Measured in number of\n    bytes. Recommended value: 1e9 (about 1GB).\"\"\"\n\n    kv_role: Optional[KVRole] = None\n    \"\"\"Whether this vLLM instance produces, consumes KV cache, or both. Choices\n    are 'kv_producer', 'kv_consumer', and 'kv_both'.\"\"\"\n\n    kv_rank: Optional[int] = None\n    \"\"\"The rank of this vLLM instance in the KV cache transfer. Typical value:\n    0 for prefill instance, 1 for decode instance.\n    Currently only 1P1D is supported.\"\"\"\n\n    kv_parallel_size: int = 1\n    \"\"\"The number of parallel instances for KV cache transfer. For\n    PyNcclConnector, this should be 2.\"\"\"\n\n    kv_ip: str = \"127.0.0.1\"\n    \"\"\"The KV connector ip, used to build distributed connection.\"\"\"\n\n    kv_port: int = 14579\n    \"\"\"The KV connector port, used to build distributed connection.\"\"\"\n\n    kv_connector_extra_config: dict[str, Any] = field(default_factory=dict)\n    \"\"\"any extra config that the connector may need.\"\"\"\n\n    kv_connector_module_path: Optional[str] = None\n    \"\"\"The Python module path to dynamically load the KV connector from.\n    Only supported in V1.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        if self.kv_role is not None and self.kv_role not in get_args(KVRole):\n            raise ValueError(f\"Unsupported kv_role: {self.kv_role}. \"\n                             f\"Supported roles are {get_args(KVRole)}\")\n\n        if self.kv_connector is not None and self.kv_role is None:\n            raise ValueError(\"Please specify kv_disagg_role when kv_connector \"\n                             f\"is set, supported roles are {get_args(KVRole)}\")\n\n    @property\n    def is_kv_transfer_instance(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in get_args(KVRole)\n\n    @property\n    def is_kv_producer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in get_args(KVProducer)\n\n    @property\n    def is_kv_consumer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in get_args(KVConsumer)\n\n    def get_from_extra_config(self, key, default) -> Any:\n        return self.kv_connector_extra_config.get(key, default)\n\n\n@config\n@dataclass\nclass KVEventsConfig:\n    \"\"\"Configuration for KV event publishing.\"\"\"\n\n    enable_kv_cache_events: bool = False\n    \"\"\"If True, enable KV cache events for tracking block storage and removal.\n    Events can be published externally by zmq using the event publisher config.\n    \"\"\"\n\n    publisher: str = \"null\"\n    \"\"\"The publisher to use for publishing kv events. Can be \"null\", \"zmq\".\n    \"\"\"\n\n    endpoint: str = \"tcp://*:5557\"\n    \"\"\"The zmq endpoint to use for publishing kv events.\n    \"\"\"\n\n    replay_endpoint: Optional[str] = None\n    \"\"\"The zmq endpoint to use for replaying kv events.\n    \"\"\"\n\n    buffer_steps: int = 10_000\n    \"\"\"The number of steps to cache for replay endpoint. Will only save\n    events from the last N steps for the replay endpoint.\n    \"\"\"\n\n    hwm: int = 100_000\n    \"\"\"The zmq high water mark for the event publisher. After queueing N events,\n    events will start dropping if the consumer is not keeping up.\n    \"\"\"\n\n    max_queue_size: int = 100_000\n    \"\"\"The maximum number of events to queue while waiting for publishing.\n    \"\"\"\n\n    topic: str = \"\"\n    \"\"\"The topic to use for the event publisher. Consumers can subscribe to\n    this topic to receive events.\n    \"\"\"\n\n\nclass CompilationLevel:\n    # constants for the levels of the compilation process\n    NO_COMPILATION = 0\n    DYNAMO_AS_IS = 1\n    DYNAMO_ONCE = 2\n    PIECEWISE = 3\n\n\n@config\n@dataclass\nclass PassConfig:\n    \"\"\"Configuration for custom Inductor passes.\n\n    This is separate from general `CompilationConfig` so that inductor passes\n    don't all have access to full configuration - that would create a cycle as\n    the `PassManager` is set as a property of config.\"\"\"\n\n    dump_graph_stages: list[str] = field(default_factory=list)\n    \"\"\"List of stages for which we want to dump the graph. Each pass defines\n    its own stages (before, after, maybe in-between).\"\"\"\n    dump_graph_dir: Path = Path(\".\")\n    \"\"\"Directory to dump the graphs.\"\"\"\n    # TODO(luka) better pass enabling system.\n    enable_fusion: bool = True\n    \"\"\"Whether to enable the custom fusion pass.\"\"\"\n    enable_noop: bool = True\n    \"\"\"Whether to enable the custom no-op elimination pass.\"\"\"\n    enable_sequence_parallelism: bool = False\n    \"\"\"Whether to enable sequence parallelism.\"\"\"\n\n    def uuid(self):\n        \"\"\"\n        Produces a hash unique to the pass configuration.\n        Any new fields that affect compilation should be added to the hash.\n        Do not include dump_graph_* in the hash - they don't affect\n        compilation.\n        \"\"\"\n        include = {\n            \"enable_fusion\", \"enable_noop\", \"enable_sequence_parallelism\"\n        }\n        dict_ = {k: v for k, v in asdict(self).items() if k in include}\n        return InductorPass.hash_dict(dict_)\n\n    def __post_init__(self) -> None:\n        if not self.enable_noop and self.enable_fusion:\n            logger.warning_once(\n                \"Fusion enabled but reshape elimination disabled. \"\n                \"RMSNorm + quant (fp8) fusion might not work\")\n\n\n@config\n@dataclass\nclass CompilationConfig:\n    \"\"\"Configuration for compilation. It has three parts:\n\n    - Top-level Compilation control:\n        - {attr}`level`\n        - {attr}`debug_dump_path`\n        - {attr}`cache_dir`\n        - {attr}`backend`\n        - {attr}`custom_ops`\n        - {attr}`splitting_ops`\n    - CudaGraph capture:\n        - {attr}`use_cudagraph`\n        - {attr}`cudagraph_capture_sizes`\n        - {attr}`cudagraph_num_of_warmups`\n        - {attr}`cudagraph_copy_inputs`\n        - {attr}`full_cuda_graph`\n    - Inductor compilation:\n        - {attr}`use_inductor`\n        - {attr}`compile_sizes`\n        - {attr}`inductor_compile_config`\n        - {attr}`inductor_passes`\n        - custom inductor passes\n\n    Why we have different sizes for cudagraph and inductor:\n    - cudagraph: a cudagraph captured for a specific size can only be used\n        for the same size. We need to capture all the sizes we want to use.\n    - inductor: a graph compiled by inductor for a general shape can be used\n        for different sizes. Inductor can also compile for specific sizes,\n        where it can have more information to optimize the graph with fully\n        static shapes. However, we find the general shape compilation is\n        sufficient for most cases. It might be beneficial to compile for\n        certain small batchsizes, where inductor is good at optimizing.\n    \"\"\"\n    # Top-level Compilation control\n    level: int = 0\n    \"\"\"The level of compilation:\n\n    - 0: no compilation.\n    - 1: dynamo as is.\n    - 2: dynamo once.\n    - 3: piecewise compilation.\"\"\"\n    debug_dump_path: str = \"\"\n    \"\"\"The path to dump the debug information.\"\"\"\n    cache_dir: str = \"\"\n    \"\"\"The directory to store the compiled graph, to accelerate Inductor\n    compilation. By default, it will use model-related information to generate\n    a cache directory.\"\"\"\n    backend: str = \"\"\n    \"\"\"The backend for compilation. It needs to be a string:\n\n    - \"\" (empty string): use the default backend.\n    - \"eager\"/\"openxla\"/...: use the specified backend registered in PyTorch.\n    - \"full.module.name\": a qualified name which can be used to import the\n\n    backend function.\n    We use string to avoid serialization issues when using compilation in a\n    distributed setting. When the compilation level is 1 or 2, the backend is\n    used for the compilation directly (it sees the whole graph). When the\n    compilation level is 3, the backend is used for the piecewise compilation\n    (it sees a part of the graph).\"\"\"\n    custom_ops: list[str] = field(default_factory=list)\n    \"\"\"Fine-grained control over which custom ops to enable/disable. Use 'all'\n    to enable all, 'none' to disable all. Also specify a list of custom op\n    names to enable (prefixed with a '+'), or disable (prefixed with a '-').\n    Examples:\n\n    - 'all,-op1' to enable all except op1\n    - 'none,+op1,+op2' to enable only op1 and op2\n\n    By default, all custom ops are enabled when running without Inductor and\n    disabled when running with Inductor (compile_level >= Inductor).\"\"\"\n    splitting_ops: list[str] = field(default_factory=list)\n    \"\"\"A list of ops to split the full graph into subgraphs, used in piecewise\n    compilation.\"\"\"\n\n    # Inductor capture\n    use_inductor: bool = True\n    \"\"\"Whether to use inductor compilation:\n\n    - False: inductor compilation is not used. graph runs in eager.\n    - True: inductor compilation is used. one graph for symbolic shape\n        is compiled. In addition, compile for compile_sizes,\n        using configurations in inductor_compile_config.\"\"\"\n    compile_sizes: Optional[list[Union[int, str]]] = None\n    \"\"\"Sizes to compile for inductor. In addition\n    to integers, it also supports \"cudagraph_capture_sizes\" to\n    specify the sizes for cudagraph capture.\"\"\"\n    inductor_compile_config: dict = field(default_factory=dict)\n    \"\"\"Additional configurations for inductor.\n    - None: use default configurations.\"\"\"\n    inductor_passes: dict[str, str] = field(default_factory=dict)\n    \"\"\"Additional passes for inductor. It is a dictionary\n    from pass name to pass function qualified name. We use function\n    name because the config uses JSON format. If we pass the config\n    from Python, functions can also be passed directly via Python object\n    constructor, e.g. `CompilationConfig(inductor_passes={\"a\": func})`.\"\"\"\n\n    # CudaGraph compilation\n    use_cudagraph: bool = False\n    \"\"\"Whether to use cudagraph inside compilation.\n    - False: cudagraph inside compilation is not used.\n    - True: cudagraph inside compilation is used. It requires\n        that all input buffers have fixed addresses, and all\n        splitting ops write their outputs to input buffers.\n    Note that this is orthogonal to the cudagraph capture logic\n    outside of compilation.\n    TODO: move outside cudagraph logic into compilation.\n    torch.compile will handle cudagraph capture logic in the future.\"\"\"\n    cudagraph_num_of_warmups: int = 0\n    \"\"\"Number of warmup runs for cudagraph.\n    It means the first several runs will be treated as warmup runs.\n    Only after that, the execution will be recorded, and the recorded\n    cudagraph will be used for subsequent runs.\"\"\"\n    cudagraph_capture_sizes: Optional[list[int]] = None\n    \"\"\"Sizes to capture cudagraph.\n    - None (default): capture sizes are inferred from vllm config.\n    - list[int]: capture sizes are specified as given.\"\"\"\n    cudagraph_copy_inputs: bool = False\n    \"\"\"Whether to copy input tensors for\n    cudagraph. If the caller can guarantee that the same input buffers\n    are always used, it can set this to False. Otherwise, it should\n    set this to True, and the compiler will copy the input to an\n    internally managed buffer. Default is False.\"\"\"\n    full_cuda_graph: bool = False\n    \"\"\"whether to use a full cuda graph for the entire forward pass rather than\n    splitting certain operations such as attention into subgraphs. Thus this\n    flag cannot be used together with splitting_ops. This may provide\n    performance benefits for smaller models.\"\"\"\n\n    pass_config: PassConfig = field(default_factory=PassConfig)\n    \"\"\"Custom inductor passes, see PassConfig for more details\"\"\"\n\n    max_capture_size: int = field(default=None, init=False)  # type: ignore\n    \"\"\"not configurable, computed after init\"\"\"\n    local_cache_dir: str = field(default=None, init=False)  # type: ignore\n    \"\"\"local cache dir for each rank\"\"\"\n    bs_to_padded_graph_size: list[int] = field(\n        default=None,  # type: ignore\n        init=False)\n    \"\"\"optimization:\n    Intuitively, bs_to_padded_graph_size should be dict[int, int].\n    since we know all keys are in a range [0, max_capture_size],\n    we can optimize it to list[int] for better lookup performance.\"\"\"\n\n    # keep track of enabled and disabled custom ops\n    enabled_custom_ops: Counter[str] = field(default_factory=Counter,\n                                             init=False)\n    \"\"\"custom ops that are enabled\"\"\"\n    disabled_custom_ops: Counter[str] = field(default_factory=Counter,\n                                              init=False)\n    \"\"\"custom ops that are disabled\"\"\"\n    traced_files: set[str] = field(default_factory=set, init=False)\n    \"\"\"files that are traced for compilation\"\"\"\n    compilation_time: float = field(default=0.0, init=False)\n    \"\"\"time taken for compilation\"\"\"\n\n    static_forward_context: dict[str, Any] = field(default_factory=dict,\n                                                   init=False)\n    \"\"\"Per-model forward context\n    Map from layer name to layer objects that need to be accessed outside\n    model code, e.g., Attention, FusedMOE when dp_size>1.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.level)\n        factors.append(self.backend)\n        factors.append(self.custom_ops)\n        factors.append(self.splitting_ops)\n        factors.append(self.use_inductor)\n        factors.append(self.inductor_compile_config)\n        factors.append(self.inductor_passes)\n        factors.append(self.pass_config.uuid())\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __repr__(self) -> str:\n        exclude = {\n            \"static_forward_context\",\n            \"enabled_custom_ops\",\n            \"disabled_custom_ops\",\n            \"compilation_time\",\n            \"bs_to_padded_graph_size\",\n            \"pass_config\",\n            \"traced_files\",\n        }\n        include = dict()\n        for k, v in asdict(self).items():\n            if k in exclude:\n                continue\n            f = get_field(CompilationConfig, k)\n            if (d := f.default) is not MISSING and d == v:\n                continue\n            if (df := f.default_factory) is not MISSING and df() == v:\n                continue\n            include[k] = v\n        return json.dumps(include)\n\n    __str__ = __repr__\n\n    @classmethod\n    def from_cli(cls, cli_value: str) -> \"CompilationConfig\":\n        \"\"\"Parse the CLI value for the compilation config.\"\"\"\n        if cli_value in [\"0\", \"1\", \"2\", \"3\"]:\n            return cls(level=int(cli_value))\n        return cls(**json.loads(cli_value))\n\n    def __post_init__(self) -> None:\n        count_none = self.custom_ops.count(\"none\")\n        count_all = self.custom_ops.count(\"all\")\n        assert count_none + count_all <= 1, \"Can only specify 'none' or 'all'\"\n\n        # TODO(zou3519/luka): There are 2 issues with auto-functionalization V2:\n        # 1. A bug in PyTorch, fixed in 2.7:\n        #    https://github.com/pytorch/pytorch/issues/147924\n        # 2. Custom passes (fusion) rely on auto-functionalization V1 and don't\n        #    work with V2. Addressing this will take extra engineering effort\n        #    and it is not yet a priority. RFC here:\n        #    https://github.com/vllm-project/vllm/issues/14703\n\n        if is_torch_equal_or_newer(\"2.6\"):\n            KEY = 'enable_auto_functionalized_v2'\n            if KEY not in self.inductor_compile_config:\n                self.inductor_compile_config[KEY] = False\n\n        for k, v in self.inductor_passes.items():\n            if not isinstance(v, str):\n                assert callable(v), (\n                    f\"pass {k} should be callable or a qualified name\")\n                self.inductor_compile_config[k] = v if isinstance(\n                    v, InductorPass) else CallableInductorPass(v)\n                continue\n\n            # resolve function from qualified name\n            names = v.split(\".\")\n            module = \".\".join(names[:-1])\n            func_name = names[-1]\n            func = __import__(module).__dict__[func_name]\n            self.inductor_compile_config[k] = func if isinstance(\n                func, InductorPass) else CallableInductorPass(func)\n\n        if isinstance(self.pass_config, dict):\n            self.pass_config = PassConfig(**self.pass_config)\n\n    def init_backend(self, vllm_config: \"VllmConfig\") -> Union[str, Callable]:\n        if self.level == CompilationLevel.NO_COMPILATION:\n            raise ValueError(\"No compilation level is set.\")\n\n        from torch._dynamo.backends.registry import list_backends\n        torch_backends = list_backends(exclude_tags=tuple())\n        if self.level in [\n                CompilationLevel.DYNAMO_AS_IS, CompilationLevel.DYNAMO_ONCE\n        ]:\n            if self.backend == \"\":\n                return \"eager\"\n            if self.backend in torch_backends:\n                return self.backend\n            return resolve_obj_by_qualname(self.backend)\n\n        # TODO: pass user-specified backend to piecewise compilation\n        # merge with the config use_inductor\n        assert self.level == CompilationLevel.PIECEWISE\n\n        from vllm.compilation.backends import VllmBackend\n        return VllmBackend(vllm_config)\n\n    def init_with_cudagraph_sizes(self,\n                                  cudagraph_capture_sizes: list[int]) -> None:\n        \"\"\"To complete the initialization of config,\n        we need to know the cudagraph sizes.\"\"\"\n\n        if self.cudagraph_capture_sizes is None:\n            self.cudagraph_capture_sizes = cudagraph_capture_sizes\n        else:\n            # de-duplicate the sizes provided by the config\n            dedup_sizes = list(set(self.cudagraph_capture_sizes))\n            if len(dedup_sizes) < len(self.cudagraph_capture_sizes):\n                logger.info((\"cudagraph sizes specified by model runner\"\n                             \" %s is overridden by config %s\"),\n                            cudagraph_capture_sizes, dedup_sizes)\n            self.cudagraph_capture_sizes = dedup_sizes\n\n        computed_compile_sizes = []\n        if self.compile_sizes is not None:\n            # de-duplicate the sizes provided by the config\n            self.compile_sizes = list(set(self.compile_sizes))\n            for x in self.compile_sizes:\n                if isinstance(x, str):\n                    assert x == \"cudagraph_capture_sizes\", \\\n                    \"Unrecognized size type in compile_sizes, \" \\\n                    f\"expect 'cudagraph_capture_sizes', got {x}\"\n                    computed_compile_sizes.extend(self.cudagraph_capture_sizes)\n                else:\n                    assert isinstance(x, int)\n                    computed_compile_sizes.append(x)\n        self.compile_sizes = computed_compile_sizes  # type: ignore\n\n        # sort to make sure cudagraph capture sizes are in descending order\n        self.cudagraph_capture_sizes.sort(reverse=True)\n        self.max_capture_size = self.cudagraph_capture_sizes[\n            0] if self.cudagraph_capture_sizes else 0\n\n        # pre-compute the mapping from batch size to padded graph size\n        self.bs_to_padded_graph_size = [\n            0 for i in range(self.max_capture_size + 1)\n        ]\n        for end, start in zip(self.cudagraph_capture_sizes,\n                              self.cudagraph_capture_sizes[1:] + [0]):\n            for bs in range(start, end):\n                if bs == start:\n                    self.bs_to_padded_graph_size[bs] = start\n                else:\n                    self.bs_to_padded_graph_size[bs] = end\n        self.bs_to_padded_graph_size[\n            self.max_capture_size] = self.max_capture_size\n\n    def set_splitting_ops_for_v1(self):\n        # NOTE: this function needs to be called\n        if self.splitting_ops and self.full_cuda_graph:\n            raise ValueError(\"full_cuda_graph cannot be used together with \"\n                             \"splitting_ops, as Full CUDA graph will override \"\n                             f\"the splitting_ops: {self.splitting_ops}\")\n\n        if not self.splitting_ops:\n            self.splitting_ops = [] if self.full_cuda_graph else [\n                \"vllm.unified_attention\",\n                \"vllm.unified_attention_with_output\",\n            ]\n\n\n@config\n@dataclass\nclass VllmConfig:\n    \"\"\"Dataclass which contains all vllm-related configuration. This\n    simplifies passing around the distinct configurations in the codebase.\n    \"\"\"\n\n    # TODO: use default_factory once default constructing ModelConfig doesn't\n    # try to download a model\n    model_config: ModelConfig = None  # type: ignore\n    \"\"\"Model configuration.\"\"\"\n    cache_config: CacheConfig = field(default_factory=CacheConfig)\n    \"\"\"Cache configuration.\"\"\"\n    parallel_config: ParallelConfig = field(default_factory=ParallelConfig)\n    \"\"\"Parallel configuration.\"\"\"\n    scheduler_config: SchedulerConfig = field(default_factory=SchedulerConfig)\n    \"\"\"Scheduler configuration.\"\"\"\n    device_config: DeviceConfig = field(default_factory=DeviceConfig)\n    \"\"\"Device configuration.\"\"\"\n    load_config: LoadConfig = field(default_factory=LoadConfig)\n    \"\"\"Load configuration.\"\"\"\n    lora_config: Optional[LoRAConfig] = None\n    \"\"\"LoRA configuration.\"\"\"\n    speculative_config: Optional[SpeculativeConfig] = None\n    \"\"\"Speculative decoding configuration.\"\"\"\n    decoding_config: DecodingConfig = field(default_factory=DecodingConfig)\n    \"\"\"Decoding configuration.\"\"\"\n    observability_config: Optional[ObservabilityConfig] = None\n    \"\"\"Observability configuration.\"\"\"\n    prompt_adapter_config: Optional[PromptAdapterConfig] = None\n    \"\"\"Prompt adapter configuration.\"\"\"\n    quant_config: Optional[QuantizationConfig] = None\n    \"\"\"Quantization configuration.\"\"\"\n    compilation_config: CompilationConfig = field(\n        default_factory=CompilationConfig)\n    \"\"\"`torch.compile` configuration for the model.\n\n    When it is a number (0, 1, 2, 3), it will be interpreted as the\n    optimization level.\n\n    NOTE: level 0 is the default level without any optimization. level 1 and 2\n    are for internal testing only. level 3 is the recommended level for\n    production.\n\n    Following the convention of traditional compilers, using `-O` without space\n    is also supported. `-O3` is equivalent to `-O 3`.\n\n    You can specify the full compilation config like so:\n    `{\"level\": 3, \"cudagraph_capture_sizes\": [1, 2, 4, 8]}`\n    \"\"\"\n    kv_transfer_config: Optional[KVTransferConfig] = None\n    \"\"\"The configurations for distributed KV cache transfer.\"\"\"\n    kv_events_config: Optional[KVEventsConfig] = None\n    \"\"\"The configurations for event publishing.\"\"\"\n    # some opaque config, only used to provide additional information\n    # for the hash computation, mainly used for testing, debugging or out of\n    # tree config registration.\n    additional_config: Union[dict, SupportsHash] = field(default_factory=dict)\n    \"\"\"Additional config for specified platform. Different platforms may\n    support different configs. Make sure the configs are valid for the platform\n    you are using. Contents must be hashable.\"\"\"\n    instance_id: str = \"\"\n    \"\"\"The ID of the vLLM instance.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n\n        # summarize vllm config\n        vllm_factors: list[Any] = []\n        from vllm import __version__\n        vllm_factors.append(__version__)\n        vllm_factors.append(envs.VLLM_USE_V1)\n        if self.model_config:\n            vllm_factors.append(self.model_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.cache_config:\n            vllm_factors.append(self.cache_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.parallel_config:\n            vllm_factors.append(self.parallel_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.scheduler_config:\n            vllm_factors.append(self.scheduler_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.device_config:\n            vllm_factors.append(self.device_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.load_config:\n            vllm_factors.append(self.load_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.lora_config:\n            vllm_factors.append(self.lora_config.compute_hash())\n            # LoRA creates static buffers based on max_num_batched_tokens.\n            # The tensor sizes and strides get captured in the torch.compile\n            # graph explicitly.\n            vllm_factors.append(\n                str(self.scheduler_config.max_num_batched_tokens))\n        else:\n            vllm_factors.append(\"None\")\n        if self.speculative_config:\n            vllm_factors.append(self.speculative_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.decoding_config:\n            vllm_factors.append(self.decoding_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.observability_config:\n            vllm_factors.append(self.observability_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.prompt_adapter_config:\n            vllm_factors.append(self.prompt_adapter_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.quant_config:\n            pass  # should be captured by model_config.quantization\n        if self.compilation_config:\n            vllm_factors.append(self.compilation_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.kv_transfer_config:\n            vllm_factors.append(self.kv_transfer_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.additional_config:\n            if isinstance(additional_config := self.additional_config, dict):\n                additional_config_hash = hashlib.md5(\n                    json.dumps(additional_config, sort_keys=True).encode(),\n                    usedforsecurity=False,\n                ).hexdigest()\n            else:\n                additional_config_hash = additional_config.compute_hash()\n            vllm_factors.append(additional_config_hash)\n        else:\n            vllm_factors.append(\"None\")\n        factors.append(vllm_factors)\n\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()[:10]\n        return hash_str\n\n    def pad_for_cudagraph(self, batch_size: int) -> int:\n        # if batch_size > self.compilation_config.max_capture_size,\n        # it should raise an IndexError.\n        # the caller should make sure the batch_size is within the range,\n        # i.e., batch_size <= self.compilation_config.max_capture_size\n        return self.compilation_config.bs_to_padded_graph_size[batch_size]\n\n    @staticmethod\n    def _get_quantization_config(\n            model_config: ModelConfig,\n            load_config: LoadConfig) -> Optional[QuantizationConfig]:\n        \"\"\"Get the quantization config.\"\"\"\n        from vllm.platforms import current_platform\n        if model_config.quantization is not None:\n            from vllm.model_executor.model_loader.weight_utils import (\n                get_quant_config)\n            quant_config = get_quant_config(model_config, load_config)\n            capability_tuple = current_platform.get_device_capability()\n\n            if capability_tuple is not None:\n                capability = capability_tuple.to_int()\n                if capability < quant_config.get_min_capability():\n                    raise ValueError(\n                        f\"The quantization method {model_config.quantization} \"\n                        \"is not supported for the current GPU. Minimum \"\n                        f\"capability: {quant_config.get_min_capability()}. \"\n                        f\"Current capability: {capability}.\")\n            supported_dtypes = quant_config.get_supported_act_dtypes()\n            if model_config.dtype not in supported_dtypes:\n                raise ValueError(\n                    f\"{model_config.dtype} is not supported for quantization \"\n                    f\"method {model_config.quantization}. Supported dtypes: \"\n                    f\"{supported_dtypes}\")\n            return quant_config\n        return None\n\n    @staticmethod\n    def get_quantization_config(\n            model_config: ModelConfig,\n            load_config: LoadConfig) -> Optional[QuantizationConfig]:\n        import copy\n\n        # For some reason, the _ version of this modifies the model_config\n        # object, so using deepcopy to avoid this problem.\n        return VllmConfig._get_quantization_config(copy.deepcopy(model_config),\n                                                   load_config)\n\n    def with_hf_config(\n        self,\n        hf_config: PretrainedConfig,\n        architectures: Optional[list[str]] = None,\n    ) -> \"VllmConfig\":\n        if architectures is not None:\n            hf_config = copy.deepcopy(hf_config)\n            hf_config.architectures = architectures\n\n        model_config = copy.deepcopy(self.model_config)\n        model_config.hf_config = hf_config\n\n        return replace(self, model_config=model_config)\n\n    def __post_init__(self):\n        \"\"\"Verify configs are valid & consistent with each other.\n        \"\"\"\n        if self.model_config is not None:\n            self.model_config.verify_async_output_proc(self.parallel_config,\n                                                       self.speculative_config,\n                                                       self.device_config)\n            self.model_config.verify_with_parallel_config(self.parallel_config)\n            self.model_config.verify_dual_chunk_attention_config(\n                self.load_config)\n\n        if self.cache_config is not None:\n            self.cache_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.lora_config:\n            self.lora_config.verify_with_cache_config(self.cache_config)\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_lora_support()\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n        if self.quant_config is None and \\\n            self.model_config is not None and self.load_config is not None:\n            self.quant_config = VllmConfig._get_quantization_config(\n                self.model_config, self.load_config)\n\n        from vllm.platforms import current_platform\n        if self.scheduler_config is not None and \\\n            self.model_config is not None and \\\n            self.scheduler_config.chunked_prefill_enabled and \\\n            self.model_config.dtype == torch.float32 and \\\n            current_platform.get_device_capability() == (7, 5):\n            logger.warning_once(\n                \"Turing devices tensor cores do not support float32 matmul. \"\n                \"To workaround this limitation, vLLM will set 'ieee' input \"\n                \"precision for chunked prefill triton kernels.\")\n\n        if self.compilation_config is None:\n            self.compilation_config = CompilationConfig()\n        if self.compilation_config.pass_config.enable_sequence_parallelism:\n            self.compilation_config.custom_ops.append(\"+rms_norm\")\n        if envs.VLLM_USE_V1 and self.model_config is not None and \\\n            not self.model_config.enforce_eager:\n            # NOTE(woosuk): Currently, we use inductor because the piecewise\n            # CUDA graphs do not work properly with the custom CUDA kernels.\n            # FIXME(woosuk): Disable inductor to reduce the compilation time\n            # and avoid any potential issues with the inductor.\n            # FIXME(rob): Add function to set all of these.\n            if not self.compilation_config.custom_ops:\n                self.compilation_config.custom_ops = [\"none\"]\n            self.compilation_config.use_cudagraph = True\n            self.compilation_config.use_inductor = True\n            self.compilation_config.cudagraph_num_of_warmups = 1\n            self.compilation_config.pass_config.enable_fusion = False\n            self.compilation_config.pass_config.enable_noop = False\n            self.compilation_config.level = CompilationLevel.PIECEWISE\n            self.compilation_config.set_splitting_ops_for_v1()\n\n        self._set_cudagraph_sizes()\n\n        if self.cache_config is not None and \\\n            self.cache_config.cpu_offload_gb > 0 and \\\n            self.compilation_config.level != CompilationLevel.NO_COMPILATION \\\n                and not envs.VLLM_USE_V1:\n            logger.warning(\n                \"CPU offload is not supported with `torch.compile` in v0 yet.\"\n                \" Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        if ((not envs.VLLM_USE_V1) and self.lora_config is not None\n                and self.compilation_config.level\n                != CompilationLevel.NO_COMPILATION):\n            logger.warning(\n                \"LoRA for V0 is not supported with `torch.compile` yet. \"\n                \"Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        if self.compilation_config.full_cuda_graph and \\\n            not self.model_config.disable_cascade_attn:\n            logger.warning_once(\n                \"full_cuda_graph is not supported with \"\n                \"cascade attention. Disabling cascade attention.\")\n            self.model_config.disable_cascade_attn = True\n\n        if self.model_config and self.model_config.use_mla and \\\n            not (current_platform.is_cuda() or current_platform.is_rocm()):\n            logger.info(\n                \"MLA is enabled on a non-GPU platform; forcing chunked \"\n                \"prefill and prefix caching to be disabled.\")\n            self.scheduler_config.enable_chunked_prefill = False\n            self.scheduler_config.chunked_prefill_enabled = False\n            self.scheduler_config.max_num_batched_tokens = max(\n                self.scheduler_config.max_model_len,\n                _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n\n            if self.cache_config is not None:\n                self.cache_config.enable_prefix_caching = False\n\n        if (self.kv_events_config\n                and self.kv_events_config.enable_kv_cache_events\n                and not self.cache_config.enable_prefix_caching):\n            logger.warning(\n                \"KV cache events are on, but prefix caching is not enabled.\"\n                \"Use --enable-prefix-caching to enable.\")\n        if (self.kv_events_config and self.kv_events_config.publisher != \"null\"\n                and not self.kv_events_config.enable_kv_cache_events):\n            logger.warning(\"KV cache events are disabled,\"\n                           \"but the scheduler is configured to publish them.\"\n                           \"Modify KVEventsConfig.enable_kv_cache_events\"\n                           \"to True to enable.\")\n        current_platform.check_and_update_config(self)\n\n        if not self.instance_id:\n            self.instance_id = random_uuid()[:5]\n\n    def update_sizes_for_sequence_parallelism(self,\n                                              possible_sizes: list) -> list:\n        # remove the sizes that not multiple of tp_size when\n        # enable sequence parallelism\n        removed_sizes = [\n            size for size in possible_sizes\n            if size % self.parallel_config.tensor_parallel_size != 0\n        ]\n        if removed_sizes:\n            logger.warning(\n                \"Batch sizes %s are removed because they are not \"\n                \"multiple of tp_size %d when \"\n                \"sequence parallelism is enabled\", removed_sizes,\n                self.parallel_config.tensor_parallel_size)\n\n        return [\n            size for size in possible_sizes\n            if size % self.parallel_config.tensor_parallel_size == 0\n        ]\n\n    def _set_cudagraph_sizes(self):\n        \"\"\"\n        cudagraph batchsize padding logic:\n\n        `[1, 2, 4] + [8 * i for i in range(1, 1025)]` is a list of all possible\n        batch sizes that cudagraph will capture.\n\n        Depending on the engine's configuration of `max_num_seqs`, the\n        candidate batch sizes to capture cudagraph will shrink to the subset\n        which just cover the range of `[1, max_num_seqs]`. In the common case,\n        `max_num_seqs` is 256, and the cudagraph batch sizes will be\n        `[1, 2, 4, 8, 16, 24, 32, 40, ..., 256]`.\n\n        However, if users specify the cudagraph capture sizes through\n        compilation config, we will use the specified sizes instead.\n\n        In the end, `vllm_config.compilation_config.cudagraph_capture_sizes`\n        will be the final sizes to capture cudagraph (in descending order).\n\n        During runtime, if batchsize is larger than\n        `vllm_config.compilation_config.cudagraph_capture_sizes`,\n        no cudagraph will be used.\n        If the batch size is no larger than\n        `vllm_config.compilation_config.cudagraph_capture_sizes`,\n        we can quickly find the padded graph size for a given batch size by\n        looking up `vllm_config.compilation_config.bs_to_padded_graph_size`.\n        \"\"\"\n\n        # calculate the default `batch_size_capture_list`\n        if not envs.VLLM_USE_V1:\n            batch_size_capture_list = []\n            max_batchsize_to_capture = 0\n            if self.scheduler_config is not None and \\\n                self.model_config is not None and \\\n                    not self.model_config.enforce_eager:\n\n                possible_sizes = [1, 2, 4] + [8 * i for i in range(1, 1025)]\n                if self.parallel_config.tensor_parallel_size > 1 and \\\n                    self.compilation_config.pass_config.enable_sequence_parallelism:\n                    possible_sizes = self.update_sizes_for_sequence_parallelism(\n                        possible_sizes)\n\n                # find the minimum size that is larger than max_num_seqs,\n                # which then becomes the max_batchsize_to_capture\n                larger_sizes = [\n                    x for x in possible_sizes\n                    if x >= self.scheduler_config.max_num_seqs\n                ]\n                if larger_sizes:\n                    max_batchsize_to_capture = larger_sizes[0]\n                else:\n                    max_batchsize_to_capture = possible_sizes[-1]\n\n                # filter out the sizes that are\n                # larger than max_batchsize_to_capture\n                batch_size_capture_list = [\n                    size for size in possible_sizes\n                    if size <= max_batchsize_to_capture\n                ]\n        else:\n            batch_size_capture_list = []\n            if self.model_config is not None and \\\n                not self.model_config.enforce_eager:\n                cuda_graph_sizes = self.scheduler_config.cuda_graph_sizes\n                if len(cuda_graph_sizes) == 1:\n                    batch_size_capture_list = [1, 2, 4] + [\n                        i for i in range(8, cuda_graph_sizes[0] + 1, 8)\n                    ]\n                elif len(cuda_graph_sizes) > 1:\n                    batch_size_capture_list = sorted(cuda_graph_sizes)\n                else:\n                    raise TypeError(f\"Invalid value for {cuda_graph_sizes=}.\")\n                if self.parallel_config.tensor_parallel_size > 1 and \\\n                    self.compilation_config.pass_config.enable_sequence_parallelism:\n                    batch_size_capture_list = \\\n                        self.update_sizes_for_sequence_parallelism(batch_size_capture_list)\n                max_num_tokens = self.scheduler_config.max_num_batched_tokens\n                batch_size_capture_list = [\n                    size for size in batch_size_capture_list\n                    if size <= max_num_tokens\n                ]\n\n        self.compilation_config.init_with_cudagraph_sizes(\n            batch_size_capture_list)\n\n    def __str__(self):\n        return (\n            f\"model={self.model_config.model!r},\"\n            f\" speculative_config={self.speculative_config!r},\"\n            f\" tokenizer={self.model_config.tokenizer!r}, \"\n            f\"skip_tokenizer_init={self.model_config.skip_tokenizer_init},\"\n            f\" tokenizer_mode={self.model_config.tokenizer_mode}, \"\n            f\"revision={self.model_config.revision}, \"\n            f\"override_neuron_config={self.model_config.override_neuron_config},\"\n            f\" tokenizer_revision={self.model_config.tokenizer_revision}, \"\n            f\"trust_remote_code={self.model_config.trust_remote_code}, \"\n            f\"dtype={self.model_config.dtype}, \"\n            f\"max_seq_len={self.model_config.max_model_len},\"\n            f\" download_dir={self.load_config.download_dir!r}, \"\n            f\"load_format={self.load_config.load_format}, \"\n            f\"tensor_parallel_size={self.parallel_config.tensor_parallel_size},\"\n            f\" pipeline_parallel_size={self.parallel_config.pipeline_parallel_size}, \"  # noqa\n            f\"disable_custom_all_reduce={self.parallel_config.disable_custom_all_reduce}, \"  # noqa\n            f\"quantization={self.model_config.quantization}, \"\n            f\"enforce_eager={self.model_config.enforce_eager}, \"\n            f\"kv_cache_dtype={self.cache_config.cache_dtype}, \"\n            f\" device_config={self.device_config.device}, \"\n            f\"decoding_config={self.decoding_config!r}, \"\n            f\"observability_config={self.observability_config!r}, \"\n            f\"seed={self.model_config.seed}, \"\n            f\"served_model_name={self.model_config.served_model_name}, \"\n            f\"num_scheduler_steps={self.scheduler_config.num_scheduler_steps}, \"\n            f\"multi_step_stream_outputs={self.scheduler_config.multi_step_stream_outputs}, \"  # noqa\n            f\"enable_prefix_caching={self.cache_config.enable_prefix_caching}, \"\n            f\"chunked_prefill_enabled={self.scheduler_config.chunked_prefill_enabled}, \"  # noqa\n            f\"use_async_output_proc={self.model_config.use_async_output_proc}, \"\n            f\"pooler_config={self.model_config.pooler_config!r}, \"\n            f\"compilation_config={self.compilation_config!r}\")\n\n\n_current_vllm_config: Optional[VllmConfig] = None\n\n\n@contextmanager\ndef set_current_vllm_config(vllm_config: VllmConfig, check_compile=False):\n    \"\"\"\n    Temporarily set the current vLLM config.\n    Used during model initialization.\n    We save the current vLLM config in a global variable,\n    so that all modules can access it, e.g. custom ops\n    can access the vLLM config to determine how to dispatch.\n    \"\"\"\n    global _current_vllm_config\n    old_vllm_config = _current_vllm_config\n    from vllm.compilation.counter import compilation_counter\n    num_models_seen = compilation_counter.num_models_seen\n    try:\n        _current_vllm_config = vllm_config\n        yield\n    except Exception:\n        raise\n    else:\n        logger.debug(\"enabled custom ops: %s\",\n                     vllm_config.compilation_config.enabled_custom_ops)\n        logger.debug(\"disabled custom ops: %s\",\n                     vllm_config.compilation_config.disabled_custom_ops)\n        if check_compile and \\\n            vllm_config.compilation_config.level == CompilationLevel.PIECEWISE \\\n            and compilation_counter.num_models_seen == num_models_seen:\n            # If the model supports compilation,\n            # compilation_counter.num_models_seen should be increased\n            # by at least 1.\n            # If it is not increased, it means the model does not support\n            # compilation (does not have @support_torch_compile decorator).\n            logger.warning(\n                \"`torch.compile` is turned on, but the model %s\"\n                \" does not support it. Please open an issue on GitHub\"\n                \" if you want it to be supported.\",\n                vllm_config.model_config.model)\n    finally:\n        _current_vllm_config = old_vllm_config\n\n\ndef get_current_vllm_config() -> VllmConfig:\n    if _current_vllm_config is None:\n        # in ci, usually when we test custom ops/modules directly,\n        # we don't set the vllm config. In that case, we set a default\n        # config.\n        logger.warning(\"Current vLLM config is not set.\")\n        from vllm.config import VllmConfig\n        return VllmConfig()\n    return _current_vllm_config\n\n\ndef contains_object_print(text):\n    \"\"\"\n    Check if the text looks like a printed Python object, e.g.\n    contains any substring matching the pattern: \"at 0xFFFFFFF>\"\n    We match against 0x followed by 2-16 hex chars (there's\n    a max of 16 on a 64 bit system).\n\n    Args:\n        text (str): The text to check\n\n    Returns:\n        bool: True if a match is found, False otherwise\n    \"\"\"\n    pattern = r'at 0x[a-fA-F0-9]{2,16}>'\n    match = re.search(pattern, text)\n    return match is not None\n\n\ndef assert_hashable(text):\n    if not contains_object_print(text):\n        return True\n    raise AssertionError(\n        f\"vLLM tried to hash some configs that may have Python objects ids \"\n        f\"in them. This is a bug, please file an issue. \"\n        f\"Text being hashed: {text}\")\n\n\nT = TypeVar(\"T\")\n\n\ndef get_layers_from_vllm_config(vllm_config: VllmConfig,\n                                layer_type: type[T]) -> dict[str, T]:\n    return {\n        layer_name: layer\n        for layer_name, layer in\n        vllm_config.compilation_config.static_forward_context.items()\n        if isinstance(layer, layer_type)\n    }\n", 4578], "/home/jeromeku/vllm/vllm/utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent\nimport contextlib\nimport datetime\nimport enum\nimport gc\nimport getpass\nimport hashlib\nimport importlib\nimport importlib.metadata\nimport importlib.util\nimport inspect\nimport ipaddress\nimport json\nimport multiprocessing\nimport os\nimport pickle\nimport re\nimport signal\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nimport threading\nimport time\nimport traceback\nimport types\nimport uuid\nimport warnings\nimport weakref\nfrom argparse import (Action, ArgumentDefaultsHelpFormatter, ArgumentParser,\n                      ArgumentTypeError, _ArgumentGroup)\nfrom asyncio import FIRST_COMPLETED, AbstractEventLoop, Task\nfrom collections import UserDict, defaultdict\nfrom collections.abc import (AsyncGenerator, Awaitable, Generator, Hashable,\n                             Iterable, Iterator, KeysView, Mapping)\nfrom concurrent.futures.process import ProcessPoolExecutor\nfrom dataclasses import dataclass, field\nfrom functools import cache, lru_cache, partial, wraps\nfrom types import MappingProxyType\nfrom typing import (TYPE_CHECKING, Any, Callable, Generic, Literal, NamedTuple,\n                    Optional, Sequence, Tuple, Type, TypeVar, Union, cast,\n                    overload)\nfrom urllib.parse import urlparse\nfrom uuid import uuid4\n\nimport cachetools\nimport cloudpickle\nimport numpy as np\nimport numpy.typing as npt\nimport psutil\nimport torch\nimport torch.types\nimport yaml\nimport zmq\nimport zmq.asyncio\nfrom packaging import version\nfrom packaging.version import Version\nfrom torch.library import Library\nfrom typing_extensions import Never, ParamSpec, TypeIs, assert_never\n\nimport vllm.envs as envs\n# NOTE: import triton_utils to make TritonPlaceholderModule work\n#       if triton is unavailable\nimport vllm.triton_utils  # noqa: F401\nfrom vllm.logger import enable_trace_function_call, init_logger\n\nif TYPE_CHECKING:\n    from argparse import Namespace\n\n    from vllm.config import ModelConfig, VllmConfig\n\nlogger = init_logger(__name__)\n\n# Exception strings for non-implemented encoder/decoder scenarios\n\n# Reminder: Please update docs/source/features/compatibility_matrix.md\n# If the feature combo become valid\n\nSTR_NOT_IMPL_ENC_DEC_SWA = \\\n    \"Sliding window attention for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_PREFIX_CACHE = \\\n    \"Prefix caching for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL = \\\n    \"Chunked prefill for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP = (\n    \"Models with logits_soft_cap \"\n    \"require FlashInfer backend, which is \"\n    \"currently not supported for encoder/decoder \"\n    \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_LORA = (\"LoRA is currently not currently \"\n                             \"supported with encoder/decoder \"\n                             \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PP = (\"Pipeline parallelism is not \"\n                           \"currently supported with \"\n                           \"encoder/decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_MM = (\"Multimodal is not currently \"\n                           \"supported with encoder/decoder \"\n                           \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_SPEC_DEC = (\"Speculative decoding is not \"\n                                 \"currently supported with encoder/\"\n                                 \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_BACKEND = (\"XFormers and Flash-Attention are the only \"\n                                \"backends currently supported with encoder/\"\n                                \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER = (\"Prompt adapters are not \"\n                                       \"currently supported with encoder/\"\n                                       \"decoder models.\")\n\n# Efficiently import all enc/dec error strings\n# rather than having to import all of the above\nSTR_NOT_IMPL_ENC_DEC_ERR_STRS = {\n    \"STR_NOT_IMPL_ENC_DEC_SWA\": STR_NOT_IMPL_ENC_DEC_SWA,\n    \"STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\": STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n    \"STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL\":\n    STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL,\n    \"STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP\": STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP,\n    \"STR_NOT_IMPL_ENC_DEC_LORA\": STR_NOT_IMPL_ENC_DEC_LORA,\n    \"STR_NOT_IMPL_ENC_DEC_PP\": STR_NOT_IMPL_ENC_DEC_PP,\n    \"STR_NOT_IMPL_ENC_DEC_MM\": STR_NOT_IMPL_ENC_DEC_MM,\n    \"STR_NOT_IMPL_ENC_DEC_SPEC_DEC\": STR_NOT_IMPL_ENC_DEC_SPEC_DEC,\n    \"STR_NOT_IMPL_ENC_DEC_BACKEND\": STR_NOT_IMPL_ENC_DEC_BACKEND,\n    \"STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER\": STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER,\n}\n\n# Constants related to forcing the attention backend selection\n\n# String name of register which may be set in order to\n# force auto-selection of attention backend by Attention\n# wrapper\nSTR_BACKEND_ENV_VAR: str = \"VLLM_ATTENTION_BACKEND\"\n\n# Possible string values of STR_BACKEND_ENV_VAR\n# register, corresponding to possible backends\nSTR_FLASHINFER_ATTN_VAL: str = \"FLASHINFER\"\nSTR_TORCH_SDPA_ATTN_VAL: str = \"TORCH_SDPA\"\nSTR_ROCM_FLASH_ATTN_VAL: str = \"ROCM_FLASH\"\nSTR_XFORMERS_ATTN_VAL: str = \"XFORMERS\"\nSTR_FLASH_ATTN_VAL: str = \"FLASH_ATTN\"\nSTR_DUAL_CHUNK_FLASH_ATTN_VAL: str = \"DUAL_CHUNK_FLASH_ATTN\"\nSTR_INVALID_VAL: str = \"INVALID\"\n\nGB_bytes = 1_000_000_000\n\"\"\"The number of bytes in one gigabyte (GB).\"\"\"\n\nGiB_bytes = 1 << 30\n\"\"\"The number of bytes in one gibibyte (GiB).\"\"\"\n\nSTR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.half,\n    \"bfloat16\": torch.bfloat16,\n    \"float\": torch.float,\n    \"fp8\": torch.uint8,\n    \"fp8_e4m3\": torch.uint8,\n    \"fp8_e5m2\": torch.uint8,\n    \"int8\": torch.int8,\n}\n\nTORCH_DTYPE_TO_NUMPY_DTYPE = {\n    torch.float16: np.float16,\n    torch.float32: np.float32,\n    torch.float64: np.float64,\n    torch.uint8: np.uint8,\n    torch.int32: np.int32,\n    torch.int64: np.int64,\n}\n\nP = ParamSpec('P')\nT = TypeVar(\"T\")\nU = TypeVar(\"U\")\n\n_K = TypeVar(\"_K\", bound=Hashable)\n_V = TypeVar(\"_V\")\n_T = TypeVar(\"_T\")\n\n\nclass _Sentinel:\n    ...\n\n\nALL_PINNED_SENTINEL = _Sentinel()\n\n\nclass Device(enum.Enum):\n    GPU = enum.auto()\n    CPU = enum.auto()\n\n\nclass LayerBlockType(enum.Enum):\n    attention = \"attention\"\n    mamba = \"mamba\"\n\n\nclass Counter:\n\n    def __init__(self, start: int = 0) -> None:\n        self.counter = start\n\n    def __next__(self) -> int:\n        i = self.counter\n        self.counter += 1\n        return i\n\n    def reset(self) -> None:\n        self.counter = 0\n\n\nclass _MappingOrderCacheView(UserDict[_K, _V]):\n\n    def __init__(self, data: Mapping[_K, _V], ordered_keys: Mapping[_K, None]):\n        super().__init__(data)\n        self.ordered_keys = ordered_keys\n\n    def __iter__(self) -> Iterator[_K]:\n        return iter(self.ordered_keys)\n\n    def keys(self) -> KeysView[_K]:\n        return KeysView(self.ordered_keys)\n\n\nclass CacheInfo(NamedTuple):\n    hits: int\n    total: int\n\n    @property\n    def hit_ratio(self) -> float:\n        if self.total == 0:\n            return 0\n\n        return self.hits / self.total\n\n    def __sub__(self, other: CacheInfo):\n        return CacheInfo(\n            hits=self.hits - other.hits,\n            total=self.total - other.total,\n        )\n\n\nclass LRUCache(cachetools.LRUCache[_K, _V], Generic[_K, _V]):\n\n    def __init__(self,\n                 capacity: float,\n                 getsizeof: Optional[Callable[[_V], float]] = None):\n        super().__init__(capacity, getsizeof)\n\n        self.pinned_items = set[_K]()\n\n        self._hits = 0\n        self._total = 0\n        self._last_info = CacheInfo(hits=0, total=0)\n\n    def __getitem__(self, key: _K, *, update_info: bool = True) -> _V:\n        value = super().__getitem__(key)\n\n        if update_info:\n            self._hits += 1\n            self._total += 1\n\n        return value\n\n    def __delitem__(self, key: _K) -> None:\n        run_on_remove = key in self\n        value = self.__getitem__(key,\n                                 update_info=False)  # type: ignore[call-arg]\n        super().__delitem__(key)\n        if key in self.pinned_items:\n            # Todo: add warning to inform that del pinned item\n            self._unpin(key)\n        if run_on_remove:\n            self._on_remove(key, value)\n\n    @property\n    def cache(self) -> Mapping[_K, _V]:\n        \"\"\"Return the internal cache dictionary in order (read-only).\"\"\"\n        return _MappingOrderCacheView(\n            self._Cache__data,  # type: ignore\n            self.order)\n\n    @property\n    def order(self) -> Mapping[_K, None]:\n        \"\"\"Return the internal order dictionary (read-only).\"\"\"\n        return MappingProxyType(self._LRUCache__order)  # type: ignore\n\n    @property\n    def capacity(self) -> float:\n        return self.maxsize\n\n    @property\n    def usage(self) -> float:\n        if self.maxsize == 0:\n            return 0\n\n        return self.currsize / self.maxsize\n\n    def stat(self, *, delta: bool = False) -> CacheInfo:\n        \"\"\"\n        Gets the cumulative number of hits and queries against this cache.\n\n        If `delta=True`, instead gets these statistics\n        since the last call that also passed `delta=True`.\n        \"\"\"\n        info = CacheInfo(hits=self._hits, total=self._total)\n\n        if delta:\n            info_delta = info - self._last_info\n            self._last_info = info\n            info = info_delta\n\n        return info\n\n    def touch(self, key: _K) -> None:\n        try:\n            self._LRUCache__order.move_to_end(key)  # type: ignore\n        except KeyError:\n            self._LRUCache__order[key] = None  # type: ignore\n\n    @overload\n    def get(self, key: _K, /) -> Optional[_V]:\n        ...\n\n    @overload\n    def get(self, key: _K, /, default: Union[_V, _T]) -> Union[_V, _T]:\n        ...\n\n    def get(self,\n            key: _K,\n            /,\n            default: Optional[Union[_V,\n                                    _T]] = None) -> Optional[Union[_V, _T]]:\n        value: Optional[Union[_V, _T]]\n        if key in self:\n            value = self.__getitem__(\n                key, update_info=False)  # type: ignore[call-arg]\n\n            self._hits += 1\n        else:\n            value = default\n\n        self._total += 1\n        return value\n\n    @overload\n    def pop(self, key: _K) -> _V:\n        ...\n\n    @overload\n    def pop(self, key: _K, default: Union[_V, _T]) -> Union[_V, _T]:\n        ...\n\n    def pop(self,\n            key: _K,\n            default: Optional[Union[_V,\n                                    _T]] = None) -> Optional[Union[_V, _T]]:\n        value: Optional[Union[_V, _T]]\n        if key not in self:\n            return default\n\n        value = self.__getitem__(key,\n                                 update_info=False)  # type: ignore[call-arg]\n        self.__delitem__(key)\n        return value\n\n    def put(self, key: _K, value: _V) -> None:\n        self.__setitem__(key, value)\n\n    def pin(self, key: _K) -> None:\n        \"\"\"\n        Pins a key in the cache preventing it from being\n        evicted in the LRU order.\n        \"\"\"\n        if key not in self:\n            raise ValueError(f\"Cannot pin key: {key} not in cache.\")\n        self.pinned_items.add(key)\n\n    def _unpin(self, key: _K) -> None:\n        \"\"\"\n        Unpins a key in the cache allowing it to be\n        evicted in the LRU order.\n        \"\"\"\n        self.pinned_items.remove(key)\n\n    def _on_remove(self, key: _K, value: Optional[_V]) -> None:\n        pass\n\n    def remove_oldest(self, *, remove_pinned: bool = False) -> None:\n        if len(self) == 0:\n            return\n\n        self.popitem(remove_pinned=remove_pinned)\n\n    def _remove_old_if_needed(self) -> None:\n        while self.currsize > self.capacity:\n            self.remove_oldest()\n\n    def popitem(self, remove_pinned: bool = False):\n        \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n        if not remove_pinned:\n            # pop the oldest item in the cache that is not pinned\n            lru_key = next(\n                (key for key in self.order if key not in self.pinned_items),\n                ALL_PINNED_SENTINEL)\n            if lru_key is ALL_PINNED_SENTINEL:\n                raise RuntimeError(\"All items are pinned, \"\n                                   \"cannot remove oldest from the cache.\")\n        else:\n            lru_key = next(iter(self.order))\n        value = self.pop(cast(_K, lru_key))\n        return (lru_key, value)\n\n    def clear(self) -> None:\n        while len(self) > 0:\n            self.remove_oldest(remove_pinned=True)\n\n        self._hits = 0\n        self._total = 0\n        self._last_info = CacheInfo(hits=0, total=0)\n\n\nclass PyObjectCache:\n    \"\"\"Used to cache python objects to avoid object allocations\n    across scheduler iterations.\n    \"\"\"\n\n    def __init__(self, obj_builder):\n        self._obj_builder = obj_builder\n        self._index = 0\n\n        self._obj_cache = []\n        for _ in range(128):\n            self._obj_cache.append(self._obj_builder())\n\n    def _grow_cache(self):\n        # Double the size of the cache\n        num_objs = len(self._obj_cache)\n        for _ in range(num_objs):\n            self._obj_cache.append(self._obj_builder())\n\n    def get_object(self):\n        \"\"\"Returns a pre-allocated cached object. If there is not enough\n        objects, then the cache size will double.\n        \"\"\"\n        if self._index >= len(self._obj_cache):\n            self._grow_cache()\n            assert self._index < len(self._obj_cache)\n\n        obj = self._obj_cache[self._index]\n        self._index += 1\n\n        return obj\n\n    def reset(self):\n        \"\"\"Makes all cached-objects available for the next scheduler iteration.\n        \"\"\"\n        self._index = 0\n\n\n@cache\ndef get_max_shared_memory_bytes(gpu: int = 0) -> int:\n    \"\"\"Returns the maximum shared memory per thread block in bytes.\"\"\"\n    from vllm import _custom_ops as ops\n    max_shared_mem = (\n        ops.get_max_shared_memory_per_block_device_attribute(gpu))\n    # value 0 will cause MAX_SEQ_LEN become negative and test_attention.py\n    # will fail\n    assert max_shared_mem > 0, \"max_shared_mem can not be zero\"\n    return int(max_shared_mem)\n\n\ndef get_cpu_memory() -> int:\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\n    return psutil.virtual_memory().total\n\n\ndef random_uuid() -> str:\n    return str(uuid.uuid4().hex)\n\n\ndef make_async(\n    func: Callable[P, T],\n    executor: Optional[concurrent.futures.Executor] = None\n) -> Callable[P, Awaitable[T]]:\n    \"\"\"Take a blocking function, and run it on in an executor thread.\n\n    This function prevents the blocking function from blocking the\n    asyncio event loop.\n    The code in this function needs to be thread safe.\n    \"\"\"\n\n    def _async_wrapper(*args: P.args, **kwargs: P.kwargs) -> asyncio.Future:\n        loop = asyncio.get_event_loop()\n        p_func = partial(func, *args, **kwargs)\n        return loop.run_in_executor(executor=executor, func=p_func)\n\n    return _async_wrapper\n\n\ndef _next_task(iterator: AsyncGenerator[T, None],\n               loop: AbstractEventLoop) -> Task:\n    # Can use anext() in python >= 3.10\n    return loop.create_task(iterator.__anext__())  # type: ignore[arg-type]\n\n\nasync def merge_async_iterators(\n    *iterators: AsyncGenerator[T,\n                               None], ) -> AsyncGenerator[tuple[int, T], None]:\n    \"\"\"Merge multiple asynchronous iterators into a single iterator.\n\n    This method handle the case where some iterators finish before others.\n    When it yields, it yields a tuple (i, item) where i is the index of the\n    iterator that yields the item.\n    \"\"\"\n    if len(iterators) == 1:\n        # Fast-path single iterator case.\n        async for item in iterators[0]:\n            yield 0, item\n        return\n\n    loop = asyncio.get_running_loop()\n\n    awaits = {_next_task(pair[1], loop): pair for pair in enumerate(iterators)}\n    try:\n        while awaits:\n            done, _ = await asyncio.wait(awaits.keys(),\n                                         return_when=FIRST_COMPLETED)\n            for d in done:\n                pair = awaits.pop(d)\n                try:\n                    item = await d\n                    i, it = pair\n                    awaits[_next_task(it, loop)] = pair\n                    yield i, item\n                except StopAsyncIteration:\n                    pass\n    finally:\n        # Cancel any remaining iterators\n        for f, (_, it) in awaits.items():\n            with contextlib.suppress(BaseException):\n                f.cancel()\n                await it.aclose()\n\n\nasync def collect_from_async_generator(\n        iterator: AsyncGenerator[T, None]) -> list[T]:\n    \"\"\"Collect all items from an async generator into a list.\"\"\"\n    items = []\n    async for item in iterator:\n        items.append(item)\n    return items\n\n\ndef get_ip() -> str:\n    host_ip = envs.VLLM_HOST_IP\n    if \"HOST_IP\" in os.environ and \"VLLM_HOST_IP\" not in os.environ:\n        logger.warning(\n            \"The environment variable HOST_IP is deprecated and ignored, as\"\n            \" it is often used by Docker and other software to\"\n            \" interact with the container's network stack. Please \"\n            \"use VLLM_HOST_IP instead to set the IP address for vLLM processes\"\n            \" to communicate with each other.\")\n    if host_ip:\n        return host_ip\n\n    # IP is not set, try to get it from the network interface\n\n    # try ipv4\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        s.connect((\"8.8.8.8\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    # try ipv6\n    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        # Google's public DNS server, see\n        # https://developers.google.com/speed/public-dns/docs/using#addresses\n        s.connect((\"2001:4860:4860::8888\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    warnings.warn(\n        \"Failed to get the IP address, using 0.0.0.0 by default.\"\n        \"The value can be set by the environment variable\"\n        \" VLLM_HOST_IP or HOST_IP.\",\n        stacklevel=2)\n    return \"0.0.0.0\"\n\n\ndef is_valid_ipv6_address(address: str) -> bool:\n    try:\n        ipaddress.IPv6Address(address)\n        return True\n    except ValueError:\n        return False\n\n\ndef get_distributed_init_method(ip: str, port: int) -> str:\n    return get_tcp_uri(ip, port)\n\n\ndef get_tcp_uri(ip: str, port: int) -> str:\n    # Brackets are not permitted in ipv4 addresses,\n    # see https://github.com/python/cpython/issues/103848\n    return f\"tcp://[{ip}]:{port}\" if \":\" in ip else f\"tcp://{ip}:{port}\"\n\n\ndef get_open_zmq_ipc_path() -> str:\n    base_rpc_path = envs.VLLM_RPC_BASE_PATH\n    return f\"ipc://{base_rpc_path}/{uuid4()}\"\n\n\ndef get_open_zmq_inproc_path() -> str:\n    return f\"inproc://{uuid4()}\"\n\n\ndef get_open_port() -> int:\n    \"\"\"\n    Get an open port for the vLLM process to listen on.\n    An edge case to handle, is when we run data parallel,\n    we need to avoid ports that are potentially used by\n    the data parallel master process.\n    Right now we reserve 10 ports for the data parallel master\n    process. Currently it uses 2 ports.\n    \"\"\"\n    if \"VLLM_DP_MASTER_PORT\" in os.environ:\n        dp_master_port = envs.VLLM_DP_MASTER_PORT\n        reserved_port_range = range(dp_master_port, dp_master_port + 10)\n        while True:\n            candidate_port = _get_open_port()\n            if candidate_port not in reserved_port_range:\n                return candidate_port\n    return _get_open_port()\n\n\ndef _get_open_port() -> int:\n    port = envs.VLLM_PORT\n    if port is not None:\n        while True:\n            try:\n                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                    s.bind((\"\", port))\n                    return port\n            except OSError:\n                port += 1  # Increment port number if already in use\n                logger.info(\"Port %d is already in use, trying port %d\",\n                            port - 1, port)\n    # try ipv4\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n    except OSError:\n        # try ipv6\n        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n\n\ndef find_process_using_port(port: int) -> Optional[psutil.Process]:\n    # TODO: We can not check for running processes with network\n    # port on macOS. Therefore, we can not have a full graceful shutdown\n    # of vLLM. For now, let's not look for processes in this case.\n    # Ref: https://www.florianreinhard.de/accessdenied-in-psutil/\n    if sys.platform.startswith(\"darwin\"):\n        return None\n\n    for conn in psutil.net_connections():\n        if conn.laddr.port == port:\n            try:\n                return psutil.Process(conn.pid)\n            except psutil.NoSuchProcess:\n                return None\n    return None\n\n\ndef update_environment_variables(envs: dict[str, str]):\n    for k, v in envs.items():\n        if k in os.environ and os.environ[k] != v:\n            logger.warning(\n                \"Overwriting environment variable %s \"\n                \"from '%s' to '%s'\", k, os.environ[k], v)\n        os.environ[k] = v\n\n\ndef chunk_list(lst: list[T], chunk_size: int):\n    \"\"\"Yield successive chunk_size chunks from lst.\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i:i + chunk_size]\n\n\ndef cdiv(a: int, b: int) -> int:\n    \"\"\"Ceiling division.\"\"\"\n    return -(a // -b)\n\n\ndef next_power_of_2(n) -> int:\n    \"\"\"The next power of 2 (inclusive)\"\"\"\n    if n < 1:\n        return 1\n    return 1 << (n - 1).bit_length()\n\n\ndef round_up(x: int, y: int) -> int:\n    return ((x + y - 1) // y) * y\n\n\ndef round_down(x: int, y: int) -> int:\n    return (x // y) * y\n\n\ndef _generate_random_fp8(\n    tensor: torch.Tensor,\n    low: float,\n    high: float,\n) -> None:\n    # NOTE(zhaoyang): Due to NaN and Inf representation for fp8 data type,\n    # it may occur Inf or NaN if we directly use torch.randint\n    # to generate random data for fp8 data.\n    # For example, s.11111.00 in fp8e5m2 format represents Inf.\n    #     | E4M3        | E5M2\n    #-----|-------------|-------------------\n    # Inf | N/A         | s.11111.00\n    # NaN | s.1111.111  | s.11111.{01,10,11}\n    from vllm import _custom_ops as ops\n    tensor_tmp = torch.empty_like(tensor, dtype=torch.float16)\n    tensor_tmp.uniform_(low, high)\n    ops.convert_fp8(tensor, tensor_tmp)\n    del tensor_tmp\n\n\ndef get_kv_cache_torch_dtype(\n        cache_dtype: Optional[Union[str, torch.dtype]],\n        model_dtype: Optional[Union[str, torch.dtype]] = None) -> torch.dtype:\n    if isinstance(cache_dtype, str):\n        if cache_dtype == \"auto\":\n            if isinstance(model_dtype, str):\n                torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[model_dtype]\n            elif isinstance(model_dtype, torch.dtype):\n                torch_dtype = model_dtype\n            else:\n                raise ValueError(f\"Invalid model dtype: {model_dtype}\")\n        elif cache_dtype in [\"half\", \"bfloat16\", \"float\"]:\n            torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_dtype]\n        elif cache_dtype == \"fp8\":\n            torch_dtype = torch.uint8\n        else:\n            raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    elif isinstance(cache_dtype, torch.dtype):\n        torch_dtype = cache_dtype\n    else:\n        raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    return torch_dtype\n\n\ndef create_kv_caches_with_random_flash(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: Optional[int] = None,\n    device: Optional[str] = \"cuda\",\n    cache_layout: Optional[str] = \"NHD\",\n) -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n    from vllm.platforms import current_platform\n    current_platform.seed_everything(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n    generic_kv_cache_shape = (num_blocks, 2, block_size, num_heads, head_size)\n    assert cache_layout in (\"NHD\", \"HND\")\n    stride_order = (0, 1, 2, 3, 4) if cache_layout == \"NHD\" else (0, 1, 3, 2,\n                                                                  4)\n\n    kv_cache_allocation_shape = tuple(generic_kv_cache_shape[i]\n                                      for i in stride_order)\n    scale = head_size**-0.5\n\n    key_caches: list[torch.Tensor] = []\n    value_caches: list[torch.Tensor] = []\n\n    for _ in range(num_layers):\n        key_value_cache = torch.empty(size=kv_cache_allocation_shape,\n                                      dtype=torch_dtype,\n                                      device=device).permute(*stride_order)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_value_cache[:, 0])\n        value_caches.append(key_value_cache[:, 1])\n    return key_caches, value_caches\n\n\ndef create_kv_caches_with_random(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: Optional[int] = None,\n    device: Optional[str] = \"cuda\",\n) -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n\n    if cache_dtype == \"fp8\" and head_size % 16:\n        raise ValueError(\n            f\"Does not support key cache of type fp8 with head_size {head_size}\"\n        )\n    from vllm.platforms import current_platform\n    current_platform.seed_everything(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n\n    scale = head_size**-0.5\n    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_caches: list[torch.Tensor] = []\n    for _ in range(num_layers):\n        key_cache = torch.empty(size=key_cache_shape,\n                                dtype=torch_dtype,\n                                device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_cache)\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_caches: list[torch.Tensor] = []\n    for _ in range(num_layers):\n        value_cache = torch.empty(size=value_cache_shape,\n                                  dtype=torch_dtype,\n                                  device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support value cache of type {cache_dtype}\")\n        value_caches.append(value_cache)\n    return key_caches, value_caches\n\n\n@cache\ndef is_pin_memory_available() -> bool:\n    from vllm.platforms import current_platform\n    return current_platform.is_pin_memory_available()\n\n\n@cache\ndef is_uva_available() -> bool:\n    \"\"\"Check if Unified Virtual Addressing (UVA) is available.\"\"\"\n    # UVA requires pinned memory.\n    # TODO: Add more requirements for UVA if needed.\n    return is_pin_memory_available()\n\n\nclass DeviceMemoryProfiler:\n\n    def __init__(self, device: Optional[torch.types.Device] = None):\n        self.device = device\n\n    def current_memory_usage(self) -> float:\n        # Return the memory usage in bytes.\n        from vllm.platforms import current_platform\n        return current_platform.get_current_memory_usage(self.device)\n\n    def __enter__(self):\n        self.initial_memory = self.current_memory_usage()\n        # This allows us to call methods of the context manager if needed\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.final_memory = self.current_memory_usage()\n        self.consumed_memory = self.final_memory - self.initial_memory\n\n        # Force garbage collection\n        gc.collect()\n\n\ndef make_ndarray_with_pad(\n    x: list[list[T]],\n    pad: T,\n    dtype: npt.DTypeLike,\n    *,\n    max_len: Optional[int] = None,\n) -> npt.NDArray:\n    \"\"\"\n    Make a padded array from 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    if max_len is None:\n        # Unlike for most functions, map is faster than a genexpr over `len`\n        max_len = max(map(len, x), default=0)\n\n    padded_x = np.full((len(x), max_len), pad, dtype=dtype)\n    for ind, blocktb in enumerate(x):\n        assert len(blocktb) <= max_len\n        padded_x[ind, :len(blocktb)] = blocktb\n\n    return padded_x\n\n\ndef make_tensor_with_pad(\n    x: list[list[T]],\n    pad: T,\n    dtype: torch.dtype,\n    *,\n    max_len: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    pin_memory: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Make a padded tensor from 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE[dtype]\n    padded_x = make_ndarray_with_pad(x, pad, np_dtype, max_len=max_len)\n\n    tensor = torch.from_numpy(padded_x).to(device)\n    if pin_memory:\n        tensor = tensor.pin_memory()\n\n    return tensor\n\n\ndef async_tensor_h2d(\n    data: list,\n    dtype: torch.dtype,\n    target_device: Union[str, torch.device],\n    pin_memory: bool,\n) -> torch.Tensor:\n    \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n    return t.to(device=target_device, non_blocking=True)\n\n\ndef get_dtype_size(dtype: torch.dtype) -> int:\n    \"\"\"Get the size of the data type in bytes.\"\"\"\n    return torch.tensor([], dtype=dtype).element_size()\n\n\n# `collections` helpers\ndef is_list_of(\n    value: object,\n    typ: Union[type[T], tuple[type[T], ...]],\n    *,\n    check: Literal[\"first\", \"all\"] = \"first\",\n) -> TypeIs[list[T]]:\n    if not isinstance(value, list):\n        return False\n\n    if check == \"first\":\n        return len(value) == 0 or isinstance(value[0], typ)\n    elif check == \"all\":\n        return all(isinstance(v, typ) for v in value)\n\n    assert_never(check)\n\n\ndef flatten_2d_lists(lists: Iterable[Iterable[T]]) -> list[T]:\n    \"\"\"Flatten a list of lists to a single list.\"\"\"\n    return [item for sublist in lists for item in sublist]\n\n\ndef full_groupby(values: Iterable[_V], *, key: Callable[[_V], _K]):\n    \"\"\"\n    Unlike {class}`itertools.groupby`, groups are not broken by\n    non-contiguous data.\n    \"\"\"\n    groups = defaultdict[_K, list[_V]](list)\n\n    for value in values:\n        groups[key(value)].append(value)\n\n    return groups.items()\n\n\n# TODO: This function can be removed if transformer_modules classes are\n# serialized by value when communicating between processes\ndef init_cached_hf_modules() -> None:\n    \"\"\"\n    Lazy initialization of the Hugging Face modules.\n    \"\"\"\n    from transformers.dynamic_module_utils import init_hf_modules\n    init_hf_modules()\n\n\n@cache\ndef find_library(lib_name: str) -> str:\n    \"\"\"\n    Find the library file in the system.\n    `lib_name` is full filename, with both prefix and suffix.\n    This function resolves `lib_name` to the full path of the library.\n    \"\"\"\n    # Adapted from https://github.com/openai/triton/blob/main/third_party/nvidia/backend/driver.py#L19 # noqa\n    # According to https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard\n    # `/sbin/ldconfig` should exist in all Linux systems.\n    # `/sbin/ldconfig` searches the library in the system\n    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n    # each line looks like the following:\n    # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n    locs = [line.split()[-1] for line in libs.splitlines() if lib_name in line]\n    # `LD_LIBRARY_PATH` searches the library in the user-defined paths\n    env_ld_library_path = envs.LD_LIBRARY_PATH\n    if not locs and env_ld_library_path:\n        locs = [\n            os.path.join(dir, lib_name)\n            for dir in env_ld_library_path.split(\":\")\n            if os.path.exists(os.path.join(dir, lib_name))\n        ]\n    if not locs:\n        raise ValueError(f\"Cannot find {lib_name} in the system.\")\n    return locs[0]\n\n\ndef find_nccl_library() -> str:\n    \"\"\"\n    We either use the library file specified by the `VLLM_NCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.VLLM_NCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\n            \"Found nccl from environment variable VLLM_NCCL_SO_PATH=%s\",\n            so_file)\n    else:\n        if torch.version.cuda is not None:\n            so_file = \"libnccl.so.2\"\n        elif torch.version.hip is not None:\n            so_file = \"librccl.so.1\"\n        else:\n            raise ValueError(\"NCCL only supports CUDA and ROCm backends.\")\n        logger.info(\"Found nccl from library %s\", so_file)\n    return so_file\n\n\nprev_set_stream = torch.cuda.set_stream\n\n_current_stream = None\n\n\ndef _patched_set_stream(stream: torch.cuda.Stream) -> None:\n    global _current_stream\n    _current_stream = stream\n    prev_set_stream(stream)\n\n\ntorch.cuda.set_stream = _patched_set_stream\n\n\ndef current_stream() -> torch.cuda.Stream:\n    \"\"\"\n    replace `torch.cuda.current_stream()` with `vllm.utils.current_stream()`.\n    it turns out that `torch.cuda.current_stream()` is quite expensive,\n    as it will construct a new stream object at each call.\n    here we patch `torch.cuda.set_stream` to keep track of the current stream\n    directly, so that we can avoid calling `torch.cuda.current_stream()`.\n\n    the underlying hypothesis is that we do not call `torch._C._cuda_setStream`\n    from C/C++ code.\n    \"\"\"\n    from vllm.platforms import current_platform\n    global _current_stream\n    if _current_stream is None:\n        # when this function is called before any stream is set,\n        # we return the default stream.\n        # On ROCm using the default 0 stream in combination with RCCL\n        # is hurting performance. Therefore creating a dedicated stream\n        # per process\n        _current_stream = torch.cuda.Stream() if current_platform.is_rocm(\n        ) else torch.cuda.current_stream()\n    return _current_stream\n\n\ndef enable_trace_function_call_for_thread(vllm_config: VllmConfig) -> None:\n    \"\"\"Set up function tracing for the current thread,\n    if enabled via the VLLM_TRACE_FUNCTION environment variable\n    \"\"\"\n\n    if envs.VLLM_TRACE_FUNCTION:\n        tmp_dir = tempfile.gettempdir()\n        # add username to tmp_dir to avoid permission issues\n        tmp_dir = os.path.join(tmp_dir, getpass.getuser())\n        filename = (f\"VLLM_TRACE_FUNCTION_for_process_{os.getpid()}\"\n                    f\"_thread_{threading.get_ident()}_\"\n                    f\"at_{datetime.datetime.now()}.log\").replace(\" \", \"_\")\n        log_path = os.path.join(tmp_dir, \"vllm\",\n                                f\"vllm-instance-{vllm_config.instance_id}\",\n                                filename)\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n        enable_trace_function_call(log_path)\n\n\n# `functools` helpers\ndef identity(value: T, **kwargs) -> T:\n    \"\"\"Returns the first provided value.\"\"\"\n    return value\n\n\nF = TypeVar('F', bound=Callable[..., Any])\n\n\ndef deprecate_args(\n    start_index: int,\n    is_deprecated: Union[bool, Callable[[], bool]] = True,\n    additional_message: Optional[str] = None,\n) -> Callable[[F], F]:\n\n    if not callable(is_deprecated):\n        is_deprecated = partial(identity, is_deprecated)\n\n    def wrapper(fn: F) -> F:\n\n        params = inspect.signature(fn).parameters\n        pos_types = (\n            inspect.Parameter.POSITIONAL_ONLY,\n            inspect.Parameter.POSITIONAL_OR_KEYWORD,\n        )\n        pos_kws = [\n            kw for kw, param in params.items() if param.kind in pos_types\n        ]\n\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            if is_deprecated():\n                deprecated_args = pos_kws[start_index:len(args)]\n                if deprecated_args:\n                    msg = (\n                        f\"The positional arguments {deprecated_args} are \"\n                        \"deprecated and will be removed in a future update.\")\n                    if additional_message is not None:\n                        msg += f\" {additional_message}\"\n\n                    warnings.warn(\n                        DeprecationWarning(msg),\n                        stacklevel=3,  # The inner function takes up one level\n                    )\n\n            return fn(*args, **kwargs)\n\n        return inner  # type: ignore\n\n    return wrapper\n\n\ndef deprecate_kwargs(\n    *kws: str,\n    is_deprecated: Union[bool, Callable[[], bool]] = True,\n    additional_message: Optional[str] = None,\n) -> Callable[[F], F]:\n    deprecated_kws = set(kws)\n\n    if not callable(is_deprecated):\n        is_deprecated = partial(identity, is_deprecated)\n\n    def wrapper(fn: F) -> F:\n\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            if is_deprecated():\n                deprecated_kwargs = kwargs.keys() & deprecated_kws\n                if deprecated_kwargs:\n                    msg = (\n                        f\"The keyword arguments {deprecated_kwargs} are \"\n                        \"deprecated and will be removed in a future update.\")\n                    if additional_message is not None:\n                        msg += f\" {additional_message}\"\n\n                    warnings.warn(\n                        DeprecationWarning(msg),\n                        stacklevel=3,  # The inner function takes up one level\n                    )\n\n            return fn(*args, **kwargs)\n\n        return inner  # type: ignore\n\n    return wrapper\n\n\n@lru_cache(maxsize=8)\ndef _cuda_device_count_stateless(\n        cuda_visible_devices: Optional[str] = None) -> int:\n    # Note: cuda_visible_devices is not used, but we keep it as an argument for\n    # LRU Cache purposes.\n\n    # Code below is based on\n    # https://github.com/pytorch/pytorch/blob/\n    # c1cd946818442aca8c7f812b16d187ce1586c3bc/\n    # torch/cuda/__init__.py#L831C1-L831C17\n    import torch.cuda\n    import torch.version\n\n    from vllm.platforms import current_platform\n    if not torch.cuda._is_compiled():\n        return 0\n    if current_platform.is_rocm():\n        # ROCm uses amdsmi instead of nvml for stateless device count\n        # This requires a sufficiently modern version of Torch 2.4.0\n        raw_count = torch.cuda._device_count_amdsmi() if (hasattr(\n            torch.cuda, \"_device_count_amdsmi\")) else -1\n    else:\n        raw_count = torch.cuda._device_count_nvml()\n    r = torch._C._cuda_getDeviceCount() if raw_count < 0 else raw_count\n    return r\n\n\ndef cuda_device_count_stateless() -> int:\n    \"\"\"Get number of CUDA devices, caching based on the value of\n    CUDA_VISIBLE_DEVICES at the time of call.\n\n    This should be used instead of torch.cuda.device_count()\n    unless CUDA_VISIBLE_DEVICES has already been set to the desired\n    value.\"\"\"\n\n    # This can be removed and simply replaced with torch.cuda.get_device_count\n    # after https://github.com/pytorch/pytorch/pull/122815 is released.\n    return _cuda_device_count_stateless(envs.CUDA_VISIBLE_DEVICES)\n\n\ndef cuda_is_initialized() -> bool:\n    \"\"\"Check if CUDA is initialized.\"\"\"\n    if not torch.cuda._is_compiled():\n        return False\n    return torch.cuda.is_initialized()\n\n\ndef cuda_get_device_properties(device,\n                               names: Sequence[str],\n                               init_cuda=False) -> tuple[Any, ...]:\n    \"\"\"Get specified CUDA device property values without initializing CUDA in\n    the current process.\"\"\"\n    if init_cuda or cuda_is_initialized():\n        props = torch.cuda.get_device_properties(device)\n        return tuple(getattr(props, name) for name in names)\n\n    # Run in subprocess to avoid initializing CUDA as a side effect.\n    mp_ctx = multiprocessing.get_context(\"fork\")\n    with ProcessPoolExecutor(max_workers=1, mp_context=mp_ctx) as executor:\n        return executor.submit(cuda_get_device_properties, device, names,\n                               True).result()\n\n\ndef weak_bind(bound_method: Callable[..., Any], ) -> Callable[..., None]:\n    \"\"\"Make an instance method that weakly references\n    its associated instance and no-ops once that\n    instance is collected.\"\"\"\n    ref = weakref.ref(bound_method.__self__)  # type: ignore[attr-defined]\n    unbound = bound_method.__func__  # type: ignore[attr-defined]\n\n    def weak_bound(*args, **kwargs) -> None:\n        if inst := ref():\n            unbound(inst, *args, **kwargs)\n\n    return weak_bound\n\n\n#From: https://stackoverflow.com/a/4104188/2749989\ndef run_once(f: Callable[P, None]) -> Callable[P, None]:\n\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> None:\n        if not wrapper.has_run:  # type: ignore[attr-defined]\n            wrapper.has_run = True  # type: ignore[attr-defined]\n            return f(*args, **kwargs)\n\n    wrapper.has_run = False  # type: ignore[attr-defined]\n    return wrapper\n\n\nclass StoreBoolean(Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if values.lower() == \"true\":\n            setattr(namespace, self.dest, True)\n        elif values.lower() == \"false\":\n            setattr(namespace, self.dest, False)\n        else:\n            raise ValueError(f\"Invalid boolean value: {values}. \"\n                             \"Expected 'true' or 'false'.\")\n\n\nclass SortedHelpFormatter(ArgumentDefaultsHelpFormatter):\n    \"\"\"SortedHelpFormatter that sorts arguments by their option strings.\"\"\"\n\n    def _split_lines(self, text, width):\n        \"\"\"\n        1. Sentences split across lines have their single newlines removed.\n        2. Paragraphs and explicit newlines are split into separate lines.\n        3. Each line is wrapped to the specified width (width of terminal).\n        \"\"\"\n        # The patterns also include whitespace after the newline\n        single_newline = re.compile(r\"(?<!\\n)\\n(?!\\n)\\s*\")\n        multiple_newlines = re.compile(r\"\\n{2,}\\s*\")\n        text = single_newline.sub(' ', text)\n        lines = re.split(multiple_newlines, text)\n        return sum([textwrap.wrap(line, width) for line in lines], [])\n\n    def add_arguments(self, actions):\n        actions = sorted(actions, key=lambda x: x.option_strings)\n        super().add_arguments(actions)\n\n\nclass FlexibleArgumentParser(ArgumentParser):\n    \"\"\"ArgumentParser that allows both underscore and dash in names.\"\"\"\n\n    _deprecated: set[Action] = set()\n\n    def __init__(self, *args, **kwargs):\n        # Set the default 'formatter_class' to SortedHelpFormatter\n        if 'formatter_class' not in kwargs:\n            kwargs['formatter_class'] = SortedHelpFormatter\n        super().__init__(*args, **kwargs)\n\n    if sys.version_info < (3, 13):\n        # Enable the deprecated kwarg for Python 3.12 and below\n\n        def parse_known_args(self, args=None, namespace=None):\n            namespace, args = super().parse_known_args(args, namespace)\n            for action in FlexibleArgumentParser._deprecated:\n                if (hasattr(namespace, dest := action.dest)\n                        and getattr(namespace, dest) != action.default):\n                    logger.warning_once(\"argument '%s' is deprecated\", dest)\n            return namespace, args\n\n        def add_argument(self, *args, **kwargs):\n            deprecated = kwargs.pop(\"deprecated\", False)\n            action = super().add_argument(*args, **kwargs)\n            if deprecated:\n                FlexibleArgumentParser._deprecated.add(action)\n            return action\n\n        class _FlexibleArgumentGroup(_ArgumentGroup):\n\n            def add_argument(self, *args, **kwargs):\n                deprecated = kwargs.pop(\"deprecated\", False)\n                action = super().add_argument(*args, **kwargs)\n                if deprecated:\n                    FlexibleArgumentParser._deprecated.add(action)\n                return action\n\n        def add_argument_group(self, *args, **kwargs):\n            group = self._FlexibleArgumentGroup(self, *args, **kwargs)\n            self._action_groups.append(group)\n            return group\n\n    def parse_args(  # type: ignore[override]\n        self,\n        args: list[str] | None = None,\n        namespace: Namespace | None = None,\n    ):\n        if args is None:\n            args = sys.argv[1:]\n\n        # Check for --model in command line arguments first\n        if args and args[0] == \"serve\":\n            model_in_cli_args = any(arg == '--model' for arg in args)\n\n            if model_in_cli_args:\n                raise ValueError(\n                    \"With `vllm serve`, you should provide the model as a \"\n                    \"positional argument or in a config file instead of via \"\n                    \"the `--model` option.\")\n\n        if '--config' in args:\n            args = self._pull_args_from_config(args)\n\n        # Convert underscores to dashes and vice versa in argument names\n        processed_args = []\n        for arg in args:\n            if arg.startswith('--'):\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    key = '--' + key[len('--'):].replace('_', '-')\n                    processed_args.append(f'{key}={value}')\n                else:\n                    processed_args.append('--' +\n                                          arg[len('--'):].replace('_', '-'))\n            elif arg.startswith('-O') and arg != '-O' and len(arg) == 2:\n                # allow -O flag to be used without space, e.g. -O3\n                processed_args.append('-O')\n                processed_args.append(arg[2:])\n            else:\n                processed_args.append(arg)\n\n        def create_nested_dict(keys: list[str], value: str):\n            \"\"\"Creates a nested dictionary from a list of keys and a value.\n\n            For example, `keys = [\"a\", \"b\", \"c\"]` and `value = 1` will create:\n            `{\"a\": {\"b\": {\"c\": 1}}}`\n            \"\"\"\n            nested_dict: Any = value\n            for key in reversed(keys):\n                nested_dict = {key: nested_dict}\n            return nested_dict\n\n        def recursive_dict_update(original: dict, update: dict):\n            \"\"\"Recursively updates a dictionary with another dictionary.\"\"\"\n            for k, v in update.items():\n                if isinstance(v, dict) and isinstance(original.get(k), dict):\n                    recursive_dict_update(original[k], v)\n                else:\n                    original[k] = v\n\n        delete = set()\n        dict_args: dict[str, dict] = defaultdict(dict)\n        for i, processed_arg in enumerate(processed_args):\n            if processed_arg.startswith(\"--\") and \".\" in processed_arg:\n                if \"=\" in processed_arg:\n                    processed_arg, value = processed_arg.split(\"=\", 1)\n                    if \".\" not in processed_arg:\n                        # False positive, . was only in the value\n                        continue\n                else:\n                    value = processed_args[i + 1]\n                    delete.add(i + 1)\n                key, *keys = processed_arg.split(\".\")\n                # Merge all values with the same key into a single dict\n                arg_dict = create_nested_dict(keys, value)\n                recursive_dict_update(dict_args[key], arg_dict)\n                delete.add(i)\n        # Filter out the dict args we set to None\n        processed_args = [\n            a for i, a in enumerate(processed_args) if i not in delete\n        ]\n        # Add the dict args back as if they were originally passed as JSON\n        for dict_arg, dict_value in dict_args.items():\n            processed_args.append(dict_arg)\n            processed_args.append(json.dumps(dict_value))\n\n        return super().parse_args(processed_args, namespace)\n\n    def check_port(self, value):\n        try:\n            value = int(value)\n        except ValueError:\n            msg = \"Port must be an integer\"\n            raise ArgumentTypeError(msg) from None\n\n        if not (1024 <= value <= 65535):\n            raise ArgumentTypeError(\"Port must be between 1024 and 65535\")\n\n        return value\n\n    def _pull_args_from_config(self, args: list[str]) -> list[str]:\n        \"\"\"Method to pull arguments specified in the config file\n        into the command-line args variable.\n\n        The arguments in config file will be inserted between\n        the argument list.\n\n        example:\n        ```yaml\n            port: 12323\n            tensor-parallel-size: 4\n        ```\n        ```python\n        $: vllm {serve,chat,complete} \"facebook/opt-12B\" \\\n            --config config.yaml -tp 2\n        $: args = [\n            \"serve,chat,complete\",\n            \"facebook/opt-12B\",\n            '--config', 'config.yaml',\n            '-tp', '2'\n        ]\n        $: args = [\n            \"serve,chat,complete\",\n            \"facebook/opt-12B\",\n            '--port', '12323',\n            '--tensor-parallel-size', '4',\n            '-tp', '2'\n            ]\n        ```\n\n        Please note how the config args are inserted after the sub command.\n        this way the order of priorities is maintained when these are args\n        parsed by super().\n        \"\"\"\n        assert args.count(\n            '--config') <= 1, \"More than one config file specified!\"\n\n        index = args.index('--config')\n        if index == len(args) - 1:\n            raise ValueError(\"No config file specified! \\\n                             Please check your command-line arguments.\")\n\n        file_path = args[index + 1]\n\n        config_args = self._load_config_file(file_path)\n\n        # 0th index is for {serve,chat,complete}\n        # optionally followed by model_tag (only for serve)\n        # followed by config args\n        # followed by rest of cli args.\n        # maintaining this order will enforce the precedence\n        # of cli > config > defaults\n        if args[0] == \"serve\":\n            model_in_cli = len(args) > 1 and not args[1].startswith('-')\n            model_in_config = any(arg == '--model' for arg in config_args)\n\n            if not model_in_cli and not model_in_config:\n                raise ValueError(\n                    \"No model specified! Please specify model either \"\n                    \"as a positional argument or in a config file.\")\n\n            if model_in_cli:\n                # Model specified as positional arg, keep CLI version\n                args = [args[0]] + [\n                    args[1]\n                ] + config_args + args[2:index] + args[index + 2:]\n            else:\n                # No model in CLI, use config if available\n                args = [args[0]\n                        ] + config_args + args[1:index] + args[index + 2:]\n        else:\n            args = [args[0]] + config_args + args[1:index] + args[index + 2:]\n\n        return args\n\n    def _load_config_file(self, file_path: str) -> list[str]:\n        \"\"\"Loads a yaml file and returns the key value pairs as a\n        flattened list with argparse like pattern\n        ```yaml\n            port: 12323\n            tensor-parallel-size: 4\n        ```\n        returns:\n            processed_args: list[str] = [\n                '--port': '12323',\n                '--tensor-parallel-size': '4'\n            ]\n        \"\"\"\n        extension: str = file_path.split('.')[-1]\n        if extension not in ('yaml', 'yml'):\n            raise ValueError(\n                \"Config file must be of a yaml/yml type.\\\n                              %s supplied\", extension)\n\n        # only expecting a flat dictionary of atomic types\n        processed_args: list[str] = []\n\n        config: dict[str, Union[int, str]] = {}\n        try:\n            with open(file_path) as config_file:\n                config = yaml.safe_load(config_file)\n        except Exception as ex:\n            logger.error(\n                \"Unable to read the config file at %s. \\\n                Make sure path is correct\", file_path)\n            raise ex\n\n        store_boolean_arguments = [\n            action.dest for action in self._actions\n            if isinstance(action, StoreBoolean)\n        ]\n\n        for key, value in config.items():\n            if isinstance(value, bool) and key not in store_boolean_arguments:\n                if value:\n                    processed_args.append('--' + key)\n            else:\n                processed_args.append('--' + key)\n                processed_args.append(str(value))\n\n        return processed_args\n\n\nasync def _run_task_with_lock(task: Callable, lock: asyncio.Lock, *args,\n                              **kwargs):\n    \"\"\"Utility function to run async task in a lock\"\"\"\n    async with lock:\n        return await task(*args, **kwargs)\n\n\ndef supports_kw(\n    callable: Callable[..., object],\n    kw_name: str,\n    *,\n    requires_kw_only: bool = False,\n    allow_var_kwargs: bool = True,\n) -> bool:\n    \"\"\"Check if a keyword is a valid kwarg for a callable; if requires_kw_only\n    disallows kwargs names that can also be positional arguments.\n    \"\"\"\n    params = inspect.signature(callable).parameters\n    if not params:\n        return False\n\n    param_val = params.get(kw_name)\n\n    # Types where the it may be valid, i.e., explicitly defined & nonvariadic\n    passable_kw_types = set((inspect.Parameter.POSITIONAL_ONLY,\n                             inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                             inspect.Parameter.KEYWORD_ONLY))\n\n    if param_val:\n        is_sig_param = param_val.kind in passable_kw_types\n        # We want kwargs only, but this is passable as a positional arg\n        if (requires_kw_only and is_sig_param\n                and param_val.kind != inspect.Parameter.KEYWORD_ONLY):\n            return False\n        if ((requires_kw_only\n             and param_val.kind == inspect.Parameter.KEYWORD_ONLY)\n                or (not requires_kw_only and is_sig_param)):\n            return True\n\n    # If we're okay with var-kwargs, it's supported as long as\n    # the kw_name isn't something like *args, **kwargs\n    if allow_var_kwargs:\n        # Get the last param; type is ignored here because params is a proxy\n        # mapping, but it wraps an ordered dict, and they appear in order.\n        # Ref: https://docs.python.org/3/library/inspect.html#inspect.Signature.parameters\n        last_param = params[next(reversed(params))]  # type: ignore\n        return (last_param.kind == inspect.Parameter.VAR_KEYWORD\n                and last_param.name != kw_name)\n    return False\n\n\ndef resolve_mm_processor_kwargs(\n    init_kwargs: Optional[Mapping[str, object]],\n    inference_kwargs: Optional[Mapping[str, object]],\n    callable: Callable[..., object],\n    *,\n    requires_kw_only: bool = True,\n    allow_var_kwargs: bool = False,\n) -> dict[str, Any]:\n    \"\"\"Applies filtering to eliminate invalid mm_processor_kwargs, i.e.,\n    those who are not explicit keywords to the given callable (of one is\n    given; otherwise no filtering is done), then merges the kwarg dicts,\n    giving priority to inference_kwargs if there are any collisions.\n\n    In the case that no kwarg overrides are provided, returns an empty\n    dict so that it can still be kwarg expanded into the callable later on.\n\n    If allow_var_kwargs=True, allows for things that can be expanded into\n    kwargs as long as they aren't naming collision for var_kwargs or potential\n    positional arguments.\n    \"\"\"\n    # Filter inference time multimodal processor kwargs provided\n    runtime_mm_kwargs = get_allowed_kwarg_only_overrides(\n        callable,\n        overrides=inference_kwargs,\n        requires_kw_only=requires_kw_only,\n        allow_var_kwargs=allow_var_kwargs,\n    )\n\n    # Filter init time multimodal processor kwargs provided\n    init_mm_kwargs = get_allowed_kwarg_only_overrides(\n        callable,\n        overrides=init_kwargs,\n        requires_kw_only=requires_kw_only,\n        allow_var_kwargs=allow_var_kwargs,\n    )\n\n    # Merge the final processor kwargs, prioritizing inference\n    # time values over the initialization time values.\n    mm_processor_kwargs = {**init_mm_kwargs, **runtime_mm_kwargs}\n    return mm_processor_kwargs\n\n\ndef get_allowed_kwarg_only_overrides(\n    callable: Callable[..., object],\n    overrides: Optional[Mapping[str, object]],\n    *,\n    requires_kw_only: bool = True,\n    allow_var_kwargs: bool = False,\n) -> dict[str, Any]:\n    \"\"\"\n    Given a callable which has one or more keyword only params and a dict\n    mapping param names to values, drop values that can be not be kwarg\n    expanded to overwrite one or more keyword-only args. This is used in a\n    few places to handle custom processor overrides for multimodal models,\n    e.g., for profiling when processor options provided by the user\n    may affect the number of mm tokens per instance.\n\n    Args:\n        callable: Callable which takes 0 or more keyword only arguments.\n                  If None is provided, all overrides names are allowed.\n        overrides: Potential overrides to be used when invoking the callable.\n        allow_var_kwargs: Allows overrides that are expandable for var kwargs.\n\n    Returns:\n        Dictionary containing the kwargs to be leveraged which may be used\n        to overwrite one or more keyword only arguments when invoking the\n        callable.\n    \"\"\"\n    if not overrides:\n        return {}\n\n    # Drop any mm_processor_kwargs provided by the user that\n    # are not kwargs, unless it can fit it var_kwargs param\n    filtered_overrides = {\n        kwarg_name: val\n        for kwarg_name, val in overrides.items()\n        if supports_kw(callable,\n                       kwarg_name,\n                       requires_kw_only=requires_kw_only,\n                       allow_var_kwargs=allow_var_kwargs)\n    }\n\n    # If anything is dropped, log a warning\n    dropped_keys = overrides.keys() - filtered_overrides.keys()\n    if dropped_keys:\n        if requires_kw_only:\n            logger.warning(\n                \"The following intended overrides are not keyword-only args \"\n                \"and will be dropped: %s\", dropped_keys)\n        else:\n            logger.warning(\n                \"The following intended overrides are not keyword args \"\n                \"and will be dropped: %s\", dropped_keys)\n\n    return filtered_overrides\n\n\n# Using dynamo with vLLM doesn't really work well with PyTorch versions < 2.4.0.\n# In particular, the FakeScalarType is not supported for earlier versions of\n# PyTorch which breaks dynamo for any ops registered using ScalarType.\ndef supports_dynamo() -> bool:\n    base_torch_version = Version(Version(torch.__version__).base_version)\n    return base_torch_version >= Version(\"2.4.0\")\n\n\n# Some backends use pytorch version < 2.4.0 which doesn't\n# support `torch.library.custom_op`.\ndef supports_custom_op() -> bool:\n    return hasattr(torch.library, \"custom_op\")\n\n\nclass AtomicCounter:\n    \"\"\"An atomic, thread-safe counter\"\"\"\n\n    def __init__(self, initial=0):\n        \"\"\"Initialize a new atomic counter to given initial value\"\"\"\n        self._value = initial\n        self._lock = threading.Lock()\n\n    def inc(self, num=1):\n        \"\"\"Atomically increment the counter by num and return the new value\"\"\"\n        with self._lock:\n            self._value += num\n            return self._value\n\n    def dec(self, num=1):\n        \"\"\"Atomically decrement the counter by num and return the new value\"\"\"\n        with self._lock:\n            self._value -= num\n            return self._value\n\n    @property\n    def value(self):\n        return self._value\n\n\n# Adapted from: https://stackoverflow.com/a/47212782/5082708\nclass LazyDict(Mapping[str, T], Generic[T]):\n\n    def __init__(self, factory: dict[str, Callable[[], T]]):\n        self._factory = factory\n        self._dict: dict[str, T] = {}\n\n    def __getitem__(self, key: str) -> T:\n        if key not in self._dict:\n            if key not in self._factory:\n                raise KeyError(key)\n            self._dict[key] = self._factory[key]()\n        return self._dict[key]\n\n    def __setitem__(self, key: str, value: Callable[[], T]):\n        self._factory[key] = value\n\n    def __iter__(self):\n        return iter(self._factory)\n\n    def __len__(self):\n        return len(self._factory)\n\n\nclass ClassRegistry(UserDict[Type[T], _V]):\n\n    def __getitem__(self, key: Type[T]) -> _V:\n        for cls in key.mro():\n            if cls in self.data:\n                return self.data[cls]\n\n        raise KeyError(key)\n\n    def __contains__(self, key: object) -> bool:\n        return self.contains(key)\n\n    def contains(self, key: object, *, strict: bool = False) -> bool:\n        if not isinstance(key, type):\n            return False\n\n        if strict:\n            return key in self.data\n\n        return any(cls in self.data for cls in key.mro())\n\n\ndef weak_ref_tensor(tensor: Any) -> Any:\n    \"\"\"\n    Create a weak reference to a tensor.\n    The new tensor will share the same data as the original tensor,\n    but will not keep the original tensor alive.\n    \"\"\"\n    if isinstance(tensor, torch.Tensor):\n        return torch.ops._C.weak_ref_tensor(tensor)\n    else:\n        return tensor\n\n\ndef weak_ref_tensors(\n    tensors: Union[torch.Tensor, list[torch.Tensor], tuple[torch.Tensor]]\n) -> Union[torch.Tensor, list[Any], tuple[Any], Any]:\n    \"\"\"\n    Convenience function to create weak references to tensors,\n    for single tensor, list of tensors or tuple of tensors.\n    \"\"\"\n    if isinstance(tensors, torch.Tensor):\n        return weak_ref_tensor(tensors)\n    if isinstance(tensors, list):\n        return [weak_ref_tensor(t) for t in tensors]\n    if isinstance(tensors, tuple):\n        return tuple(weak_ref_tensor(t) for t in tensors)\n    raise ValueError(\"Invalid type for tensors\")\n\n\ndef get_cuda_view_from_cpu_tensor(cpu_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get a CUDA view of a CPU tensor using Unified Virtual Addressing (UVA).\n    \"\"\"\n    assert cpu_tensor.is_pinned(), \"CPU tensor must be pinned\"\n    return torch.ops._C.get_cuda_view_from_cpu_tensor(cpu_tensor)\n\n\ndef is_in_doc_build() -> bool:\n    try:\n        from sphinx.ext.autodoc.mock import _MockModule\n        return isinstance(zmq, _MockModule)\n    except ModuleNotFoundError:\n        return False\n\n\ndef import_from_path(module_name: str, file_path: Union[str, os.PathLike]):\n    \"\"\"\n    Import a Python file according to its file path.\n\n    Based on the official recipe:\n    https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n    \"\"\"\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    if spec is None:\n        raise ModuleNotFoundError(f\"No module named '{module_name}'\")\n\n    assert spec.loader is not None\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n    return module\n\n\n@cache\ndef get_vllm_optional_dependencies():\n    metadata = importlib.metadata.metadata(\"vllm\")\n    requirements = metadata.get_all(\"Requires-Dist\", [])\n    extras = metadata.get_all(\"Provides-Extra\", [])\n\n    return {\n        extra: [\n            re.split(r\";|>=|<=|==\", req)[0] for req in requirements\n            if req.endswith(f'extra == \"{extra}\"')\n        ]\n        for extra in extras\n    }\n\n\nclass _PlaceholderBase:\n    \"\"\"\n    Disallows downstream usage of placeholder modules.\n\n    We need to explicitly override each dunder method because\n    {meth}`__getattr__` is not called when they are accessed.\n\n    :::{seealso}\n    [Special method lookup](https://docs.python.org/3/reference/datamodel.html#special-lookup)\n    :::\n    \"\"\"\n\n    def __getattr__(self, key: str) -> Never:\n        \"\"\"\n        The main class should implement this to throw an error\n        for attribute accesses representing downstream usage.\n        \"\"\"\n        raise NotImplementedError\n\n    # [Basic customization]\n\n    def __lt__(self, other: object):\n        return self.__getattr__(\"__lt__\")\n\n    def __le__(self, other: object):\n        return self.__getattr__(\"__le__\")\n\n    def __eq__(self, other: object):\n        return self.__getattr__(\"__eq__\")\n\n    def __ne__(self, other: object):\n        return self.__getattr__(\"__ne__\")\n\n    def __gt__(self, other: object):\n        return self.__getattr__(\"__gt__\")\n\n    def __ge__(self, other: object):\n        return self.__getattr__(\"__ge__\")\n\n    def __hash__(self):\n        return self.__getattr__(\"__hash__\")\n\n    def __bool__(self):\n        return self.__getattr__(\"__bool__\")\n\n    # [Callable objects]\n\n    def __call__(self, *args: object, **kwargs: object):\n        return self.__getattr__(\"__call__\")\n\n    # [Container types]\n\n    def __len__(self):\n        return self.__getattr__(\"__len__\")\n\n    def __getitem__(self, key: object):\n        return self.__getattr__(\"__getitem__\")\n\n    def __setitem__(self, key: object, value: object):\n        return self.__getattr__(\"__setitem__\")\n\n    def __delitem__(self, key: object):\n        return self.__getattr__(\"__delitem__\")\n\n    # __missing__ is optional according to __getitem__ specification,\n    # so it is skipped\n\n    # __iter__ and __reversed__ have a default implementation\n    # based on __len__ and __getitem__, so they are skipped.\n\n    # [Numeric Types]\n\n    def __add__(self, other: object):\n        return self.__getattr__(\"__add__\")\n\n    def __sub__(self, other: object):\n        return self.__getattr__(\"__sub__\")\n\n    def __mul__(self, other: object):\n        return self.__getattr__(\"__mul__\")\n\n    def __matmul__(self, other: object):\n        return self.__getattr__(\"__matmul__\")\n\n    def __truediv__(self, other: object):\n        return self.__getattr__(\"__truediv__\")\n\n    def __floordiv__(self, other: object):\n        return self.__getattr__(\"__floordiv__\")\n\n    def __mod__(self, other: object):\n        return self.__getattr__(\"__mod__\")\n\n    def __divmod__(self, other: object):\n        return self.__getattr__(\"__divmod__\")\n\n    def __pow__(self, other: object, modulo: object = ...):\n        return self.__getattr__(\"__pow__\")\n\n    def __lshift__(self, other: object):\n        return self.__getattr__(\"__lshift__\")\n\n    def __rshift__(self, other: object):\n        return self.__getattr__(\"__rshift__\")\n\n    def __and__(self, other: object):\n        return self.__getattr__(\"__and__\")\n\n    def __xor__(self, other: object):\n        return self.__getattr__(\"__xor__\")\n\n    def __or__(self, other: object):\n        return self.__getattr__(\"__or__\")\n\n    # r* and i* methods have lower priority than\n    # the methods for left operand so they are skipped\n\n    def __neg__(self):\n        return self.__getattr__(\"__neg__\")\n\n    def __pos__(self):\n        return self.__getattr__(\"__pos__\")\n\n    def __abs__(self):\n        return self.__getattr__(\"__abs__\")\n\n    def __invert__(self):\n        return self.__getattr__(\"__invert__\")\n\n    # __complex__, __int__ and __float__ have a default implementation\n    # based on __index__, so they are skipped.\n\n    def __index__(self):\n        return self.__getattr__(\"__index__\")\n\n    def __round__(self, ndigits: object = ...):\n        return self.__getattr__(\"__round__\")\n\n    def __trunc__(self):\n        return self.__getattr__(\"__trunc__\")\n\n    def __floor__(self):\n        return self.__getattr__(\"__floor__\")\n\n    def __ceil__(self):\n        return self.__getattr__(\"__ceil__\")\n\n    # [Context managers]\n\n    def __enter__(self):\n        return self.__getattr__(\"__enter__\")\n\n    def __exit__(self, *args: object, **kwargs: object):\n        return self.__getattr__(\"__exit__\")\n\n\nclass PlaceholderModule(_PlaceholderBase):\n    \"\"\"\n    A placeholder object to use when a module does not exist.\n\n    This enables more informative errors when trying to access attributes\n    of a module that does not exists.\n    \"\"\"\n\n    def __init__(self, name: str) -> None:\n        super().__init__()\n\n        # Apply name mangling to avoid conflicting with module attributes\n        self.__name = name\n\n    def placeholder_attr(self, attr_path: str):\n        return _PlaceholderModuleAttr(self, attr_path)\n\n    def __getattr__(self, key: str):\n        name = self.__name\n\n        try:\n            importlib.import_module(name)\n        except ImportError as exc:\n            for extra, names in get_vllm_optional_dependencies().items():\n                if name in names:\n                    msg = f\"Please install vllm[{extra}] for {extra} support\"\n                    raise ImportError(msg) from exc\n\n            raise exc\n\n        raise AssertionError(\"PlaceholderModule should not be used \"\n                             \"when the original module can be imported\")\n\n\nclass _PlaceholderModuleAttr(_PlaceholderBase):\n\n    def __init__(self, module: PlaceholderModule, attr_path: str) -> None:\n        super().__init__()\n\n        # Apply name mangling to avoid conflicting with module attributes\n        self.__module = module\n        self.__attr_path = attr_path\n\n    def placeholder_attr(self, attr_path: str):\n        return _PlaceholderModuleAttr(self.__module,\n                                      f\"{self.__attr_path}.{attr_path}\")\n\n    def __getattr__(self, key: str):\n        getattr(self.__module, f\"{self.__attr_path}.{key}\")\n\n        raise AssertionError(\"PlaceholderModule should not be used \"\n                             \"when the original module can be imported\")\n\n\n# create a library to hold the custom op\nvllm_lib = Library(\"vllm\", \"FRAGMENT\")  # noqa\n\n\ndef direct_register_custom_op(\n        op_name: str,\n        op_func: Callable,\n        mutates_args: list[str],\n        fake_impl: Optional[Callable] = None,\n        target_lib: Optional[Library] = None,\n        dispatch_key: str = \"CUDA\",\n        tags: Tuple[torch.Tag, ...] = (),\n):\n    \"\"\"\n    `torch.library.custom_op` can have significant overhead because it\n    needs to consider complicated dispatching logic. This function\n    directly registers a custom op and dispatches it to the CUDA backend.\n    See https://gist.github.com/youkaichao/ecbea9ec9fc79a45d2adce1784d7a9a5\n    for more details.\n\n    By default, the custom op is registered to the vLLM library. If you\n    want to register it to a different library, you can pass the library\n    object to the `target_lib` argument.\n\n    IMPORTANT: the lifetime of the operator is tied to the lifetime of the\n    library object. If you want to bind the operator to a different library,\n    make sure the library object is alive when the operator is used.\n    \"\"\"\n    if not supports_custom_op():\n        from vllm.platforms import current_platform\n        assert not current_platform.is_cuda_alike(), (\n            \"cuda platform needs torch>=2.4 to support custom op, \"\n            \"chances are you are using an old version of pytorch \"\n            \"or a custom build of pytorch. It is recommended to \"\n            \"use vLLM in a fresh new environment and let it install \"\n            \"the required dependencies.\")\n        return\n\n    import torch.library\n    if hasattr(torch.library, \"infer_schema\"):\n        schema_str = torch.library.infer_schema(op_func,\n                                                mutates_args=mutates_args)\n    else:\n        # for pytorch 2.4\n        import torch._custom_op.impl\n        schema_str = torch._custom_op.impl.infer_schema(op_func, mutates_args)\n    my_lib = target_lib or vllm_lib\n    my_lib.define(op_name + schema_str, tags=tags)\n    my_lib.impl(op_name, op_func, dispatch_key=dispatch_key)\n    if fake_impl is not None:\n        my_lib._register_fake(op_name, fake_impl)\n\n\ndef resolve_obj_by_qualname(qualname: str) -> Any:\n    \"\"\"\n    Resolve an object by its fully qualified name.\n    \"\"\"\n    module_name, obj_name = qualname.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, obj_name)\n\n\ndef kill_process_tree(pid: int):\n    \"\"\"\n    Kills all descendant processes of the given pid by sending SIGKILL.\n\n    Args:\n        pid (int): Process ID of the parent process\n    \"\"\"\n    try:\n        parent = psutil.Process(pid)\n    except psutil.NoSuchProcess:\n        return\n\n    # Get all children recursively\n    children = parent.children(recursive=True)\n\n    # Send SIGKILL to all children first\n    for child in children:\n        with contextlib.suppress(ProcessLookupError):\n            os.kill(child.pid, signal.SIGKILL)\n\n    # Finally kill the parent\n    with contextlib.suppress(ProcessLookupError):\n        os.kill(pid, signal.SIGKILL)\n\n\n@dataclass\nclass MemorySnapshot:\n    \"\"\"Memory snapshot.\"\"\"\n    torch_peak: int = 0\n    cuda_memory: int = 0\n    torch_memory: int = 0\n    non_torch_memory: int = 0\n    timestamp: float = 0.0\n    auto_measure: bool = True\n\n    def __post_init__(self):\n        if self.auto_measure:\n            self.measure()\n\n    def measure(self):\n        # we measure the torch peak memory usage via allocated_bytes,\n        # rather than `torch.cuda.memory_reserved()` .\n        # After `torch.cuda.reset_peak_memory_stats()`,\n        # `torch.cuda.memory_reserved()` will keep growing, and only shrink\n        # when we call `torch.cuda.empty_cache()` or OOM happens.\n        self.torch_peak = torch.cuda.memory_stats().get(\n            \"allocated_bytes.all.peak\", 0)\n\n        self.cuda_memory = torch.cuda.mem_get_info(\n        )[1] - torch.cuda.mem_get_info()[0]\n\n        # torch.cuda.memory_reserved() is how many bytes\n        # PyTorch gets from cuda (by calling cudaMalloc, etc.)\n        # this is used to measure the non-torch memory usage\n        self.torch_memory = torch.cuda.memory_reserved()\n\n        self.non_torch_memory = self.cuda_memory - self.torch_memory\n        self.timestamp = time.time()\n\n    def __sub__(self, other: MemorySnapshot) -> MemorySnapshot:\n        return MemorySnapshot(\n            torch_peak=self.torch_peak - other.torch_peak,\n            cuda_memory=self.cuda_memory - other.cuda_memory,\n            torch_memory=self.torch_memory - other.torch_memory,\n            non_torch_memory=self.non_torch_memory - other.non_torch_memory,\n            timestamp=self.timestamp - other.timestamp,\n            auto_measure=False,\n        )\n\n\n@dataclass\nclass MemoryProfilingResult:\n    \"\"\"Memory profiling result. All numbers are in bytes.\n    \"\"\"\n    non_kv_cache_memory: int = 0\n    torch_peak_increase: int = 0\n    non_torch_increase: int = 0\n    weights_memory: float = 0\n    before_create: MemorySnapshot = field(default_factory=MemorySnapshot)\n    before_profile: MemorySnapshot = field(default_factory=MemorySnapshot)\n    after_profile: MemorySnapshot = field(default_factory=MemorySnapshot)\n    profile_time: float = 0.0\n\n\n@contextlib.contextmanager\ndef memory_profiling(\n        baseline_snapshot: MemorySnapshot,\n        weights_memory: int) -> Generator[MemoryProfilingResult, None, None]:\n    \"\"\"Memory profiling context manager.\n    baseline_snapshot: the memory snapshot before the current vLLM instance.\n    weights_memory: memory used by PyTorch when loading the model weights.\n        Note that, before loading the model weights, we also initialize the device\n        and distributed environment, which may consume some memory. This part is not\n        included in the weights_memory because PyTorch does not control it.\n\n    The memory in one GPU can be classified into 3 categories:\n    1. memory used by anything other than the current vLLM instance.\n    2. memory used by torch in the current vLLM instance.\n    3. memory used in the current vLLM instance, but not by torch.\n\n    A quantitive example:\n\n    Before creating the current vLLM instance:\n        category 1: 1 GiB\n        category 2: 0 GiB\n        category 3: 0 GiB\n\n    After creating the current vLLM instance and loading the model,\n    (i.e. before profiling):\n        category 1: 1 GiB\n        category 2: 2 GiB (model weights take 2 GiB)\n        category 3: 0.5 GiB (memory used by NCCL)\n\n    During profiling (peak):\n        category 1: 1 GiB\n        category 2: 4 GiB (peak activation tensors take 2 GiB)\n        category 3: 1 GiB (memory used by NCCL + buffers for some attention backends)\n\n    After profiling:\n        category 1: 1 GiB\n        category 2: 3 GiB (after garbage-collecting activation tensors)\n        category 3: 1 GiB (memory used by NCCL + buffers for some attention backends)\n\n    In this case, non-kv cache takes 5 GiB in total, including:\n    a. 2 GiB used by the model weights (category 2)\n    b. 2 GiB reserved for the peak activation tensors (category 2)\n    c. 1 GiB used by non-torch components (category 3)\n\n    The memory used for loading weights (a.) is directly given from the argument `weights_memory`.\n\n    The increase of `torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"]` during profiling gives (b.).\n\n    The increase of `non_torch_memory` from creating the current vLLM instance until after profiling to get (c.).\n    \"\"\" # noqa\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    result = MemoryProfilingResult()\n\n    result.before_create = baseline_snapshot\n    # the part of memory used for holding the model weights\n    result.weights_memory = weights_memory\n\n    result.before_profile.measure()\n\n    yield result\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    result.after_profile.measure()\n\n    diff_profile = result.after_profile - result.before_profile\n    diff_from_create = result.after_profile - result.before_create\n    result.torch_peak_increase = diff_profile.torch_peak\n    result.non_torch_increase = diff_from_create.non_torch_memory\n    result.profile_time = diff_profile.timestamp\n    result.non_kv_cache_memory = result.non_torch_increase + result.torch_peak_increase + result.weights_memory  # noqa\n\n\n# Adapted from: https://github.com/sgl-project/sglang/blob/v0.4.1/python/sglang/srt/utils.py#L630 # noqa: E501\ndef set_ulimit(target_soft_limit=65535):\n    if sys.platform.startswith('win'):\n        logger.info(\"Windows detected, skipping ulimit adjustment.\")\n        return\n\n    import resource\n    resource_type = resource.RLIMIT_NOFILE\n    current_soft, current_hard = resource.getrlimit(resource_type)\n\n    if current_soft < target_soft_limit:\n        try:\n            resource.setrlimit(resource_type,\n                               (target_soft_limit, current_hard))\n        except ValueError as e:\n            logger.warning(\n                \"Found ulimit of %s and failed to automatically increase \"\n                \"with error %s. This can cause fd limit errors like \"\n                \"`OSError: [Errno 24] Too many open files`. Consider \"\n                \"increasing with ulimit -n\", current_soft, e)\n\n\n# Adapted from: https://github.com/sgl-project/sglang/blob/v0.4.1/python/sglang/utils.py#L28 # noqa: E501\ndef get_exception_traceback():\n    etype, value, tb = sys.exc_info()\n    err_str = \"\".join(traceback.format_exception(etype, value, tb))\n    return err_str\n\n\ndef split_zmq_path(path: str) -> Tuple[str, str, str]:\n    \"\"\"Split a zmq path into its parts.\"\"\"\n    parsed = urlparse(path)\n    if not parsed.scheme:\n        raise ValueError(f\"Invalid zmq path: {path}\")\n\n    scheme = parsed.scheme\n    host = parsed.hostname or \"\"\n    port = str(parsed.port or \"\")\n\n    if scheme == \"tcp\" and not all((host, port)):\n        # The host and port fields are required for tcp\n        raise ValueError(f\"Invalid zmq path: {path}\")\n\n    if scheme != \"tcp\" and port:\n        # port only makes sense with tcp\n        raise ValueError(f\"Invalid zmq path: {path}\")\n\n    return scheme, host, port\n\n\ndef make_zmq_path(scheme: str, host: str, port: Optional[int] = None) -> str:\n    \"\"\"Make a ZMQ path from its parts.\n\n    Args:\n        scheme: The ZMQ transport scheme (e.g. tcp, ipc, inproc).\n        host: The host - can be an IPv4 address, IPv6 address, or hostname.\n        port: Optional port number, only used for TCP sockets.\n\n    Returns:\n        A properly formatted ZMQ path string.\n    \"\"\"\n    if not port:\n        return f\"{scheme}://{host}\"\n    if is_valid_ipv6_address(host):\n        return f\"{scheme}://[{host}]:{port}\"\n    return f\"{scheme}://{host}:{port}\"\n\n\n# Adapted from: https://github.com/sgl-project/sglang/blob/v0.4.1/python/sglang/srt/utils.py#L783 # noqa: E501\ndef make_zmq_socket(\n    ctx: Union[zmq.asyncio.Context, zmq.Context],  # type: ignore[name-defined]\n    path: str,\n    socket_type: Any,\n    bind: Optional[bool] = None,\n    identity: Optional[bytes] = None,\n) -> Union[zmq.Socket, zmq.asyncio.Socket]:  # type: ignore[name-defined]\n    \"\"\"Make a ZMQ socket with the proper bind/connect semantics.\"\"\"\n\n    mem = psutil.virtual_memory()\n    socket = ctx.socket(socket_type)\n\n    # Calculate buffer size based on system memory\n    total_mem = mem.total / 1024**3\n    available_mem = mem.available / 1024**3\n    # For systems with substantial memory (>32GB total, >16GB available):\n    # - Set a large 0.5GB buffer to improve throughput\n    # For systems with less memory:\n    # - Use system default (-1) to avoid excessive memory consumption\n    if total_mem > 32 and available_mem > 16:\n        buf_size = int(0.5 * 1024**3)  # 0.5GB in bytes\n    else:\n        buf_size = -1  # Use system default buffer size\n\n    if bind is None:\n        bind = socket_type != zmq.PUSH\n\n    if socket_type in (zmq.PULL, zmq.DEALER, zmq.ROUTER):\n        socket.setsockopt(zmq.RCVHWM, 0)\n        socket.setsockopt(zmq.RCVBUF, buf_size)\n\n    if socket_type in (zmq.PUSH, zmq.DEALER, zmq.ROUTER):\n        socket.setsockopt(zmq.SNDHWM, 0)\n        socket.setsockopt(zmq.SNDBUF, buf_size)\n\n    if identity is not None:\n        socket.setsockopt(zmq.IDENTITY, identity)\n\n    # Determine if the path is a TCP socket with an IPv6 address.\n    # Enable IPv6 on the zmq socket if so.\n    scheme, host, _ = split_zmq_path(path)\n    if scheme == \"tcp\" and is_valid_ipv6_address(host):\n        socket.setsockopt(zmq.IPV6, 1)\n\n    if bind:\n        socket.bind(path)\n    else:\n        socket.connect(path)\n\n    return socket\n\n\n@contextlib.contextmanager\ndef zmq_socket_ctx(\n    path: str,\n    socket_type: Any,\n    bind: Optional[bool] = None,\n    linger: int = 0,\n    identity: Optional[bytes] = None,\n) -> Iterator[zmq.Socket]:\n    \"\"\"Context manager for a ZMQ socket\"\"\"\n\n    ctx = zmq.Context()  # type: ignore[attr-defined]\n    try:\n        yield make_zmq_socket(ctx,\n                              path,\n                              socket_type,\n                              bind=bind,\n                              identity=identity)\n    except KeyboardInterrupt:\n        logger.debug(\"Got Keyboard Interrupt.\")\n\n    finally:\n        ctx.destroy(linger=linger)\n\n\ndef is_in_ray_actor():\n    \"\"\"Check if we are in a Ray actor.\"\"\"\n\n    try:\n        import ray\n        return (ray.is_initialized()\n                and ray.get_runtime_context().get_actor_id() is not None)\n    except ImportError:\n        return False\n\n\ndef _maybe_force_spawn():\n    \"\"\"Check if we need to force the use of the `spawn` multiprocessing start\n    method.\n    \"\"\"\n    if os.environ.get(\"VLLM_WORKER_MULTIPROC_METHOD\") == \"spawn\":\n        return\n\n    reason = None\n    if cuda_is_initialized():\n        reason = \"CUDA is initialized\"\n    elif is_in_ray_actor():\n        # even if we choose to spawn, we need to pass the ray address\n        # to the subprocess so that it knows how to connect to the ray cluster.\n        # env vars are inherited by subprocesses, even if we use spawn.\n        import ray\n        os.environ[\"RAY_ADDRESS\"] = ray.get_runtime_context().gcs_address\n        reason = \"In a Ray actor and can only be spawned\"\n\n    if reason is not None:\n        logger.warning(\n            \"We must use the `spawn` multiprocessing start method. \"\n            \"Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. \"\n            \"See https://docs.vllm.ai/en/latest/getting_started/\"\n            \"troubleshooting.html#python-multiprocessing \"\n            \"for more information. Reason: %s\", reason)\n        os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n\n\ndef get_mp_context():\n    \"\"\"Get a multiprocessing context with a particular method (spawn or fork).\n    By default we follow the value of the VLLM_WORKER_MULTIPROC_METHOD to\n    determine the multiprocessing method (default is fork). However, under\n    certain conditions, we may enforce spawn and override the value of\n    VLLM_WORKER_MULTIPROC_METHOD.\n    \"\"\"\n    _maybe_force_spawn()\n    mp_method = envs.VLLM_WORKER_MULTIPROC_METHOD\n    return multiprocessing.get_context(mp_method)\n\n\ndef bind_kv_cache(\n        ctx: dict[str, Any],\n        kv_cache: list[list[torch.Tensor]],  # [virtual_engine][layer_index]\n) -> None:\n    # Bind the kv_cache tensor to Attention modules, similar to\n    # ctx[layer_name].kv_cache[ve]=kv_cache[ve][extract_layer_index(layer_name)]\n    # Special things handled here:\n    # 1. Some models have non-attention layers, e.g., Jamba\n    # 2. Pipeline parallelism, each rank only has a subset of layers\n    # 3. Encoder attention has no kv cache\n    # 4. Encoder-decoder models, encoder-decoder attention and decoder-only\n    #    attention of the same layer (e.g., bart's decoder.layers.1.self_attn\n    #    and decoder.layers.1.encoder_attn) is mapped to the same kv cache\n    #    tensor\n    from vllm.attention import AttentionType\n    from vllm.model_executor.models.utils import extract_layer_index\n    layer_need_kv_cache = [\n        layer_name for layer_name in ctx\n        if (hasattr(ctx[layer_name], 'attn_type') and ctx[layer_name].attn_type\n            in (AttentionType.DECODER, AttentionType.ENCODER_DECODER))\n    ]\n    layer_index_sorted = sorted(\n        set(\n            extract_layer_index(layer_name)\n            for layer_name in layer_need_kv_cache))\n    for layer_name in layer_need_kv_cache:\n        kv_cache_idx = layer_index_sorted.index(\n            extract_layer_index(layer_name))\n        forward_ctx = ctx[layer_name]\n        assert len(forward_ctx.kv_cache) == len(kv_cache)\n        for ve, ve_kv_cache in enumerate(kv_cache):\n            forward_ctx.kv_cache[ve] = ve_kv_cache[kv_cache_idx]\n\n\ndef run_method(obj: Any, method: Union[str, bytes, Callable], args: tuple[Any],\n               kwargs: dict[str, Any]) -> Any:\n    \"\"\"\n    Run a method of an object with the given arguments and keyword arguments.\n    If the method is string, it will be converted to a method using getattr.\n    If the method is serialized bytes and will be deserialized using\n    cloudpickle.\n    If the method is a callable, it will be called directly.\n    \"\"\"\n    if isinstance(method, bytes):\n        func = partial(cloudpickle.loads(method), obj)\n    elif isinstance(method, str):\n        try:\n            func = getattr(obj, method)\n        except AttributeError:\n            raise NotImplementedError(f\"Method {method!r} is not\"\n                                      \" implemented.\") from None\n    else:\n        func = partial(method, obj)  # type: ignore\n    return func(*args, **kwargs)\n\n\ndef import_pynvml():\n    \"\"\"\n    Historical comments:\n\n    libnvml.so is the library behind nvidia-smi, and\n    pynvml is a Python wrapper around it. We use it to get GPU\n    status without initializing CUDA context in the current process.\n    Historically, there are two packages that provide pynvml:\n    - `nvidia-ml-py` (https://pypi.org/project/nvidia-ml-py/): The official\n        wrapper. It is a dependency of vLLM, and is installed when users\n        install vLLM. It provides a Python module named `pynvml`.\n    - `pynvml` (https://pypi.org/project/pynvml/): An unofficial wrapper.\n        Prior to version 12.0, it also provides a Python module `pynvml`,\n        and therefore conflicts with the official one. What's worse,\n        the module is a Python package, and has higher priority than\n        the official one which is a standalone Python file.\n        This causes errors when both of them are installed.\n        Starting from version 12.0, it migrates to a new module\n        named `pynvml_utils` to avoid the conflict.\n    It is so confusing that many packages in the community use the\n    unofficial one by mistake, and we have to handle this case.\n    For example, `nvcr.io/nvidia/pytorch:24.12-py3` uses the unofficial\n    one, and it will cause errors, see the issue\n    https://github.com/vllm-project/vllm/issues/12847 for example.\n    After all the troubles, we decide to copy the official `pynvml`\n    module to our codebase, and use it directly.\n    \"\"\"\n    import vllm.third_party.pynvml as pynvml\n    return pynvml\n\n\ndef warn_for_unimplemented_methods(cls: type[T]) -> type[T]:\n    \"\"\"\n    A replacement for `abc.ABC`.\n    When we use `abc.ABC`, subclasses will fail to instantiate\n    if they do not implement all abstract methods.\n    Here, we only require `raise NotImplementedError` in the\n    base class, and log a warning if the method is not implemented\n    in the subclass.\n    \"\"\"\n\n    original_init = cls.__init__\n\n    def find_unimplemented_methods(self: object):\n        unimplemented_methods = []\n        for attr_name in dir(self):\n            # bypass inner method\n            if attr_name.startswith('_'):\n                continue\n\n            try:\n                attr = getattr(self, attr_name)\n                # get the func of callable method\n                if callable(attr):\n                    attr_func = attr.__func__\n            except AttributeError:\n                continue\n            src = inspect.getsource(attr_func)\n            if \"NotImplementedError\" in src:\n                unimplemented_methods.append(attr_name)\n        if unimplemented_methods:\n            method_names = ','.join(unimplemented_methods)\n            msg = (f\"Methods {method_names} not implemented in {self}\")\n            logger.warning(msg)\n\n    @wraps(original_init)\n    def wrapped_init(self, *args, **kwargs) -> None:\n        original_init(self, *args, **kwargs)\n        find_unimplemented_methods(self)\n\n    type.__setattr__(cls, '__init__', wrapped_init)\n    return cls\n\n\nclass LazyLoader(types.ModuleType):\n    \"\"\"\n    LazyLoader module borrowed from Tensorflow\n    https://github.com/tensorflow/tensorflow/blob/main/tensorflow/python/util/lazy_loader.py\n    with a addition of \"module caching\".\n\n    Lazily import a module, mainly to avoid pulling in large dependencies.\n    Modules such as `xgrammar` might do additional side effects, so we\n    only want to use this when it is needed, delaying all eager effects\n    \"\"\"\n\n    def __init__(\n        self,\n        local_name: str,\n        parent_module_globals: dict[str, Any],\n        name: str,\n    ):\n        self._local_name = local_name\n        self._parent_module_globals = parent_module_globals\n        self._module: types.ModuleType | None = None\n\n        super().__init__(str(name))\n\n    def _load(self) -> types.ModuleType:\n        # Import the target module and insert it into the parent's namespace\n        try:\n            module = importlib.import_module(self.__name__)\n            self._parent_module_globals[self._local_name] = module\n            # The additional add to sys.modules\n            # ensures library is actually loaded.\n            sys.modules[self._local_name] = module\n        except ModuleNotFoundError as err:\n            raise err from None\n\n        # Update this object's dict so that if someone keeps a\n        # reference to the LazyLoader, lookups are efficient\n        # (__getattr__ is only called on lookups that fail).\n        self.__dict__.update(module.__dict__)\n        return module\n\n    def __getattr__(self, item: Any) -> Any:\n        if self._module is None:\n            self._module = self._load()\n        return getattr(self._module, item)\n\n    def __dir__(self) -> list[str]:\n        if self._module is None:\n            self._module = self._load()\n        return dir(self._module)\n\n\ndef swap_dict_values(obj: dict[_K, _V], key1: _K, key2: _K) -> None:\n    \"\"\"\n    Helper function to swap values for two keys\n    \"\"\"\n    v1 = obj.get(key1)\n    v2 = obj.get(key2)\n    if v1 is not None:\n        obj[key2] = v1\n    else:\n        obj.pop(key2, None)\n    if v2 is not None:\n        obj[key1] = v2\n    else:\n        obj.pop(key1, None)\n\n\n@contextlib.contextmanager\ndef cprofile_context(save_file: Optional[str] = None):\n    \"\"\"Run a cprofile\n\n    Args:\n        save_file: path to save the profile result. \"1\" or\n          None will result in printing to stdout.\n    \"\"\"\n    import cProfile\n\n    prof = cProfile.Profile()\n    prof.enable()\n\n    try:\n        yield\n    finally:\n        prof.disable()\n        if save_file and save_file != \"1\":\n            prof.dump_stats(save_file)\n        else:\n            prof.print_stats(sort=\"cumtime\")\n\n\ndef cprofile(save_file: Optional[str] = None, enabled: bool = True):\n    \"\"\"Decorator to profile a Python method using cProfile.\n\n    Args:\n        save_file: Path to save the profile result.\n            If \"1\", None, or \"\", results will be printed to stdout.\n        enabled: Set to false to turn this into a no-op\n    \"\"\"\n\n    def decorator(func: Callable):\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if not enabled:\n                # If profiling is disabled, just call the function directly.\n                return func(*args, **kwargs)\n\n            with cprofile_context(save_file):\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n# Only relevant for models using ALiBi (e.g, MPT)\ndef check_use_alibi(model_config: ModelConfig) -> bool:\n    return (getattr(model_config.hf_text_config, \"alibi\", False)  # Falcon\n            or (\"BloomForCausalLM\" in getattr(model_config.hf_config,\n                                              \"architectures\", []))  # Bloom\n            or getattr(model_config.hf_text_config, \"position_encoding_type\",\n                       \"\") == \"alibi\"  # codellm_1b_alibi\n            or\n            (hasattr(model_config.hf_text_config, \"attn_config\")  # MPT\n             and model_config.hf_text_config.attn_config.get(\"alibi\", False)))\n\n\ndef sha256(input) -> int:\n    \"\"\"Hash any picklable Python object using SHA-256.\n\n    The input is serialized using pickle before hashing, which allows\n    arbitrary Python objects to be used. Note that this function does\n    not use a hash seed\u2014if you need one, prepend it explicitly to the input.\n\n    Args:\n        input: Any picklable Python object.\n\n    Returns:\n        An integer representing the SHA-256 hash of the serialized input.\n    \"\"\"\n    input_bytes = pickle.dumps(input, protocol=pickle.HIGHEST_PROTOCOL)\n    return int.from_bytes(hashlib.sha256(input_bytes).digest(),\n                          byteorder=\"big\")\n\n\ndef is_torch_equal_or_newer(target: str) -> bool:\n    \"\"\"Check if the installed torch version is >= the target version.\n\n    Args:\n        target: a version string, like \"2.6.0\".\n\n    Returns:\n        Whether the condition meets.\n    \"\"\"\n    try:\n        torch_version = version.parse(str(torch.__version__))\n        return torch_version >= version.parse(target)\n    except Exception:\n        # Fallback to PKG-INFO to load the package info, needed by the doc gen.\n        return Version(importlib.metadata.version('torch')) >= Version(target)\n", 2834], "/home/jeromeku/vllm/vllm/v1/executor/abstract.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom concurrent.futures import Future\nfrom typing import Callable, Union\n\nimport torch\nimport torch.distributed as dist\n\nfrom vllm.config import VllmConfig\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.executor.uniproc_executor import (  # noqa\n    ExecutorWithExternalLauncher as ExecutorWithExternalLauncherV0)\nfrom vllm.executor.uniproc_executor import (  # noqa\n    UniProcExecutor as UniProcExecutorV0)\nfrom vllm.v1.kv_cache_interface import KVCacheConfig, KVCacheSpec\nfrom vllm.v1.outputs import ModelRunnerOutput\n\nFailureCallback = Callable[[], None]\n\n\nclass Executor(ExecutorBase):\n    \"\"\"\n    Abstract class for v1 executors, mainly define some methods for v1.\n    For methods shared by v0 and v1, define them in ExecutorBase\"\"\"\n\n    @staticmethod\n    def get_class(vllm_config: VllmConfig) -> type[\"Executor\"]:\n        executor_class: type[Executor]\n        parallel_config = vllm_config.parallel_config\n        distributed_executor_backend = (\n            parallel_config.distributed_executor_backend)\n        # distributed_executor_backend must be set in VllmConfig.__post_init__\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            executor_class = distributed_executor_backend\n        elif distributed_executor_backend == \"ray\":\n            from vllm.v1.executor.ray_distributed_executor import (  # noqa\n                RayDistributedExecutor)\n            executor_class = RayDistributedExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.v1.executor.multiproc_executor import MultiprocExecutor\n            executor_class = MultiprocExecutor\n        elif distributed_executor_backend == \"uni\":\n            executor_class = UniProcExecutor\n        elif distributed_executor_backend == \"external_launcher\":\n            # TODO: make v1 scheduling deterministic\n            # to support external launcher\n            executor_class = ExecutorWithExternalLauncher\n        else:\n            raise ValueError(\"Unknown distributed executor backend: \"\n                             f\"{distributed_executor_backend}\")\n        return executor_class\n\n    def initialize_from_config(self,\n                               kv_cache_configs: list[KVCacheConfig]) -> None:\n        \"\"\"\n        Initialize the KV caches and begin the model execution loop of the\n        underlying workers.\n        \"\"\"\n        self.collective_rpc(\"initialize_from_config\",\n                            args=(kv_cache_configs, ))\n        self.collective_rpc(\"compile_or_warm_up_model\")\n\n    def register_failure_callback(self, callback: FailureCallback):\n        \"\"\"\n        Register a function to be called if the executor enters a permanent\n        failed state.\n        \"\"\"\n        pass\n\n    def determine_available_memory(self) -> list[int]:  # in bytes\n        output = self.collective_rpc(\"determine_available_memory\")\n        return output\n\n    def get_kv_cache_specs(self) -> list[dict[str, KVCacheSpec]]:\n        output = self.collective_rpc(\"get_kv_cache_spec\")\n        return output\n\n    def execute_model(\n        self,\n        scheduler_output,\n    ) -> Union[ModelRunnerOutput, Future[ModelRunnerOutput]]:\n        output = self.collective_rpc(\"execute_model\",\n                                     args=(scheduler_output, ))\n        return output[0]\n\n    @property\n    def max_concurrent_batches(self) -> int:\n        return 1\n\n    def profile(self, is_start: bool = True):\n        self.collective_rpc(\"profile\", args=(is_start, ))\n\n\nclass UniProcExecutor(UniProcExecutorV0, Executor):\n    pass\n\n\nclass ExecutorWithExternalLauncher(ExecutorWithExternalLauncherV0, Executor):\n\n    def determine_available_memory(self) -> list[int]:  # in bytes\n        # same as determine_num_available_blocks in v0,\n        # we need to get the min across all ranks.\n        memory = super().determine_available_memory()\n        from vllm.distributed.parallel_state import get_world_group\n        cpu_group = get_world_group().cpu_group\n        memory_tensor = torch.tensor([memory], device=\"cpu\", dtype=torch.int64)\n        dist.all_reduce(memory_tensor, group=cpu_group, op=dist.ReduceOp.MIN)\n        return [memory_tensor.item()]\n", 112], "/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport contextlib\nimport copy\nimport os\nimport warnings\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nimport huggingface_hub\nfrom transformers import (AutoTokenizer, PreTrainedTokenizer,\n                          PreTrainedTokenizerFast)\n\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizer_base import (TokenizerBase,\n                                                    TokenizerRegistry)\nfrom vllm.transformers_utils.tokenizers import MistralTokenizer\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.utils import make_async\n\nif TYPE_CHECKING:\n    from vllm.config import ModelConfig\n\nlogger = init_logger(__name__)\n\nAnyTokenizer = Union[PreTrainedTokenizer, PreTrainedTokenizerFast,\n                     TokenizerBase]\n\n\ndef decode_tokens(\n    tokenizer: AnyTokenizer,\n    token_ids: list[int],\n    *,\n    skip_special_tokens: Optional[bool] = None,\n) -> str:\n    \"\"\"\n    Backend-agnostic equivalent of HF's\n    `tokenizer.decode(token_ids, ...)`.\n\n    `skip_special_tokens=None` means to use the backend's default\n    settings.\n    \"\"\"\n    if skip_special_tokens is not None:\n        return tokenizer.decode(token_ids,\n                                skip_special_tokens=skip_special_tokens)\n\n    return tokenizer.decode(token_ids)\n\n\ndef encode_tokens(\n    tokenizer: AnyTokenizer,\n    text: str,\n    *,\n    truncation: Optional[bool] = None,\n    max_length: Optional[int] = None,\n    add_special_tokens: Optional[bool] = None,\n) -> list[int]:\n    \"\"\"\n    Backend-agnostic equivalent of HF's\n    `tokenizer.encode(text, ...)`.\n\n    `add_special_tokens=None` means to use the backend's default\n    settings.\n    \"\"\"\n\n    kw_args: dict[str, Any] = {}\n    if max_length is not None:\n        kw_args[\"max_length\"] = max_length\n\n    if truncation is not None:\n        kw_args[\"truncation\"] = truncation\n\n    if add_special_tokens is not None:\n        kw_args[\"add_special_tokens\"] = add_special_tokens\n\n    return tokenizer.encode(text, **kw_args)\n\n\ndef get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n    \"\"\"\n    By default, transformers will recompute multiple tokenizer properties\n    each time they are called, leading to a significant slowdown.\n    This proxy caches these properties for faster access.\n    \"\"\"\n    cached_tokenizer = copy.copy(tokenizer)\n\n    tokenizer_all_special_ids = tokenizer.all_special_ids\n    tokenizer_all_special_tokens = tokenizer.all_special_tokens\n    tokenizer_all_special_tokens_extended = (\n        tokenizer.all_special_tokens_extended)\n    tokenizer_vocab = tokenizer.get_vocab()\n    tokenizer_len = len(tokenizer)\n\n    max_token_id = max(tokenizer_vocab.values())\n    # Some tokenizers (e.g., QwenTokenizer) have special tokens that\n    # are added and included in the implementation of the vocab_size\n    # property, but not in get_vocab(); if there is an implementation\n    # of vocab size, we should take the greater value.\n    if hasattr(tokenizer, \"vocab_size\"):\n        with contextlib.suppress(NotImplementedError):\n            max_token_id = max(max_token_id, tokenizer.vocab_size)\n\n    class CachedTokenizer(tokenizer.__class__):  # type: ignore\n\n        @property\n        def all_special_ids(self) -> list[int]:\n            return tokenizer_all_special_ids\n\n        @property\n        def all_special_tokens(self) -> list[str]:\n            return tokenizer_all_special_tokens\n\n        @property\n        def all_special_tokens_extended(self) -> list[str]:\n            return tokenizer_all_special_tokens_extended\n\n        @property\n        def max_token_id(self) -> int:\n            return max_token_id\n\n        def get_vocab(self) -> dict[str, int]:\n            return tokenizer_vocab\n\n        def __len__(self) -> int:\n            return tokenizer_len\n\n        def __reduce__(self):\n            return get_cached_tokenizer, (tokenizer, )\n\n    CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n\n    cached_tokenizer.__class__ = CachedTokenizer\n    return cached_tokenizer\n\n\ndef patch_padding_side(tokenizer: PreTrainedTokenizer) -> None:\n    \"\"\"Patch _pad method to accept `padding_side` for older tokenizers.\"\"\"\n    orig_pad = tokenizer._pad\n\n    def _pad(\n        self: PreTrainedTokenizer,\n        *args,\n        padding_side: Optional[str] = None,\n        **kwargs,\n    ):\n        if padding_side is not None and padding_side != self.padding_side:\n            msg = (\"`padding_side` argument is not supported by \"\n                   f\"{type(tokenizer).__name__} and will be ignored.\")\n            warnings.warn(msg, stacklevel=2)\n\n        return orig_pad(*args, **kwargs)\n\n    tokenizer._pad = MethodType(_pad, tokenizer)\n\n\ndef get_tokenizer(\n    tokenizer_name: Union[str, Path],\n    *args,\n    tokenizer_mode: str = \"auto\",\n    trust_remote_code: bool = False,\n    revision: Optional[str] = None,\n    download_dir: Optional[str] = None,\n    **kwargs,\n) -> AnyTokenizer:\n    \"\"\"Gets a tokenizer for the given model name via HuggingFace or ModelScope.\n    \"\"\"\n    if VLLM_USE_MODELSCOPE:\n        # download model from ModelScope hub,\n        # lazy import so that modelscope is not required for normal use.\n        # pylint: disable=C.\n        from modelscope.hub.snapshot_download import snapshot_download\n\n        # avoid circuit import\n        from vllm.model_executor.model_loader.weight_utils import get_lock\n\n        # Only set the tokenizer here, model will be downloaded on the workers.\n        if not os.path.exists(tokenizer_name):\n            # Use file lock to prevent multiple processes from\n            # downloading the same file at the same time.\n            with get_lock(tokenizer_name, download_dir):\n                tokenizer_path = snapshot_download(\n                    model_id=tokenizer_name,\n                    cache_dir=download_dir,\n                    revision=revision,\n                    local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,\n                    # Ignore weights - we only need the tokenizer.\n                    ignore_file_pattern=[\".*.pt\", \".*.safetensors\", \".*.bin\"])\n                tokenizer_name = tokenizer_path\n\n    if tokenizer_mode == \"slow\":\n        if kwargs.get(\"use_fast\", False):\n            raise ValueError(\n                \"Cannot use the fast tokenizer in slow tokenizer mode.\")\n        kwargs[\"use_fast\"] = False\n\n    if \"truncation_side\" not in kwargs:\n        kwargs[\"truncation_side\"] = \"left\"\n\n    # Separate model folder from file path for GGUF models\n    is_gguf = check_gguf_file(tokenizer_name)\n    if is_gguf:\n        kwargs[\"gguf_file\"] = Path(tokenizer_name).name\n        tokenizer_name = Path(tokenizer_name).parent\n\n    # if tokenizer is from official mistral org\n    is_from_mistral_org = str(tokenizer_name).split(\"/\")[0] == \"mistralai\"\n    if is_from_mistral_org and tokenizer_mode != \"mistral\":\n        warnings.warn(\n            'It is strongly recommended to run mistral models with '\n            '`--tokenizer-mode \"mistral\"` to ensure correct '\n            'encoding and decoding.',\n            FutureWarning,\n            stacklevel=2)\n\n    tokenizer: AnyTokenizer\n    if tokenizer_mode == \"mistral\":\n        tokenizer = MistralTokenizer.from_pretrained(str(tokenizer_name),\n                                                     revision=revision)\n    elif tokenizer_mode == \"custom\":\n        tokenizer = TokenizerRegistry.get_tokenizer(str(tokenizer_name),\n                                                    *args,\n                                                    revision=revision,\n                                                    download_dir=download_dir,\n                                                    **kwargs)\n    else:\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                tokenizer_name,\n                *args,\n                trust_remote_code=trust_remote_code,\n                revision=revision,\n                **kwargs,\n            )\n        except ValueError as e:\n            # If the error pertains to the tokenizer class not existing or not\n            # currently being imported,\n            # suggest using the --trust-remote-code flag.\n            if not trust_remote_code and (\n                    \"does not exist or is not currently imported.\" in str(e)\n                    or \"requires you to execute the tokenizer file\" in str(e)):\n                err_msg = (\"Failed to load the tokenizer. If the tokenizer \"\n                           \"is a custom tokenizer not yet available in the \"\n                           \"HuggingFace transformers library, consider \"\n                           \"setting `trust_remote_code=True` in LLM or using \"\n                           \"the `--trust-remote-code` flag in the CLI.\")\n                raise RuntimeError(err_msg) from e\n            else:\n                raise e\n\n        # NOTE: We can remove this after https://github.com/THUDM/ChatGLM3/issues/1324\n        if type(tokenizer).__name__ in (\"ChatGLMTokenizer\",\n                                        \"ChatGLM4Tokenizer\"):\n            assert isinstance(tokenizer, PreTrainedTokenizer)\n            patch_padding_side(tokenizer)\n\n        if not isinstance(tokenizer, PreTrainedTokenizerFast):\n            logger.warning(\n                \"Using a slow tokenizer. This might cause a significant \"\n                \"slowdown. Consider using a fast tokenizer instead.\")\n        tokenizer = get_cached_tokenizer(tokenizer)\n\n    return tokenizer\n\n\ncached_get_tokenizer = lru_cache(get_tokenizer)\n\n\ndef cached_tokenizer_from_config(\n    model_config: \"ModelConfig\",\n    **kwargs: Any,\n):\n    return cached_get_tokenizer(\n        model_config.tokenizer,\n        tokenizer_mode=model_config.tokenizer_mode,\n        tokenizer_revision=model_config.tokenizer_revision,\n        trust_remote_code=model_config.trust_remote_code,\n        **kwargs,\n    )\n\n\ndef get_lora_tokenizer(lora_request: LoRARequest, *args,\n                       **kwargs) -> Optional[AnyTokenizer]:\n    if lora_request is None:\n        return None\n    try:\n        tokenizer = get_tokenizer(lora_request.lora_path, *args, **kwargs)\n    except Exception as e:\n        # No tokenizer was found in the LoRA folder,\n        # use base model tokenizer\n        logger.warning(\n            \"No tokenizer found in %s, using base model tokenizer instead. \"\n            \"(Exception: %s)\", lora_request.lora_path, e)\n        tokenizer = None\n    return tokenizer\n\n\nget_lora_tokenizer_async = make_async(get_lora_tokenizer)\n", 301], "/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom typing import Optional\n\nfrom vllm.config import LoRAConfig, ModelConfig, SchedulerConfig\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizer import (AnyTokenizer, encode_tokens,\n                                               get_lora_tokenizer,\n                                               get_lora_tokenizer_async,\n                                               get_tokenizer)\nfrom vllm.utils import LRUCache\n\n\nclass TokenizerGroup:\n    \"\"\"A group of tokenizers that can be used for LoRA adapters.\"\"\"\n\n    def __init__(self, tokenizer_id: str, enable_lora: bool, max_num_seqs: int,\n                 max_input_length: Optional[int], **tokenizer_config):\n        self.tokenizer_id = tokenizer_id\n        self.tokenizer_config = tokenizer_config\n        self.enable_lora = enable_lora\n        self.max_input_length = max_input_length\n        self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n        max_loras = tokenizer_config.get(\"max_loras\", 0)\n        self.lora_tokenizers = LRUCache[int, AnyTokenizer](\n            capacity=max(max_loras, max_num_seqs) if enable_lora else 0)\n\n    def get_max_input_len(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        \"\"\"Get the maximum input length for the LoRA request.\"\"\"\n        return self.max_input_length\n\n    def _raise_if_input_too_long(self,\n                                 encoded_tokens: list[int],\n                                 lora_request: Optional[LoRARequest] = None):\n        input_length = len(encoded_tokens)\n        if lora_request:\n            max_input_length = (lora_request.long_lora_max_len\n                                or self.max_input_length)\n        else:\n            max_input_length = self.max_input_length\n        if max_input_length is not None and input_length > max_input_length:\n            raise ValueError(\"Input too long.\", input_length, max_input_length)\n\n    def encode(self,\n               prompt: str,\n               max_length: Optional[int] = None,\n               truncation: Optional[bool] = None,\n               lora_request: Optional[LoRARequest] = None,\n               add_special_tokens: Optional[bool] = None) -> list[int]:\n\n        tokenizer = self.get_lora_tokenizer(lora_request)\n        ret = encode_tokens(tokenizer,\n                            prompt,\n                            max_length=max_length,\n                            truncation=truncation,\n                            add_special_tokens=add_special_tokens)\n        self._raise_if_input_too_long(ret, lora_request)\n        return ret\n\n    async def encode_async(\n            self,\n            prompt: str,\n            max_length: Optional[int] = None,\n            truncation: Optional[bool] = None,\n            lora_request: Optional[LoRARequest] = None,\n            add_special_tokens: Optional[bool] = None) -> list[int]:\n        tokenizer = await self.get_lora_tokenizer_async(lora_request)\n        ret = encode_tokens(tokenizer,\n                            prompt,\n                            max_length=max_length,\n                            truncation=truncation,\n                            add_special_tokens=add_special_tokens)\n        self._raise_if_input_too_long(ret, lora_request)\n        return ret\n\n    def get_lora_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        if not lora_request or not self.enable_lora:\n            return self.tokenizer\n        if lora_request.lora_int_id not in self.lora_tokenizers:\n            tokenizer = (get_lora_tokenizer(\n                lora_request, **self.tokenizer_config) or self.tokenizer)\n            self.lora_tokenizers.put(lora_request.lora_int_id, tokenizer)\n            return tokenizer\n        else:\n            return self.lora_tokenizers[lora_request.lora_int_id]\n\n    async def get_lora_tokenizer_async(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        if not lora_request or not self.enable_lora:\n            return self.tokenizer\n        if lora_request.lora_int_id not in self.lora_tokenizers:\n            tokenizer = (await get_lora_tokenizer_async(\n                lora_request, **self.tokenizer_config) or self.tokenizer)\n            self.lora_tokenizers.put(lora_request.lora_int_id, tokenizer)\n            return tokenizer\n        else:\n            return self.lora_tokenizers[lora_request.lora_int_id]\n\n\ndef init_tokenizer_from_configs(model_config: ModelConfig,\n                                scheduler_config: SchedulerConfig,\n                                lora_config: Optional[LoRAConfig]):\n    return TokenizerGroup(\n        tokenizer_id=model_config.tokenizer,\n        enable_lora=bool(lora_config),\n        max_num_seqs=scheduler_config.max_num_seqs,\n        max_loras=lora_config.max_loras if lora_config else 0,\n        max_input_length=None,\n        tokenizer_mode=model_config.tokenizer_mode,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.tokenizer_revision,\n        truncation_side=model_config.truncation_side)\n", 119], "/home/jeromeku/vllm/vllm/transformers_utils/config.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport enum\nimport json\nimport os\nimport time\nfrom functools import cache\nfrom pathlib import Path\nfrom typing import Any, Callable, Literal, Optional, Union\n\nimport huggingface_hub\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub import list_repo_files as hf_list_repo_files\nfrom huggingface_hub import try_to_load_from_cache\nfrom huggingface_hub.utils import (EntryNotFoundError, HfHubHTTPError,\n                                   HFValidationError, LocalEntryNotFoundError,\n                                   RepositoryNotFoundError,\n                                   RevisionNotFoundError)\nfrom torch import nn\nfrom transformers import GenerationConfig, PretrainedConfig\nfrom transformers.models.auto.image_processing_auto import (\n    get_image_processor_config)\nfrom transformers.models.auto.modeling_auto import (\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\nfrom transformers.utils import CONFIG_NAME as HF_CONFIG_NAME\n\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\n# yapf conflicts with isort for this block\n# yapf: disable\nfrom vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n                                             DbrxConfig, DeepseekVLV2Config,\n                                             EAGLEConfig, ExaoneConfig,\n                                             H2OVLChatConfig,\n                                             InternVLChatConfig, JAISConfig,\n                                             KimiVLConfig, MedusaConfig,\n                                             MiniMaxText01Config,\n                                             MiniMaxVL01Config, MllamaConfig,\n                                             MLPSpeculatorConfig, MPTConfig,\n                                             NemotronConfig, NVLM_D_Config,\n                                             OvisConfig, RWConfig,\n                                             SkyworkR1VChatConfig, SolarConfig,\n                                             Telechat2Config, UltravoxConfig)\n# yapf: enable\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.utils import resolve_obj_by_qualname\n\nif VLLM_USE_MODELSCOPE:\n    from modelscope import AutoConfig\nelse:\n    from transformers import AutoConfig\n\nMISTRAL_CONFIG_NAME = \"params.json\"\nHF_TOKEN = os.getenv('HF_TOKEN', None)\n\nlogger = init_logger(__name__)\n\n_CONFIG_REGISTRY_OVERRIDE_HF: dict[str, type[PretrainedConfig]] = {\n    \"mllama\": MllamaConfig\n}\n\n_CONFIG_REGISTRY: dict[str, type[PretrainedConfig]] = {\n    \"chatglm\": ChatGLMConfig,\n    \"cohere2\": Cohere2Config,\n    \"dbrx\": DbrxConfig,\n    \"deepseek_vl_v2\": DeepseekVLV2Config,\n    \"kimi_vl\": KimiVLConfig,\n    \"mpt\": MPTConfig,\n    \"RefinedWeb\": RWConfig,  # For tiiuae/falcon-40b(-instruct)\n    \"RefinedWebModel\": RWConfig,  # For tiiuae/falcon-7b(-instruct)\n    \"jais\": JAISConfig,\n    \"mlp_speculator\": MLPSpeculatorConfig,\n    \"medusa\": MedusaConfig,\n    \"eagle\": EAGLEConfig,\n    \"exaone\": ExaoneConfig,\n    \"h2ovl_chat\": H2OVLChatConfig,\n    \"internvl_chat\": InternVLChatConfig,\n    \"minimax_text_01\": MiniMaxText01Config,\n    \"minimax_vl_01\": MiniMaxVL01Config,\n    \"nemotron\": NemotronConfig,\n    \"NVLM_D\": NVLM_D_Config,\n    \"ovis\": OvisConfig,\n    \"solar\": SolarConfig,\n    \"skywork_chat\": SkyworkR1VChatConfig,\n    \"telechat\": Telechat2Config,\n    \"ultravox\": UltravoxConfig,\n    **_CONFIG_REGISTRY_OVERRIDE_HF\n}\n\n\nclass ConfigFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    HF = \"hf\"\n    MISTRAL = \"mistral\"\n\n\ndef with_retry(func: Callable[[], Any],\n               log_msg: str,\n               max_retries: int = 2,\n               retry_delay: int = 2):\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                logger.error(\"%s: %s\", log_msg, e)\n                raise\n            logger.error(\"%s: %s, retrying %d of %d\", log_msg, e, attempt + 1,\n                         max_retries)\n            time.sleep(retry_delay)\n            retry_delay *= 2\n\n\n# @cache doesn't cache exceptions\n@cache\ndef list_repo_files(\n    repo_id: str,\n    *,\n    revision: Optional[str] = None,\n    repo_type: Optional[str] = None,\n    token: Union[str, bool, None] = None,\n) -> list[str]:\n\n    def lookup_files() -> list[str]:\n        # directly list files if model is local\n        if (local_path := Path(repo_id)).exists():\n            return [\n                str(file.relative_to(local_path))\n                for file in local_path.rglob('*') if file.is_file()\n            ]\n        # if model is remote, use hf_hub api to list files\n        try:\n            if VLLM_USE_MODELSCOPE:\n                from vllm.transformers_utils.utils import (\n                    modelscope_list_repo_files)\n                return modelscope_list_repo_files(repo_id,\n                                                  revision=revision,\n                                                  token=token)\n            return hf_list_repo_files(repo_id,\n                                      revision=revision,\n                                      repo_type=repo_type,\n                                      token=token)\n        except huggingface_hub.errors.OfflineModeIsEnabled:\n            # Don't raise in offline mode,\n            # all we know is that we don't have this\n            # file cached.\n            return []\n\n    return with_retry(lookup_files, \"Error retrieving file list\")\n\n\ndef file_exists(\n    repo_id: str,\n    file_name: str,\n    *,\n    repo_type: Optional[str] = None,\n    revision: Optional[str] = None,\n    token: Union[str, bool, None] = None,\n) -> bool:\n    file_list = list_repo_files(repo_id,\n                                repo_type=repo_type,\n                                revision=revision,\n                                token=token)\n    return file_name in file_list\n\n\n# In offline mode the result can be a false negative\ndef file_or_path_exists(model: Union[str, Path], config_name: str,\n                        revision: Optional[str]) -> bool:\n    if (local_path := Path(model)).exists():\n        return (local_path / config_name).is_file()\n\n    # Offline mode support: Check if config file is cached already\n    cached_filepath = try_to_load_from_cache(repo_id=model,\n                                             filename=config_name,\n                                             revision=revision)\n    if isinstance(cached_filepath, str):\n        # The config file exists in cache- we can continue trying to load\n        return True\n\n    # NB: file_exists will only check for the existence of the config file on\n    # hf_hub. This will fail in offline mode.\n\n    # Call HF to check if the file exists\n    return file_exists(str(model),\n                       config_name,\n                       revision=revision,\n                       token=HF_TOKEN)\n\n\ndef patch_rope_scaling(config: PretrainedConfig) -> None:\n    \"\"\"Provide backwards compatibility for RoPE.\"\"\"\n    text_config = getattr(config, \"text_config\", None)\n    if text_config is not None:\n        patch_rope_scaling(text_config)\n\n    rope_scaling = getattr(config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        patch_rope_scaling_dict(rope_scaling)\n\n\ndef patch_rope_scaling_dict(rope_scaling: dict[str, Any]) -> None:\n    if \"rope_type\" in rope_scaling and \"type\" in rope_scaling:\n        rope_type = rope_scaling[\"rope_type\"]\n        rope_type_legacy = rope_scaling[\"type\"]\n        if rope_type != rope_type_legacy:\n            raise ValueError(\n                f\"Found conflicts between 'rope_type={rope_type}' (modern \"\n                f\"field) and 'type={rope_type_legacy}' (legacy field). \"\n                \"You should only specify one of them.\")\n\n    if \"rope_type\" not in rope_scaling and \"type\" in rope_scaling:\n        rope_scaling[\"rope_type\"] = rope_scaling[\"type\"]\n        logger.info(\"Replacing legacy 'type' key with 'rope_type'\")\n\n    if \"rope_type\" not in rope_scaling:\n        raise ValueError(\"rope_scaling should have a 'rope_type' key\")\n\n    if rope_scaling[\"rope_type\"] == \"su\":\n        rope_scaling[\"rope_type\"] = \"longrope\"\n        logger.warning(\"Replacing legacy rope_type 'su' with 'longrope'\")\n    elif rope_scaling[\"rope_type\"] == \"mrope\":\n        assert \"mrope_section\" in rope_scaling\n        rope_scaling[\"rope_type\"] = \"default\"\n        logger.warning(\"Replacing legacy rope_type 'mrope' with 'default'\")\n\n\ndef _uses_mrope(config: PretrainedConfig) -> bool:\n    rope_scaling = getattr(config, \"rope_scaling\", None)\n    if rope_scaling is None:\n        return False\n\n    return \"mrope_section\" in rope_scaling\n\n\ndef uses_mrope(config: PretrainedConfig) -> bool:\n    \"\"\"Detect if the model with this config uses M-ROPE.\"\"\"\n    return _uses_mrope(config) or thinker_uses_mrope(config)\n\n\ndef thinker_uses_mrope(config: PretrainedConfig) -> bool:\n    \"\"\"Detect if the model contains a thinker config and it uses M-ROPE.\"\"\"\n    thinker_config = getattr(config, \"thinker_config\", None)\n    if thinker_config is None:\n        return False\n\n    thinker_text_config = getattr(thinker_config, \"text_config\", None)\n    if thinker_text_config is None:\n        return False\n\n    return uses_mrope(thinker_text_config)\n\n\ndef is_encoder_decoder(config: PretrainedConfig) -> bool:\n    \"\"\"Detect if the model with this config is used as an encoder/decoder.\"\"\"\n    text_config = getattr(config, \"text_config\", None)\n    if text_config is not None:\n        return is_encoder_decoder(text_config)\n\n    return getattr(config, \"is_encoder_decoder\", False)\n\n\ndef get_config(\n    model: Union[str, Path],\n    trust_remote_code: bool,\n    revision: Optional[str] = None,\n    code_revision: Optional[str] = None,\n    config_format: ConfigFormat = ConfigFormat.AUTO,\n    **kwargs,\n) -> PretrainedConfig:\n    # Separate model folder from file path for GGUF models\n\n    is_gguf = check_gguf_file(model)\n    if is_gguf:\n        kwargs[\"gguf_file\"] = Path(model).name\n        model = Path(model).parent\n\n    if config_format == ConfigFormat.AUTO:\n        try:\n            if is_gguf or file_or_path_exists(\n                    model, HF_CONFIG_NAME, revision=revision):\n                config_format = ConfigFormat.HF\n            elif file_or_path_exists(model,\n                                     MISTRAL_CONFIG_NAME,\n                                     revision=revision):\n                config_format = ConfigFormat.MISTRAL\n            else:\n                raise ValueError(\n                    \"Could not detect config format for no config file found. \"\n                    \"Ensure your model has either config.json (HF format) \"\n                    \"or params.json (Mistral format).\")\n\n        except Exception as e:\n            error_message = (\n                \"Invalid repository ID or local directory specified:\"\n                \" '{model}'.\\nPlease verify the following requirements:\\n\"\n                \"1. Provide a valid Hugging Face repository ID.\\n\"\n                \"2. Specify a local directory that contains a recognized \"\n                \"configuration file.\\n\"\n                \"   - For Hugging Face models: ensure the presence of a \"\n                \"'config.json'.\\n\"\n                \"   - For Mistral models: ensure the presence of a \"\n                \"'params.json'.\\n\").format(model=model)\n\n            raise ValueError(error_message) from e\n\n    if config_format == ConfigFormat.HF:\n        config_dict, _ = PretrainedConfig.get_config_dict(\n            model,\n            revision=revision,\n            code_revision=code_revision,\n            token=HF_TOKEN,\n            **kwargs,\n        )\n\n        # Use custom model class if it's in our registry\n        model_type = config_dict.get(\"model_type\")\n        if model_type in _CONFIG_REGISTRY:\n            config_class = _CONFIG_REGISTRY[model_type]\n            config = config_class.from_pretrained(\n                model,\n                revision=revision,\n                code_revision=code_revision,\n                token=HF_TOKEN,\n                **kwargs,\n            )\n        else:\n            try:\n                config = AutoConfig.from_pretrained(\n                    model,\n                    trust_remote_code=trust_remote_code,\n                    revision=revision,\n                    code_revision=code_revision,\n                    token=HF_TOKEN,\n                    **kwargs,\n                )\n            except ValueError as e:\n                if (not trust_remote_code\n                        and \"requires you to execute the configuration file\"\n                        in str(e)):\n                    err_msg = (\n                        \"Failed to load the model config. If the model \"\n                        \"is a custom model not yet available in the \"\n                        \"HuggingFace transformers library, consider setting \"\n                        \"`trust_remote_code=True` in LLM or using the \"\n                        \"`--trust-remote-code` flag in the CLI.\")\n                    raise RuntimeError(err_msg) from e\n                else:\n                    raise e\n\n    elif config_format == ConfigFormat.MISTRAL:\n        config = load_params_config(model, revision, token=HF_TOKEN, **kwargs)\n    else:\n        supported_formats = [\n            fmt.value for fmt in ConfigFormat if fmt != ConfigFormat.AUTO\n        ]\n        raise ValueError(\n            f\"Unsupported config format: {config_format}. \"\n            f\"Supported formats are: {', '.join(supported_formats)}. \"\n            f\"Ensure your model uses one of these configuration formats \"\n            f\"or specify the correct format explicitly.\")\n\n    # Special architecture mapping check for GGUF models\n    if is_gguf:\n        if config.model_type not in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES:\n            raise RuntimeError(\n                f\"Can't get gguf config for {config.model_type}.\")\n        model_type = MODEL_FOR_CAUSAL_LM_MAPPING_NAMES[config.model_type]\n        config.update({\"architectures\": [model_type]})\n\n    patch_rope_scaling(config)\n\n    if trust_remote_code:\n        maybe_register_config_serialize_by_value()\n\n    return config\n\n\ndef try_get_local_file(model: Union[str, Path],\n                       file_name: str,\n                       revision: Optional[str] = 'main') -> Optional[Path]:\n    file_path = Path(model) / file_name\n    if file_path.is_file():\n        return file_path\n    else:\n        try:\n            cached_filepath = try_to_load_from_cache(repo_id=model,\n                                                     filename=file_name,\n                                                     revision=revision)\n            if isinstance(cached_filepath, str):\n                return Path(cached_filepath)\n        except HFValidationError:\n            ...\n    return None\n\n\ndef get_hf_file_to_dict(file_name: str,\n                        model: Union[str, Path],\n                        revision: Optional[str] = 'main'):\n    \"\"\"\n    Downloads a file from the Hugging Face Hub and returns\n    its contents as a dictionary.\n\n    Parameters:\n    - file_name (str): The name of the file to download.\n    - model (str): The name of the model on the Hugging Face Hub.\n    - revision (str): The specific version of the model.\n\n    Returns:\n    - config_dict (dict): A dictionary containing\n    the contents of the downloaded file.\n    \"\"\"\n\n    file_path = try_get_local_file(model=model,\n                                   file_name=file_name,\n                                   revision=revision)\n\n    if file_path is None:\n        try:\n            hf_hub_file = hf_hub_download(model, file_name, revision=revision)\n        except huggingface_hub.errors.OfflineModeIsEnabled:\n            return None\n        except (RepositoryNotFoundError, RevisionNotFoundError,\n                EntryNotFoundError, LocalEntryNotFoundError) as e:\n            logger.debug(\"File or repository not found in hf_hub_download\", e)\n            return None\n        except HfHubHTTPError as e:\n            logger.warning(\n                \"Cannot connect to Hugging Face Hub. Skipping file \"\n                \"download for '%s':\",\n                file_name,\n                exc_info=e)\n            return None\n        file_path = Path(hf_hub_file)\n\n    if file_path is not None and file_path.is_file():\n        with open(file_path) as file:\n            return json.load(file)\n\n    return None\n\n\n@cache\ndef get_pooling_config(model: str, revision: Optional[str] = 'main'):\n    \"\"\"\n    This function gets the pooling and normalize\n    config from the model - only applies to\n    sentence-transformers models.\n\n    Args:\n        model (str): The name of the Hugging Face model.\n        revision (str, optional): The specific version\n        of the model to use. Defaults to 'main'.\n\n    Returns:\n        dict: A dictionary containing the pooling\n        type and whether normalization is used.\n    \"\"\"\n\n    modules_file_name = \"modules.json\"\n\n    modules_dict = None\n    if file_or_path_exists(model=model,\n                           config_name=modules_file_name,\n                           revision=revision):\n        modules_dict = get_hf_file_to_dict(modules_file_name, model, revision)\n\n    if modules_dict is None:\n        return None\n\n    logger.info(\"Found sentence-transformers modules configuration.\")\n\n    pooling = next((item for item in modules_dict\n                    if item[\"type\"] == \"sentence_transformers.models.Pooling\"),\n                   None)\n    normalize = bool(\n        next((item for item in modules_dict\n              if item[\"type\"] == \"sentence_transformers.models.Normalize\"),\n             False))\n\n    if pooling:\n\n        pooling_file_name = \"{}/config.json\".format(pooling[\"path\"])\n        pooling_dict = get_hf_file_to_dict(pooling_file_name, model, revision)\n        pooling_type_name = next(\n            (item for item, val in pooling_dict.items() if val is True), None)\n\n        if pooling_type_name is not None:\n            pooling_type_name = get_pooling_config_name(pooling_type_name)\n\n        logger.info(\"Found pooling configuration.\")\n        return {\"pooling_type\": pooling_type_name, \"normalize\": normalize}\n\n    return None\n\n\ndef get_pooling_config_name(pooling_name: str) -> Union[str, None]:\n    if \"pooling_mode_\" in pooling_name:\n        pooling_name = pooling_name.replace(\"pooling_mode_\", \"\")\n\n    if \"_\" in pooling_name:\n        pooling_name = pooling_name.split(\"_\")[0]\n\n    if \"lasttoken\" in pooling_name:\n        pooling_name = \"last\"\n\n    supported_pooling_types = ['LAST', 'ALL', 'CLS', 'STEP', 'MEAN']\n    pooling_type_name = pooling_name.upper()\n\n    try:\n        if pooling_type_name in supported_pooling_types:\n            return pooling_type_name\n    except NotImplementedError as e:\n        logger.debug(\"Pooling type not supported\", e)\n        return None\n    return None\n\n\n@cache\ndef get_sentence_transformer_tokenizer_config(model: str,\n                                              revision: Optional[str] = 'main'\n                                              ):\n    \"\"\"\n    Returns the tokenization configuration dictionary for a\n    given Sentence Transformer BERT model.\n\n    Parameters:\n    - model (str): The name of the Sentence Transformer\n    BERT model.\n    - revision (str, optional): The revision of the m\n    odel to use. Defaults to 'main'.\n\n    Returns:\n    - dict: A dictionary containing the configuration parameters\n    for the Sentence Transformer BERT model.\n    \"\"\"\n    sentence_transformer_config_files = [\n        \"sentence_bert_config.json\",\n        \"sentence_roberta_config.json\",\n        \"sentence_distilbert_config.json\",\n        \"sentence_camembert_config.json\",\n        \"sentence_albert_config.json\",\n        \"sentence_xlm-roberta_config.json\",\n        \"sentence_xlnet_config.json\",\n    ]\n    encoder_dict = None\n\n    for config_file in sentence_transformer_config_files:\n        if try_get_local_file(model=model,\n                              file_name=config_file,\n                              revision=revision) is not None:\n            encoder_dict = get_hf_file_to_dict(config_file, model, revision)\n            if encoder_dict:\n                break\n\n    if not encoder_dict and not model.startswith(\"/\"):\n        try:\n            # If model is on HuggingfaceHub, get the repo files\n            repo_files = list_repo_files(model,\n                                         revision=revision,\n                                         token=HF_TOKEN)\n        except Exception:\n            repo_files = []\n\n        for config_name in sentence_transformer_config_files:\n            if config_name in repo_files:\n                encoder_dict = get_hf_file_to_dict(config_name, model,\n                                                   revision)\n                if encoder_dict:\n                    break\n\n    if not encoder_dict:\n        return None\n\n    logger.info(\"Found sentence-transformers tokenize configuration.\")\n\n    if all(k in encoder_dict for k in (\"max_seq_length\", \"do_lower_case\")):\n        return encoder_dict\n    return None\n\n\ndef maybe_register_config_serialize_by_value() -> None:\n    \"\"\"Try to register HF model configuration class to serialize by value\n\n        If trust_remote_code is set, and the model's config file specifies an\n        `AutoConfig` class, then the config class is typically an instance of\n        a custom class imported from the HF modules cache.\n\n        Examples:\n\n        >>> from transformers import AutoConfig\n        >>> klass = AutoConfig.from_pretrained('meta-llama/Meta-Llama-3-8B', trust_remote_code=True)\n        >>> klass.__class__ # transformers.models.llama.configuration_llama.LlamaConfig\n        >>> import transformers_modules # error, not initialized\n        >>> klass = AutoConfig.from_pretrained('deepseek-ai/DeepSeek-V2.5', trust_remote_code=True)\n        >>> import transformers_modules # success, initialized\n        >>> klass.__class__ # transformers_modules.deepseek-ai.DeepSeek-V2.5.98b11844770b2c3ffc18b175c758a803640f4e77.configuration_deepseek.DeepseekV2Config\n\n        In the DeepSeek example, the config class is an instance of a custom\n        class that is not serializable by default. This class will not be\n        importable in spawned workers, and won't exist at all on\n        other nodes, which breaks serialization of the config.\n\n        In this function we tell the cloudpickle serialization library to pass\n        instances of these generated classes by value instead of by reference,\n        i.e. the class definition is serialized along with its data so that the\n        class module does not need to be importable on the receiving end.\n\n        See: https://github.com/cloudpipe/cloudpickle?tab=readme-ov-file#overriding-pickles-serialization-mechanism-for-importable-constructs\n    \"\"\" # noqa\n    try:\n        import transformers_modules\n    except ImportError:\n        # the config does not need trust_remote_code\n        return\n\n    try:\n        import cloudpickle\n        cloudpickle.register_pickle_by_value(transformers_modules)\n\n        # ray vendors its own version of cloudpickle\n        from vllm.executor.ray_utils import ray\n        if ray:\n            ray.cloudpickle.register_pickle_by_value(transformers_modules)\n\n        # multiprocessing uses pickle to serialize arguments when using spawn\n        # Here we get pickle to use cloudpickle to serialize config objects\n        # that contain instances of the custom config class to avoid\n        # serialization problems if the generated module (and model) has a `.`\n        # in its name\n        import multiprocessing\n        import pickle\n\n        from vllm.config import VllmConfig\n\n        def _reduce_config(config: VllmConfig):\n            return (pickle.loads, (cloudpickle.dumps(config), ))\n\n        multiprocessing.reducer.register(VllmConfig, _reduce_config)\n\n    except Exception as e:\n        logger.warning(\n            \"Unable to register remote classes used by\"\n            \" trust_remote_code with by-value serialization. This may\"\n            \" lead to a later error. If remote code is not needed\"\n            \" remove `--trust-remote-code`\",\n            exc_info=e)\n\n\ndef load_params_config(model: Union[str, Path], revision: Optional[str],\n                       **kwargs) -> PretrainedConfig:\n    # This function loads a params.json config which\n    # should be used when loading models in mistral format\n\n    config_file_name = \"params.json\"\n\n    config_dict = get_hf_file_to_dict(config_file_name, model, revision)\n    if config_dict is None:\n        raise ValueError(\n            f\"Failed to load mistral '{config_file_name}' config for model \"\n            f\"{model}. Please check if the model is a mistral-format model \"\n            f\"and if the config file exists.\")\n    assert isinstance(config_dict, dict)\n\n    config_mapping = {\n        \"dim\": \"hidden_size\",\n        \"norm_eps\": \"rms_norm_eps\",\n        \"n_kv_heads\": \"num_key_value_heads\",\n        \"n_layers\": \"num_hidden_layers\",\n        \"n_heads\": \"num_attention_heads\",\n        \"hidden_dim\": \"intermediate_size\",\n    }\n\n    def recurse_elems(elem: Any):\n        if isinstance(elem, dict):\n            config_dict = {}\n            for key, value in elem.items():\n                key = config_mapping.get(key, key)\n                config_dict[key] = recurse_elems(value)\n\n            return config_dict\n        else:\n            return elem\n\n    config_dict[\"model_type\"] = config_dict.get(\"model_type\", \"transformer\")\n    config_dict[\"hidden_act\"] = config_dict.get(\"activation\", \"silu\")\n    config_dict[\"tie_word_embeddings\"] = config_dict.get(\n        \"tie_embeddings\", False)\n\n    if config_dict.get(\"max_position_embeddings\") is None:\n        max_position_embeddings = 128_000\n        try:\n            trust_remote_code_val = kwargs.get(\"trust_remote_code\", False)\n            hf_config = get_config(model=model,\n                                   trust_remote_code=trust_remote_code_val,\n                                   revision=revision,\n                                   config_format=ConfigFormat.HF)\n            if hf_value := hf_config.get_text_config().max_position_embeddings:\n                max_position_embeddings = hf_value\n        except Exception as e:\n            logger.warning(\n                \"The params.json file is missing 'max_position_embeddings'\"\n                \" and could not get a value from the HF config.\"\n                \" Defaulting to 128000\",\n                exc_info=e)\n        config_dict[\"max_position_embeddings\"] = max_position_embeddings\n\n    if config_dict.get(\"quantization\") is not None:\n        quantization = config_dict.get(\"quantization\", {})\n        if quantization.get(\"qformat_weight\") == \"fp8_e4m3\":\n            # This maps to the FP8 static per-tensor quantization scheme\n            quantization_config = {\n                \"quant_method\": \"fp8\",\n                \"activation_scheme\": \"static\"\n            }\n        elif quantization.get(\"quant_method\") == \"compressed-tensors\":\n            # Pass through the quantization config to compressed-tensors\n            quantization_config = quantization\n        else:\n            raise ValueError(\n                f\"Found unknown quantization='{quantization}' in config\")\n\n        config_dict[\"quantization_config\"] = quantization_config\n\n    config_type: Literal[\"text\",\n                         \"multimodal\"] = \"multimodal\" if config_dict.get(\n                             \"vision_encoder\") is not None else \"text\"\n\n    if config_dict.get(\"moe\") is not None:\n        config_dict[\"architectures\"] = [\"MixtralForCausalLM\"]\n    else:\n        config_dict[\"architectures\"] = [\"MistralForCausalLM\"]\n\n    if config_type == \"multimodal\":\n        multimodal_config = config_dict.pop(\"vision_encoder\")\n        quantization_config = config_dict.get(\"quantization_config\", {})\n\n        config_dict = {\n            \"text_config\": config_dict,\n            \"vision_config\": multimodal_config\n        }\n        config_dict[\"architectures\"] = [\"PixtralForConditionalGeneration\"]\n        config_dict[\"model_type\"] = \"pixtral\"\n        if quantization_config:\n            config_dict[\"quantization_config\"] = quantization_config\n\n    config_dict.update(kwargs)\n\n    config_dict = recurse_elems(config_dict)\n\n    # transform to HF config format\n    if config_type == \"multimodal\":\n        config_dict[\"text_config\"] = PretrainedConfig(\n            **config_dict[\"text_config\"])\n        config_dict[\"vision_config\"] = PretrainedConfig(\n            **config_dict[\"vision_config\"])\n\n    return PretrainedConfig(**config_dict)\n\n\ndef get_hf_image_processor_config(\n    model: Union[str, Path],\n    hf_token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    **kwargs,\n) -> dict[str, Any]:\n    # ModelScope does not provide an interface for image_processor\n    if VLLM_USE_MODELSCOPE:\n        return dict()\n    # Separate model folder from file path for GGUF models\n    if check_gguf_file(model):\n        model = Path(model).parent\n    return get_image_processor_config(model,\n                                      token=hf_token,\n                                      revision=revision,\n                                      **kwargs)\n\n\ndef get_hf_text_config(config: PretrainedConfig):\n    \"\"\"Get the \"sub\" config relevant to llm for multi modal models.\n    No op for pure text models.\n    \"\"\"\n    # This block should be unnecessary after https://github.com/huggingface/transformers/pull/37517\n    if hasattr(config, \"thinker_config\"):\n        # TODO(suyang.fy): Refactor code.\n        #  For Qwen2.5-Omni, change hf_text_config to\n        #  thinker_config.text_config.\n        return config.thinker_config.text_config\n\n    text_config = config.get_text_config()\n\n    if text_config is not config:\n        # The code operates under the assumption that text_config should have\n        # `num_attention_heads` (among others). Assert here to fail early\n        # if transformers config doesn't align with this assumption.\n        assert hasattr(text_config, \"num_attention_heads\")\n\n    return text_config\n\n\ndef try_get_generation_config(\n    model: str,\n    trust_remote_code: bool,\n    revision: Optional[str] = None,\n) -> Optional[GenerationConfig]:\n    try:\n        return GenerationConfig.from_pretrained(\n            model,\n            revision=revision,\n        )\n    except OSError:  # Not found\n        try:\n            config = get_config(\n                model,\n                trust_remote_code=trust_remote_code,\n                revision=revision,\n            )\n            return GenerationConfig.from_model_config(config)\n        except OSError:  # Not found\n            return None\n\n\ndef get_cross_encoder_activation_function(config: PretrainedConfig):\n    if (hasattr(config, \"sbert_ce_default_activation_function\")\n            and config.sbert_ce_default_activation_function is not None):\n\n        function_name = config.sbert_ce_default_activation_function\n        assert function_name.startswith(\"torch.nn.modules.\"), \\\n            \"Loading of activation functions is restricted to \" \\\n            \"torch.nn.modules for security reasons\"\n        return resolve_obj_by_qualname(function_name)()\n    else:\n        return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()\n", 833], "/home/jeromeku/vllm/vllm/inputs/preprocess.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport asyncio\nfrom collections.abc import Mapping\nfrom typing import Any, Optional, Union, cast\n\nfrom typing_extensions import assert_never\n\nfrom vllm.config import ModelConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.multimodal.inputs import (MultiModalDataDict, MultiModalEncDecInputs,\n                                    MultiModalInputs)\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\n\nfrom .data import (DecoderOnlyInputs, EmbedsInputs, EmbedsPrompt,\n                   EncoderDecoderInputs, ProcessorInputs, PromptType,\n                   SingletonInputs, SingletonPrompt, TextPrompt, TokenInputs,\n                   TokensPrompt, embeds_inputs, token_inputs)\nfrom .parse import is_explicit_encoder_decoder_prompt, parse_singleton_prompt\n\nlogger = init_logger(__name__)\n\n\nclass InputPreprocessor:\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        tokenizer: Optional[TokenizerGroup],\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ) -> None:\n        super().__init__()\n\n        self.model_config = model_config\n        self.tokenizer = tokenizer\n        self.mm_registry = mm_registry\n\n    def get_tokenizer_group(self) -> TokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(\"You cannot pass text prompts when \"\n                             \"`skip_tokenizer_init` is True\")\n\n        return self.tokenizer\n\n    def get_bos_token_id(self,\n                         lora_request: Optional[LoRARequest] = None\n                         ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for BOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).bos_token_id\n\n    def get_eos_token_id(self,\n                         lora_request: Optional[LoRARequest] = None\n                         ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for EOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).eos_token_id\n\n    def get_decoder_start_token_id(self) -> Optional[int]:\n        '''\n        Obtain the decoder start token id employed by an encoder/decoder\n        model. Returns None for non-encoder/decoder models or if the\n        model config is unavailable.\n        '''\n\n        if not self.model_config.is_encoder_decoder:\n            logger.warning_once(\n                \"Using None for decoder start token id because \"\n                \"this is not an encoder/decoder model.\")\n            return None\n\n        if (self.model_config is None or self.model_config.hf_config is None):\n            logger.warning_once(\n                \"Using None for decoder start token id because \"\n                \"model config is not available.\")\n            return None\n\n        dec_start_token_id = getattr(self.model_config.hf_config,\n                                     'decoder_start_token_id', None)\n        if dec_start_token_id is None:\n            logger.warning_once(\n                \"Falling back on <BOS> for decoder start token \"\n                \"id because decoder start token id is not \"\n                \"available.\")\n            dec_start_token_id = self.get_bos_token_id()\n\n        return dec_start_token_id\n\n    def _get_default_enc_dec_decoder_prompt(self) -> list[int]:\n        '''\n        Specifically for encoder/decoder models:\n        generate a default decoder prompt for when\n        the user specifies only the encoder prompt.\n\n        Encoder/decoder models utilize the decoder\n        prompt in different ways; as new models are\n        added, it is intended that this function\n        will be extended to produce differing\n        default decoder prompts, depending on the\n        model variety.\n\n        Absent a special case, the default behavior\n        of this method is to mirror the behavior of\n        the HuggingFace (HF) GenerationMixin for a None\n        decoder prompt, which is to employ a logit processor\n        setting to force the first decoded token to be <BOS>.\n        Here, this behavior is approximated by having the\n        \"default\" decoder prompt be <BOS>.\n\n        However, it is possible that in the future\n        other models may have different or more\n        complex logic for the default decoder prompt.\n        This motivates having a special helper method\n        for default decoder prompts.\n\n        Returns:\n\n        * prompt_token_ids\n        '''\n\n        bos_token_id = self.get_bos_token_id()\n        assert bos_token_id is not None\n        return [bos_token_id]\n\n    def _prepare_decoder_input_ids_for_generation(\n        self,\n        decoder_input_ids: Optional[list[int]],\n    ) -> list[int]:\n        \"\"\"\n        Prepares `decoder_input_ids` for generation with encoder-decoder models.\n\n        Based on:\n        https://github.com/huggingface/transformers/blob/4037a2b5b1278736e566aec12e169100275545ea/src/transformers/generation/utils.py\n        specifically,\n        `GenerationMixin._prepare_decoder_input_ids_for_generation()`.\n\n        Arguments:\n\n        * decoder_input_ids: input token ids to preprocess\n\n        Returns:\n\n        * Processed token list\n        \"\"\"\n\n        decoder_start_token_id = self.get_decoder_start_token_id()\n        assert decoder_start_token_id is not None\n\n        if decoder_input_ids is None:\n            # no decoder prompt input ->\n            # use decoder_start_token_id as decoder_input_ids\n            decoder_input_ids = self._get_default_enc_dec_decoder_prompt()\n\n        if (len(decoder_input_ids) == 0\n                or decoder_input_ids[0] != decoder_start_token_id):\n            decoder_input_ids = [decoder_start_token_id] + decoder_input_ids\n\n        return decoder_input_ids\n\n    def _apply_prompt_adapter(\n        self,\n        prompt_token_ids: list[int],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n    ) -> list[int]:\n        if prompt_adapter_request:\n            prompt_token_ids = (\n                [0] * prompt_adapter_request.prompt_adapter_num_virtual_tokens\n                + prompt_token_ids)\n\n        return prompt_token_ids\n\n    def _get_tokenization_kw(\n        self,\n        overrides: Optional[dict[str, Any]] = None,\n    ) -> dict[str, Any]:\n        kwargs = dict[str, Any]()\n\n        if self.model_config.hf_config.model_type == \"whisper\":\n            # For Whisper, special tokens should be provided by the user based\n            # on the task and language of their request. Also needed to avoid\n            # appending an EOS token to the prompt which disrupts generation.\n            kwargs[\"add_special_tokens\"] = False\n\n        if overrides:\n            kwargs.update(overrides)\n\n        return kwargs\n\n    def _tokenize_prompt(\n        self,\n        prompt: str,\n        lora_request: Optional[LoRARequest],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> list[int]:\n        \"\"\"\n        Apply the model's tokenizer to a text prompt, returning the\n        corresponding token IDs.\n        \"\"\"\n        tokenizer = self.get_tokenizer_group()\n        tokenization_kwargs = self._get_tokenization_kw(tokenization_kwargs)\n\n        encoder_config = self.model_config.encoder_config\n\n        if encoder_config and encoder_config.get(\"do_lower_case\", False):\n            prompt = prompt.lower()\n\n        return tokenizer.encode(prompt=prompt,\n                                lora_request=lora_request,\n                                **tokenization_kwargs)\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        lora_request: Optional[LoRARequest],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> list[int]:\n        \"\"\"Async version of {meth}`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group()\n        tokenization_kwargs = self._get_tokenization_kw(tokenization_kwargs)\n\n        return await tokenizer.encode_async(prompt=prompt,\n                                            lora_request=lora_request,\n                                            **tokenization_kwargs)\n\n    def _get_mm_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest],\n    ) -> AnyTokenizer:\n        # PrithviGeoSpatialMAE needs to be initialized without a tokenizer\n        # while using also multi-modal input\n        if not self.tokenizer:\n            return cast(AnyTokenizer, object())  # Dummy\n\n        tokenizer_group = self.get_tokenizer_group()\n        return tokenizer_group.get_lora_tokenizer(lora_request)\n\n    async def _get_mm_tokenizer_async(\n        self,\n        lora_request: Optional[LoRARequest],\n    ) -> AnyTokenizer:\n        # PrithviGeoSpatialMAE needs to be initialized without a tokenizer\n        # while using also multi-modal input\n        if not self.tokenizer:\n            return cast(AnyTokenizer, object())  # Dummy\n\n        tokenizer_group = self.get_tokenizer_group()\n        return await tokenizer_group.get_lora_tokenizer_async(lora_request)\n\n    def _process_multimodal(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        mm_processor_kwargs: Optional[Mapping[str, object]],\n        lora_request: Optional[LoRARequest],\n        return_mm_hashes: bool = False,\n    ) -> MultiModalInputs:\n        \"\"\"\n        Apply the model's multi-modal processor to a multi-modal prompt,\n        returning the corresponding token IDs and metadata.\n        \"\"\"\n        tokenizer = self._get_mm_tokenizer(lora_request)\n\n        mm_processor = self.mm_registry.create_processor(self.model_config,\n                                                         tokenizer=tokenizer)\n\n        if mm_processor_kwargs is None:\n            mm_processor_kwargs = {}\n\n        return mm_processor.apply(prompt, mm_data, mm_processor_kwargs,\n                                  return_mm_hashes)\n\n    async def _process_multimodal_async(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        mm_processor_kwargs: Optional[Mapping[str, object]],\n        lora_request: Optional[LoRARequest],\n        return_mm_hashes: bool = False,\n    ) -> MultiModalInputs:\n        \"\"\"Async version of {meth}`_process_multimodal`.\"\"\"\n        tokenizer = await self._get_mm_tokenizer_async(lora_request)\n\n        mm_processor = self.mm_registry.create_processor(self.model_config,\n                                                         tokenizer=tokenizer)\n        if mm_processor_kwargs is None:\n            mm_processor_kwargs = {}\n\n        return mm_processor.apply(prompt, mm_data, mm_processor_kwargs,\n                                  return_mm_hashes)\n\n    def _process_embeds(\n        self,\n        parsed_content: EmbedsPrompt,\n    ) -> EmbedsInputs:\n        if not self.model_config.enable_prompt_embeds:\n            raise ValueError(\"You must set `--enable-prompt-embeds` to input \"\n                             \"`prompt_embeds`.\")\n\n        prompt_embeds = parsed_content[\"prompt_embeds\"]\n\n        # prompt_embeds must be (seq_len, hidden_size), but if the user\n        # passes in a batch of size 1, i.e. (1, seq_len, hidden_size),\n        # we can unambiguously process the intent by squeezing the batch\n        # dimension.\n        if prompt_embeds.ndim == 3:\n            prompt_embeds = prompt_embeds.squeeze(dim=0)\n\n        if prompt_embeds.ndim != 2:\n            raise ValueError(\n                \"prompt_embeds must be of shape (seq_len, hidden_size).\")\n\n        return embeds_inputs(prompt_embeds=prompt_embeds,\n                             cache_salt=parsed_content.get(\"cache_salt\"))\n\n    async def _process_embeds_async(\n        self,\n        parsed_content: EmbedsPrompt,\n    ) -> EmbedsInputs:\n        return self._process_embeds(parsed_content)\n\n    def _process_tokens(\n        self,\n        parsed_content: TokensPrompt,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_token_ids = parsed_content[\"prompt_token_ids\"]\n        token_type_ids = parsed_content.get(\"token_type_ids\")\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = self._process_multimodal(\n                prompt_token_ids,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            inputs = token_inputs(\n                prompt_token_ids=prompt_token_ids,\n                token_type_ids=token_type_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    async def _process_tokens_async(\n        self,\n        parsed_content: TokensPrompt,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_token_ids = parsed_content[\"prompt_token_ids\"]\n        token_type_ids = parsed_content.get(\"token_type_ids\")\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = await self._process_multimodal_async(\n                prompt_token_ids,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            inputs = token_inputs(\n                prompt_token_ids=prompt_token_ids,\n                token_type_ids=token_type_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    def _process_text(\n        self,\n        parsed_content: TextPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_text = parsed_content[\"prompt\"]\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = self._process_multimodal(\n                prompt_text,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            prompt_token_ids = self._tokenize_prompt(\n                prompt_text,\n                lora_request=lora_request,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            inputs = token_inputs(\n                prompt=prompt_text,\n                prompt_token_ids=prompt_token_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    async def _process_text_async(\n        self,\n        parsed_content: TextPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_text = parsed_content[\"prompt\"]\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = await self._process_multimodal_async(\n                prompt_text,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt_text,\n                lora_request=lora_request,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            inputs = token_inputs(\n                prompt=prompt_text,\n                prompt_token_ids=prompt_token_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    def _prompt_to_llm_inputs(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> SingletonInputs:\n        \"\"\"\n        Extract the singleton inputs from a prompt.\n\n        Arguments:\n\n        * prompt: single encoder or decoder input prompt\n        * lora_request: this is only valid for decoder prompts\n        * return_mm_hashes: whether to return multimodal hashes\n\n        Returns:\n\n        * {class}`SingletonInputs` instance\n        \"\"\"\n        parsed = parse_singleton_prompt(prompt)\n\n        if parsed[\"type\"] == \"embeds\":\n            return self._process_embeds(parsed[\"content\"])\n        if parsed[\"type\"] == \"tokens\":\n            return self._process_tokens(\n                parsed[\"content\"],\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"text\":\n            return self._process_text(\n                parsed[\"content\"],\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"str\":\n            return self._process_text(\n                TextPrompt(prompt=parsed[\"content\"]),\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n\n        assert_never(parsed)\n\n    async def _prompt_to_llm_inputs_async(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> SingletonInputs:\n        \"\"\"Async version of {meth}`_prompt_to_llm_inputs`.\"\"\"\n        parsed = parse_singleton_prompt(prompt)\n\n        if parsed[\"type\"] == \"embeds\":\n            return await self._process_embeds_async(parsed[\"content\"])\n        if parsed[\"type\"] == \"tokens\":\n            return await self._process_tokens_async(\n                parsed[\"content\"],\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"text\":\n            return await self._process_text_async(\n                parsed[\"content\"],\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"str\":\n            return await self._process_text_async(\n                TextPrompt(prompt=parsed[\"content\"]),\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n\n        assert_never(parsed)\n\n    def _build_enc_dec_llm_inputs(\n        self,\n        encoder_inputs: SingletonInputs,\n        decoder_inputs: Optional[SingletonInputs],\n    ) -> EncoderDecoderInputs:\n        if (encoder_inputs[\"type\"] == \"embeds\"\n                or decoder_inputs and decoder_inputs[\"type\"] == \"embeds\"):\n            raise ValueError(\"Embedding inputs are not supported for encoder-\"\n                             \"decoder models\")\n\n        # Needed for mypy\n        encoder_inputs = cast(Union[TokenInputs, MultiModalInputs],\n                              encoder_inputs)\n        decoder_inputs = cast(Optional[Union[TokenInputs, MultiModalInputs]],\n                              decoder_inputs)\n\n        if decoder_inputs is None:\n            if self.model_config.hf_config.model_type == \"whisper\":\n                # For Whisper models, the text prompt should go to the decoder.\n                # If no explicit encoder/decoder inputs, then copy the prompt\n                # from the encoder to the decoder. The encoder tokens are later\n                # overridden by the audio features.\n                dec_token_ids = encoder_inputs[\"prompt_token_ids\"].copy()\n            else:\n                dec_token_ids = self._prepare_decoder_input_ids_for_generation(\n                    None)\n            decoder_inputs = token_inputs(dec_token_ids)\n        else:\n            if \"multi_modal_data\" in decoder_inputs:\n                raise ValueError(\"Multi-modal decoder inputs of encoder-\"\n                                 \"decoder models are not supported yet\")\n\n            dec_token_ids = self._prepare_decoder_input_ids_for_generation(\n                decoder_inputs[\"prompt_token_ids\"])\n            decoder_inputs[\"prompt_token_ids\"] = dec_token_ids\n\n        return EncoderDecoderInputs(\n            encoder=encoder_inputs,\n            decoder=decoder_inputs,\n        )\n\n    def _split_enc_dec_mm_inputs(\n        self,\n        inputs: Union[SingletonInputs, MultiModalEncDecInputs],\n        decoder_inputs_to_override: Optional[SingletonInputs] = None,\n    ) -> tuple[SingletonInputs, SingletonInputs]:\n        \"\"\"\n        For encoder/decoder models only:\n        Separate Encoder/Decoder inputs from a MultiModalEncDecInputs\n        \"\"\"\n        if (inputs[\"type\"] == \"embeds\" or decoder_inputs_to_override\n                and decoder_inputs_to_override[\"type\"] == \"embeds\"):\n            raise ValueError(\"Embedding inputs are not supported for encoder-\"\n                             \"decoder models\")\n\n        # Needed for mypy\n        inputs = cast(\n            Union[TokenInputs, MultiModalInputs, MultiModalEncDecInputs],\n            inputs,\n        )\n        decoder_inputs_to_override = cast(\n            Optional[Union[TokenInputs, MultiModalInputs]],\n            decoder_inputs_to_override,\n        )\n\n        encoder_inputs: SingletonInputs\n        decoder_inputs: SingletonInputs\n\n        if inputs[\"type\"] == \"multimodal\":  # Multimodal data inputs\n            if not (\"encoder_prompt\" in inputs\n                    and \"encoder_prompt_token_ids\" in inputs):\n                raise RuntimeError(\"You should register an encoder-decoder \"\n                                   \"multi-modal processor for encoder-decoder \"\n                                   \"models.\")\n            inputs = cast(MultiModalEncDecInputs, inputs)\n\n            encoder_inputs = token_inputs(\n                prompt=inputs[\"encoder_prompt\"],\n                prompt_token_ids=inputs[\"encoder_prompt_token_ids\"],\n            )\n\n            decoder_prompt_inputs = decoder_inputs_to_override or inputs\n            decoder_inputs = MultiModalInputs(\n                type=\"multimodal\",\n                prompt=decoder_prompt_inputs.get(\"prompt\", \"\"),\n                prompt_token_ids=decoder_prompt_inputs[\"prompt_token_ids\"],\n                mm_kwargs=inputs[\"mm_kwargs\"],\n                mm_hashes=inputs[\"mm_hashes\"],\n                mm_placeholders=inputs[\"mm_placeholders\"],\n            )\n            if cache_salt := inputs.get(\"cache_salt\"):\n                decoder_inputs[\"cache_salt\"] = cache_salt\n\n        elif inputs[\"type\"] == \"token\":  # Text-only inputs\n            encoder_inputs = token_inputs(prompt=\"\", prompt_token_ids=[])\n            decoder_inputs = decoder_inputs_to_override or inputs\n        else:\n            assert_never(inputs)  # type: ignore[arg-type]\n\n        return encoder_inputs, decoder_inputs\n\n    def _process_encoder_decoder_prompt(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> EncoderDecoderInputs:\n        \"\"\"\n        For encoder/decoder models only:\n        Process an input prompt into an {class}`EncoderDecoderInputs` instance.\n\n        There are two types of input prompts:\n        singleton prompts which carry only the\n        encoder prompt, and explicit encoder/decoder\n        prompts which carry both the encoder and the\n        decoder prompts as member variables.\n\n        This function handles the following scenarios:\n        * Singleton encoder prompt: extract encoder prompt\n          token ids & infer default decoder prompt token ids\n        * Explicit encoder/decoder prompt: extract encoder\n          and decoder prompt token ids\n\n        Note that for Explicit encoder/decoder prompts,\n        each sub-prompt (encoder or decoder prompt) can\n        have any possible singleton type; thus this\n        method relies on helper functions to obtain\n        token ids for the sub-prompts.\n\n        Arguments:\n\n        * prompt: an input prompt\n\n        Returns:\n\n        * {class}`EncoderDecoderInputs` instance\n        \"\"\"\n        encoder_inputs: SingletonInputs\n        decoder_inputs: Optional[SingletonInputs]\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            encoder_inputs = self._prompt_to_llm_inputs(\n                prompt[\"encoder_prompt\"],\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            if (decoder_input := prompt[\"decoder_prompt\"]) is None:\n                decoder_inputs = None\n            else:\n                decoder_inputs = self._prompt_to_llm_inputs(decoder_input)\n            # For multimodal model, override decoder prompt from processor\n            # with explicit decoder prompt.\n            if self.model_config.is_multimodal_model:\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(encoder_inputs,\n                                                  decoder_inputs))\n        else:\n            inputs = self._prompt_to_llm_inputs(\n                prompt,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            if self.model_config.is_multimodal_model:\n                # Encoder-Decoder Multimodal model\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(inputs))\n            else:\n                encoder_inputs = inputs\n                decoder_inputs = None\n\n        return self._build_enc_dec_llm_inputs(encoder_inputs, decoder_inputs)\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> EncoderDecoderInputs:\n        \"\"\"Async version of {meth}`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_inputs: SingletonInputs\n        decoder_inputs: Optional[SingletonInputs]\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            encoder_task = self._prompt_to_llm_inputs_async(\n                prompt[\"encoder_prompt\"],\n                tokenization_kwargs=tokenization_kwargs,\n            )\n\n            if (decoder_input := prompt[\"decoder_prompt\"]) is None:\n                encoder_inputs = await encoder_task\n                decoder_inputs = None\n            else:\n                decoder_task = self._prompt_to_llm_inputs_async(\n                    decoder_input,\n                    tokenization_kwargs=tokenization_kwargs,\n                )\n\n                encoder_inputs, decoder_inputs = await asyncio.gather(\n                    encoder_task, decoder_task)\n\n            # For multimodal model, override decoder prompt from processor\n            # with explicit decoder prompt.\n            if self.model_config.is_multimodal_model:\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(encoder_inputs,\n                                                  decoder_inputs))\n        else:\n            inputs = await self._prompt_to_llm_inputs_async(\n                prompt,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            if self.model_config.is_multimodal_model:\n                # Encoder-Decoder Multimodal model\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(inputs))\n            else:\n                encoder_inputs = inputs\n                decoder_inputs = None\n\n        return self._build_enc_dec_llm_inputs(encoder_inputs, decoder_inputs)\n\n    def _build_decoder_only_llm_inputs(\n        self,\n        prompt_inputs: DecoderOnlyInputs,\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n    ) -> DecoderOnlyInputs:\n        if \"prompt_token_ids\" in prompt_inputs:\n            prompt_inputs = cast(Union[TokenInputs, MultiModalInputs],\n                                 prompt_inputs)  # Needed for mypy\n            prompt_inputs[\"prompt_token_ids\"] = self._apply_prompt_adapter(\n                prompt_inputs[\"prompt_token_ids\"],\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return prompt_inputs\n\n    def _process_decoder_only_prompt(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> DecoderOnlyInputs:\n        \"\"\"\n        For decoder-only models:\n        Process an input prompt into an {class}`DecoderOnlyInputs` instance.\n\n        Arguments:\n\n        * prompt: input prompt\n        * lora_request\n        * prompt_adapter_request\n        * return_mm_hashes\n\n        Returns:\n\n        * {class}`DecoderOnlyInputs` instance\n        \"\"\"\n\n        prompt_comps = self._prompt_to_llm_inputs(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> DecoderOnlyInputs:\n        \"\"\"Async version of {meth}`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._prompt_to_llm_inputs_async(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    def preprocess(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> ProcessorInputs:\n        \"\"\"Preprocess the input prompt.\"\"\"\n        if self.model_config.is_encoder_decoder:\n            assert not return_mm_hashes, (\n                \"Multimodal hashes for encoder-decoder models should not be \",\n                \"returned until they are supported on vLLM V1.\")\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            return self._process_encoder_decoder_prompt(prompt)\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                             \"to decoder-only models\")\n\n        # Decoder-only operation\n        return self._process_decoder_only_prompt(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n    async def preprocess_async(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> ProcessorInputs:\n        \"\"\"Async version of {meth}`preprocess`.\"\"\"\n        if self.model_config.is_encoder_decoder:\n            assert not return_mm_hashes, (\n                \"Multimodal hashes for encoder-decoder models should not be \",\n                \"returned until they are supported on vLLM V1.\")\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            return await self._process_encoder_decoder_prompt_async(prompt)\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                             \"to decoder-only models\")\n\n        # Decoder-only operation\n        return await self._process_decoder_only_prompt_async(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n", 886], "/home/jeromeku/vllm/vllm/multimodal/processing.py": ["# SPDX-License-Identifier: Apache-2.0\nimport json\nimport re\nimport sys\nfrom abc import ABC, abstractmethod\nfrom collections import defaultdict\nfrom collections.abc import (Callable, Generator, ItemsView, Iterable, Mapping,\n                             Sequence)\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom functools import lru_cache\nfrom typing import (TYPE_CHECKING, Generic, NamedTuple, Optional, Protocol,\n                    TypeVar, Union, cast)\n\nimport torch\nfrom typing_extensions import assert_never\n\nfrom vllm.inputs import InputProcessingContext\nfrom vllm.jsontree import json_map_leaves, json_reduce_leaves\nfrom vllm.logger import init_logger\nfrom vllm.transformers_utils.tokenizer import (AnyTokenizer, decode_tokens,\n                                               encode_tokens)\nfrom vllm.utils import GiB_bytes, LRUCache, flatten_2d_lists, full_groupby\n\nfrom .hasher import MultiModalHasher\nfrom .inputs import (MultiModalDataDict, MultiModalEncDecInputs,\n                     MultiModalFieldConfig, MultiModalInputs, MultiModalKwargs,\n                     MultiModalKwargsItem, NestedTensors, PlaceholderRange)\nfrom .parse import (DictEmbeddingItems, EmbeddingItems, MultiModalDataItems,\n                    MultiModalDataParser)\n\nif TYPE_CHECKING:\n    from transformers.configuration_utils import PretrainedConfig\n    from transformers.feature_extraction_utils import BatchFeature\n    from transformers.processing_utils import ProcessorMixin\n\n    from .profiling import BaseDummyInputsBuilder\n\nlogger = init_logger(__name__)\n\n_S = TypeVar(\"_S\", str, list[int])\n\nPromptSeq = Union[str, list[int]]\n\"\"\"A token sequence (list of token IDs) or text.\"\"\"\n\n\n@dataclass\nclass PromptIndex:\n    \"\"\"Resolves to an index in the prompt.\"\"\"\n    get_match_index: Callable[[AnyTokenizer, PromptSeq], Optional[int]]\n\n\nclass PromptIndexTargets:\n\n    @staticmethod\n    def start() -> PromptIndex:\n        \"\"\"\n        Resolves to the start of the prompt (before the first token).\n\n        This results in a match even if the prompt is empty.\n        \"\"\"\n        return PromptIndex(lambda tok, prompt: 0)\n\n    @staticmethod\n    def prefix(seq: PromptSeq) -> PromptIndex:\n        \"\"\"\n        Resolves to a location in the prompt after the given prefix.\n        \"\"\"\n\n        def get_match_index(\n            tokenizer: AnyTokenizer,\n            prompt: PromptSeq,\n        ) -> Optional[int]:\n            prefix = seq\n\n            if isinstance(prompt, str):\n                if not isinstance(prefix, str):\n                    # Make both `str`\n                    prefix = decode_tokens(tokenizer, prefix)\n            else:\n                if isinstance(prefix, str):\n                    # Make both `list[int]`\n                    prefix = encode_tokens(tokenizer,\n                                           prefix,\n                                           add_special_tokens=False)\n\n            match_idx = len(prefix)\n            return match_idx if prompt[:match_idx] == prefix else None\n\n        return PromptIndex(get_match_index)\n\n    @staticmethod\n    def end() -> PromptIndex:\n        \"\"\"\n        Resolves to the end of the prompt (after the last token).\n\n        This results in a match even if the prompt is empty.\n        \"\"\"\n        return PromptIndex(lambda tok, prompt: len(prompt))\n\n\nPromptTarget = Union[PromptSeq, PromptIndex]\n\"\"\"\nThe token sequence or text to update.\n\"\"\"\n\n\n@dataclass\nclass PromptUpdateDetails(Generic[_S]):\n    \"\"\"Details about the token sequence or text that are part of the update.\"\"\"\n\n    full: _S\n    \"\"\"The full content.\"\"\"\n\n    is_embed: Optional[Callable[[\"_BoundPromptSequence\"], torch.Tensor]] = None\n    \"\"\"\n    Given {attr}`full`, return a boolean mask of shape `(len(full),)`\n    indicating which positions of `full` to assign embeddings to.\n\n    `None` (default) means to assign embeddings to all positions of `full`.\n\n    The embeddings are obtained by calling\n    {class}`SupportsMultiModal.get_multimodal_embeddings`.\n    \"\"\"\n\n    @staticmethod\n    def from_seq(seq: _S) -> \"PromptUpdateDetails[_S]\":\n        return PromptUpdateDetails(full=seq)\n\n    @staticmethod\n    def select_text(\n        seq: _S,\n        embed_text: str,\n    ) -> \"PromptUpdateDetails[_S]\":\n\n        def is_embed(full: \"_BoundPromptSequence\") -> torch.Tensor:\n            embed_token_ids = encode_tokens(full.tokenizer, embed_text)\n\n            return torch.isin(\n                torch.tensor(full.token_ids),\n                torch.tensor(embed_token_ids),\n            )\n\n        return PromptUpdateDetails(full=seq, is_embed=is_embed)\n\n    @staticmethod\n    def select_token_id(\n        seq: _S,\n        embed_token_id: int,\n    ) -> \"PromptUpdateDetails[_S]\":\n        return PromptUpdateDetails(\n            full=seq,\n            is_embed=lambda f: torch.tensor(f.token_ids) == embed_token_id,\n        )\n\n\nPromptUpdateInfo = Union[PromptSeq, PromptUpdateDetails]\n\"\"\"\nThe token sequence or text that are part of the update.\n\nIf only part of the content corresponds to feature placeholders, you can\nuse {class}`PromptUpdateDetails` to specify which part.\n\"\"\"\n\nPromptUpdateContent = Union[Callable[[int], PromptUpdateInfo],\n                            PromptUpdateInfo]\n\"\"\"\nGiven the index of the processed item within {attr}`modality`,\noutput the corresponding token sequence (or text).\n\nFor convenience, you can directly pass in the token sequence (or text)\ninstead of a function if it does not depend on the input.\n\"\"\"\n\n\nclass UpdateMode(str, Enum):\n    INSERT = \"insert\"\n    REPLACE = \"replace\"\n\n\n@dataclass\nclass PromptUpdate(ABC):\n    \"\"\"\n    Defines how to update a prompt with placeholder tokens.\n    \"\"\"\n\n    modality: str\n    \"\"\"The modality for which the update is made.\"\"\"\n\n    target: PromptTarget\n    \"\"\"The token sequence (or text) to update.\"\"\"\n\n    @property\n    @abstractmethod\n    def content(self) -> PromptUpdateContent:\n        \"\"\"The placeholder tokens that are part of the update.\"\"\"\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def mode(self) -> UpdateMode:\n        \"\"\"Defines how to update the prompt.\"\"\"\n        raise NotImplementedError\n\n    def bind(self, tokenizer: AnyTokenizer) -> \"BoundPromptUpdate\":\n        return BoundPromptUpdate(\n            _origin=self,\n            tokenizer=tokenizer,\n        )\n\n\n@dataclass\nclass PromptInsertion(PromptUpdate):\n    \"\"\"\n    Defines how to insert placeholder tokens into a prompt.\n\n    Example:\n\n    For each image, insert a number of ``<image>`` feature placeholders\n    equal to the feature size of the vision encoder after the ``<s>`` token:\n\n    ```python\n    PromptInsertion(\n        modality=\"image\",\n        target=\"<s>\",\n        insertion=\"<image>\" * image_feature_size,\n    )\n    ```\n\n    Insert these tokens at the start of the prompt:\n\n    ```python\n    PromptInsertion(\n        modality=\"image\",\n        target=PromptIndexTargets.start(),\n        insertion=\"<image>\" * image_feature_size,\n    )\n    ```\n\n    Insert these tokens after a prefix ``Images:``:\n\n    ```python\n    PromptInsertion(\n        modality=\"image\",\n        target=PromptIndexTargets.prefix(\"Images:\"),\n        insertion=\"<image>\" * image_feature_size,\n    )\n    ```\n\n    Insert these tokens at the end of the prompt:\n\n    ```python\n    PromptInsertion(\n        modality=\"image\",\n        target=PromptIndexTargets.end(),\n        insertion=\"<image>\" * image_feature_size,\n    )\n    ```\n    \"\"\"\n\n    insertion: PromptUpdateContent = field(repr=False)\n    \"\"\"\n    Given the index of the processed item within {attr}`modality`,\n    output the token sequence (or text) to insert right after {attr}`target`.\n\n    For convenience, you can directly pass in the token sequence (or text)\n    instead of a function if it does not depend on the input.\n    \"\"\"\n\n    @property\n    def content(self) -> PromptUpdateContent:\n        return self.insertion\n\n    @property\n    def mode(self) -> UpdateMode:\n        return UpdateMode.INSERT\n\n\n@dataclass\nclass PromptReplacement(PromptUpdate):\n    \"\"\"\n    Defines how to replace portions of an input prompt with placeholder tokens.\n\n    Example:\n\n    For each image, replace one ``<image>`` input placeholder in the prompt\n    with a number of ``<image>`` feature placeholders\n    equal to the feature size of the vision encoder:\n\n    ```python\n    PromptReplacement(\n        modality=\"image\",\n        target=\"<image>\",\n        replacement=\"<image>\" * image_feature_size,\n    )\n    ```\n\n    As above, but further pad the feature placeholders with ``<image_bos>``\n    and `<image_eos>``, which are not supposed to be passed to the vision\n    encoder:\n\n    ```python\n    PromptReplacement(\n        modality=\"image\",\n        target=\"<image>\",\n        replacement=PromptUpdateDetails(\n            full=\"\".join([\n                \"<image_bos>\",\n                \"<image>\" * image_feature_size,\n                \"<image_eos>\",\n            ]),\n            features=\"<image>\" * image_feature_size,\n        ),\n    )\n    ```\n\n    To avoid unnecessary tokenization during prompt replacement,\n    we recommended passing token sequences instead of text:\n\n    ```python\n    PromptReplacement(\n        modality=\"image\",\n        target=[image_token_id],\n        replacement=PromptUpdateDetails(\n            full=([image_bos_id] + [image_token_id] * image_feature_size\n                    + [image_eos_id]),\n            features=[image_token_id] * image_feature_size,\n        ),\n    )\n    ```\n    \"\"\"\n\n    replacement: PromptUpdateContent = field(repr=False)\n    \"\"\"\n    Given the index of the processed item within {attr}`modality`,\n    output the token sequence (or text) to replace {attr}`target`.\n\n    For convenience, you can directly pass in the token sequence (or text)\n    instead of a function if it does not depend on the input.\n    \"\"\"\n\n    @property\n    def content(self) -> PromptUpdateContent:\n        return self.replacement\n\n    @property\n    def mode(self) -> UpdateMode:\n        return UpdateMode.REPLACE\n\n\n@lru_cache(maxsize=2048)\ndef _cached_encode(\n    tokenizer: AnyTokenizer,\n    text: str,\n    *,\n    add_special_tokens: Optional[bool] = None,\n) -> list[int]:\n    return encode_tokens(tokenizer,\n                         text,\n                         add_special_tokens=add_special_tokens)\n\n\n@lru_cache(maxsize=2048)\ndef _cached_decode(\n    tokenizer: AnyTokenizer,\n    token_ids: tuple[int, ...],\n    *,\n    skip_special_tokens: Optional[bool] = None,\n) -> str:\n    return decode_tokens(tokenizer,\n                         list(token_ids),\n                         skip_special_tokens=skip_special_tokens)\n\n\nclass _HasModalityAttr(Protocol):\n    modality: str\n\n\nclass _HasModalityProp(Protocol):\n\n    @property\n    def modality(self) -> str:\n        ...\n\n\n_M = TypeVar(\"_M\", bound=Union[_HasModalityAttr, _HasModalityProp])\n\n\ndef full_groupby_modality(values: Iterable[_M]) -> ItemsView[str, list[_M]]:\n    \"\"\"Convenience function to apply {func}`full_groupby` based on modality.\"\"\"\n    return full_groupby(values, key=lambda x: x.modality)\n\n\n@dataclass\nclass _BoundPromptSequence:\n    \"\"\"\n    A {data}`_PromptSeq` bound to a tokenizer to automatically\n    convert between token sequence and text representations.\n    \"\"\"\n    tokenizer: AnyTokenizer = field(repr=False)\n\n    _text: Optional[str]\n    _token_ids: Optional[list[int]]\n\n    @staticmethod\n    def from_seq(\n        tokenizer: AnyTokenizer,\n        seq: PromptSeq,\n    ) -> \"_BoundPromptSequence\":\n        return _BoundPromptSequence(\n            tokenizer=tokenizer,\n            _text=seq if isinstance(seq, str) else None,\n            _token_ids=seq if isinstance(seq, list) else None,\n        )\n\n    def __post_init__(self) -> None:\n        if self._text is None and self._token_ids is None:\n            raise ValueError(\"At least one of 'text' and 'token_ids' must be \"\n                             \"specified\")\n\n    @property\n    def text(self) -> str:\n        if self._text is None:\n            assert self._token_ids is not None\n            self._text = _cached_decode(self.tokenizer, tuple(self._token_ids))\n\n        return self._text\n\n    @property\n    def token_ids(self) -> list[int]:\n        if self._token_ids is None:\n            assert self._text is not None\n            self._token_ids = _cached_encode(self.tokenizer,\n                                             self._text,\n                                             add_special_tokens=False)\n\n        return self._token_ids\n\n\n@dataclass\nclass _BoundPromptContent:\n    full: _BoundPromptSequence\n    is_embed: Optional[Callable[[\"_BoundPromptSequence\"], torch.Tensor]]\n\n\n@dataclass\nclass BoundPromptUpdate:\n    \"\"\"\n    A {class}`PromptUpdate` bound to a tokenizer to automatically convert\n    {attr}`target` and the result of {meth}`get_content` between\n    token sequence and text representations.\n    \"\"\"\n    _origin: PromptUpdate\n    tokenizer: AnyTokenizer = field(repr=False)\n\n    def __post_init__(self) -> None:\n        self._content_cache = dict[int, _BoundPromptContent]()\n\n    @property\n    def modality(self) -> str:\n        return self._origin.modality\n\n    @property\n    def target(self) -> Union[_BoundPromptSequence, PromptIndex]:\n        \"\"\"The token sequence (or text) to update.\"\"\"\n        target = self._origin.target\n\n        if isinstance(target, PromptIndex):\n            return target\n\n        return _BoundPromptSequence.from_seq(self.tokenizer, target)\n\n    @property\n    def content(self) -> PromptUpdateContent:\n        \"\"\"The placeholder tokens that are part of the update.\"\"\"\n        return self._origin.content\n\n    @property\n    def mode(self) -> UpdateMode:\n        \"\"\"Defines how to update the prompt.\"\"\"\n        return self._origin.mode\n\n    def get_content(self, item_idx: int) -> _BoundPromptContent:\n        \"\"\"\n        Given the index of the processed item within {attr}`modality`,\n        output the token sequence (or text) to update.\n        \"\"\"\n        content = self.content\n        if callable(content):\n            cache_key = item_idx\n            if cache_key in self._content_cache:\n                return self._content_cache[cache_key]\n\n            content = content(item_idx)\n        else:\n            cache_key = None\n\n        if not isinstance(content, PromptUpdateDetails):\n            content = PromptUpdateDetails.from_seq(content)\n\n        bound_full = _BoundPromptSequence.from_seq(self.tokenizer,\n                                                   content.full)\n        bound_content = _BoundPromptContent(full=bound_full,\n                                            is_embed=content.is_embed)\n\n        if cache_key is not None:\n            self._content_cache[cache_key] = bound_content\n\n        return bound_content\n\n\nclass _TokenMatch(NamedTuple):\n    start_idx: int\n    end_idx: int\n\n\ndef iter_token_matches(\n    token_ids: list[int],\n    match_ids: list[int],\n) -> Generator[_TokenMatch]:\n    \"\"\"\n    Yield each occurrence of `match_ids` in `token_ids`.\n\n    Note that empty matches are ignored.\n    \"\"\"\n    prompt_len = len(token_ids)\n    match_len = len(match_ids)\n\n    if match_len == 0:\n        return\n\n    start_idx = 0\n    while start_idx < prompt_len - match_len + 1:\n        end_idx = start_idx + match_len\n\n        if token_ids[start_idx:end_idx] == match_ids:\n            yield _TokenMatch(start_idx=start_idx, end_idx=end_idx)\n\n            # Exclude overlapping matches\n            start_idx = end_idx\n        else:\n            start_idx += 1\n\n\ndef replace_token_matches(\n    token_ids: list[int],\n    match_ids: list[int],\n    new_ids: list[int],\n) -> list[int]:\n    \"\"\"\n    Replace each occurrence of `match_ids` in `token_ids`\n    with `new_ids`.\n\n    Note that empty matches are ignored.\n    \"\"\"\n    out_seqs = list[list[int]]()\n    prev_end_idx = 0\n\n    for match in iter_token_matches(token_ids, match_ids):\n        start_idx = match.start_idx\n        end_idx = match.end_idx\n\n        out_seqs.append(token_ids[prev_end_idx:start_idx])\n        out_seqs.append(new_ids)\n        prev_end_idx = end_idx\n\n    out_seqs.append(token_ids[prev_end_idx:])\n\n    return flatten_2d_lists(out_seqs)\n\n\n@dataclass(repr=False)\nclass PromptTargetMatch(ABC):\n    _origin: BoundPromptUpdate\n\n    @property\n    def modality(self) -> str:\n        return self._origin.modality\n\n    @property\n    @abstractmethod\n    def start_idx(self) -> int:\n        raise NotImplementedError\n\n    @property\n    @abstractmethod\n    def end_idx(self) -> int:\n        raise NotImplementedError\n\n    def __repr__(self) -> str:\n        return (f\"{type(self).__name__}(modality={self.modality!r}, \"\n                f\"start_idx={self.start_idx!r}, end_idx={self.end_idx!r})\")\n\n\n@dataclass(repr=False)\nclass _PromptTargetIndexMatch(PromptTargetMatch):\n    match_idx: int\n\n    @property\n    def start_idx(self) -> int:\n        return self.match_idx\n\n    @property\n    def end_idx(self) -> int:\n        return self.match_idx\n\n\n@dataclass(repr=False)\nclass _PromptTargetTokenMatch(PromptTargetMatch):\n    match: _TokenMatch\n\n    @property\n    def start_idx(self) -> int:\n        return self.match.start_idx\n\n    @property\n    def end_idx(self) -> int:\n        return self.match.end_idx\n\n\n@dataclass(repr=False)\nclass _PromptTargetTextMatch(PromptTargetMatch):\n    match: re.Match[str]\n\n    @property\n    def start_idx(self) -> int:\n        return self.match.start()\n\n    @property\n    def end_idx(self) -> int:\n        return self.match.end()\n\n\n@dataclass\nclass PlaceholderFeaturesInfo:\n    modality: str\n    item_idx: int\n    start_idx: int\n    tokens: list[int]\n    is_embed: Optional[torch.Tensor]\n\n    @property\n    def length(self) -> int:\n        return len(self.tokens)\n\n    def to_range(self) -> PlaceholderRange:\n        # TODO: Is it worth it to optimize this by stripping the\n        # leading and ending positions where `is_embed=False`?\n        return PlaceholderRange(\n            offset=self.start_idx,\n            length=self.length,\n            is_embed=self.is_embed,\n        )\n\n\ndef find_token_matches(\n    prompt: list[int],\n    prompt_updates: Sequence[BoundPromptUpdate],\n) -> Sequence[PromptTargetMatch]:\n    \"\"\"Return each target of `prompt_updates` found in `prompt`.\"\"\"\n\n    def get_matches(update: BoundPromptUpdate):\n        target = update.target\n\n        if isinstance(target, PromptIndex):\n            match_idx = target.get_match_index(update.tokenizer, prompt)\n            if match_idx is None:\n                return []\n\n            return [_PromptTargetIndexMatch(update, match_idx)]\n\n        return [\n            _PromptTargetTokenMatch(update, match)\n            for match in iter_token_matches(prompt, target.token_ids)\n        ]\n\n    return [\n        match for update in prompt_updates for match in get_matches(update)\n    ]\n\n\ndef find_text_matches(\n    prompt: str,\n    prompt_updates: Sequence[BoundPromptUpdate],\n) -> Sequence[PromptTargetMatch]:\n    \"\"\"Return each target of `prompt_updates` found in `prompt`.\"\"\"\n\n    def get_matches(update: BoundPromptUpdate):\n        target = update.target\n\n        if isinstance(target, PromptIndex):\n            match_idx = target.get_match_index(update.tokenizer, prompt)\n            if match_idx is None:\n                return []\n\n            return [_PromptTargetIndexMatch(update, match_idx)]\n\n        return [\n            _PromptTargetTextMatch(update, match)\n            for match in re.finditer(re.escape(target.text), prompt)\n        ]\n\n    return [\n        match for update in prompt_updates for match in get_matches(update)\n    ]\n\n\ndef _resolve_matches(\n    prompt: PromptSeq,\n    mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n) -> list[PromptTargetMatch]:\n    \"\"\"\n    Resolve `mm_matches` to ensure that there are no overlapping matches,\n    and sort them such that earlier matches take priority over later ones.\n    \"\"\"\n    matches = [m for matches in mm_matches.values() for m in matches]\n\n    seen_matches: list[Optional[PromptTargetMatch]] = [None] * len(prompt)\n\n    for match in matches:\n        for idx in range(match.start_idx, match.end_idx):\n            if seen_matches[idx] is not None:\n                raise ValueError(\"Found overlapping matches \"\n                                 f\"({seen_matches[idx]} and {match}) \"\n                                 f\"at index={idx} of prompt={prompt}\")\n\n            seen_matches[idx] = match\n\n    return sorted(matches, key=lambda x: x.start_idx)\n\n\ndef _apply_matches(\n    prompt: _S,\n    mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n    mm_item_counts: Mapping[str, int],\n) -> list[_S]:\n    \"\"\"Apply the updates in `mm_matches` to `prompt`.\"\"\"\n    out_seqs = list[Union[str, list[int]]]()\n    prev_end_idx = 0\n    next_idx_by_modality = defaultdict[str, int](lambda: 0)\n\n    for match in _resolve_matches(prompt, mm_matches):\n        modality = match.modality\n\n        item_start_idx = next_idx_by_modality[modality]\n        max_item_count = mm_item_counts.get(modality, 0)\n        if item_start_idx >= max_item_count:\n            continue\n\n        start_idx = match.start_idx\n        end_idx = match.end_idx\n        origin = match._origin\n        mode = origin.mode\n\n        if mode == UpdateMode.INSERT:\n            out_seqs.append(prompt[prev_end_idx:end_idx])\n            num_inserts = max_item_count\n        elif mode == UpdateMode.REPLACE:\n            out_seqs.append(prompt[prev_end_idx:start_idx])\n            num_inserts = max_item_count if start_idx == end_idx else 1\n        else:\n            assert_never(mode)\n\n        item_end_idx = min(item_start_idx + num_inserts, max_item_count)\n\n        for item_idx in range(item_start_idx, item_end_idx):\n            content = origin.get_content(item_idx)\n            insert_seq = (content.full.text if isinstance(prompt, str) else\n                          content.full.token_ids)\n\n            out_seqs.append(insert_seq)\n\n        prev_end_idx = end_idx\n        next_idx_by_modality[modality] += item_end_idx - item_start_idx\n\n    out_seqs.append(prompt[prev_end_idx:])\n\n    return cast(list[_S], out_seqs)\n\n\ndef apply_token_matches(\n    prompt: list[int],\n    mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n    mm_item_counts: Mapping[str, int],\n) -> list[int]:\n    \"\"\"Apply the updates in `mm_matches` to `prompt`.\"\"\"\n    if not mm_matches:\n        return prompt\n\n    token_id_seqs = _apply_matches(prompt, mm_matches, mm_item_counts)\n\n    return flatten_2d_lists(token_id_seqs)\n\n\ndef apply_text_matches(\n    prompt: str,\n    mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n    mm_item_counts: Mapping[str, int],\n) -> str:\n    \"\"\"Apply the updates in `mm_matches` to `prompt`.\"\"\"\n    if not mm_matches:\n        return prompt\n\n    texts = _apply_matches(prompt, mm_matches, mm_item_counts)\n\n    return \"\".join(texts)\n\n\ndef _iter_placeholders(\n    mm_prompt_updates: Mapping[str, Sequence[BoundPromptUpdate]],\n    prompt: list[int],\n    mm_item_counts: Mapping[str, int],\n) -> Iterable[PlaceholderFeaturesInfo]:\n    \"\"\"\n    Yield each set of placeholder tokens found in `prompt`.\n\n    Matches are exclusive even when multiple modalities share\n    the same placeholder tokens. In that case, the modality that\n    appears earlier in `mm_prompt_updates` takes priority.\n\n    Note that empty matches are ignored.\n    \"\"\"\n    prompt_len = len(prompt)\n    item_idx_by_modality = defaultdict[str, int](lambda: 0)\n\n    start_idx = 0\n    while start_idx < prompt_len:\n        found = False\n\n        for modality, modality_updates in mm_prompt_updates.items():\n            item_idx = item_idx_by_modality[modality]\n            if item_idx >= mm_item_counts.get(modality, 0):\n                continue\n\n            for update_info in modality_updates:\n                content = update_info.get_content(item_idx)\n                content_tokens_full = content.full.token_ids\n                content_len_full = len(content_tokens_full)\n                end_idx_full = start_idx + content_len_full\n\n                if content_len_full == 0 or end_idx_full > prompt_len:\n                    continue\n\n                if prompt[start_idx:end_idx_full] == content_tokens_full:\n                    content_is_embed = content.is_embed\n                    if content_is_embed is not None:\n                        content_is_embed = content_is_embed(content.full)\n\n                    yield PlaceholderFeaturesInfo(\n                        modality=modality,\n                        item_idx=item_idx,\n                        start_idx=start_idx,\n                        tokens=content_tokens_full,\n                        is_embed=content_is_embed,\n                    )\n\n                    # Exclude overlapping matches\n                    start_idx = end_idx_full\n                    item_idx_by_modality[modality] += 1\n                    found = True\n                    break\n\n            if found:\n                break  # Go back to the outer while loop\n\n        if not found:\n            start_idx += 1\n\n\ndef find_mm_placeholders(\n    mm_prompt_updates: Mapping[str, Sequence[BoundPromptUpdate]],\n    prompt: list[int],\n    mm_item_counts: Mapping[str, int],\n) -> Mapping[str, list[PlaceholderFeaturesInfo]]:\n    it = _iter_placeholders(mm_prompt_updates, prompt, mm_item_counts)\n    return dict(full_groupby_modality(it))\n\n\n_V = TypeVar(\"_V\", bound=\"Union[MultiModalKwargs, MultiModalKwargsItem]\")\n\n\nclass ProcessingCacheOptionalItem(NamedTuple):\n    key: str\n    value: Optional[MultiModalKwargsItem]\n\n\nclass ProcessingCacheItem(NamedTuple):\n    key: str\n    value: MultiModalKwargsItem\n\n\nclass ProcessingCache:\n\n    @staticmethod\n    def get_lru_cache(\n        capacity_gb: float,\n        value_type: type[_V],\n        *,\n        debug: bool = False,\n    ) -> LRUCache[str, _V]:\n\n        def get_leaf_size(leaf: object) -> int:\n            # MultiModalKwargs is not a subclass of dict\n            if isinstance(leaf, MultiModalKwargs):\n                return get_item_size(leaf.data)\n\n            # MultiModalKwargsItem is not a subclass of dict\n            if isinstance(leaf, MultiModalKwargsItem):\n                leaf_data = {k: v.data for k, v in leaf.items()}\n                return get_item_size(leaf_data)\n\n            # sys.getsizeof doesn't work for tensors\n            if isinstance(leaf, torch.Tensor):\n                return leaf.nbytes\n\n            return sys.getsizeof(leaf)\n\n        def get_item_size(\n            value: Union[MultiModalKwargs, MultiModalKwargsItem,\n                         Mapping[str, NestedTensors]]\n        ) -> int:\n            size = json_reduce_leaves(\n                lambda a, b: a + b,\n                json_map_leaves(get_leaf_size, value),\n            )\n\n            if debug:\n                logger.debug(\"Calculated size of %s to be %.2f GiB\",\n                             type(value), size / GiB_bytes)\n\n            return size\n\n        return LRUCache(GiB_bytes * capacity_gb, getsizeof=get_item_size)\n\n    def __init__(\n        self,\n        capacity_gb: float,\n        *,\n        debug_cache_hit_ratio_steps: Optional[int] = None,\n    ) -> None:\n        super().__init__()\n\n        self.debug_cache_hit_ratio_steps = debug_cache_hit_ratio_steps\n        self.debug_cache_hits = 0\n        self.debug_cache_total = 0\n\n        self._cache = self.get_lru_cache(\n            capacity_gb,\n            MultiModalKwargsItem,\n            debug=bool(debug_cache_hit_ratio_steps),\n        )\n\n    def _maybe_log_cache_stats(self) -> None:\n        steps = self.debug_cache_hit_ratio_steps\n        if not steps:\n            return\n\n        total = self.debug_cache_total\n        if total > 0 and total % steps == 0:\n            logger.debug(\"ProcessingCache: hit_ratio = %.2f\",\n                         self.debug_cache_hits / total)\n            logger.debug(\"ProcessingCache: size = %.2f / %.2f GiB\",\n                         self._cache.currsize / GiB_bytes,\n                         self._cache.maxsize / GiB_bytes)\n\n    def get(\n        self,\n        model_id: str,\n        modality: str,\n        input_item: object,\n        input_kwargs: Mapping[str, object],\n    ) -> Optional[MultiModalKwargsItem]:\n        \"\"\"\n        Get a processed multi-modal item from the cache\n        according to its dependencies, including:\n\n        - The model ID\n        - The modality of the item\n        - The original data item passed to the HF processor\n        - The configuration options of the HF processor\n        \"\"\"\n        self._maybe_log_cache_stats()\n\n        cache_key = MultiModalHasher.hash_kwargs(model_id=model_id,\n                                                 **{modality: input_item},\n                                                 **input_kwargs)\n\n        if self.debug_cache_hit_ratio_steps:\n            if cache_key in self._cache:\n                self.debug_cache_hits += 1\n\n            self.debug_cache_total += 1\n\n        return self._cache.get(cache_key)\n\n    def get_item(\n        self,\n        model_id: str,\n        modality: str,\n        input_item: object,\n        input_kwargs: Mapping[str, object],\n    ) -> ProcessingCacheOptionalItem:\n        cache_key = MultiModalHasher.hash_kwargs(model_id=model_id,\n                                                 **{modality: input_item},\n                                                 **input_kwargs)\n\n        return ProcessingCacheOptionalItem(\n            key=cache_key,\n            value=self._cache.get(cache_key),\n        )\n\n    def put(\n        self,\n        model_id: str,\n        modality: str,\n        input_item: object,\n        input_kwargs: Mapping[str, object],\n        output_kwargs: MultiModalKwargsItem,\n    ) -> None:\n        \"\"\"\n        Put a processed multi-modal item into the cache\n        according to its dependencies (see {meth}`get`).\n        \"\"\"\n        cache_key = MultiModalHasher.hash_kwargs(model_id=model_id,\n                                                 **{modality: input_item},\n                                                 **input_kwargs)\n        self._cache[cache_key] = output_kwargs\n\n    def put_item(self, item: ProcessingCacheItem) -> None:\n        self._cache[item.key] = item.value\n\n    def reset(self) -> bool:\n        self._cache.clear()\n\n        return True\n\n\nclass BaseProcessingInfo:\n    \"\"\"Base class to provide the information necessary for data processing.\"\"\"\n\n    def __init__(self, ctx: InputProcessingContext) -> None:\n        super().__init__()\n\n        self.ctx = ctx\n\n    @property\n    def model_id(self) -> str:\n        return self.ctx.model_config.model\n\n    def get_tokenizer(self) -> AnyTokenizer:\n        return self.ctx.tokenizer\n\n    def get_hf_config(self) -> \"PretrainedConfig\":\n        return self.ctx.get_hf_config()\n\n    def get_hf_processor(self, **kwargs: object) -> \"ProcessorMixin\":\n        \"\"\"\n        Subclasses can override this method to handle\n        specific kwargs from model config or user inputs.\n        \"\"\"\n        return self.ctx.get_hf_processor(**kwargs)\n\n    @abstractmethod\n    def get_supported_mm_limits(self) -> Mapping[str, Optional[int]]:\n        \"\"\"\n        Return the maximum supported number of items for each modality.\n\n        A value of `None` means unlimited number of items.\n\n        Omitting a modality from the returned dictionary means that\n        it is not supported at all.\n        \"\"\"\n        raise NotImplementedError\n\n    def get_allowed_mm_limits(self) -> Mapping[str, int]:\n        \"\"\"Return the maximum allowed number of items for each modality.\"\"\"\n        supported_mm_limits = self.get_supported_mm_limits()\n        mm_config = self.ctx.get_mm_config()\n\n        allowed_limits = dict[str, int]()\n        for modality, supported_limit in supported_mm_limits.items():\n            user_limit = mm_config.get_limit_per_prompt(modality)\n\n            allowed_limits[modality] = (user_limit if supported_limit is None\n                                        else min(user_limit, supported_limit))\n\n        return allowed_limits\n\n\n_I = TypeVar(\"_I\", bound=BaseProcessingInfo)\n\nMultiModalHashes = dict[str, list[str]]\n\"\"\"\nA collection of hashes with a similar structure as {class}`MultiModalKwargs`.\n\"\"\"\n\n\nclass BaseMultiModalProcessor(ABC, Generic[_I]):\n    \"\"\"\n    Abstract base class to process multi-modal inputs to be used in vLLM.\n\n    Not to be confused with {class}`transformers.ProcessorMixin`.\n    \"\"\"\n\n    def __init__(self,\n                 info: _I,\n                 dummy_inputs: \"BaseDummyInputsBuilder[_I]\",\n                 *,\n                 cache: Optional[ProcessingCache] = None) -> None:\n        super().__init__()\n\n        self.info = info\n        self.dummy_inputs = dummy_inputs\n        self.cache = cache\n\n        self.data_parser = self._get_data_parser()\n\n    def __call__(\n        self,\n        prompt: str,\n        mm_data: MultiModalDataDict,\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> MultiModalInputs:\n        return self.apply(prompt, mm_data, hf_processor_mm_kwargs)\n\n    def _get_data_parser(self) -> MultiModalDataParser:\n        \"\"\"\n        Construct a parser to preprocess multi-modal data items\n        before passing them to {meth}`_get_hf_mm_data`.\n\n        You can support additional modalities by creating a subclass\n        of {class}`MultiModalDataParser` that has additional subparsers.\n        \"\"\"\n        return MultiModalDataParser()\n\n    def _to_mm_items(\n        self,\n        mm_data: MultiModalDataDict,\n    ) -> MultiModalDataItems:\n        \"\"\"\n        Normalize {class}`MultiModalDataDict` to {class}`MultiModalDataItems`\n        before passing them to {meth}`_get_hf_mm_data`.\n        \"\"\"\n        mm_items = self.data_parser.parse_mm_data(mm_data)\n        supported_mm_limits = self.info.get_supported_mm_limits()\n        allowed_mm_limits = self.info.get_allowed_mm_limits()\n\n        for modality, items in mm_items.items():\n            supported_limit = supported_mm_limits.get(modality, 0)\n            allowed_limit = allowed_mm_limits.get(modality, 0)\n            num_items = len(items)\n\n            if supported_limit is not None and num_items > supported_limit:\n                raise ValueError(\n                    f\"The model only supports at most {supported_limit} \"\n                    f\"{modality} items, but you passed {num_items} \"\n                    f\"{modality} items in the same prompt.\")\n\n            if num_items > allowed_limit:\n                raise ValueError(\n                    \"You set or defaulted to \"\n                    f\"'{json.dumps({modality: allowed_limit})}' in \"\n                    f\"`--limit-mm-per-prompt`, but passed {num_items} \"\n                    f\"{modality} items in the same prompt.\")\n\n        return mm_items\n\n    @abstractmethod\n    def _get_mm_fields_config(\n        self,\n        hf_inputs: \"BatchFeature\",\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> Mapping[str, MultiModalFieldConfig]:\n        \"\"\"Given the HF-processed data, output the metadata of each field.\"\"\"\n        raise NotImplementedError\n\n    @abstractmethod\n    def _get_prompt_updates(\n        self,\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        out_mm_kwargs: MultiModalKwargs,\n    ) -> Sequence[PromptUpdate]:\n        \"\"\"\n        Given the original multi-modal items for this modality\n        and HF-processed data, output the updates to perform.\n\n        The information returned by this method is used to update token inputs\n        which bypass the HF processor. It is also used to update the output of\n        HF processor if the HF process does not apply prompt updates to text\n        inputs.\n\n        Moreover, this information is critical to determine the token positions\n        in order to construct  {class}`~vllm-multimodal.input.PlaceholderRange`\n        for each multi-modal item.\n        \"\"\"\n        raise NotImplementedError\n\n    def _find_mm_placeholders(\n        self,\n        mm_prompt_updates: Mapping[str, Sequence[BoundPromptUpdate]],\n        new_token_ids: list[int],\n        mm_item_counts: Mapping[str, int],\n    ) -> Mapping[str, list[PlaceholderFeaturesInfo]]:\n        return find_mm_placeholders(mm_prompt_updates, new_token_ids,\n                                    mm_item_counts)\n\n    def _get_hf_mm_data(\n        self,\n        mm_items: MultiModalDataItems,\n    ) -> tuple[Mapping[str, object], Mapping[str, object]]:\n        processor_data = dict[str, object]()\n        passthrough_data = dict[str, object]()\n\n        for items in mm_items.values():\n            processor_data.update(items.get_processor_data())\n            passthrough_data.update(items.get_passthrough_data())\n\n        return processor_data, passthrough_data\n\n    def _call_hf_processor(\n        self,\n        prompt: str,\n        # Not to be confused with `mm_data` in `self.apply`.\n        # This refers to the data to be passed to HF processor.\n        mm_data: Mapping[str, object],\n        mm_kwargs: Mapping[str, object],\n    ) -> \"BatchFeature\":\n        \"\"\"\n        Call the HF processor on the prompt text and\n        associated multi-modal data.\n        \"\"\"\n        return self.info.ctx.call_hf_processor(\n            self.info.get_hf_processor(**mm_kwargs),\n            dict(text=prompt, **mm_data),\n            mm_kwargs,\n        )\n\n    def _hf_processor_applies_updates(\n        self,\n        prompt_text: str,\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> bool:\n        \"\"\"\n        Return whether the HF processor applies prompt updates.\n\n        For most HF processors, this should be `True` when multi-modal\n        data items are passed, but `False` when multi-modal embeddings\n        are passed.\n        \"\"\"\n        return not any(\n            isinstance(items, (EmbeddingItems, DictEmbeddingItems))\n            for items in mm_items.values())\n\n    def _apply_hf_processor_text_mm(\n        self,\n        prompt_text: str,\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> tuple[list[int], MultiModalKwargs, bool]:\n        \"\"\"\n        Apply the HF processor on the prompt text and multi-modal data\n        together.\n\n        In addition, return whether prompt updates have been applied.\n        \"\"\"\n        processor_data, passthrough_data = self._get_hf_mm_data(mm_items)\n\n        processed_data = self._call_hf_processor(\n            prompt=prompt_text,\n            mm_data=processor_data,\n            mm_kwargs=hf_processor_mm_kwargs,\n        )\n        processed_data.update(passthrough_data)\n\n        prompt_ids, = processed_data.pop(\"input_ids\").tolist()\n\n        mm_kwargs = MultiModalKwargs.from_hf_inputs(\n            processed_data,\n            self._get_mm_fields_config(processed_data, hf_processor_mm_kwargs),\n        )\n\n        is_update_applied = self._hf_processor_applies_updates(\n            prompt_text=prompt_text,\n            mm_items=mm_items,\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n        )\n\n        return prompt_ids, mm_kwargs, is_update_applied\n\n    def _apply_hf_processor_text_only(self, prompt_text: str) -> list[int]:\n        \"\"\"\n        Apply the HF processor on the prompt text only.\n\n        Since HF processor requires that text and multi-modal items\n        correspond to each other, we create dummy multi-modal items\n        to go along with the text.\n        \"\"\"\n        prompt_ids, _, _ = self._apply_hf_processor_text_mm(\n            prompt_text=prompt_text,\n            mm_items=MultiModalDataItems({}),\n            hf_processor_mm_kwargs={},\n        )\n\n        return prompt_ids\n\n    def _apply_hf_processor_tokens_only(\n        self,\n        prompt_tokens: list[int],\n    ) -> list[int]:\n        \"\"\"\n        Apply the HF processor on the prompt tokens only.\n\n        Most HF processors accept prompt text but not prompt tokens.\n        If the HF processor adds or removes tokens that are not related to\n        multi-modal data, you should override this method so it is consistent\n        with the output of {meth}`_apply_hf_processor_text_only` on the\n        corresponding text.\n        \"\"\"\n        return prompt_tokens\n\n    def _apply_hf_processor_mm_only(\n        self,\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> MultiModalKwargs:\n        \"\"\"\n        Apply the HF processor on the multi-modal data only.\n\n        Since HF processor requires that text and multi-modal items\n        correspond to each other, we generate dummy text using\n        {class}`DummyInputsBuilder` to go along with the multi-modal data.\n        \"\"\"\n        mm_counts = mm_items.get_all_counts()\n\n        _, mm_kwargs, _ = self._apply_hf_processor_text_mm(\n            prompt_text=self.dummy_inputs.get_dummy_text(mm_counts),\n            mm_items=mm_items,\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n        )\n\n        return mm_kwargs\n\n    def _apply_hf_processor_main(\n        self,\n        prompt: Union[str, list[int]],\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        *,\n        enable_hf_prompt_update: bool,\n    ) -> tuple[list[int], MultiModalKwargs, bool]:\n        \"\"\"\n        Apply the HF processor on the prompt text and multi-modal data.\n\n        In addition, return whether prompt updates have been applied\n        (for most HF processors, this should be `True`).\n\n        Note:\n            If `enable_hf_prompt_update=False`, we use HF processor\n            to perform prompt updates if available; HF processor requires\n            that the prompt corresponds to multi-modal items.\n        \"\"\"\n        if isinstance(prompt, str):\n            if enable_hf_prompt_update:\n                return self._apply_hf_processor_text_mm(\n                    prompt_text=prompt,\n                    mm_items=mm_items,\n                    hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n                )\n\n            prompt_ids = self._apply_hf_processor_text_only(prompt)\n        else:\n            prompt_ids = self._apply_hf_processor_tokens_only(prompt)\n\n        mm_kwargs = self._apply_hf_processor_mm_only(\n            mm_items=mm_items,\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n        )\n\n        return prompt_ids, mm_kwargs, False\n\n    def _get_cache_missing_items(\n        self,\n        cache: ProcessingCache,\n        mm_data_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> tuple[dict[str, list[ProcessingCacheOptionalItem]], dict[\n            str, list[object]]]:\n        model_id = self.info.model_id\n\n        mm_cache_items = {\n            modality: [\n                cache.get_item(model_id, modality, item,\n                               hf_processor_mm_kwargs) for item in items\n            ]\n            for modality, items in mm_data_items.items()\n        }\n\n        mm_missing_idxs = {\n            modality: [\n                idx for idx, item in enumerate(cache_items)\n                if item.value is None\n            ]\n            for modality, cache_items in mm_cache_items.items()\n        }\n        mm_missing_data = {\n            modality: [mm_data_items[modality][idx] for idx in idxs]\n            for modality, idxs in mm_missing_idxs.items()\n        }\n\n        return mm_cache_items, mm_missing_data\n\n    def _hash_mm_items(\n        self,\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n    ) -> MultiModalHashes:\n        \"\"\"Create MM hashes to be returned (only used in V1).\"\"\"\n        model_id = self.info.model_id\n\n        return {\n            modality: [\n                MultiModalHasher.hash_kwargs(model_id=model_id,\n                                             **{modality: item},\n                                             **hf_processor_mm_kwargs)\n                for item in items\n            ]\n            for modality, items in mm_items.items()\n        }\n\n    def _merge_mm_kwargs(\n        self,\n        cache: ProcessingCache,\n        mm_cache_items: dict[str, list[ProcessingCacheOptionalItem]],\n        mm_missing_data: dict[str, list[object]],\n        mm_missing_kwargs: MultiModalKwargs,\n    ) -> dict[str, list[ProcessingCacheItem]]:\n        mm_missing_next_idx = {modality: 0 for modality in mm_missing_data}\n\n        merged_items = defaultdict[str, list[ProcessingCacheItem]](list)\n        for modality, cache_items in mm_cache_items.items():\n            for cache_item in cache_items:\n                if cache_item.value is None:\n                    kw_item = mm_missing_kwargs.get_item(\n                        modality,\n                        mm_missing_next_idx[modality],\n                    )\n                    cache_item_new = ProcessingCacheItem(\n                        key=cache_item.key,\n                        value=kw_item,\n                    )\n\n                    cache.put_item(cache_item_new)\n                    mm_missing_next_idx[modality] += 1\n                else:\n                    cache_item_new = ProcessingCacheItem(\n                        key=cache_item.key,\n                        value=cache_item.value,\n                    )\n\n                merged_items[modality].append(cache_item_new)\n\n        return dict(merged_items)\n\n    def _apply_hf_processor(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        *,\n        return_mm_hashes: bool,\n    ) -> tuple[list[int], MultiModalKwargs, Optional[MultiModalHashes], bool]:\n        (\n            prompt_ids,\n            mm_kwargs,\n            is_update_applied,\n        ) = self._apply_hf_processor_main(\n            prompt=prompt,\n            mm_items=mm_data_items,\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n            enable_hf_prompt_update=True,\n        )\n\n        mm_hashes = (self._hash_mm_items(mm_data_items, hf_processor_mm_kwargs)\n                     if return_mm_hashes else None)\n\n        return prompt_ids, mm_kwargs, mm_hashes, is_update_applied\n\n    def _cached_apply_hf_processor(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        *,\n        return_mm_hashes: bool,\n    ) -> tuple[list[int], MultiModalKwargs, Optional[MultiModalHashes], bool]:\n        \"\"\"\n        Apply the HF processor on the full prompt text,\n        caching the results and reusing cached results.\n        \"\"\"\n        cache = self.cache\n\n        _, passthrough_data = self._get_hf_mm_data(mm_data_items)\n        if cache is None or passthrough_data:\n            return self._apply_hf_processor(\n                prompt=prompt,\n                mm_data_items=mm_data_items,\n                hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n                return_mm_hashes=return_mm_hashes,\n            )\n\n        (\n            mm_cache_items,\n            mm_missing_data,\n        ) = self._get_cache_missing_items(\n            cache=cache,\n            mm_data_items=mm_data_items,\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n        )\n\n        # NOTE: `prompt` does not correspond to `mm_missing_data_items`,\n        # so we can't apply prompt updates until the new multimodal\n        # items are combined with the cached multimodal items\n        (\n            prompt_ids,\n            mm_missing_kwargs,\n            is_update_applied,\n        ) = self._apply_hf_processor_main(\n            prompt=prompt,\n            mm_items=self._to_mm_items(mm_missing_data),\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n            enable_hf_prompt_update=False,\n        )\n\n        mm_cache_items_merged = self._merge_mm_kwargs(\n            cache,\n            mm_cache_items=mm_cache_items,\n            mm_missing_data=mm_missing_data,\n            mm_missing_kwargs=mm_missing_kwargs,\n        )\n\n        mm_kwargs = MultiModalKwargs.from_items([\n            item.value for cache_items in mm_cache_items_merged.values()\n            for item in cache_items\n        ])\n\n        mm_hashes = {\n            modality: [item.key for item in cache_items]\n            for modality, cache_items in mm_cache_items_merged.items()\n        } if return_mm_hashes else None\n\n        return prompt_ids, mm_kwargs, mm_hashes, is_update_applied\n\n    def _bind_and_group_updates(\n        self,\n        prompt_updates: Sequence[PromptUpdate],\n    ) -> dict[str, Sequence[BoundPromptUpdate]]:\n        tokenizer = self.info.get_tokenizer()\n\n        it = (update.bind(tokenizer) for update in prompt_updates)\n        return dict(full_groupby_modality(it))\n\n    def _apply_token_matches(\n        self,\n        prompt: list[int],\n        mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n        mm_item_counts: Mapping[str, int],\n    ) -> list[int]:\n        return apply_token_matches(prompt, mm_matches, mm_item_counts)\n\n    def _apply_text_matches(\n        self,\n        prompt: str,\n        mm_matches: Mapping[str, Sequence[PromptTargetMatch]],\n        mm_item_counts: Mapping[str, int],\n    ) -> str:\n        return apply_text_matches(prompt, mm_matches, mm_item_counts)\n\n    def _apply_prompt_updates(\n        self,\n        token_ids: list[int],\n        mm_prompt_updates: Mapping[str, Sequence[BoundPromptUpdate]],\n        mm_item_counts: Mapping[str, int],\n    ) -> tuple[list[int], str, Mapping[str, list[PlaceholderFeaturesInfo]]]:\n        tokenizer = self.info.get_tokenizer()\n\n        mm_token_matches = {\n            modality: find_token_matches(token_ids, updates)\n            for modality, updates in mm_prompt_updates.items()\n        }\n        mm_match_counts = {\n            modality: len(matches)\n            for modality, matches in mm_token_matches.items()\n        }\n\n        # If the search text does not represent a special token,\n        # it may have different token IDs in the prompt, because\n        # the tokens may go across the boundaries of the search text.\n        # ----\n        # e.g. when searching for \"foo\" in \"food\", if \"food\" itself makes\n        # up a token, then the token ID of \"foo\" will not appear at all\n        # ----\n        # Since it is inefficient to search for all possible tokenizations\n        # of the search text in the prompt, we instead perform string-based\n        # updates on the decoded token IDs, then encode them back.\n        if all(\n            mm_match_counts.get(modality, 0) >= item_count\n            for modality, item_count in mm_item_counts.items()\n        ):  # yapf: disable\n            token_ids = self._apply_token_matches(\n                token_ids,\n                mm_token_matches,\n                mm_item_counts,\n            )\n\n            text = decode_tokens(tokenizer, token_ids)\n            matched_updates = {\n                modality: [match._origin for match in token_matches]\n                for modality, token_matches in mm_token_matches.items()\n            }\n        else:\n            text = decode_tokens(tokenizer, token_ids)\n\n            mm_text_matches = {\n                modality: find_text_matches(text, updates)\n                for modality, updates in mm_prompt_updates.items()\n            }\n            text = self._apply_text_matches(\n                text,\n                mm_text_matches,\n                mm_item_counts,\n            )\n\n            token_ids = encode_tokens(tokenizer,\n                                      text,\n                                      add_special_tokens=False)\n            matched_updates = {\n                modality: [match._origin for match in token_matches]\n                for modality, token_matches in mm_text_matches.items()\n            }\n\n        placeholders = self._find_mm_placeholders(\n            matched_updates,\n            token_ids,\n            mm_item_counts,\n        )\n\n        return token_ids, text, placeholders\n\n    def _validate_mm_kwargs(\n        self,\n        mm_kwargs: MultiModalKwargs,\n        mm_item_counts: Mapping[str, int],\n    ) -> None:\n        for modality, item_count in mm_item_counts.items():\n            if modality in mm_kwargs.modalities:\n                items = mm_kwargs.get_items(modality)\n            else:\n                items = []\n\n            if len(items) != item_count:\n                raise RuntimeError(\n                    f\"Expected there to be {item_count} {modality} items in \"\n                    f\"keyword arguments corresponding to {item_count} \"\n                    f\"{modality} data items, but only found {len(items)}! \"\n                    \"There is likely a problem with your \"\n                    \"implementation of merged multi-modal processor for this \"\n                    \"model (usually arising from an inconsistency between \"\n                    \"`_call_hf_processor` and `_get_mm_fields_config`).\")\n\n    def _validate_mm_placeholders(\n        self,\n        mm_placeholders: Mapping[str, list[PlaceholderFeaturesInfo]],\n        mm_item_counts: Mapping[str, int],\n    ) -> None:\n        for modality, item_count in mm_item_counts.items():\n            placeholders = mm_placeholders.get(modality, [])\n\n            if len(placeholders) != item_count:\n                # NOTE: If you are a model developer, this can also arise from\n                # an inconsistency between `_call_hf_processor` and\n                # `_get_mm_fields_config` implementations\n                raise RuntimeError(\n                    f\"Expected there to be {item_count} prompt updates \"\n                    f\"corresponding to {item_count} {modality} items, but \"\n                    f\"instead found {len(placeholders)} prompt updates! \"\n                    \"This is likely because you forgot to include input \"\n                    \"placeholder tokens (e.g., `<image>`, `<|image_pad|>`) \"\n                    \"in the prompt. If the model has a chat template, make \"\n                    \"sure you have applied it before calling `LLM.generate`.\")\n\n    def _maybe_apply_prompt_updates(\n        self,\n        mm_items: MultiModalDataItems,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        prompt_ids: list[int],\n        mm_kwargs: MultiModalKwargs,\n        is_update_applied: bool,\n    ) -> tuple[list[int], str, Mapping[str, list[PlaceholderFeaturesInfo]]]:\n        unbound_prompt_updates = self._get_prompt_updates(\n            mm_items,\n            hf_processor_mm_kwargs,\n            mm_kwargs,\n        )\n        mm_prompt_updates = self._bind_and_group_updates(\n            unbound_prompt_updates)\n\n        mm_item_counts = mm_items.get_all_counts()\n        self._validate_mm_kwargs(mm_kwargs, mm_item_counts)\n\n        if is_update_applied:\n            mm_placeholders = self._find_mm_placeholders(\n                mm_prompt_updates,\n                prompt_ids,\n                mm_item_counts,\n            )\n            self._validate_mm_placeholders(mm_placeholders, mm_item_counts)\n\n            tokenizer = self.info.get_tokenizer()\n            prompt = decode_tokens(tokenizer, prompt_ids)\n        else:\n            (\n                prompt_ids,\n                prompt,\n                mm_placeholders,\n            ) = self._apply_prompt_updates(\n                prompt_ids,\n                mm_prompt_updates,\n                mm_item_counts,\n            )\n            self._validate_mm_placeholders(mm_placeholders, mm_item_counts)\n\n        return prompt_ids, prompt, mm_placeholders\n\n    def apply(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        return_mm_hashes: bool = False,\n    ) -> MultiModalInputs:\n        \"\"\"\n        Process multi-modal inputs to be used in vLLM.\n\n        The main steps are:\n\n        1. Apply HF Processor on prompt text and multi-modal data together,\n           outputting token IDs and processed tensors.\n        2. Find and update sequences in the token IDs with placeholder tokens.\n           The number of placeholder tokens equals the feature size of the\n           multi-modal data outputted by the multi-modal encoder.\n        3. Extract information about the placeholder tokens from the\n           processed token IDs.\n        \"\"\"\n        mm_items = self._to_mm_items(mm_data)\n\n        (\n            prompt_ids,\n            mm_kwargs,\n            mm_hashes,\n            is_update_applied,\n        ) = self._cached_apply_hf_processor(\n            prompt,\n            mm_items,\n            hf_processor_mm_kwargs,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n        prompt_ids, prompt, mm_placeholders = self._maybe_apply_prompt_updates(\n            mm_items=mm_items,\n            hf_processor_mm_kwargs=hf_processor_mm_kwargs,\n            prompt_ids=prompt_ids,\n            mm_kwargs=mm_kwargs,\n            is_update_applied=is_update_applied,\n        )\n\n        mm_placeholder_ranges = {\n            modality: [item.to_range() for item in placeholders]\n            for modality, placeholders in mm_placeholders.items()\n        }\n\n        return MultiModalInputs(\n            type=\"multimodal\",\n            prompt=prompt,\n            prompt_token_ids=prompt_ids,\n            mm_kwargs=mm_kwargs,\n            mm_hashes=mm_hashes,\n            mm_placeholders=mm_placeholder_ranges,\n        )\n\n\nclass EncDecMultiModalProcessor(BaseMultiModalProcessor[_I]):\n\n    @abstractmethod\n    def create_encoder_prompt(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n    ) -> Union[str, list[int]]:\n        \"\"\"\n        Create input prompt for the encoder. HF processor will be applied on\n        this prompt during profiling and generation.\n        \"\"\"\n        raise NotImplementedError\n\n    @property\n    def pad_dummy_encoder_prompt(self) -> bool:\n        return False\n\n    def create_decoder_prompt(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n    ) -> Union[str, list[int]]:\n        \"\"\"Create input prompt for the decoder.\"\"\"\n        return prompt\n\n    def _get_enc_dec_inputs(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        encoder_inputs: MultiModalInputs,\n    ):\n        tokenizer = self.info.get_tokenizer()\n        decoder_prompt = self.create_decoder_prompt(prompt, mm_data)\n        if isinstance(decoder_prompt, str):\n            decoder_prompt_ids = encode_tokens(tokenizer,\n                                               decoder_prompt,\n                                               add_special_tokens=False)\n        else:\n            decoder_prompt_ids = decoder_prompt\n            decoder_prompt = decode_tokens(tokenizer, decoder_prompt)\n\n        mm_inputs = MultiModalEncDecInputs(\n            encoder_prompt=encoder_inputs[\"prompt\"],\n            encoder_prompt_token_ids=encoder_inputs[\"prompt_token_ids\"],\n            **encoder_inputs)\n        mm_inputs.update({\n            \"prompt\": decoder_prompt,\n            \"prompt_token_ids\": decoder_prompt_ids\n        })\n        return mm_inputs\n\n    def apply(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        hf_processor_mm_kwargs: Mapping[str, object],\n        return_mm_hashes: bool = False,\n    ) -> MultiModalEncDecInputs:\n        \"\"\"\n        Process multi-modal inputs to be used in vLLM.\n        The main processing steps are modified to fit encoder-decoder model:\n        1. Create encoder prompt from input prompt text.\n        2. Apply the HF processor on encoder prompt.\n        3. Copy the input prompt text as decoder prompt inputs.\n        \"\"\"\n        encoder_prompt = self.create_encoder_prompt(prompt, mm_data)\n        encoder_inputs = super().apply(\n            encoder_prompt,\n            mm_data,\n            hf_processor_mm_kwargs,\n            return_mm_hashes,\n        )\n\n        return self._get_enc_dec_inputs(\n            prompt=prompt,\n            mm_data=mm_data,\n            encoder_inputs=encoder_inputs,\n        )\n", 1871], "/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py": ["# SPDX-License-Identifier: Apache-2.0\nfrom collections.abc import Sequence\nfrom typing import Optional\n\nfrom vllm.envs import VLLM_MM_INPUT_CACHE_GIB\nfrom vllm.multimodal import MultiModalKwargs\nfrom vllm.multimodal.processing import ProcessingCache\nfrom vllm.utils import is_list_of\n\n# The idea of multimodal preprocessing caching is based on having a client and\n# a server, where the client executes in the frontend process (=P0) and the\n# server in the core process (=P1).\n#\n# -- Client:\n#  - BaseMultiModalProcessor to process MultiModalData into MultiModalKwargs\n#    with built-in caching functionality, with mm_hash as its identifier.\n#  - MirroredProcessingCache to keep track of the cached entries and\n#    determine whether to send the MultiModalKwargs to P1.\n#\n# -- Server:\n#  - MirroredProcessingCache to store the MultiModalKwargs from P0.\n#\n# The caching for both client and server is mirrored, and this allows us\n# to avoid the serialization of \"mm_inputs\" (like pixel values) between\n# client (=P0) and server (=P1) processes if the mm_hash is found in the client\n# cache.\n\n# Both Client and Server must use the same cache size\n# (to perform mirrored caching). This cache size is set by the environment\n# variable VLLM_MM_INPUT_CACHE_GIB.\n\n\nclass MirroredProcessingCache:\n\n    def __init__(self, model_config):\n        mm_config = model_config.multimodal_config\n        disable_mm_preprocessor_cache = mm_config is not None and \\\n            not mm_config.disable_mm_preprocessor_cache\n        self.use_cache = not disable_mm_preprocessor_cache\n        self.mm_cache = ProcessingCache.get_lru_cache(VLLM_MM_INPUT_CACHE_GIB,\n                                                      MultiModalKwargs)\n\n    def get_and_update_p0(\n        self,\n        mm_inputs: Sequence[MultiModalKwargs],\n        mm_hashes: list[str],\n    ) -> Sequence[Optional[MultiModalKwargs]]:\n        assert len(mm_inputs) == len(mm_hashes)\n\n        if not self.use_cache:\n            assert is_list_of(mm_inputs, MultiModalKwargs)\n            return mm_inputs\n\n        full_mm_inputs = list[Optional[MultiModalKwargs]]()\n        for mm_input, mm_hash in zip(mm_inputs, mm_hashes):\n            if self.mm_cache.get(mm_hash) is not None:\n                mm_input = None\n            else:\n                self.mm_cache[mm_hash] = mm_input\n\n            full_mm_inputs.append(mm_input)\n\n        return full_mm_inputs\n\n    def get_and_update_p1(\n        self,\n        mm_inputs: Sequence[Optional[MultiModalKwargs]],\n        mm_hashes: list[str],\n    ) -> Sequence[MultiModalKwargs]:\n        assert len(mm_inputs) == len(mm_hashes)\n\n        if not self.use_cache:\n            assert is_list_of(mm_inputs, MultiModalKwargs)\n            return mm_inputs\n\n        full_mm_inputs = list[MultiModalKwargs]()\n        for mm_input, mm_hash in zip(mm_inputs, mm_hashes):\n            if mm_input is None:\n                mm_input = self.mm_cache[mm_hash]\n            else:\n                self.mm_cache[mm_hash] = mm_input\n\n            full_mm_inputs.append(mm_input)\n\n        return full_mm_inputs\n\n    def reset(self) -> bool:\n        self.mm_cache.clear()\n\n        return True\n", 90], "/home/jeromeku/vllm/vllm/v1/engine/processor.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport time\nfrom collections.abc import Mapping, Sequence\nfrom typing import Any, Literal, Optional, Union\n\nfrom vllm.config import VllmConfig\nfrom vllm.inputs import ProcessorInputs, PromptType, SingletonInputs\nfrom vllm.inputs.parse import split_enc_dec_inputs\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, MultiModalKwargs,\n                             MultiModalRegistry)\nfrom vllm.multimodal.inputs import PlaceholderRange\nfrom vllm.multimodal.processing import EncDecMultiModalProcessor\nfrom vllm.multimodal.utils import merge_and_sort_multimodal_metadata\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\nfrom vllm.v1.engine import EngineCoreRequest\nfrom vllm.v1.engine.mm_input_cache import MirroredProcessingCache\nfrom vllm.v1.structured_output.backend_guidance import (\n    validate_guidance_grammar)\nfrom vllm.v1.structured_output.backend_xgrammar import (\n    validate_xgrammar_grammar)\n\n\nclass Processor:\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        tokenizer: TokenizerGroup,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ):\n\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.decoding_config = vllm_config.decoding_config\n        self.tokenizer = tokenizer\n\n        self.generation_config_fields = (\n            self.model_config.try_get_generation_config())\n        self.input_preprocessor = InputPreprocessor(self.model_config,\n                                                    self.tokenizer,\n                                                    mm_registry)\n\n        self.mm_input_cache_client = MirroredProcessingCache(self.model_config)\n\n        # Multi-modal hasher (for images)\n        self.use_hash = self.mm_input_cache_client.use_cache or \\\n            self.cache_config.enable_prefix_caching\n\n    @property\n    def mm_registry(self):\n        return self.input_preprocessor.mm_registry\n\n    def _validate_logprobs(\n        self,\n        params: SamplingParams,\n    ) -> None:\n        max_logprobs = self.model_config.max_logprobs\n        # Validate sample logprobs.\n        if params.logprobs and params.logprobs > max_logprobs:\n            raise ValueError(\n                f\"Requested sample logprobs of {params.logprobs}, \"\n                f\"which is greater than max allowed: {max_logprobs}\")\n\n        # Validate prompt logprobs.\n        if params.prompt_logprobs and params.prompt_logprobs > max_logprobs:\n            raise ValueError(\n                f\"Requested prompt logprobs of {params.prompt_logprobs}, \"\n                f\"which is greater than max allowed: {max_logprobs}\")\n\n    def _validate_sampling_params(\n        self,\n        params: SamplingParams,\n        lora_request: Optional[LoRARequest],\n    ) -> None:\n        self._validate_structured_output(params)\n        self._validate_logit_bias(params)\n\n        if params.allowed_token_ids is None:\n            return\n        if not params.allowed_token_ids:\n            raise ValueError(\"allowed_token_ids is not None and empty!\")\n        tokenizer = self.tokenizer.get_lora_tokenizer(lora_request)\n        vocab_size = len(tokenizer)\n        if not all(0 <= tid < vocab_size for tid in params.allowed_token_ids):\n            raise ValueError(\n                \"allowed_token_ids contains out-of-vocab token id!\")\n\n    def _validate_logit_bias(\n        self,\n        params: SamplingParams,\n    ) -> None:\n        \"\"\"Validate logit_bias token IDs are within vocabulary range.\"\"\"\n        if not params.logit_bias:\n            return\n\n        vocab_size = self.model_config.get_vocab_size()\n        invalid_token_ids = []\n\n        for token_id in params.logit_bias:\n            if token_id < 0 or token_id >= vocab_size:\n                invalid_token_ids.append(token_id)\n\n        if invalid_token_ids:\n            raise ValueError(\n                f\"token_id(s) {invalid_token_ids} in logit_bias contain \"\n                f\"out-of-vocab token ids. Vocabulary size: {vocab_size}\")\n\n    def _validate_supported_sampling_params(\n        self,\n        params: SamplingParams,\n    ) -> None:\n        # Best of not yet supported.\n        if params.best_of is not None and params.best_of > 1:\n            raise ValueError(\"vLLM V1 does not yet support best_of.\")\n        # Logits processors not supported.\n        if params.logits_processors:\n            raise ValueError(\"vLLM V1 does not support per request \"\n                             \"user provided logits processors.\")\n\n    def _validate_params(\n        self,\n        params: Union[SamplingParams, PoolingParams],\n        lora_request: Optional[LoRARequest],\n    ):\n        \"\"\"\n        Validate supported SamplingParam.\n        Should raise ValueError if unsupported for API Server.\n        \"\"\"\n\n        if not isinstance(params, SamplingParams):\n            raise ValueError(\"V1 does not yet support Pooling models.\")\n\n        self._validate_logprobs(params)\n        self._validate_sampling_params(params, lora_request)\n        self._validate_supported_sampling_params(params)\n\n    def _validate_lora(self, lora_request: Optional[LoRARequest]) -> None:\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n\n    def _validate_structured_output(self, params: SamplingParams) -> None:\n        if not params.guided_decoding or not self.decoding_config:\n            return\n\n        engine_level_backend = self.decoding_config.backend\n        if params.guided_decoding.backend:\n            # Request-level backend selection is not supported in V1.\n            # The values may differ if `params` is reused and was set\n            # to a specific backend based on `auto` behavior in a previous\n            # request. We remember that it was set as a result of `auto`\n            # using the `_auto` option set on the backend in the params.\n            if (params.guided_decoding.backend != engine_level_backend\n                    and not (engine_level_backend == \"auto\"\n                             and params.guided_decoding.backend_was_auto)):\n                raise ValueError(\n                    \"Request-level structured output backend selection is no \"\n                    \"longer supported. The request specified \"\n                    f\"'{params.guided_decoding.backend}', but vLLM was \"\n                    f\"initialised with '{engine_level_backend}'. This error \"\n                    \"can be resolved by removing backend selection from the \"\n                    \"request.\")\n        else:\n            params.guided_decoding.backend = engine_level_backend\n\n        # Request content validation\n        if engine_level_backend.startswith(\"xgrammar\"):\n            # xgrammar with no fallback\n            validate_xgrammar_grammar(params)\n        elif engine_level_backend.startswith(\"guidance\"):\n            # TODO: ideally we would have the LLTokenizer here as Lark syntax\n            # allows <|special_token|> and similar, see\n            # https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md#special-tokens\n            # Without tokenizer these are disallowed in grammars.\n            validate_guidance_grammar(params, tokenizer=None)\n        else:\n            # NOTE: engine_level_backend must be \"auto\" here, because we have\n            # checked supported_backends above.\n            # \"auto\" is an opt-in to opinionated behavior where we try to\n            # choose a backend based on request contents. This is not the\n            # default as it is less predictable and subject to change\n            # between releases as feature support changes.\n            try:\n                validate_xgrammar_grammar(params)\n                params.guided_decoding.backend = \"xgrammar\"\n            except ValueError:\n                # The request either failed validation\n                # or includes some jsonschema feature(s) that\n                # are not supported in xgrammar. Fall back to guidance.\n                validate_guidance_grammar(params, tokenizer=None)\n                params.guided_decoding.backend = \"guidance\"\n            # Remember that this backend was set automatically\n            params.guided_decoding.backend_was_auto = True\n\n    def process_inputs(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> tuple[Optional[str], EngineCoreRequest]:\n\n        # TODO(woosuk): Support pooling models.\n        # TODO(woosuk): Support encoder-decoder models.\n        self._validate_lora(lora_request)\n        self._validate_params(params, lora_request)\n        if priority != 0:\n            raise ValueError(\"V1 does not support priority yet.\")\n        if trace_headers is not None:\n            raise ValueError(\"V1 does not support tracing yet.\")\n        if prompt_adapter_request is not None:\n            raise ValueError(\"V1 does not support prompt_adapter_request.\")\n\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        # Process inputs, which includes:\n        # 1. Tokenize text prompt, with LoRA request if one exists.\n        # 2. For multimodal models with a merged preprocessor, preprocess\n        #   multimodal data and expand prompt token ids accordingly.\n        # 3. Apply prompt adapter to prompt token ids if one exists.\n        processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            return_mm_hashes=self.use_hash,\n        )\n        from vllm.platforms import current_platform\n        current_platform.validate_request(\n            prompt=prompt,\n            params=params,\n            processed_inputs=processed_inputs,\n        )\n        eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)\n\n        self._validate_model_inputs(processed_inputs, lora_request)\n\n        encoder_inputs, decoder_inputs = split_enc_dec_inputs(processed_inputs)\n\n        # TODO: Impl encoder-decoder\n        if encoder_inputs is not None:\n            raise NotImplementedError\n\n        assert isinstance(params, SamplingParams)\n        # TODO: can we avoid cloning here in multiproc case?\n        sampling_params = params.clone()\n        # If unset max tokens, then generate up to the max_model_len.\n        if sampling_params.max_tokens is None:\n            sampling_params.max_tokens = (\n                self.model_config.max_model_len -\n                len(decoder_inputs[\"prompt_token_ids\"]))\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields, eos_token_id)\n        sampling_params.update_from_tokenizer(\n            self.tokenizer.get_lora_tokenizer(lora_request))\n\n        # Multimodal related.\n        sorted_mm_inputs: Optional[Sequence[Optional[MultiModalKwargs]]] = None\n        sorted_mm_positions: Optional[list[PlaceholderRange]] = None\n        sorted_mm_hashes: Optional[list[str]] = None\n        if decoder_inputs[\"type\"] == \"multimodal\":\n            decoder_mm_inputs = decoder_inputs[\"mm_kwargs\"]\n\n            # Merge and flatten multimodal placeholders, hashes and inputs\n            # from dictionaries to lists, and sort them by each item's position\n            # in the input sequence.\n            (\n                sorted_item_modalities,\n                sorted_mm_positions,\n                sorted_mm_hashes,\n            ) = merge_and_sort_multimodal_metadata(\n                decoder_inputs[\"mm_placeholders\"],\n                decoder_inputs[\"mm_hashes\"] if self.use_hash else None,\n            )\n\n            # The output of merged multi-modal processor (`decoder_mm_inputs`)\n            # is a single MultiModalKwargs for all items from all modalities.\n            # This code flattens kwargs for individual items in a list and\n            # sorts them by each item's position in the input sequence if there\n            # are multiple modalities.\n            unique_modalities = set(sorted_item_modalities)\n            if len(unique_modalities) > 1:\n                orig_sorted_mm_inputs = []\n                used_indices = {modality: 0 for modality in unique_modalities}\n\n                for modality in sorted_item_modalities:\n                    items = decoder_mm_inputs.get_items(modality)\n                    item = items[used_indices[modality]]\n\n                    orig_sorted_mm_inputs.append(\n                        MultiModalKwargs.from_items([item]))\n                    used_indices[modality] += 1\n            else:\n                orig_sorted_mm_inputs = [\n                    MultiModalKwargs.from_items([item]) for item in\n                    decoder_mm_inputs.get_items(sorted_item_modalities[0])\n                ]\n\n            if sorted_mm_hashes is not None:\n                sorted_mm_inputs = self.mm_input_cache_client.get_and_update_p0(\n                    orig_sorted_mm_inputs, sorted_mm_hashes)\n            else:\n                sorted_mm_inputs = orig_sorted_mm_inputs\n\n        return decoder_inputs.get(\"prompt\"), EngineCoreRequest(\n            request_id=request_id,\n            prompt_token_ids=decoder_inputs[\"prompt_token_ids\"],\n            mm_inputs=sorted_mm_inputs,\n            mm_hashes=sorted_mm_hashes,\n            mm_placeholders=sorted_mm_positions,\n            sampling_params=sampling_params,\n            eos_token_id=eos_token_id,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            cache_salt=decoder_inputs.get(\"cache_salt\"),\n        )\n\n    def _validate_model_inputs(self,\n                               inputs: ProcessorInputs,\n                               lora_request: Optional[LoRARequest] = None):\n        encoder_inputs, decoder_inputs = split_enc_dec_inputs(inputs)\n\n        if encoder_inputs is not None:\n            self._validate_model_input(encoder_inputs,\n                                       lora_request,\n                                       prompt_type=\"encoder\")\n\n        self._validate_model_input(decoder_inputs,\n                                   lora_request,\n                                   prompt_type=\"decoder\")\n\n    def _validate_model_input(\n        self,\n        prompt_inputs: SingletonInputs,\n        lora_request: Optional[LoRARequest],\n        *,\n        prompt_type: Literal[\"encoder\", \"decoder\"],\n    ):\n        model_config = self.model_config\n        tokenizer = self.tokenizer.get_lora_tokenizer(lora_request)\n\n        prompt_ids = prompt_inputs[\"prompt_token_ids\"]\n        if not prompt_ids:\n            if prompt_type == \"encoder\" and model_config.is_multimodal_model:\n                pass  # Mllama may have empty encoder inputs for text-only data\n            else:\n                raise ValueError(f\"The {prompt_type} prompt cannot be empty\")\n\n        max_input_id = max(prompt_ids, default=0)\n        if max_input_id > tokenizer.max_token_id:\n            raise ValueError(f\"Token id {max_input_id} is out of vocabulary\")\n\n        max_prompt_len = self.model_config.max_model_len\n        if len(prompt_ids) > max_prompt_len:\n            if prompt_type == \"encoder\" and model_config.is_multimodal_model:\n                mm_registry = self.input_preprocessor.mm_registry\n                mm_processor = mm_registry.create_processor(\n                    model_config,\n                    tokenizer=tokenizer,\n                )\n                assert isinstance(mm_processor, EncDecMultiModalProcessor)\n\n                if mm_processor.pad_dummy_encoder_prompt:\n                    return  # Skip encoder length check for Whisper\n\n            if model_config.is_multimodal_model:\n                suggestion = (\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens plus multimodal tokens. For image \"\n                    \"inputs, the number of image tokens depends on the number \"\n                    \"of images, and possibly their aspect ratios as well.\")\n            else:\n                suggestion = (\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens.\")\n\n            raise ValueError(\n                f\"The {prompt_type} prompt (length {len(prompt_ids)}) is \"\n                f\"longer than the maximum model length of {max_prompt_len}. \"\n                f\"{suggestion}\")\n\n            # TODO: Find out how many placeholder tokens are there so we can\n            # check that chunked prefill does not truncate them\n            # max_batch_len = self.scheduler_config.max_num_batched_tokens\n", 398], "/home/jeromeku/vllm/vllm/v1/metrics/stats.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Optional\n\nfrom vllm.v1.spec_decode.metrics import SpecDecodingStats\n\nif TYPE_CHECKING:\n    from vllm.v1.engine import EngineCoreEvent, EngineCoreOutput, FinishReason\n    from vllm.v1.engine.output_processor import RequestState\n\n\n@dataclass\nclass PrefixCacheStats:\n    \"\"\"Stores prefix cache hit statistics.\"\"\"\n    # Whether reset_prefix_cache was invoked.\n    reset: bool = False\n    # The number of requests in this update.\n    requests: int = 0\n    # The number of queries in these requests. Note that \"queries\" here\n    # means the number of tokens that were queried from the cache.\n    queries: int = 0\n    # The number of hits in these requests.\n    hits: int = 0\n\n\n@dataclass\nclass SchedulerStats:\n    \"\"\"Stats associated with the scheduler.\"\"\"\n\n    num_running_reqs: int = 0\n    num_waiting_reqs: int = 0\n\n    gpu_cache_usage: float = 0.0\n\n    prefix_cache_stats: PrefixCacheStats = field(\n        default_factory=PrefixCacheStats)\n\n    spec_decoding_stats: Optional[SpecDecodingStats] = None\n\n\n@dataclass\nclass LoRAStats:\n    waiting_requests: set[str] = field(default_factory=set)\n    running_requests: set[str] = field(default_factory=set)\n\n\n@dataclass\nclass RequestStateStats:\n    \"\"\"Stats that need to be tracked across delta updates.\"\"\"\n\n    num_generation_tokens: int = 0\n\n    # This is a engine frontend timestamp (wall-clock)\n    arrival_time: float = 0.0\n\n    # These are engine core timestamps (monotonic)\n    queued_ts: float = 0.0\n    scheduled_ts: float = 0.0\n    first_token_ts: float = 0.0\n    last_token_ts: float = 0.0\n\n\n@dataclass\nclass FinishedRequestStats:\n    \"\"\"Stats associated with a finished request.\"\"\"\n\n    finish_reason: \"FinishReason\"\n    e2e_latency: float = 0.0\n    num_prompt_tokens: int = 0\n    num_generation_tokens: int = 0\n    max_tokens_param: Optional[int] = None\n    queued_time: float = 0.0\n    prefill_time: float = 0.0\n    inference_time: float = 0.0\n    decode_time: float = 0.0\n\n\nclass IterationStats:\n    \"\"\"Stats associated with a single set of EngineCoreOutputs.\"\"\"\n\n    def __init__(self):\n        self.iteration_timestamp = time.time()\n        self.num_generation_tokens = 0\n        self.num_prompt_tokens = 0\n        self.num_preempted_reqs = 0\n        self.finished_requests: list[FinishedRequestStats] = []\n        self.max_num_generation_tokens_iter: list[int] = []\n        self.n_params_iter: list[int] = []\n        self.time_to_first_tokens_iter: list[float] = []\n        self.time_per_output_tokens_iter: list[float] = []\n        self.waiting_lora_adapters: dict[str, int] = {}\n        self.running_lora_adapters: dict[str, int] = {}\n\n    def _time_since(self, start: float) -> float:\n        \"\"\"Calculate an interval relative to this iteration's timestamp.\"\"\"\n        return self.iteration_timestamp - start\n\n    def update_from_output(self, output: \"EngineCoreOutput\",\n                           engine_core_timestamp: float, is_prefilling: bool,\n                           prompt_len: int, req_stats: RequestStateStats,\n                           lora_stats: Optional[LoRAStats]):\n        num_new_generation_tokens = len(output.new_token_ids)\n\n        self.num_generation_tokens += num_new_generation_tokens\n        if is_prefilling:\n            assert num_new_generation_tokens > 0\n            self.num_prompt_tokens += prompt_len\n\n            first_token_latency = self._time_since(req_stats.arrival_time)\n            self.time_to_first_tokens_iter.append(first_token_latency)\n\n        req_stats.num_generation_tokens += num_new_generation_tokens\n\n        # Process request-level engine core events\n        if output.events is not None:\n            self.update_from_events(output.request_id, output.events,\n                                    is_prefilling, req_stats, lora_stats)\n\n        # Process the batch-level \"new tokens\" engine core event\n        if is_prefilling:\n            req_stats.first_token_ts = engine_core_timestamp\n        else:\n            tpot = engine_core_timestamp - req_stats.last_token_ts\n            self.time_per_output_tokens_iter.append(tpot)\n\n        req_stats.last_token_ts = engine_core_timestamp\n\n    def update_from_events(self, req_id: str, events: list[\"EngineCoreEvent\"],\n                           is_prefilling: bool, req_stats: RequestStateStats,\n                           lora_stats: Optional[LoRAStats]):\n        # Avoid circular dependency\n        from vllm.v1.engine import EngineCoreEventType\n        for event in events:\n            if event.type == EngineCoreEventType.QUEUED:\n                req_stats.queued_ts = event.timestamp\n                if lora_stats is not None:\n                    lora_stats.waiting_requests.add(req_id)\n            elif event.type == EngineCoreEventType.SCHEDULED:\n                if req_stats.scheduled_ts == 0.0:  # ignore preemptions\n                    req_stats.scheduled_ts = event.timestamp\n                LoRARequestStates.scheduled_request(lora_stats, req_id)\n            elif event.type == EngineCoreEventType.PREEMPTED:\n                self.num_preempted_reqs += 1\n                LoRARequestStates.preempted_request(lora_stats, req_id)\n\n    def update_from_finished_request(self, finish_reason: \"FinishReason\",\n                                     num_prompt_tokens: int,\n                                     max_tokens_param: Optional[int],\n                                     req_stats: RequestStateStats):\n        e2e_latency = self._time_since(req_stats.arrival_time)\n\n        # Queued interval is from first QUEUED event to first SCHEDULED\n        queued_time = req_stats.scheduled_ts - req_stats.queued_ts\n\n        # Prefill interval is from first SCHEDULED to first NEW_TOKEN\n        # Any preemptions during prefill is included in the interval\n        prefill_time = req_stats.first_token_ts - req_stats.scheduled_ts\n\n        # Decode interval is from first NEW_TOKEN to last NEW_TOKEN\n        # Any preemptions during decode are included\n        decode_time = req_stats.last_token_ts - req_stats.first_token_ts\n\n        # Inference interval is from first SCHEDULED to last NEW_TOKEN\n        # Any preemptions during prefill or decode are included\n        inference_time = req_stats.last_token_ts - req_stats.scheduled_ts\n\n        finished_req = \\\n            FinishedRequestStats(finish_reason=finish_reason,\n                                 e2e_latency=e2e_latency,\n                                 num_prompt_tokens=num_prompt_tokens,\n                                 num_generation_tokens=req_stats.num_generation_tokens,\n                                 max_tokens_param=max_tokens_param,\n                                 queued_time=queued_time,\n                                 prefill_time=prefill_time,\n                                 inference_time=inference_time,\n                                 decode_time=decode_time)\n        self.finished_requests.append(finished_req)\n\n\nclass LoRARequestStates:\n    \"\"\"Per-LoRA request state stats.\"\"\"\n\n    def __init__(self):\n        self.lora_name_to_stats: dict[str, LoRAStats] = {}\n\n    def get_stats(self, req_state: 'RequestState') -> Optional[LoRAStats]:\n        if req_state.lora_name is None:\n            return None\n        if req_state.lora_name not in self.lora_name_to_stats:\n            self.lora_name_to_stats[req_state.lora_name] = LoRAStats()\n        return self.lora_name_to_stats[req_state.lora_name]\n\n    def add_request(self, req_state: 'RequestState'):\n        if (lora_stats := self.get_stats(req_state)) is not None:\n            lora_stats.waiting_requests.add(req_state.request_id)\n\n    def finish_request(self, req_state: 'RequestState'):\n        if req_state.lora_name is None:\n            return\n        lora_stats = self.lora_name_to_stats[req_state.lora_name]\n        lora_stats.running_requests.remove(req_state.request_id)\n\n    def abort_request(self, req_state: 'RequestState'):\n        if req_state.lora_name is None:\n            return\n        lora_stats = self.lora_name_to_stats[req_state.lora_name]\n        lora_stats.waiting_requests.discard(req_state.request_id)\n        lora_stats.running_requests.discard(req_state.request_id)\n\n    # Break the pattern for this lifecycle methods so we can\n    # call this from IterationStats.update_from_events()\n    @staticmethod\n    def scheduled_request(lora_stats: Optional[LoRAStats], request_id: str):\n        if lora_stats is None:\n            return\n        lora_stats.waiting_requests.remove(request_id)\n        lora_stats.running_requests.add(request_id)\n\n    @staticmethod\n    def preempted_request(lora_stats: Optional[LoRAStats], request_id: str):\n        if lora_stats is None:\n            return\n        lora_stats.running_requests.remove(request_id)\n        lora_stats.waiting_requests.add(request_id)\n\n    def update_iteration_stats(self,\n                               iteration_stats: Optional[IterationStats]):\n        if iteration_stats is None:\n            return\n        for lora_name, stats in self.lora_name_to_stats.items():\n            if stats.waiting_requests:\n                iteration_stats.waiting_lora_adapters[lora_name] = \\\n                    len(stats.waiting_requests)\n            if stats.running_requests:\n                iteration_stats.running_lora_adapters[lora_name] = \\\n                    len(stats.running_requests)\n", 238], "/home/jeromeku/vllm/vllm/v1/engine/output_processor.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport asyncio\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Union\n\nfrom vllm.outputs import CompletionOutput, RequestOutput\nfrom vllm.sampling_params import RequestOutputKind\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\nfrom vllm.v1.engine import EngineCoreOutput, EngineCoreRequest, FinishReason\nfrom vllm.v1.engine.detokenizer import IncrementalDetokenizer\nfrom vllm.v1.engine.logprobs import LogprobsProcessor\nfrom vllm.v1.engine.parallel_sampling import ParentRequest\nfrom vllm.v1.metrics.stats import (IterationStats, LoRARequestStates,\n                                   RequestStateStats)\n\n\nclass RequestOutputCollector:\n    \"\"\"\n    Collects streamed RequestOutputs per individual request,\n    for hand-off to the consuming asyncio generate task.\n\n    When streaming deltas, RequestOutputs are merged if the\n    producer gets ahead of the consumer.\n    \"\"\"\n\n    def __init__(self, output_kind: RequestOutputKind):\n        self.aggregate = output_kind == RequestOutputKind.DELTA\n        self.output: Optional[Union[RequestOutput, Exception]] = None\n        self.ready = asyncio.Event()\n\n    def put(self, output: Union[RequestOutput, Exception]) -> None:\n        \"\"\"Non-blocking put operation.\"\"\"\n        if self.output is None or isinstance(output, Exception):\n            self.output = output\n            self.ready.set()\n        elif isinstance(self.output, RequestOutput):\n            # This ensures that request outputs with different request indexes\n            # (if n > 1) do not override each other.\n            self.output.add(output, aggregate=self.aggregate)\n\n    async def get(self) -> RequestOutput:\n        \"\"\"Get operation blocks on put event.\"\"\"\n        while (output := self.output) is None:\n            await self.ready.wait()\n        self.output = None\n        self.ready.clear()\n        if isinstance(output, Exception):\n            raise output\n        return output\n\n    def get_nowait(self) -> Optional[RequestOutput]:\n        \"\"\"Non-blocking get operation.\"\"\"\n        output = self.output\n        if output is not None:\n            self.output = None\n            self.ready.clear()\n        if isinstance(output, Exception):\n            raise output\n        return output\n\n\n@dataclass\nclass OutputProcessorOutput:\n\n    request_outputs: list[RequestOutput]\n    reqs_to_abort: list[str]\n\n\nclass RequestState:\n\n    def __init__(\n        self,\n        request_id: str,\n        parent_req: Optional[ParentRequest],\n        request_index: int,\n        lora_name: Optional[str],\n        output_kind: RequestOutputKind,\n        prompt: Optional[str],\n        prompt_token_ids: list[int],\n        logprobs_processor: LogprobsProcessor,\n        detokenizer: IncrementalDetokenizer,\n        max_tokens_param: Optional[int],\n        arrival_time: float,\n        queue: Optional[RequestOutputCollector],\n        log_stats: bool,\n    ):\n        self.request_id = request_id\n        self.parent_req = parent_req\n        self.request_index = request_index\n        self.lora_name = lora_name\n        self.output_kind = output_kind\n        self.prompt = prompt\n        self.prompt_token_ids = prompt_token_ids\n        self.prompt_len = len(prompt_token_ids)\n        self.logprobs_processor = logprobs_processor\n        self.detokenizer = detokenizer\n        self.max_tokens_param = max_tokens_param\n        self.is_prefilling = True\n        self.queue = queue\n\n        self.stats = RequestStateStats(\n            arrival_time=arrival_time) if log_stats else None\n\n    @classmethod\n    def from_new_request(\n        cls,\n        tokenizer: AnyTokenizer,\n        request: EngineCoreRequest,\n        prompt: Optional[str],\n        parent_req: Optional[ParentRequest],\n        request_index: int,\n        queue: Optional[RequestOutputCollector],\n        log_stats: bool,\n    ) -> \"RequestState\":\n        if not request.sampling_params.detokenize:\n            tokenizer = None\n        return cls(\n            request_id=request.request_id,\n            parent_req=parent_req,\n            request_index=request_index,\n            lora_name=(request.lora_request.name\n                       if request.lora_request is not None else None),\n            output_kind=request.sampling_params.output_kind,\n            prompt=prompt,\n            prompt_token_ids=request.prompt_token_ids,\n            logprobs_processor=LogprobsProcessor.from_new_request(\n                tokenizer=tokenizer,\n                request=request,\n            ),\n            detokenizer=IncrementalDetokenizer.from_new_request(\n                tokenizer=tokenizer,\n                request=request,\n            ),\n            max_tokens_param=(request.sampling_params.max_tokens if\n                              request.sampling_params is not None else None),\n            arrival_time=request.arrival_time,\n            queue=queue,\n            log_stats=log_stats,\n        )\n\n    def make_request_output(\n        self,\n        new_token_ids: list[int],\n        finish_reason: Optional[FinishReason],\n        stop_reason: Union[int, str, None],\n        kv_transfer_params: Optional[dict[str, Any]] = None,\n    ) -> Optional[RequestOutput]:\n\n        finished = finish_reason is not None\n        final_only = self.output_kind == RequestOutputKind.FINAL_ONLY\n\n        if not finished and final_only:\n            # Only the final output is required in FINAL_ONLY mode.\n            return None\n\n        completion_output = self._new_completion_output(\n            new_token_ids, finish_reason, stop_reason)\n\n        request_id = self.request_id\n        if self.parent_req is None:\n            outputs = [completion_output]\n        else:\n            request_id, outputs, finished = self.parent_req.get_outputs(\n                request_id, completion_output)\n            if not outputs:\n                return None\n\n        return self._new_request_output(request_id, outputs, finished,\n                                        kv_transfer_params)\n\n    def _new_request_output(\n        self,\n        request_id: str,\n        outputs: list[CompletionOutput],\n        finished: bool,\n        kv_transfer_params: Optional[dict[str, Any]] = None,\n    ) -> RequestOutput:\n\n        if self.output_kind == RequestOutputKind.DELTA:\n            # Side effect: logprobs processor forgets prompt logprobs\n            prompt_logprobs = self.logprobs_processor.pop_prompt_logprobs()\n        else:\n            prompt_logprobs = self.logprobs_processor.prompt_logprobs\n\n        return RequestOutput(\n            request_id=request_id,\n            prompt=self.prompt,\n            prompt_token_ids=self.prompt_token_ids,\n            prompt_logprobs=prompt_logprobs,\n            outputs=outputs,\n            finished=finished,\n            kv_transfer_params=kv_transfer_params,\n        )\n\n    def _new_completion_output(\n        self,\n        token_ids: list[int],\n        finish_reason: Optional[FinishReason],\n        stop_reason: Union[int, str, None],\n    ) -> CompletionOutput:\n\n        finished = finish_reason is not None\n        delta = self.output_kind == RequestOutputKind.DELTA\n\n        # Prepare text and token_ids, based on delta mode\n        text = self.detokenizer.get_next_output_text(finished, delta)\n        if not delta:\n            token_ids = self.detokenizer.output_token_ids\n\n        # Prepare logprobs, based on delta mode\n        logprobs = self.logprobs_processor.logprobs\n        if delta and logprobs:\n            logprobs = logprobs[-len(token_ids):]\n\n        return CompletionOutput(\n            index=self.request_index,\n            text=text,\n            token_ids=token_ids,\n            logprobs=logprobs,\n            cumulative_logprob=self.logprobs_processor.cumulative_logprob,\n            finish_reason=str(finish_reason) if finished else None,\n            stop_reason=stop_reason if finished else None)\n\n\nclass OutputProcessor:\n    \"\"\"Process EngineCoreOutputs into RequestOutputs.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: TokenizerGroup,\n        log_stats: bool,\n    ):\n        self.log_stats = log_stats\n        self.tokenizer = tokenizer\n        self.request_states: dict[str, RequestState] = {}\n        self.parent_requests: dict[str, ParentRequest] = {}\n        self.lora_states = LoRARequestStates()\n\n    def get_num_unfinished_requests(self):\n        return len(self.request_states)\n\n    def has_unfinished_requests(self) -> bool:\n        return len(self.request_states) > 0\n\n    def propagate_error(self, e: Exception):\n        \"\"\"Propagate error to all generate() tasks.\"\"\"\n\n        for _, state in self.request_states.items():\n            assert state.queue is not None\n            state.queue.put(e)\n\n    def abort_requests(\n        self,\n        request_ids: Iterable[str],\n    ) -> list[str]:\n        request_ids_to_abort = []\n        for request_id in request_ids:\n            req_state = self.request_states.pop(request_id, None)\n            if req_state is not None:\n                self.lora_states.abort_request(req_state)\n                request_ids_to_abort.append(request_id)\n            else:\n                parent = self.parent_requests.pop(request_id, None)\n                if parent and parent.child_requests:\n                    self.abort_requests(parent.child_requests)\n                    request_ids_to_abort.extend(parent.child_requests)\n        return request_ids_to_abort\n\n    def add_request(\n        self,\n        request: EngineCoreRequest,\n        prompt: Optional[str],\n        parent_req: Optional[ParentRequest] = None,\n        request_index: int = 0,\n        queue: Optional[RequestOutputCollector] = None,\n    ) -> None:\n        request_id = request.request_id\n        if request_id in self.request_states:\n            raise ValueError(f\"Request id {request_id} already running.\")\n\n        req_state = RequestState.from_new_request(\n            tokenizer=self.tokenizer.get_lora_tokenizer(request.lora_request),\n            request=request,\n            prompt=prompt,\n            parent_req=parent_req,\n            request_index=request_index,\n            queue=queue,\n            log_stats=self.log_stats)\n        self.request_states[request_id] = req_state\n        self.lora_states.add_request(req_state)\n        if parent_req:\n            self.parent_requests[parent_req.request_id] = parent_req\n\n    def process_outputs(\n        self,\n        engine_core_outputs: list[EngineCoreOutput],\n        engine_core_timestamp: Optional[float] = None,\n        iteration_stats: Optional[IterationStats] = None,\n    ) -> OutputProcessorOutput:\n        \"\"\"\n        Process the EngineCoreOutputs:\n        1) Compute stats for logging\n        2) Detokenize\n        3) Create and handle RequestOutput objects:\n            * If there is a queue (for usage with AsyncLLM), \n              put the RequestOutput objects into the queue for\n              handling by the per-request generate() tasks.\n\n            * If there is no queue (for usage with LLMEngine), \n              return a list of RequestOutput objects.\n\n        NOTE FOR DEVELOPERS\n\n        vLLM V1 minimizes the number of python loops over the full\n        batch to ensure system overheads are minimized. This is the \n        only function that should loop over EngineCoreOutputs.\n\n        If you need to touch every element of the batch, do it from\n        within the loop below.\n        \"\"\"\n\n        request_outputs: list[RequestOutput] = []\n        reqs_to_abort: list[str] = []\n        for engine_core_output in engine_core_outputs:\n            req_id = engine_core_output.request_id\n            req_state = self.request_states.get(req_id)\n            if req_state is None:\n                # Ignore output for already-aborted request.\n                continue\n\n            # 1) Compute stats for this iteration.\n            self._update_stats_from_output(req_state, engine_core_output,\n                                           engine_core_timestamp,\n                                           iteration_stats)\n\n            new_token_ids = engine_core_output.new_token_ids\n            finish_reason = engine_core_output.finish_reason\n            stop_reason = engine_core_output.stop_reason\n            kv_transfer_params = engine_core_output.kv_transfer_params\n\n            req_state.is_prefilling = False\n\n            # 2) Detokenize the token ids into text and perform stop checks.\n            stop_string = req_state.detokenizer.update(\n                new_token_ids, finish_reason == FinishReason.STOP)\n            if stop_string:\n                finish_reason = FinishReason.STOP\n                stop_reason = stop_string\n\n            # 3) Compute sample and prompt logprobs for request, if required.\n            req_state.logprobs_processor.update_from_output(engine_core_output)\n\n            # 4) Create and handle RequestOutput objects.\n            if request_output := req_state.make_request_output(\n                    new_token_ids, finish_reason, stop_reason,\n                    kv_transfer_params):\n                if req_state.queue is not None:\n                    # AsyncLLM: put into queue for handling by generate().\n                    req_state.queue.put(request_output)\n                else:\n                    # LLMEngine: return list of RequestOutputs.\n                    request_outputs.append(request_output)\n\n            # Free completed requests.\n            if finish_reason is not None:\n                self.request_states.pop(req_id)\n                # Remove parent request if applicable.\n                parent_req = req_state.parent_req\n                if parent_req and not parent_req.child_requests:\n                    self.parent_requests.pop(parent_req.request_id, None)\n                if not engine_core_output.finished:\n                    # If req not finished in EngineCore, but Detokenizer\n                    # detected stop string, abort needed in EngineCore.\n                    reqs_to_abort.append(req_id)\n\n                # Track per-request stats\n                self._update_stats_from_finished(req_state, finish_reason,\n                                                 iteration_stats)\n\n        self.lora_states.update_iteration_stats(iteration_stats)\n\n        return OutputProcessorOutput(\n            request_outputs=request_outputs,\n            reqs_to_abort=reqs_to_abort,\n        )\n\n    def _update_stats_from_output(self, req_state: RequestState,\n                                  engine_core_output: EngineCoreOutput,\n                                  engine_core_timestamp: Optional[float],\n                                  iteration_stats: Optional[IterationStats]):\n        if iteration_stats is None:\n            return\n\n        lora_stats = self.lora_states.get_stats(req_state)\n\n        assert engine_core_timestamp is not None\n        assert req_state.stats is not None\n        iteration_stats.update_from_output(engine_core_output,\n                                           engine_core_timestamp,\n                                           req_state.is_prefilling,\n                                           req_state.prompt_len,\n                                           req_state.stats, lora_stats)\n\n    def _update_stats_from_finished(self, req_state: RequestState,\n                                    finish_reason: Optional[FinishReason],\n                                    iteration_stats: Optional[IterationStats]):\n        if iteration_stats is None:\n            return\n\n        assert finish_reason is not None\n        assert req_state.stats is not None\n        iteration_stats.update_from_finished_request(\n            finish_reason=finish_reason,\n            num_prompt_tokens=len(req_state.prompt_token_ids),\n            max_tokens_param=req_state.max_tokens_param,\n            req_stats=req_state.stats)\n        self.lora_states.finish_request(req_state)\n\n        ParentRequest.observe_finished_request(\n            req_state.parent_req, iteration_stats,\n            req_state.stats.num_generation_tokens)\n", 424], "/home/jeromeku/vllm/vllm/v1/serial_utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport dataclasses\nimport pickle\nfrom collections.abc import Sequence\nfrom inspect import isclass\nfrom types import FunctionType\nfrom typing import Any, Optional, Union\n\nimport cloudpickle\nimport numpy as np\nimport torch\nimport zmq\nfrom msgspec import msgpack\n\nfrom vllm import envs\nfrom vllm.logger import init_logger\nfrom vllm.multimodal.inputs import (BaseMultiModalField,\n                                    MultiModalBatchedField,\n                                    MultiModalFieldConfig, MultiModalFieldElem,\n                                    MultiModalFlatField, MultiModalKwargs,\n                                    MultiModalKwargsItem,\n                                    MultiModalSharedField, NestedTensors)\n\nlogger = init_logger(__name__)\n\nCUSTOM_TYPE_PICKLE = 1\nCUSTOM_TYPE_CLOUDPICKLE = 2\nCUSTOM_TYPE_RAW_VIEW = 3\n\n# MultiModalField class serialization type map.\n# These need to list all possible field types and match them\n# to factory methods in `MultiModalFieldConfig`.\nMMF_CLASS_TO_FACTORY: dict[type[BaseMultiModalField], str] = {\n    MultiModalFlatField: \"flat\",\n    MultiModalSharedField: \"shared\",\n    MultiModalBatchedField: \"batched\",\n}\n\nbytestr = Union[bytes, bytearray, memoryview, zmq.Frame]\n\n\ndef _log_insecure_serialization_warning():\n    logger.warning_once(\"Allowing insecure serialization using pickle due to \"\n                        \"VLLM_ALLOW_INSECURE_SERIALIZATION=1\")\n\n\nclass MsgpackEncoder:\n    \"\"\"Encoder with custom torch tensor and numpy array serialization.\n\n    Note that unlike vanilla `msgspec` Encoders, this interface is generally\n    not thread-safe when encoding tensors / numpy arrays.\n\n    By default, arrays below 256B are serialized inline Larger will get sent \n    via dedicated messages. Note that this is a per-tensor limit.\n    \"\"\"\n\n    def __init__(self, size_threshold: Optional[int] = None):\n        if size_threshold is None:\n            size_threshold = envs.VLLM_MSGPACK_ZERO_COPY_THRESHOLD\n        self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)\n        # This is used as a local stash of buffers that we can then access from\n        # our custom `msgspec` hook, `enc_hook`. We don't have a way to\n        # pass custom data to the hook otherwise.\n        self.aux_buffers: Optional[list[bytestr]] = None\n        self.size_threshold = size_threshold\n        if envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            _log_insecure_serialization_warning()\n\n    def encode(self, obj: Any) -> Sequence[bytestr]:\n        try:\n            self.aux_buffers = bufs = [b'']\n            bufs[0] = self.encoder.encode(obj)\n            # This `bufs` list allows us to collect direct pointers to backing\n            # buffers of tensors and np arrays, and return them along with the\n            # top-level encoded buffer instead of copying their data into the\n            # new buffer.\n            return bufs\n        finally:\n            self.aux_buffers = None\n\n    def encode_into(self, obj: Any, buf: bytearray) -> Sequence[bytestr]:\n        try:\n            self.aux_buffers = [buf]\n            bufs = self.aux_buffers\n            self.encoder.encode_into(obj, buf)\n            return bufs\n        finally:\n            self.aux_buffers = None\n\n    def enc_hook(self, obj: Any) -> Any:\n        if isinstance(obj, torch.Tensor):\n            return self._encode_tensor(obj)\n\n        # Fall back to pickle for object or void kind ndarrays.\n        if isinstance(obj, np.ndarray) and obj.dtype.kind not in ('O', 'V'):\n            return self._encode_ndarray(obj)\n\n        if isinstance(obj, slice):\n            # We are assuming only int-based values will be used here.\n            return tuple(\n                int(v) if v is not None else None\n                for v in (obj.start, obj.stop, obj.step))\n\n        if isinstance(obj, MultiModalKwargs):\n            mm: MultiModalKwargs = obj\n            if not mm.modalities:\n                # just return the main dict if there are no modalities.\n                return dict(mm)\n\n            # ignore the main dict, it will be re-indexed.\n            # Encode a list of MultiModalKwargsItems as plain dicts\n            # + special handling for .field.\n            # Any tensors *not* indexed by modality will be ignored.\n            return [[{\n                \"modality\": elem.modality,\n                \"key\": elem.key,\n                \"data\": self._encode_nested_tensors(elem.data),\n                \"field\": self._encode_mm_field(elem.field),\n            } for elem in item.values()]\n                    for itemlist in mm._items_by_modality.values()\n                    for item in itemlist]\n\n        if not envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            raise TypeError(f\"Object of type {type(obj)} is not serializable\"\n                            \"Set VLLM_ALLOW_INSECURE_SERIALIZATION=1 to allow \"\n                            \"fallback to pickle-based serialization.\")\n\n        if isinstance(obj, FunctionType):\n            # `pickle` is generally faster than cloudpickle, but can have\n            # problems serializing methods.\n            return msgpack.Ext(CUSTOM_TYPE_CLOUDPICKLE, cloudpickle.dumps(obj))\n\n        return msgpack.Ext(CUSTOM_TYPE_PICKLE,\n                           pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))\n\n    def _encode_ndarray(\n        self, obj: np.ndarray\n    ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n        assert self.aux_buffers is not None\n        # If the array is non-contiguous, we need to copy it first\n        arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()\n        if not obj.shape or obj.nbytes < self.size_threshold:\n            # Encode small arrays and scalars inline. Using this extension type\n            # ensures we can avoid copying when decoding.\n            data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)\n        else:\n            # Otherwise encode index of backing buffer to avoid copy.\n            data = len(self.aux_buffers)\n            self.aux_buffers.append(arr_data)\n\n        # We serialize the ndarray as a tuple of native types.\n        # The data is either inlined if small, or an index into a list of\n        # backing buffers that we've stashed in `aux_buffers`.\n        return obj.dtype.str, obj.shape, data\n\n    def _encode_tensor(\n        self, obj: torch.Tensor\n    ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n        assert self.aux_buffers is not None\n        # this creates a copy of the tensor if it's not already contiguous\n        obj = obj.contiguous()\n        #  view the tensor as a 1D array of bytes\n        arr = obj.view((obj.numel(), )).view(torch.uint8).numpy()\n        if obj.nbytes < self.size_threshold:\n            # Smaller tensors are encoded inline, just like ndarrays.\n            data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr.data)\n        else:\n            # Otherwise encode index of backing buffer to avoid copy.\n            data = len(self.aux_buffers)\n            self.aux_buffers.append(arr.data)\n        dtype = str(obj.dtype)[6:]  # remove 'torch.' prefix\n        return dtype, obj.shape, data\n\n    def _encode_nested_tensors(self, nt: NestedTensors) -> Any:\n        if isinstance(nt, torch.Tensor):\n            return self._encode_tensor(nt)\n        if isinstance(nt, (int, float)):\n            # Although it violates NestedTensors type, MultiModalKwargs\n            # values are sometimes floats.\n            return nt\n        return [self._encode_nested_tensors(x) for x in nt]\n\n    def _encode_mm_field(self, field: BaseMultiModalField):\n        # Figure out the factory name for the field type.\n        name = MMF_CLASS_TO_FACTORY.get(field.__class__)\n        if not name:\n            raise TypeError(f\"Unsupported field type: {field.__class__}\")\n        # We just need to copy all of the field values in order\n        # which will be then used to reconstruct the field.\n        field_values = (getattr(field, f.name)\n                        for f in dataclasses.fields(field))\n        return name, *field_values\n\n\nclass MsgpackDecoder:\n    \"\"\"Decoder with custom torch tensor and numpy array serialization.\n\n    Note that unlike vanilla `msgspec` Decoders, this interface is generally\n    not thread-safe when encoding tensors / numpy arrays.\n    \"\"\"\n\n    def __init__(self, t: Optional[Any] = None):\n        args = () if t is None else (t, )\n        self.decoder = msgpack.Decoder(*args,\n                                       ext_hook=self.ext_hook,\n                                       dec_hook=self.dec_hook)\n        self.aux_buffers: Sequence[bytestr] = ()\n        if envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            _log_insecure_serialization_warning()\n\n    def decode(self, bufs: Union[bytestr, Sequence[bytestr]]) -> Any:\n        if isinstance(bufs, (bytes, bytearray, memoryview, zmq.Frame)):\n            # TODO - This check can become `isinstance(bufs, bytestr)`\n            # as of Python 3.10.\n            return self.decoder.decode(bufs)\n\n        self.aux_buffers = bufs\n        try:\n            return self.decoder.decode(bufs[0])\n        finally:\n            self.aux_buffers = ()\n\n    def dec_hook(self, t: type, obj: Any) -> Any:\n        # Given native types in `obj`, convert to type `t`.\n        if isclass(t):\n            if issubclass(t, np.ndarray):\n                return self._decode_ndarray(obj)\n            if issubclass(t, torch.Tensor):\n                return self._decode_tensor(obj)\n            if t is slice:\n                return slice(*obj)\n            if issubclass(t, MultiModalKwargs):\n                if isinstance(obj, list):\n                    return MultiModalKwargs.from_items(\n                        self._decode_mm_items(obj))\n                return MultiModalKwargs({\n                    k: self._decode_nested_tensors(v)\n                    for k, v in obj.items()\n                })\n        return obj\n\n    def _decode_ndarray(self, arr: Any) -> np.ndarray:\n        dtype, shape, data = arr\n        # zero-copy decode. We assume the ndarray will not be kept around,\n        # as it now locks the whole received message buffer in memory.\n        buffer = self.aux_buffers[data] if isinstance(data, int) else data\n        return np.ndarray(buffer=buffer, dtype=np.dtype(dtype), shape=shape)\n\n    def _decode_tensor(self, arr: Any) -> torch.Tensor:\n        dtype, shape, data = arr\n        # Copy from inline representation, to decouple the memory storage\n        # of the message from the original buffer. And also make Torch\n        # not complain about a readonly memoryview.\n        buffer = self.aux_buffers[data] if isinstance(data, int) \\\n            else bytearray(data)\n        # Create numpy wrapper around the bytes\n        arr = np.ndarray(buffer=buffer, dtype=np.uint8, shape=(len(buffer), ))\n        torch_dtype = getattr(torch, dtype)\n        assert isinstance(torch_dtype, torch.dtype)\n        # Convert back to proper shape & type\n        return torch.from_numpy(arr).view(torch_dtype).view(shape)\n\n    def _decode_mm_items(self, obj: list) -> list[MultiModalKwargsItem]:\n        decoded_items = []\n        for item in obj:\n            elems = []\n            for v in item:\n                v[\"data\"] = self._decode_nested_tensors(v[\"data\"])\n                # Reconstruct the field processor using MultiModalFieldConfig\n                factory_meth_name, *field_args = v[\"field\"]\n                factory_meth = getattr(MultiModalFieldConfig,\n                                       factory_meth_name)\n\n                # Special case: decode the union \"slices\" field of\n                # MultiModalFlatField\n                if factory_meth_name == \"flat\":\n                    field_args[0] = self._decode_nested_slices(field_args[0])\n\n                v[\"field\"] = factory_meth(None, *field_args).field\n                elems.append(MultiModalFieldElem(**v))\n            decoded_items.append(MultiModalKwargsItem.from_elems(elems))\n        return decoded_items\n\n    def _decode_nested_tensors(self, obj: Any) -> NestedTensors:\n        if isinstance(obj, (int, float)):\n            # Although it violates NestedTensors type, MultiModalKwargs\n            # values are sometimes floats.\n            return obj\n        if not isinstance(obj, list):\n            raise TypeError(f\"Unexpected NestedTensors contents: {type(obj)}\")\n        if obj and isinstance(obj[0], str):\n            return self._decode_tensor(obj)\n        return [self._decode_nested_tensors(x) for x in obj]\n\n    def _decode_nested_slices(self, obj: Any) -> Any:\n        assert isinstance(obj, (list, tuple))\n        if obj and not isinstance(obj[0], (list, tuple)):\n            return slice(*obj)\n        return [self._decode_nested_slices(x) for x in obj]\n\n    def ext_hook(self, code: int, data: memoryview) -> Any:\n        if code == CUSTOM_TYPE_RAW_VIEW:\n            return data\n\n        if envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            if code == CUSTOM_TYPE_PICKLE:\n                return pickle.loads(data)\n            if code == CUSTOM_TYPE_CLOUDPICKLE:\n                return cloudpickle.loads(data)\n\n        raise NotImplementedError(\n            f\"Extension type code {code} is not supported\")\n", 313], "/home/jeromeku/vllm/vllm/v1/engine/core_client.py": ["# SPDX-License-Identifier: Apache-2.0\nimport asyncio\nimport contextlib\nimport queue\nimport uuid\nimport weakref\nfrom abc import ABC, abstractmethod\nfrom collections import deque\nfrom collections.abc import Awaitable, Sequence\nfrom concurrent.futures import Future\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom threading import Thread\nfrom typing import Any, Callable, Optional, TypeVar, Union\n\nimport msgspec\nimport zmq\nimport zmq.asyncio\n\nfrom vllm.config import ParallelConfig, VllmConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.utils import (\n    get_open_port,\n    get_open_zmq_inproc_path,\n    get_open_zmq_ipc_path,\n    get_tcp_uri,\n    make_zmq_socket,\n)\nfrom vllm.v1.engine import (\n    EngineCoreOutputs,\n    EngineCoreRequest,\n    EngineCoreRequestType,\n    UtilityOutput,\n)\nfrom vllm.v1.engine.core import EngineCore, EngineCoreProc\nfrom vllm.v1.engine.exceptions import EngineDeadError\nfrom vllm.v1.executor.abstract import Executor\nfrom vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder, bytestr\nfrom vllm.v1.utils import CoreEngineProcManager\n\nlogger = init_logger(__name__)\n\nAnyFuture = Union[asyncio.Future[Any], Future[Any]]\n\n_R = TypeVar('_R')  # Return type for collective_rpc\n\nSTARTUP_POLL_PERIOD_MS = 10000\n\n\nclass EngineCoreClient(ABC):\n    \"\"\"\n    EngineCoreClient: subclasses handle different methods for pushing \n        and pulling from the EngineCore for asyncio / multiprocessing.\n\n    Subclasses:\n    * InprocClient: In process EngineCore (for V0-style LLMEngine use)\n    * SyncMPClient: ZMQ + background proc EngineCore (for LLM)\n    * AsyncMPClient: ZMQ + background proc EngineCore w/ asyncio (for AsyncLLM)\n    \"\"\"\n\n    @staticmethod\n    def make_client(\n        multiprocess_mode: bool,\n        asyncio_mode: bool,\n        vllm_config: VllmConfig,\n        executor_class: type[Executor],\n        log_stats: bool,\n    ) -> \"EngineCoreClient\":\n\n        # TODO: support this for debugging purposes.\n        if asyncio_mode and not multiprocess_mode:\n            raise NotImplementedError(\n                \"Running EngineCore in asyncio without multiprocessing \"\n                \"is not currently supported.\")\n\n        if multiprocess_mode and asyncio_mode:\n            if vllm_config.parallel_config.data_parallel_size > 1:\n                return DPAsyncMPClient(vllm_config, executor_class, log_stats)\n\n            return AsyncMPClient(vllm_config, executor_class, log_stats)\n\n        if multiprocess_mode and not asyncio_mode:\n            return SyncMPClient(vllm_config, executor_class, log_stats)\n\n        return InprocClient(vllm_config, executor_class, log_stats)\n\n    @abstractmethod\n    def shutdown(self):\n        ...\n\n    def get_output(self) -> EngineCoreOutputs:\n        raise NotImplementedError\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        raise NotImplementedError\n\n    def profile(self, is_start: bool = True) -> None:\n        raise NotImplementedError\n\n    def reset_mm_cache(self) -> None:\n        raise NotImplementedError\n\n    def reset_prefix_cache(self) -> None:\n        raise NotImplementedError\n\n    def sleep(self, level: int = 1) -> None:\n        raise NotImplementedError\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        raise NotImplementedError\n\n    def is_sleeping(self) -> bool:\n        raise NotImplementedError\n\n    def execute_dummy_batch(self) -> None:\n        raise NotImplementedError\n\n    async def execute_dummy_batch_async(self) -> None:\n        raise NotImplementedError\n\n    def abort_requests(self, request_ids: list[str]) -> None:\n        raise NotImplementedError\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        raise NotImplementedError\n\n    def remove_lora(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    def list_loras(self) -> set[int]:\n        raise NotImplementedError\n\n    def pin_lora(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    def save_sharded_state(self,\n                           path: str,\n                           pattern: Optional[str] = None,\n                           max_size: Optional[int] = None) -> None:\n        raise NotImplementedError\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        raise NotImplementedError\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        raise NotImplementedError\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        raise NotImplementedError\n\n    async def profile_async(self, is_start: bool = True) -> None:\n        raise NotImplementedError\n\n    async def reset_mm_cache_async(self) -> None:\n        raise NotImplementedError\n\n    async def reset_prefix_cache_async(self) -> None:\n        raise NotImplementedError\n\n    async def sleep_async(self, level: int = 1) -> None:\n        raise NotImplementedError\n\n    async def wake_up_async(self, tags: Optional[list[str]] = None) -> None:\n        raise NotImplementedError\n\n    async def is_sleeping_async(self) -> bool:\n        raise NotImplementedError\n\n    async def abort_requests_async(self, request_ids: list[str]) -> None:\n        raise NotImplementedError\n\n    async def add_lora_async(self, lora_request: LoRARequest) -> bool:\n        raise NotImplementedError\n\n    async def remove_lora_async(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    async def list_loras_async(self) -> set[int]:\n        raise NotImplementedError\n\n    async def pin_lora_async(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    async def save_sharded_state_async(self,\n                                       path: str,\n                                       pattern: Optional[str] = None,\n                                       max_size: Optional[int] = None) -> None:\n        raise NotImplementedError\n\n    async def collective_rpc_async(\n            self,\n            method: Union[str, Callable[..., _R]],\n            timeout: Optional[float] = None,\n            args: tuple = (),\n            kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        raise NotImplementedError\n\n\nclass InprocClient(EngineCoreClient):\n    \"\"\"\n    InprocClient: client for in-process EngineCore. Intended \n    for use in LLMEngine for V0-style add_request() and step()\n        EngineCore setup in this process (no busy loop).\n\n        * pushes EngineCoreRequest directly into the EngineCore\n        * pulls EngineCoreOutputs by stepping the EngineCore\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.engine_core = EngineCore(*args, **kwargs)\n\n    def get_output(self) -> EngineCoreOutputs:\n        return self.engine_core.step()\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        self.engine_core.add_request(request)\n\n    def abort_requests(self, request_ids: list[str]) -> None:\n        if len(request_ids) > 0:\n            self.engine_core.abort_requests(request_ids)\n\n    def shutdown(self) -> None:\n        self.engine_core.shutdown()\n\n    def profile(self, is_start: bool = True) -> None:\n        self.engine_core.profile(is_start)\n\n    def reset_mm_cache(self) -> None:\n        self.engine_core.reset_mm_cache()\n\n    def reset_prefix_cache(self) -> None:\n        self.engine_core.reset_prefix_cache()\n\n    def sleep(self, level: int = 1) -> None:\n        self.engine_core.sleep(level)\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        self.engine_core.wake_up(tags)\n\n    def is_sleeping(self) -> bool:\n        return self.engine_core.is_sleeping()\n\n    def execute_dummy_batch(self) -> None:\n        self.engine_core.execute_dummy_batch()\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.engine_core.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.engine_core.remove_lora(lora_id)\n\n    def list_loras(self) -> set[int]:\n        return self.engine_core.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.engine_core.pin_lora(lora_id)\n\n    def save_sharded_state(self,\n                           path: str,\n                           pattern: Optional[str] = None,\n                           max_size: Optional[int] = None) -> None:\n        self.engine_core.save_sharded_state(path, pattern, max_size)\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.engine_core.collective_rpc(method, timeout, args, kwargs)\n\n\nclass CoreEngineState(Enum):\n    NEW = auto()\n    CONNECTED = auto()\n    READY = auto()\n\n\nclass CoreEngine:\n    \"\"\"One per data parallel rank.\"\"\"\n\n    def __init__(self, index: int = 0, local: bool = True):\n        self.local = local\n        self.index = index\n        self.identity = index.to_bytes(length=2, byteorder=\"little\")\n\n        self.state = CoreEngineState.NEW\n        self.num_reqs_in_flight = 0\n\n\n@dataclass\nclass BackgroundResources:\n    \"\"\"Used as a finalizer for clean shutdown, avoiding\n    circular reference back to the client object.\"\"\"\n\n    ctx: Union[zmq.Context]\n    local_engine_manager: Optional[CoreEngineProcManager] = None\n    output_socket: Optional[Union[zmq.Socket, zmq.asyncio.Socket]] = None\n    input_socket: Optional[Union[zmq.Socket, zmq.asyncio.Socket]] = None\n    output_queue_task: Optional[asyncio.Task] = None\n    shutdown_path: Optional[str] = None\n\n    # Set if any of the engines are dead. Here so that the output\n    # processing threads can access it without holding a ref to the client.\n    engine_dead: bool = False\n\n    def __call__(self):\n        \"\"\"Clean up background resources.\"\"\"\n\n        self.engine_dead = True\n        if self.local_engine_manager is not None:\n            self.local_engine_manager.close()\n\n        if self.output_queue_task is not None:\n            self.output_queue_task.cancel()\n\n        # ZMQ context termination can hang if the sockets\n        # aren't explicitly closed first.\n        if self.output_socket is not None:\n            self.output_socket.close(linger=0)\n        if self.input_socket is not None:\n            self.input_socket.close(linger=0)\n        if self.shutdown_path is not None:\n            # We must ensure that the sync output socket is\n            # closed cleanly in its own thread.\n            with self.ctx.socket(zmq.PAIR) as shutdown_sender:\n                shutdown_sender.connect(self.shutdown_path)\n                # Send shutdown signal.\n                shutdown_sender.send(b'')\n\n    def validate_alive(self, frames: Sequence[zmq.Frame]):\n        if len(frames) == 1 and (frames[0].buffer\n                                 == EngineCoreProc.ENGINE_CORE_DEAD):\n            self.engine_dead = True\n            raise EngineDeadError()\n\n\nclass MPClient(EngineCoreClient):\n    \"\"\"\n    MPClient: base client for multi-proc EngineCore.\n        EngineCore runs in a background process busy loop, getting\n        new EngineCoreRequests and returning EngineCoreOutputs\n\n        * pushes EngineCoreRequests via input_socket\n        * pulls EngineCoreOutputs via output_socket\n    \n        * AsyncMPClient subclass for AsyncLLM usage\n        * SyncMPClient subclass for LLM usage\n    \"\"\"\n\n    def __init__(\n        self,\n        asyncio_mode: bool,\n        vllm_config: VllmConfig,\n        executor_class: type[Executor],\n        log_stats: bool,\n    ):\n        self.vllm_config = vllm_config\n        # Serialization setup.\n        self.encoder = MsgpackEncoder()\n        self.decoder = MsgpackDecoder(EngineCoreOutputs)\n\n        # ZMQ setup.\n        sync_ctx = zmq.Context(io_threads=2)\n        self.ctx = zmq.asyncio.Context(sync_ctx) if asyncio_mode else sync_ctx\n\n        # This will ensure resources created so far are closed\n        # when the client is garbage collected, even if an\n        # exception is raised mid-construction.\n        self.resources = BackgroundResources(ctx=sync_ctx)\n        self._finalizer = weakref.finalize(self, self.resources)\n        success = False\n        try:\n            parallel_config = vllm_config.parallel_config\n            local_engine_count = parallel_config.data_parallel_size_local\n            start_index = parallel_config.data_parallel_rank\n            local_start_index = parallel_config.data_parallel_rank_local\n\n            # SPMD mode is where there is an LLM instance per DP rank and\n            # one core engine per LLM, see\n            # examples/offline_inference/data_parallel.py.\n            spmd_mode = local_start_index is not None\n            if spmd_mode:\n                assert local_engine_count == 1\n                self.core_engines = [\n                    CoreEngine(index=local_start_index, local=True)\n                ]\n            else:\n                assert start_index == 0\n                local_start_index = 0\n                self.core_engines = [\n                    CoreEngine(index=i, local=(i < local_engine_count))\n                    for i in range(parallel_config.data_parallel_size)\n                ]\n\n            input_address, output_address = self._get_zmq_addresses(\n                parallel_config, spmd_mode)\n\n            # Create input and output sockets.\n            self.input_socket = self.resources.input_socket = make_zmq_socket(\n                self.ctx, input_address, zmq.ROUTER, bind=True)\n\n            self.resources.output_socket = make_zmq_socket(\n                self.ctx, output_address, zmq.constants.PULL)\n            # Start local engines.\n            if local_engine_count:\n                # In server mode, start_index and local_start_index will\n                # both be 0.\n                self.resources.local_engine_manager = CoreEngineProcManager(\n                    EngineCoreProc.run_engine_core,\n                    vllm_config=vllm_config,\n                    executor_class=executor_class,\n                    log_stats=log_stats,\n                    input_address=input_address,\n                    on_head_node=True,\n                    local_engine_count=local_engine_count,\n                    start_index=start_index,\n                    local_start_index=local_start_index)\n\n            self.core_engine = self.core_engines[0]\n\n            # Wait for engine core process(es) to start.\n            self._wait_for_engine_startup(output_address, parallel_config)\n\n            self.utility_results: dict[int, AnyFuture] = {}\n\n            # Request objects which may contain pytorch-allocated tensors\n            # that we need to keep references to until zmq is done with the\n            # underlying data.\n            self.pending_messages = deque[tuple[zmq.MessageTracker, Any]]()\n\n            success = True\n        finally:\n            if not success:\n                self._finalizer()\n\n    @staticmethod\n    def _get_zmq_addresses(parallel_config: ParallelConfig,\n                           spmd_mode: bool) -> tuple[str, str]:\n        \"\"\"Returns (input_address, output_address).\"\"\"\n        dp_size = parallel_config.data_parallel_size\n        local_engine_count = parallel_config.data_parallel_size_local\n\n        if local_engine_count == dp_size or spmd_mode:\n            input_address = get_open_zmq_ipc_path()\n            output_address = get_open_zmq_ipc_path()\n        else:\n            host = parallel_config.data_parallel_master_ip\n            input_port = parallel_config.data_parallel_rpc_port\n            output_port = get_open_port()\n            input_address = get_tcp_uri(host, input_port)\n            output_address = get_tcp_uri(host, output_port)\n\n        return input_address, output_address\n\n    def _wait_for_engine_startup(self, output_address: str,\n                                 parallel_config: ParallelConfig):\n        # Get a sync handle to the socket which can be sync or async.\n        sync_input_socket = zmq.Socket.shadow(self.input_socket)\n\n        # Wait for engine core process(es) to send ready messages.\n        local_count = parallel_config.data_parallel_size_local\n        remote_count = len(self.core_engines) - local_count\n        # [local, remote] counts\n        conn_pending, start_pending = [local_count, remote_count], [0, 0]\n\n        poller = zmq.Poller()\n        poller.register(sync_input_socket, zmq.POLLIN)\n        proc_manager = self.resources.local_engine_manager\n        if proc_manager is not None:\n            for sentinel in proc_manager.sentinels():\n                poller.register(sentinel, zmq.POLLIN)\n        while any(conn_pending) or any(start_pending):\n            events = poller.poll(STARTUP_POLL_PERIOD_MS)\n            if not events:\n                if any(conn_pending):\n                    logger.debug(\n                        \"Waiting for %d local, %d remote core engine proc(s) \"\n                        \"to connect.\", *conn_pending)\n                if any(start_pending):\n                    logger.debug(\n                        \"Waiting for %d local, %d remote core engine proc(s) \"\n                        \"to start.\", *start_pending)\n                continue\n            if len(events) > 1 or events[0][0] != sync_input_socket:\n                # One of the local core processes exited.\n                finished = proc_manager.finished_procs(\n                ) if proc_manager else {}\n                raise RuntimeError(\"Engine core initialization failed. \"\n                                   \"See root cause above. \"\n                                   f\"Failed core proc(s): {finished}\")\n\n            # Receive HELLO and READY messages from the input socket.\n            eng_identity, ready_msg_bytes = sync_input_socket.recv_multipart()\n            eng_index = int.from_bytes(eng_identity, byteorder=\"little\")\n            engine = next(\n                (e for e in self.core_engines if e.identity == eng_identity),\n                None)\n            if engine is None:\n                raise RuntimeError(f\"Message from engine with unexpected data \"\n                                   f\"parallel rank: {eng_index}\")\n            msg = msgspec.msgpack.decode(ready_msg_bytes)\n            status, local = msg[\"status\"], msg[\"local\"]\n            if local != engine.local:\n                raise RuntimeError(f\"{status} message from \"\n                                   f\"{'local' if local else 'remote'} \"\n                                   f\"engine {eng_index}, expected it to be \"\n                                   f\"{'local' if engine.local else 'remote'}\")\n\n            if status == \"HELLO\" and engine.state == CoreEngineState.NEW:\n\n                # Send init message with DP config info.\n                init_message = self.encoder.encode({\n                    \"output_socket_address\": output_address,\n                    \"parallel_config\": {\n                        \"data_parallel_master_ip\":\n                        parallel_config.data_parallel_master_ip,\n                        \"data_parallel_master_port\":\n                        parallel_config.data_parallel_master_port,\n                        \"data_parallel_size\":\n                        parallel_config.data_parallel_size,\n                    },\n                })\n                sync_input_socket.send_multipart((eng_identity, *init_message),\n                                                 copy=False)\n                conn_pending[0 if local else 1] -= 1\n                start_pending[0 if local else 1] += 1\n                engine.state = CoreEngineState.CONNECTED\n            elif status == \"READY\" and (engine.state\n                                        == CoreEngineState.CONNECTED):\n                # Setup KV cache config with initialization state from\n                # engine core process. Sum values from all engines in DP case.\n                cache_config = self.vllm_config.cache_config\n                num_gpu_blocks = cache_config.num_gpu_blocks or 0\n                num_gpu_blocks += msg['num_gpu_blocks']\n                cache_config.num_gpu_blocks = num_gpu_blocks\n\n                start_pending[0 if local else 1] -= 1\n                engine.state = CoreEngineState.READY\n            else:\n                raise RuntimeError(f\"Unexpected {status} message for \"\n                                   f\"{'local' if local else 'remote'} engine \"\n                                   f\"{eng_index} in {engine.state} state.\")\n\n            logger.debug(\"%s from %s core engine process %s.\", status,\n                         \"local\" if local else \"remote\", eng_index)\n\n    def shutdown(self):\n        # Terminate background resources.\n        self._finalizer()\n\n    def _format_exception(self, e: Exception) -> Exception:\n        \"\"\"If errored, use EngineDeadError so root cause is clear.\"\"\"\n        return EngineDeadError(\n            suppress_context=True) if self.resources.engine_dead else e\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n            raise EngineDeadError()\n\n    def add_pending_message(self, tracker: zmq.MessageTracker, msg: Any):\n        if not tracker.done:\n            self.pending_messages.appendleft((tracker, msg))\n\n    def free_pending_messages(self):\n        while self.pending_messages and self.pending_messages[-1][0].done:\n            self.pending_messages.pop()\n\n\ndef _process_utility_output(output: UtilityOutput,\n                            utility_results: dict[int, AnyFuture]):\n    \"\"\"Set the result from a utility method in the waiting future\"\"\"\n    future = utility_results.pop(output.call_id)\n    if output.failure_message is not None:\n        future.set_exception(Exception(output.failure_message))\n    else:\n        future.set_result(output.result)\n\n\nclass SyncMPClient(MPClient):\n    \"\"\"Synchronous client for multi-proc EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n                 log_stats: bool):\n        super().__init__(\n            asyncio_mode=False,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=log_stats,\n        )\n\n        self.outputs_queue = queue.Queue[Union[EngineCoreOutputs, Exception]]()\n\n        # Ensure that the outputs socket processing thread does not have\n        # a ref to the client which prevents gc.\n        ctx = self.ctx\n        out_socket = self.resources.output_socket\n        assert out_socket is not None\n        decoder = self.decoder\n        utility_results = self.utility_results\n        outputs_queue = self.outputs_queue\n\n        shutdown_path = get_open_zmq_inproc_path()\n        resources = self.resources\n        resources.shutdown_path = shutdown_path\n\n        def process_outputs_socket():\n            shutdown_socket = ctx.socket(zmq.PAIR)\n            try:\n                shutdown_socket.bind(shutdown_path)\n                poller = zmq.Poller()\n                poller.register(shutdown_socket)\n                poller.register(out_socket)\n                while True:\n                    socks = poller.poll()\n                    if not socks:\n                        continue\n                    if len(socks) == 2 or socks[0][0] == shutdown_socket:\n                        # shutdown signal, exit thread.\n                        break\n\n                    frames = out_socket.recv_multipart(copy=False)\n                    resources.validate_alive(frames)\n                    outputs = decoder.decode(frames)\n                    if outputs.utility_output:\n                        _process_utility_output(outputs.utility_output,\n                                                utility_results)\n                    else:\n                        outputs_queue.put_nowait(outputs)\n            except Exception as e:\n                outputs_queue.put_nowait(e)\n            finally:\n                # Close sockets.\n                shutdown_socket.close(linger=0)\n                out_socket.close(linger=0)\n\n        # Process outputs from engine in separate thread.\n        self.output_queue_thread = Thread(target=process_outputs_socket,\n                                          name=\"EngineCoreOutputQueueThread\",\n                                          daemon=True)\n        self.output_queue_thread.start()\n\n        # The thread takes on responsibility for closing the socket.\n        self.resources.output_socket = None\n\n    def get_output(self) -> EngineCoreOutputs:\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        outputs = self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n            raise self._format_exception(outputs) from None\n        return outputs\n\n    def _send_input(self, request_type: EngineCoreRequestType, request: Any):\n        self.ensure_alive()\n        self.free_pending_messages()\n        # (Identity, RequestType, SerializedRequest)\n        msg = (self.core_engine.identity, request_type.value,\n               *self.encoder.encode(request))\n\n        if len(msg) <= 3:\n            # No auxiliary buffers => no tensor backing buffers in request.\n            self.input_socket.send_multipart(msg, copy=False)\n            return\n\n        tracker = self.input_socket.send_multipart(msg, copy=False, track=True)\n        self.add_pending_message(tracker, request)\n\n    def call_utility(self, method: str, *args) -> Any:\n        call_id = uuid.uuid1().int >> 64\n        future: Future[Any] = Future()\n        self.utility_results[call_id] = future\n        self._send_input(EngineCoreRequestType.UTILITY,\n                         (call_id, method, args))\n\n        return future.result()\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        self._send_input(EngineCoreRequestType.ADD, request)\n\n    def abort_requests(self, request_ids: list[str]) -> None:\n        if request_ids and not self.resources.engine_dead:\n            self._send_input(EngineCoreRequestType.ABORT, request_ids)\n\n    def profile(self, is_start: bool = True) -> None:\n        self.call_utility(\"profile\", is_start)\n\n    def reset_mm_cache(self) -> None:\n        self.call_utility(\"reset_mm_cache\")\n\n    def reset_prefix_cache(self) -> None:\n        self.call_utility(\"reset_prefix_cache\")\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.call_utility(\"add_lora\", lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.call_utility(\"remove_lora\", lora_id)\n\n    def list_loras(self) -> set[int]:\n        return self.call_utility(\"list_loras\")\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.call_utility(\"pin_lora\", lora_id)\n\n    def sleep(self, level: int = 1) -> None:\n        self.call_utility(\"sleep\", level)\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        self.call_utility(\"wake_up\", tags)\n\n    def is_sleeping(self) -> bool:\n        return self.call_utility(\"is_sleeping\")\n\n    def execute_dummy_batch(self) -> None:\n        self.call_utility(\"execute_dummy_batch\")\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.call_utility(\"collective_rpc\", method, timeout, args,\n                                 kwargs)\n\n    def save_sharded_state(self,\n                           path: str,\n                           pattern: Optional[str] = None,\n                           max_size: Optional[int] = None) -> None:\n        self.call_utility(\"save_sharded_state\", path, pattern, max_size)\n\n\nclass AsyncMPClient(MPClient):\n    \"\"\"Asyncio-compatible client for multi-proc EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n                 log_stats: bool):\n        super().__init__(\n            asyncio_mode=True,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=log_stats,\n        )\n\n        self.outputs_queue = asyncio.Queue[Union[EngineCoreOutputs,\n                                                 Exception]]()\n        try:\n            # If we are running in an asyncio event loop, start the queue task.\n            # Otherwise, it will be started lazily. If it is not started here,\n            # we could miss EXECUTOR_FAILED messages from engine core if they\n            # occur prior to any requests being sent.\n            asyncio.get_running_loop()\n            self._ensure_output_queue_task()\n        except RuntimeError:\n            pass\n\n    def _ensure_output_queue_task(self):\n        resources = self.resources\n        if resources.output_queue_task is not None:\n            return\n\n        # Perform IO in separate task to parallelize as much as possible.\n        # Avoid task having direct reference back to the client.\n        decoder = self.decoder\n        utility_results = self.utility_results\n        outputs_queue = self.outputs_queue\n        output_handler: Optional[Callable[[AsyncMPClient, EngineCoreOutputs],\n                                          Awaitable[None]]] = getattr(\n                                              self.__class__,\n                                              \"process_engine_outputs\", None)\n        _self_ref = weakref.ref(self) if output_handler else None\n        output_socket = resources.output_socket\n        assert output_socket is not None\n\n        async def process_outputs_socket():\n            try:\n                while True:\n                    frames = await output_socket.recv_multipart(copy=False)\n                    resources.validate_alive(frames)\n                    outputs: EngineCoreOutputs = decoder.decode(frames)\n                    if outputs.utility_output:\n                        _process_utility_output(outputs.utility_output,\n                                                utility_results)\n                        continue\n\n                    if output_handler is not None:\n                        assert _self_ref is not None\n                        _self = _self_ref()\n                        if not _self:\n                            # Client has been garbage collected, abort.\n                            return\n                        await output_handler(_self, outputs)\n\n                    if outputs.outputs or outputs.scheduler_stats:\n                        outputs_queue.put_nowait(outputs)\n            except Exception as e:\n                outputs_queue.put_nowait(e)\n\n        resources.output_queue_task = asyncio.create_task(\n            process_outputs_socket(), name=\"EngineCoreOutputQueueTask\")\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        self._ensure_output_queue_task()\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        assert self.outputs_queue is not None\n        outputs = await self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n            raise self._format_exception(outputs) from None\n        return outputs\n\n    def _send_input(self,\n                    request_type: EngineCoreRequestType,\n                    request: Any,\n                    engine: Optional[CoreEngine] = None) -> Awaitable[Any]:\n        self.ensure_alive()\n        if engine is None:\n            engine = self.core_engine\n\n        message = (request_type.value, *self.encoder.encode(request))\n        return self._send_input_message(message, engine, request)\n\n    def _send_input_message(self, message: tuple[bytestr,\n                                                 ...], engine: CoreEngine,\n                            objects: Any) -> Awaitable[Any]:\n        \"\"\"\n        objects is a reference to retain until zmq is finished with the\n        buffers, in case they were extracted from tensors in the request.\n        \"\"\"\n        self.ensure_alive()\n        self.free_pending_messages()\n\n        msg = (engine.identity, ) + message\n        if not objects or len(msg) <= 3:\n            # No auxiliary buffers => no tensor backing buffers in request.\n            return self.input_socket.send_multipart(msg, copy=False)\n\n        future: asyncio.Future[zmq.MessageTracker]\n        future = self.input_socket.send_multipart(msg, copy=False, track=True)\n\n        def add_pending(f: asyncio.Future[zmq.MessageTracker]):\n            with contextlib.suppress(BaseException):\n                self.add_pending_message(f.result(), objects)\n\n        future.add_done_callback(add_pending)\n        return future\n\n    async def call_utility_async(self, method: str, *args) -> Any:\n        return await self._call_utility_async(method,\n                                              *args,\n                                              engine=self.core_engine)\n\n    async def _call_utility_async(self, method: str, *args,\n                                  engine: CoreEngine) -> Any:\n        call_id = uuid.uuid1().int >> 64\n        future = asyncio.get_running_loop().create_future()\n        self.utility_results[call_id] = future\n        message = (EngineCoreRequestType.UTILITY.value, *self.encoder.encode(\n            (call_id, method, args)))\n        await self._send_input_message(message, engine, args)\n        self._ensure_output_queue_task()\n        return await future\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        await self._send_input(EngineCoreRequestType.ADD, request)\n        self._ensure_output_queue_task()\n\n    async def abort_requests_async(self, request_ids: list[str]) -> None:\n        if request_ids and not self.resources.engine_dead:\n            await self._send_input(EngineCoreRequestType.ABORT, request_ids)\n\n    async def profile_async(self, is_start: bool = True) -> None:\n        await self.call_utility_async(\"profile\", is_start)\n\n    async def reset_mm_cache_async(self) -> None:\n        await self.call_utility_async(\"reset_mm_cache\")\n\n    async def reset_prefix_cache_async(self) -> None:\n        await self.call_utility_async(\"reset_prefix_cache\")\n\n    async def sleep_async(self, level: int = 1) -> None:\n        await self.call_utility_async(\"sleep\", level)\n\n    async def wake_up_async(self, tags: Optional[list[str]] = None) -> None:\n        await self.call_utility_async(\"wake_up\", tags)\n\n    async def is_sleeping_async(self) -> bool:\n        return await self.call_utility_async(\"is_sleeping\")\n\n    async def execute_dummy_batch_async(self) -> None:\n        await self.call_utility_async(\"execute_dummy_batch\")\n\n    async def add_lora_async(self, lora_request: LoRARequest) -> bool:\n        return await self.call_utility_async(\"add_lora\", lora_request)\n\n    async def remove_lora_async(self, lora_id: int) -> bool:\n        return await self.call_utility_async(\"remove_lora\", lora_id)\n\n    async def list_loras_async(self) -> set[int]:\n        return await self.call_utility_async(\"list_loras\")\n\n    async def pin_lora_async(self, lora_id: int) -> bool:\n        return await self.call_utility_async(\"pin_lora\", lora_id)\n\n    async def save_sharded_state_async(self,\n                                       path: str,\n                                       pattern: Optional[str] = None,\n                                       max_size: Optional[int] = None) -> None:\n        await self.call_utility_async(\"save_sharded_state\", path, pattern,\n                                      max_size)\n\n    async def collective_rpc_async(\n            self,\n            method: Union[str, Callable[..., _R]],\n            timeout: Optional[float] = None,\n            args: tuple = (),\n            kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return await self.call_utility_async(\"collective_rpc\", method, timeout,\n                                             args, kwargs)\n\n\nclass DPAsyncMPClient(AsyncMPClient):\n    \"\"\"Asyncio-compatible client for multi-proc, multi-engine (data parallel)\n    EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n                 log_stats: bool):\n\n        self.current_wave = 0\n        self.engines_running = False\n        self.reqs_in_flight: dict[str, CoreEngine] = {}\n\n        super().__init__(vllm_config, executor_class, log_stats)\n\n        assert len(self.core_engines) > 1\n\n    async def call_utility_async(self, method: str, *args) -> Any:\n        # Only the result from the first engine is returned.\n        return (await asyncio.gather(*[\n            self._call_utility_async(method, *args, engine=engine)\n            for engine in self.core_engines\n        ]))[0]\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        request.current_wave = self.current_wave\n\n        chosen_engine = self.get_core_engine_for_request()\n        self.reqs_in_flight[request.request_id] = chosen_engine\n        chosen_engine.num_reqs_in_flight += 1\n\n        to_await = self._send_input(EngineCoreRequestType.ADD, request,\n                                    chosen_engine)\n        if not self.engines_running:\n            # Send request to chosen engine and dp start loop\n            # control message to all other engines.\n            self.engines_running = True\n            to_await = asyncio.gather(\n                to_await,  # type: ignore[assignment]\n                *self._start_wave_coros(exclude_index=chosen_engine.index))\n\n        await to_await\n\n        self._ensure_output_queue_task()\n\n    def get_core_engine_for_request(self) -> CoreEngine:\n        return min(self.core_engines, key=lambda e: e.num_reqs_in_flight)\n\n    @staticmethod\n    async def process_engine_outputs(self: \"DPAsyncMPClient\",\n                                     outputs: EngineCoreOutputs):\n        if self.reqs_in_flight:\n            for req_id in outputs.finished_requests or ():\n                if engine := self.reqs_in_flight.pop(req_id, None):\n                    engine.num_reqs_in_flight -= 1\n\n        if outputs.wave_complete is not None:\n            # Current wave is complete, move to next wave number\n            # and mark engines as paused.\n            if self.current_wave <= outputs.wave_complete:\n                self.current_wave = outputs.wave_complete + 1\n                self.engines_running = False\n\n        elif outputs.start_wave is not None and (\n                outputs.start_wave > self.current_wave or\n            (outputs.start_wave == self.current_wave\n             and not self.engines_running)):\n            # Engine received request for a non-current wave so we must ensure\n            # that other engines progress to the next wave.\n            self.current_wave = outputs.start_wave\n            self.engines_running = True\n            await asyncio.gather(*self._start_wave_coros(\n                exclude_index=outputs.engine_index))\n\n    def _start_wave_coros(self, exclude_index: int) -> list[Awaitable[None]]:\n        logger.debug(\"Sending start DP wave %d.\", self.current_wave)\n        return [\n            self._send_input(EngineCoreRequestType.START_DP_WAVE,\n                             self.current_wave, engine)\n            for engine in self.core_engines if engine.index != exclude_index\n        ]\n\n    async def abort_requests_async(self, request_ids: list[str]) -> None:\n        if not request_ids:\n            return\n\n        if len(request_ids) == 1:\n            # Fast-path common case.\n            if engine := self.reqs_in_flight.get(request_ids[0]):\n                await self._abort_requests(request_ids, engine)\n            return\n\n        by_engine: dict[CoreEngine, list[str]] = {}\n        for req_id in request_ids:\n            if engine := self.reqs_in_flight.get(req_id):\n                by_engine.setdefault(engine, []).append(req_id)\n        for engine, req_ids in by_engine.items():\n            await self._abort_requests(req_ids, engine)\n\n    async def _abort_requests(self, request_ids: list[str],\n                              engine: CoreEngine) -> None:\n        if not self.resources.engine_dead:\n            await self._send_input(EngineCoreRequestType.ABORT, request_ids,\n                                   engine)\n", 1029], "/home/jeromeku/vllm/vllm/v1/utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport os\nimport time\nimport weakref\nfrom collections import defaultdict\nfrom collections.abc import Sequence\nfrom multiprocessing import Process, connection\nfrom typing import (TYPE_CHECKING, Callable, Generic, Optional, TypeVar, Union,\n                    overload)\n\nimport torch\n\nfrom vllm.config import VllmConfig\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.models.utils import extract_layer_index\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import get_mp_context, kill_process_tree\nfrom vllm.v1.executor.abstract import Executor\n\nif TYPE_CHECKING:\n    from vllm.attention.layer import Attention\n\nlogger = init_logger(__name__)\n\nT = TypeVar(\"T\")\n\n\nclass ConstantList(Generic[T], Sequence):\n\n    def __init__(self, x: list[T]) -> None:\n        self._x = x\n\n    def append(self, item):\n        raise Exception(\"Cannot append to a constant list\")\n\n    def extend(self, item):\n        raise Exception(\"Cannot extend a constant list\")\n\n    def insert(self, item):\n        raise Exception(\"Cannot insert into a constant list\")\n\n    def pop(self, item):\n        raise Exception(\"Cannot pop from a constant list\")\n\n    def remove(self, item):\n        raise Exception(\"Cannot remove from a constant list\")\n\n    def clear(self):\n        raise Exception(\"Cannot clear a constant list\")\n\n    def index(self,\n              item: T,\n              start: int = 0,\n              stop: Optional[int] = None) -> int:\n        return self._x.index(item, start,\n                             stop if stop is not None else len(self._x))\n\n    @overload\n    def __getitem__(self, item: int) -> T:\n        ...\n\n    @overload\n    def __getitem__(self, s: slice, /) -> list[T]:\n        ...\n\n    def __getitem__(self, item: Union[int, slice]) -> Union[T, list[T]]:\n        return self._x[item]\n\n    @overload\n    def __setitem__(self, item: int, value: T):\n        ...\n\n    @overload\n    def __setitem__(self, s: slice, value: T, /):\n        ...\n\n    def __setitem__(self, item: Union[int, slice], value: Union[T, list[T]]):\n        raise Exception(\"Cannot set item in a constant list\")\n\n    def __delitem__(self, item):\n        raise Exception(\"Cannot delete item from a constant list\")\n\n    def __iter__(self):\n        return iter(self._x)\n\n    def __contains__(self, item):\n        return item in self._x\n\n    def __len__(self):\n        return len(self._x)\n\n    def __repr__(self):\n        return f\"ConstantList({self._x})\"\n\n\nclass CoreEngineProcManager:\n    \"\"\"\n    Utility class to handle creation, readiness, and shutdown\n    of background processes used by the AsyncLLM and LLMEngine.\n    \"\"\"\n\n    def __init__(\n        self,\n        target_fn: Callable,\n        local_engine_count: int,\n        start_index: int,\n        local_start_index: int,\n        vllm_config: VllmConfig,\n        on_head_node: bool,\n        input_address: str,\n        executor_class: type[Executor],\n        log_stats: bool,\n    ):\n        context = get_mp_context()\n        common_kwargs = {\n            \"vllm_config\": vllm_config,\n            \"on_head_node\": on_head_node,\n            \"input_address\": input_address,\n            \"executor_class\": executor_class,\n            \"log_stats\": log_stats,\n        }\n\n        self.processes: list[Process] = []\n        for index in range(local_engine_count):\n            local_index = local_start_index + index\n            global_index = start_index + index\n            # Start EngineCore in background process.\n            self.processes.append(\n                context.Process(target=target_fn,\n                                name=f\"EngineCore_{global_index}\",\n                                kwargs=common_kwargs | {\n                                    \"dp_rank\": global_index,\n                                    \"local_dp_rank\": local_index,\n                                }))\n\n        self._finalizer = weakref.finalize(self, shutdown, self.processes,\n                                           input_address)\n        try:\n            for proc in self.processes:\n                proc.start()\n        finally:\n            # Kill other procs if not all are running.\n            if self.finished_procs():\n                self.close()\n\n    def close(self):\n        \"\"\"Shutdown all procs.\"\"\"\n        self._finalizer()\n\n    def join_first(self):\n        \"\"\"Wait for any process to exit.\"\"\"\n        connection.wait(proc.sentinel for proc in self.processes)\n\n    def sentinels(self) -> list:\n        return [proc.sentinel for proc in self.processes]\n\n    def finished_procs(self) -> dict[str, int]:\n        \"\"\"Returns dict of proc name -> exit code for any finished procs.\"\"\"\n        return {\n            proc.name: proc.exitcode\n            for proc in self.processes if proc.exitcode is not None\n        }\n\n\n# Note(rob): shutdown function cannot be a bound method,\n# else the gc cannot collect the objedecoupct.\ndef shutdown(procs: list[Process], input_address: str):\n    # Shutdown the process.\n    for proc in procs:\n        if proc.is_alive():\n            proc.terminate()\n\n    # Allow 5 seconds for remaining procs to terminate.\n    deadline = time.monotonic() + 5\n    for proc in procs:\n        remaining = deadline - time.monotonic()\n        if remaining <= 0:\n            break\n        if proc.is_alive():\n            proc.join(remaining)\n\n    for proc in procs:\n        if proc.is_alive() and (pid := proc.pid) is not None:\n            kill_process_tree(pid)\n\n    # Remove zmq ipc socket files.\n    if input_address.startswith(\"ipc://\"):\n        socket_file = input_address[len(\"ipc://\"):]\n        if os and os.path.exists(socket_file):\n            os.remove(socket_file)\n\n\ndef bind_kv_cache(\n    kv_caches: dict[str, torch.Tensor],\n    forward_context: dict[str, \"Attention\"],\n    runner_kv_caches: list[torch.Tensor],\n) -> None:\n    \"\"\"\n    Bind the allocated KV cache to both ModelRunner and forward context so\n    that the KV cache can be used in the forward pass.\n\n    This function:\n      1) Fills the ModelRunner's kv cache list (`runner_kv_caches`) with\n         kv_caches.\n      2) Associates each attention layer in the `forward_context` with its \n         corresponding KV cache in kv_caches.\n\n    Args:\n        kv_caches: The allocated kv_caches with layer names as keys.\n        forward_context: The global forward context containing all Attention \n        layers with layer names as keys.\n        runner_kv_caches: The kv_cache declared by ModelRunner.\n    \"\"\"\n    # Bind kv_caches to ModelRunner\n    assert len(runner_kv_caches) == 0\n\n    # Convert kv_caches dict to a list of tensors in the order of layer_index.\n    index2name = defaultdict(list)\n    for layer_name in kv_caches:\n        index2name[extract_layer_index(layer_name)].append(layer_name)\n\n    for layer_index in sorted(index2name.keys()):\n        layer_names = index2name[layer_index]\n        if len(layer_names) > 1:\n            # One typical case is encoder-decoder model, e.g., bart.\n            # The cross attention and self attention in the same decoder layer\n            # has different layer_name but the same layer_index.\n            raise NotImplementedError\n        layer_name = layer_names[0]\n        runner_kv_caches.append(kv_caches[layer_name])\n\n    # Bind kv_caches to forward context\n    for layer_name, kv_cache in kv_caches.items():\n        # NOTE: Use list because of v0 PP virtual engine.\n        forward_context[layer_name].kv_cache = [kv_cache]\n\n\ndef copy_slice(from_tensor: torch.Tensor, to_tensor: torch.Tensor,\n               length: int) -> torch.Tensor:\n    \"\"\"\n    Copy the first length elements of a tensor into another tensor in a\n    non-blocking manner.\n\n    Used to copy pinned CPU tensor data to pre-allocated GPU tensors.\n\n    Returns the sliced target tensor.\n    \"\"\"\n    return to_tensor[:length].copy_(from_tensor[:length], non_blocking=True)\n\n\ndef report_usage_stats(\n        vllm_config,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT) -> None:\n    \"\"\"Report usage statistics if enabled.\"\"\"\n\n    if not is_usage_stats_enabled():\n        return\n\n    from vllm.model_executor.model_loader import get_architecture_class_name\n\n    usage_message.report_usage(\n        get_architecture_class_name(vllm_config.model_config),\n        usage_context,\n        extra_kvs={\n            # Common configuration\n            \"dtype\":\n            str(vllm_config.model_config.dtype),\n            \"tensor_parallel_size\":\n            vllm_config.parallel_config.tensor_parallel_size,\n            \"block_size\":\n            vllm_config.cache_config.block_size,\n            \"gpu_memory_utilization\":\n            vllm_config.cache_config.gpu_memory_utilization,\n\n            # Quantization\n            \"quantization\":\n            vllm_config.model_config.quantization,\n            \"kv_cache_dtype\":\n            str(vllm_config.cache_config.cache_dtype),\n\n            # Feature flags\n            \"enable_lora\":\n            bool(vllm_config.lora_config),\n            \"enable_prompt_adapter\":\n            bool(vllm_config.prompt_adapter_config),\n            \"enable_prefix_caching\":\n            vllm_config.cache_config.enable_prefix_caching,\n            \"enforce_eager\":\n            vllm_config.model_config.enforce_eager,\n            \"disable_custom_all_reduce\":\n            vllm_config.parallel_config.disable_custom_all_reduce,\n        })\n", 294], "/home/jeromeku/vllm/vllm/multimodal/registry.py": ["# SPDX-License-Identifier: Apache-2.0\nfrom collections.abc import Mapping\nfrom dataclasses import dataclass\nfrom typing import TYPE_CHECKING, Generic, Optional, Protocol, TypeVar\n\nimport torch.nn as nn\nfrom typing_extensions import deprecated\n\nfrom vllm.envs import VLLM_MM_INPUT_CACHE_GIB\nfrom vllm.inputs import InputProcessingContext\nfrom vllm.logger import init_logger\nfrom vllm.transformers_utils.tokenizer import (AnyTokenizer,\n                                               cached_tokenizer_from_config)\nfrom vllm.utils import ClassRegistry\n\nfrom .processing import (BaseMultiModalProcessor, BaseProcessingInfo,\n                         ProcessingCache)\nfrom .profiling import (BaseDummyInputsBuilder, DummyDecoderData,\n                        DummyEncoderData, MultiModalProfiler)\n\nif TYPE_CHECKING:\n    from vllm.config import ModelConfig\n\nlogger = init_logger(__name__)\n\nN = TypeVar(\"N\", bound=type[nn.Module])\n_I = TypeVar(\"_I\", bound=BaseProcessingInfo)\n_I_co = TypeVar(\"_I_co\", bound=BaseProcessingInfo, covariant=True)\n\n\nclass ProcessingInfoFactory(Protocol[_I_co]):\n    \"\"\"Constructs a {class}`MultiModalProcessor` instance from the context.\"\"\"\n\n    def __call__(\n        self,\n        ctx: InputProcessingContext,\n    ) -> _I_co:\n        ...\n\n\nclass DummyInputsBuilderFactory(Protocol[_I]):\n    \"\"\"\n    Constructs a {class}`BaseDummyInputsBuilder` instance from the context.\n    \"\"\"\n\n    def __call__(self, info: _I) -> BaseDummyInputsBuilder[_I]:\n        ...\n\n\nclass MultiModalProcessorFactory(Protocol[_I]):\n    \"\"\"Constructs a {class}`MultiModalProcessor` instance from the context.\"\"\"\n\n    def __call__(\n        self,\n        info: _I,\n        dummy_inputs: BaseDummyInputsBuilder[_I],\n        *,\n        cache: Optional[ProcessingCache] = None,\n    ) -> BaseMultiModalProcessor[_I]:\n        ...\n\n\n@dataclass(frozen=True)\nclass _ProcessorFactories(Generic[_I]):\n    info: ProcessingInfoFactory[_I]\n    processor: MultiModalProcessorFactory[_I]\n    dummy_inputs: DummyInputsBuilderFactory[_I]\n\n    def build_processor(\n        self,\n        ctx: InputProcessingContext,\n        *,\n        cache: Optional[ProcessingCache] = None,\n    ):\n        info = self.info(ctx)\n        dummy_inputs_builder = self.dummy_inputs(info)\n        return self.processor(info, dummy_inputs_builder, cache=cache)\n\n\nclass MultiModalRegistry:\n    \"\"\"\n    A registry that dispatches data processing according to the model.\n    \"\"\"\n\n    def __init__(self) -> None:\n        self._processor_factories = ClassRegistry[nn.Module,\n                                                  _ProcessorFactories]()\n\n        self._processing_cache = ProcessingCache(VLLM_MM_INPUT_CACHE_GIB)\n\n    def reset_processor_cache(self) -> bool:\n        \"\"\"Reset the multi-modal processing cache.\"\"\"\n        self._processing_cache.reset()\n\n        return True  # Success\n\n    @deprecated(\"Legacy input processor/mapper pipeline has been removed. \"\n                \"Please update your model runner to use \"\n                \"`seq_group_metadata.multi_modal_data` directly without \"\n                \"further processing.\")\n    def create_input_mapper(self, model_config: \"ModelConfig\"):\n        return lambda data, mm_processor_kwargs: data\n\n    def get_max_tokens_per_item_by_modality(\n        self,\n        model_config: \"ModelConfig\",\n    ) -> Mapping[str, int]:\n        \"\"\"\n        Get the maximum number of tokens per data item from each modality based\n        on underlying model configuration.\n        \"\"\"\n        if not model_config.is_multimodal_model:\n            return {}\n\n        processor = self.create_processor(model_config, disable_cache=False)\n        profiler = MultiModalProfiler(processor)\n\n        seq_len = model_config.max_model_len\n        mm_limits = self.get_mm_limits_per_prompt(model_config)\n\n        return profiler.get_mm_max_tokens(\n            seq_len,\n            {\n                modality: 1\n                for modality, limit in mm_limits.items() if limit > 0\n            },\n        )\n\n    def get_max_tokens_per_item_by_nonzero_modality(\n        self,\n        model_config: \"ModelConfig\",\n    ) -> Mapping[str, int]:\n        \"\"\"\n        Get the maximum number of tokens per data item from each modality based\n        on underlying model configuration, excluding modalities that user\n        explicitly disabled via `limit_mm_per_prompt`.\n\n        Note:\n            This is currently directly used only in V1 for profiling the memory\n            usage of a model.\n        \"\"\"\n        mm_limits = self.get_mm_limits_per_prompt(model_config)\n\n        return {\n            key: max_tokens_per_mm_item\n            for key, max_tokens_per_mm_item in\n            self.get_max_tokens_per_item_by_modality(model_config).items()\n            if mm_limits[key] > 0\n        }\n\n    def get_max_tokens_by_modality(\n        self,\n        model_config: \"ModelConfig\",\n    ) -> Mapping[str, int]:\n        \"\"\"\n        Get the maximum number of tokens from each modality\n        for profiling the memory usage of a model.\n\n        See {meth}`MultiModalPlugin.get_max_multimodal_tokens` for more details.\n        \"\"\"\n        mm_limits = self.get_mm_limits_per_prompt(model_config)\n\n        return {\n            key: mm_limits[key] * max_tokens_per_mm_item\n            for key, max_tokens_per_mm_item in\n            self.get_max_tokens_per_item_by_modality(model_config).items()\n        }\n\n    def get_max_multimodal_tokens(self, model_config: \"ModelConfig\") -> int:\n        \"\"\"\n        Get the maximum number of multi-modal tokens\n        for profiling the memory usage of a model.\n\n        See {meth}`MultiModalPlugin.get_max_multimodal_tokens` for more details.\n        \"\"\"\n        return sum(self.get_max_tokens_by_modality(model_config).values())\n\n    @deprecated(\"Legacy input processor/mapper pipeline has been removed. \"\n                \"Please update your model runner to use \"\n                \"`seq_group_metadata.multi_modal_data` directly without \"\n                \"further processing.\")\n    def init_mm_limits_per_prompt(\n        self,\n        model_config: \"ModelConfig\",\n    ) -> None:\n        pass\n\n    def get_mm_limits_per_prompt(\n        self,\n        model_config: \"ModelConfig\",\n    ) -> Mapping[str, int]:\n        \"\"\"\n        Get the maximum number of multi-modal input instances for each modality\n        that are allowed per prompt for a model class.\n        \"\"\"\n        if not model_config.is_multimodal_model:\n            return {}\n\n        processor = self.create_processor(model_config, disable_cache=False)\n        profiler = MultiModalProfiler(processor)\n        return profiler.get_mm_limits()\n\n    def register_processor(\n        self,\n        processor: MultiModalProcessorFactory[_I],\n        *,\n        info: ProcessingInfoFactory[_I],\n        dummy_inputs: DummyInputsBuilderFactory[_I],\n    ):\n        \"\"\"\n        Register a multi-modal processor to a model class. The processor\n        is constructed lazily, hence a factory method should be passed.\n\n        When the model receives multi-modal data, the provided function is\n        invoked to transform the data into a dictionary of model inputs.\n\n        :::{seealso}\n        {ref}`mm-processing`\n        :::\n        \"\"\"\n\n        def wrapper(model_cls: N) -> N:\n            if self._processor_factories.contains(model_cls, strict=True):\n                logger.warning(\n                    \"Model class %s already has a multi-modal processor \"\n                    \"registered to %s. It is overwritten by the new one.\",\n                    model_cls, self)\n\n            self._processor_factories[model_cls] = _ProcessorFactories(\n                info=info,\n                dummy_inputs=dummy_inputs,\n                processor=processor,\n            )\n\n            return model_cls\n\n        return wrapper\n\n    def _get_model_cls(self, model_config: \"ModelConfig\"):\n        # Avoid circular import\n        from vllm.model_executor.model_loader import get_model_architecture\n\n        model_cls, _ = get_model_architecture(model_config)\n        return model_cls\n\n    @deprecated(\"Legacy input processor/mapper pipeline has been removed. \"\n                \"Please update your model runner to use \"\n                \"`seq_group_metadata.multi_modal_data` directly without \"\n                \"further processing.\")\n    def has_processor(self, model_config: \"ModelConfig\") -> bool:\n        return True\n\n    def create_processor(\n        self,\n        model_config: \"ModelConfig\",\n        *,\n        tokenizer: Optional[AnyTokenizer] = None,\n        disable_cache: Optional[bool] = None,\n    ) -> BaseMultiModalProcessor[BaseProcessingInfo]:\n        \"\"\"\n        Create a multi-modal processor for a specific model and tokenizer.\n\n        :::{seealso}\n        {ref}`mm-processing`\n        :::\n        \"\"\"\n        if not model_config.is_multimodal_model:\n            raise ValueError(f\"{model_config.model} is not a multimodal model\")\n\n        if tokenizer is None:\n            tokenizer = cached_tokenizer_from_config(model_config)\n        if disable_cache is None:\n            mm_config = model_config.get_multimodal_config()\n            disable_cache = mm_config.disable_mm_preprocessor_cache\n\n        model_cls = self._get_model_cls(model_config)\n        factories = self._processor_factories[model_cls]\n\n        ctx = InputProcessingContext(model_config, tokenizer)\n        cache = None if disable_cache else self._processing_cache\n\n        return factories.build_processor(ctx, cache=cache)\n\n    def get_decoder_dummy_data(\n        self,\n        model_config: \"ModelConfig\",\n        seq_len: int,\n        mm_counts: Optional[Mapping[str, int]] = None,\n    ) -> DummyDecoderData:\n        \"\"\"\n        Create dummy data for profiling the memory usage of a model.\n\n        The model is identified by ``model_config``.\n        \"\"\"\n        processor = self.create_processor(model_config, disable_cache=False)\n        profiler = MultiModalProfiler(processor)\n        dummy_data = profiler.get_decoder_dummy_data(seq_len, mm_counts)\n\n        # Having more tokens is over-conservative but otherwise fine\n        token_ids = dummy_data.prompt_token_ids\n        if len(token_ids) < seq_len:\n            raise AssertionError(\n                f\"Expected at least {seq_len} dummy tokens for profiling, \"\n                f\"but found {len(token_ids)} tokens instead.\")\n\n        return dummy_data\n\n    def get_encoder_dummy_data(\n        self,\n        model_config: \"ModelConfig\",\n        seq_len: int,\n        mm_counts: Optional[Mapping[str, int]] = None,\n    ) -> DummyEncoderData:\n        \"\"\"\n        Create dummy data for profiling the memory usage of a model.\n\n        The model is identified by ``model_config``.\n        \"\"\"\n        processor = self.create_processor(model_config, disable_cache=False)\n        profiler = MultiModalProfiler(processor)\n        dummy_data = profiler.get_encoder_dummy_data(seq_len, mm_counts)\n\n        # Having more tokens is over-conservative but otherwise fine\n        token_ids = dummy_data.prompt_token_ids\n        if len(token_ids) < seq_len:\n            logger.warning_once(\n                \"Expected at least %d dummy encoder tokens for profiling, but found %d tokens instead.\",  # noqa: E501\n                seq_len,\n                len(token_ids),\n            )\n\n        return dummy_data\n", 332], "/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom collections.abc import Mapping\nfrom copy import copy\nfrom typing import Any, Callable, Optional, Union\n\nfrom typing_extensions import TypeVar\n\nimport vllm.envs as envs\nfrom vllm.config import ParallelConfig, VllmConfig\nfrom vllm.distributed import stateless_destroy_torch_distributed_process_group\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.inputs import PromptType\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.outputs import RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.transformers_utils.tokenizer_group import (\n    TokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import Device\nfrom vllm.v1.engine.core_client import EngineCoreClient\nfrom vllm.v1.engine.output_processor import OutputProcessor\nfrom vllm.v1.engine.parallel_sampling import ParentRequest\nfrom vllm.v1.engine.processor import Processor\nfrom vllm.v1.executor.abstract import Executor\nfrom vllm.v1.metrics.loggers import StatLoggerFactory\n\nlogger = init_logger(__name__)\n\n_R = TypeVar(\"_R\", default=Any)\n\n\nclass LLMEngine:\n    \"\"\"Legacy LLMEngine for backwards compatibility.\"\"\"\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        executor_class: type[Executor],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[list[StatLoggerFactory]] = None,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n        use_cached_outputs: bool = False,\n        multiprocess_mode: bool = False,\n    ) -> None:\n        if not envs.VLLM_USE_V1:\n            raise ValueError(\n                \"Using V1 LLMEngine, but envs.VLLM_USE_V1=False. \"\n                \"This should not happen. As a workaround, try using \"\n                \"LLMEngine.from_vllm_config(...) or explicitly set \"\n                \"VLLM_USE_V1=0 or 1 and report this issue on Github.\")\n\n        if stat_loggers is not None:\n            raise NotImplementedError(\n                \"Passing StatLoggers to LLMEngine in V1 is not yet supported. \"\n                \"Set VLLM_USE_V1=0 and file and issue on Github.\")\n\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n\n        # important: init dp group before init the engine_core\n        # In the decoupled engine case this is handled in EngineCoreProc.\n        parallel_config = vllm_config.parallel_config\n        if not multiprocess_mode and parallel_config.data_parallel_size > 1:\n            self.dp_group = parallel_config.stateless_init_dp_group()\n        else:\n            self.dp_group = None\n        self.should_execute_dummy_batch = False\n\n        # Tokenizer (+ ensure liveness if running in another process).\n        self.tokenizer = init_tokenizer_from_configs(\n            model_config=vllm_config.model_config,\n            scheduler_config=vllm_config.scheduler_config,\n            lora_config=vllm_config.lora_config)\n\n        # Processor (convert Inputs --> EngineCoreRequests)\n        self.processor = Processor(vllm_config=vllm_config,\n                                   tokenizer=self.tokenizer,\n                                   mm_registry=mm_registry)\n\n        # OutputProcessor (convert EngineCoreOutputs --> RequestOutput).\n        self.output_processor = OutputProcessor(self.tokenizer,\n                                                log_stats=False)\n\n        # EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\n        self.engine_core = EngineCoreClient.make_client(\n            multiprocess_mode=multiprocess_mode,\n            asyncio_mode=False,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=False,  # FIXME: implement\n        )\n\n        if not multiprocess_mode:\n            # for v0 compatibility\n            self.model_executor = self.engine_core.engine_core.model_executor  # type: ignore\n\n        # Don't keep the dummy data in memory\n        self.reset_mm_cache()\n\n    @classmethod\n    def from_vllm_config(\n        cls,\n        vllm_config: VllmConfig,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[list[StatLoggerFactory]] = None,\n        disable_log_stats: bool = False,\n    ) -> \"LLMEngine\":\n        return cls(vllm_config=vllm_config,\n                   executor_class=Executor.get_class(vllm_config),\n                   log_stats=(not disable_log_stats),\n                   usage_context=usage_context,\n                   stat_loggers=stat_loggers,\n                   multiprocess_mode=envs.VLLM_ENABLE_V1_MULTIPROCESSING)\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[list[StatLoggerFactory]] = None,\n        enable_multiprocessing: bool = False,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n\n        # Create the engine configs.\n        vllm_config = engine_args.create_engine_config(usage_context)\n        executor_class = Executor.get_class(vllm_config)\n\n        if envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n            logger.debug(\"Enabling multiprocessing for LLMEngine.\")\n            enable_multiprocessing = True\n\n        # Create the LLMEngine.\n        return cls(vllm_config=vllm_config,\n                   executor_class=executor_class,\n                   log_stats=not engine_args.disable_log_stats,\n                   usage_context=usage_context,\n                   stat_loggers=stat_loggers,\n                   multiprocess_mode=enable_multiprocessing)\n\n    def get_num_unfinished_requests(self) -> int:\n        return self.output_processor.get_num_unfinished_requests()\n\n    def has_unfinished_requests(self) -> bool:\n        has_unfinished = self.output_processor.has_unfinished_requests()\n        if self.dp_group is None:\n            return has_unfinished\n        return self.has_unfinished_requests_dp(has_unfinished)\n\n    def has_unfinished_requests_dp(self, has_unfinished: bool) -> bool:\n        aggregated_has_unfinished = ParallelConfig.has_unfinished_dp(\n            self.dp_group, has_unfinished)\n        if not has_unfinished and aggregated_has_unfinished:\n            self.should_execute_dummy_batch = True\n        return aggregated_has_unfinished\n\n    @classmethod\n    def validate_outputs(cls, outputs, output_type):\n        return outputs\n\n    def abort_request(self, request_ids: list[str]) -> None:\n        \"\"\"Remove request_ids from EngineCore and Detokenizer.\"\"\"\n\n        request_ids = self.output_processor.abort_requests(request_ids)\n        self.engine_core.abort_requests(request_ids)\n\n    def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        # Process raw inputs into the request.\n        prompt_str, request = self.processor.process_inputs(\n            request_id, prompt, params, arrival_time, lora_request,\n            tokenization_kwargs, trace_headers, prompt_adapter_request,\n            priority)\n\n        n = params.n if isinstance(params, SamplingParams) else 1\n\n        if n == 1:\n            # Make a new RequestState and queue.\n            self.output_processor.add_request(request, prompt_str, None, 0)\n            # Add the request to EngineCore.\n            self.engine_core.add_request(request)\n            return\n\n        # Fan out child requests (for n>1).\n        parent_req = ParentRequest(request_id, params)\n        for idx in range(n):\n            request_id, params = parent_req.get_child_info(idx)\n            child_request = request if idx == n - 1 else copy(request)\n            child_request.request_id = request_id\n            child_request.sampling_params = params\n\n            # Make a new RequestState and queue.\n            self.output_processor.add_request(child_request, prompt_str,\n                                              parent_req, idx)\n            # Add the request to EngineCore.\n            self.engine_core.add_request(child_request)\n\n    def step(self) -> list[RequestOutput]:\n\n        if self.should_execute_dummy_batch:\n            self.should_execute_dummy_batch = False\n            self.engine_core.execute_dummy_batch()\n            return []\n\n        # 1) Get EngineCoreOutput from the EngineCore.\n        outputs = self.engine_core.get_output()\n\n        # 2) Process EngineCoreOutputs.\n        processed_outputs = self.output_processor.process_outputs(\n            outputs.outputs)\n\n        # 3) Abort any reqs that finished due to stop strings.\n        self.engine_core.abort_requests(processed_outputs.reqs_to_abort)\n\n        return processed_outputs.request_outputs\n\n    def get_vllm_config(self):\n        return self.vllm_config\n\n    def get_model_config(self):\n        return self.model_config\n\n    def start_profile(self):\n        self.engine_core.profile(True)\n\n    def stop_profile(self):\n        self.engine_core.profile(False)\n\n    def reset_mm_cache(self):\n        self.processor.mm_registry.reset_processor_cache()\n        self.processor.mm_input_cache_client.reset()\n        self.engine_core.reset_mm_cache()\n\n    def reset_prefix_cache(self, device: Optional[Device] = None):\n        self.engine_core.reset_prefix_cache()\n\n    def sleep(self, level: int = 1):\n        self.engine_core.sleep(level)\n\n    def wake_up(self, tags: Optional[list[str]] = None):\n        self.engine_core.wake_up(tags)\n\n    def is_sleeping(self) -> bool:\n        return self.engine_core.is_sleeping()\n\n    def get_tokenizer_group(self) -> TokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(\"Unable to get tokenizer because \"\n                             \"skip_tokenizer_init is True\")\n\n        return self.tokenizer\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        \"\"\"Load a new LoRA adapter into the engine for future requests.\"\"\"\n        return self.engine_core.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        \"\"\"Remove an already loaded LoRA adapter.\"\"\"\n        return self.engine_core.remove_lora(lora_id)\n\n    def list_loras(self) -> set[int]:\n        \"\"\"List all registered adapters.\"\"\"\n        return self.engine_core.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        \"\"\"Prevent an adapter from being evicted.\"\"\"\n        return self.engine_core.pin_lora(lora_id)\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.engine_core.collective_rpc(method, timeout, args, kwargs)\n\n    def __del__(self):\n        if dp_group := getattr(self, \"dp_group\", None):\n            stateless_destroy_torch_distributed_process_group(dp_group)\n", 295], "/home/jeromeku/vllm/vllm/engine/llm_engine.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport copy\nimport time\nfrom collections import Counter as collectionsCounter\nfrom collections import deque\nfrom contextlib import contextmanager\nfrom dataclasses import dataclass\nfrom functools import partial\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Deque, Dict,\n                    Iterable, List, Literal, Mapping, NamedTuple, Optional)\nfrom typing import Sequence as GenericSequence\nfrom typing import Set, Type, Union, cast, overload\n\nimport torch\nfrom typing_extensions import TypeVar, deprecated\n\nimport vllm.envs as envs\nfrom vllm.config import (DecodingConfig, LoRAConfig, ModelConfig,\n                         ObservabilityConfig, ParallelConfig, SchedulerConfig,\n                         VllmConfig)\nfrom vllm.core.scheduler import ScheduledSequenceGroup, SchedulerOutputs\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.engine.metrics_types import StatLoggerBase, Stats\nfrom vllm.engine.output_processor.interfaces import (\n    SequenceGroupOutputProcessor)\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.engine.output_processor.util import create_output_by_sequence_group\nfrom vllm.entrypoints.openai.logits_processors import (\n    get_logits_processors as get_openai_logits_processors)\nfrom vllm.executor.executor_base import ExecutorBase\nfrom vllm.inputs import ProcessorInputs, PromptType, SingletonInputs\nfrom vllm.inputs.parse import split_enc_dec_inputs\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.logger import init_logger\nfrom vllm.logits_process import get_bad_words_logits_processors\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.guided_decoding import (\n    get_local_guided_decoding_logits_processor)\nfrom vllm.model_executor.layers.sampler import SamplerOutput\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.multimodal.processing import EncDecMultiModalProcessor\nfrom vllm.outputs import (PoolingRequestOutput, RequestOutput,\n                          RequestOutputFactory)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import RequestOutputKind, SamplingParams\nfrom vllm.sequence import (ExecuteModelRequest, ParallelSampleSequenceGroup,\n                           PoolingSequenceGroupOutput, Sequence, SequenceGroup,\n                           SequenceGroupBase, SequenceGroupMetadata,\n                           SequenceGroupOutput, SequenceStatus)\nfrom vllm.tracing import (SpanAttributes, SpanKind, extract_trace_context,\n                          init_tracer)\nfrom vllm.transformers_utils.detokenizer import Detokenizer\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import (\n    TokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import (UsageContext, is_usage_stats_enabled,\n                                  usage_message)\nfrom vllm.utils import (Counter, Device, deprecate_kwargs,\n                        resolve_obj_by_qualname, weak_bind)\nfrom vllm.version import __version__ as VLLM_VERSION\nfrom vllm.worker.model_runner_base import InputProcessingError\n\nlogger = init_logger(__name__)\n_LOCAL_LOGGING_INTERVAL_SEC = 5\n\n_O = TypeVar(\"_O\", RequestOutput, PoolingRequestOutput)\n_R = TypeVar(\"_R\", default=Any)\n\n\n@dataclass\nclass SchedulerOutputState:\n    \"\"\"Caches the scheduler outputs for a virtual engine. Used for Multi-Step\"\"\"\n    seq_group_metadata_list: Optional[List[SequenceGroupMetadata]] = None\n    scheduler_outputs: Optional[SchedulerOutputs] = None\n    allow_async_output_proc: bool = False\n    last_output: Optional[SamplerOutput] = None\n\n\nclass OutputData(NamedTuple):\n    outputs: List[SamplerOutput]\n    seq_group_metadata_list: List[SequenceGroupMetadata]\n    scheduler_outputs: SchedulerOutputs\n    is_async: bool\n    is_last_step: bool\n    # Indicates if this output is from the first step of the\n    # multi-step. When multi-step is disabled, this is always\n    # set to True.\n    # is_first_step_output is invalid when `outputs` has\n    # outputs from multiple steps.\n    is_first_step_output: Optional[bool]\n    skip: List[int]\n\n\nclass SchedulerContext:\n\n    def __init__(self, multi_step_stream_outputs: bool = False):\n        self.output_queue: Deque[OutputData] = deque()\n        self.request_outputs: List[Union[RequestOutput,\n                                         PoolingRequestOutput]] = []\n        self.seq_group_metadata_list: Optional[\n            List[SequenceGroupMetadata]] = None\n        self.scheduler_outputs: Optional[SchedulerOutputs] = None\n\n        self.multi_step_stream_outputs: bool = multi_step_stream_outputs\n\n    def append_output(self, outputs: List[SamplerOutput],\n                      seq_group_metadata_list: List[SequenceGroupMetadata],\n                      scheduler_outputs: SchedulerOutputs, is_async: bool,\n                      is_last_step: bool,\n                      is_first_step_output: Optional[bool]):\n        self.output_queue.append(\n            OutputData(outputs=outputs,\n                       seq_group_metadata_list=seq_group_metadata_list,\n                       scheduler_outputs=scheduler_outputs,\n                       is_async=is_async,\n                       is_last_step=is_last_step,\n                       is_first_step_output=is_first_step_output,\n                       skip=[]))\n\n\nclass LLMEngine:\n    \"\"\"An LLM engine that receives requests and generates texts.\n\n    This is the main class for the vLLM engine. It receives requests\n    from clients and generates texts from the LLM. It includes a tokenizer, a\n    language model (possibly distributed across multiple GPUs), and GPU memory\n    space allocated for intermediate states (aka KV cache). This class utilizes\n    iteration-level scheduling and efficient memory management to maximize the\n    serving throughput.\n\n    The {class}`~vllm.LLM` class wraps this class for offline batched inference\n    and the {class}`AsyncLLMEngine` class wraps this class for online serving.\n\n    The config arguments are derived from {class}`~vllm.EngineArgs`. (See\n    {ref}`engine-args`)\n\n    Args:\n        model_config: The configuration related to the LLM model.\n        cache_config: The configuration related to the KV cache memory\n            management.\n        parallel_config: The configuration related to distributed execution.\n        scheduler_config: The configuration related to the request scheduler.\n        device_config: The configuration related to the device.\n        lora_config (Optional): The configuration related to serving multi-LoRA.\n        speculative_config (Optional): The configuration related to speculative\n            decoding.\n        executor_class: The model executor class for managing distributed\n            execution.\n        prompt_adapter_config (Optional): The configuration related to serving\n            prompt adapters.\n        log_stats: Whether to log statistics.\n        usage_context: Specified entry point, used for usage info collection.\n    \"\"\"\n\n    DO_VALIDATE_OUTPUT: ClassVar[bool] = False\n    \"\"\"A flag to toggle whether to validate the type of request output.\"\"\"\n\n    @classmethod\n    @contextmanager\n    def enable_output_validation(cls):\n        cls.DO_VALIDATE_OUTPUT = True\n\n        yield\n\n        cls.DO_VALIDATE_OUTPUT = False\n\n    @classmethod\n    def validate_output(\n        cls,\n        output: object,\n        output_type: Type[_O],\n    ) -> _O:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        if ((TYPE_CHECKING or do_validate)\n                and not isinstance(output, output_type)):\n            raise TypeError(f\"Expected output of type {output_type}, \"\n                            f\"but found type {type(output)}\")\n\n        return cast(_O, output)\n\n    @classmethod\n    def validate_outputs(\n        cls,\n        outputs: GenericSequence[object],\n        output_type: Type[_O],\n    ) -> List[_O]:\n        do_validate = cls.DO_VALIDATE_OUTPUT\n\n        outputs_: List[_O]\n        if TYPE_CHECKING or do_validate:\n            outputs_ = []\n            for output in outputs:\n                if not isinstance(output, output_type):\n                    raise TypeError(f\"Expected output of type {output_type}, \"\n                                    f\"but found type {type(output)}\")\n\n                outputs_.append(output)\n        else:\n            outputs_ = outputs\n\n        return outputs_\n\n    tokenizer: Optional[TokenizerGroup]\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        executor_class: Type[ExecutorBase],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n        use_cached_outputs: bool = False,\n    ) -> None:\n        if envs.VLLM_USE_V1:\n            raise ValueError(\n                \"Using V0 LLMEngine, but envs.VLLM_USE_V1=True. \"\n                \"This should not happen. As a workaround, try using \"\n                \"LLMEngine.from_vllm_config(...) or explicitly set \"\n                \"VLLM_USE_V1=0 or 1 and report this issue on Github.\")\n\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.parallel_config = vllm_config.parallel_config\n        self.scheduler_config = vllm_config.scheduler_config\n        self.device_config = vllm_config.device_config\n        self.speculative_config = vllm_config.speculative_config  # noqa\n        self.load_config = vllm_config.load_config\n        self.decoding_config = vllm_config.decoding_config or DecodingConfig(  # noqa\n        )\n        self.prompt_adapter_config = vllm_config.prompt_adapter_config  # noqa\n        self.observability_config = vllm_config.observability_config or ObservabilityConfig(  # noqa\n        )\n\n        logger.info(\n            \"Initializing a V0 LLM engine (v%s) with config: %s, \"\n            \"use_cached_outputs=%s, \",\n            VLLM_VERSION,\n            vllm_config,\n            use_cached_outputs,\n        )\n\n        self.log_stats = log_stats\n        self.use_cached_outputs = use_cached_outputs\n\n        if not self.model_config.skip_tokenizer_init:\n            self.tokenizer = self._init_tokenizer()\n            self.detokenizer = Detokenizer(self.tokenizer)\n            tokenizer_group = self.get_tokenizer_group()\n        else:\n            self.tokenizer = None\n            self.detokenizer = None\n            tokenizer_group = None\n\n        # Ensure that the function doesn't contain a reference to self,\n        # to avoid engine GC issues\n        def get_tokenizer_for_seq(sequence: Sequence) -> AnyTokenizer:\n            assert tokenizer_group, (\"tokenizer_group cannot be None, \"\n                                     \"make sure skip_tokenizer_init is False\")\n            return tokenizer_group.get_lora_tokenizer(sequence.lora_request)\n\n        self.seq_counter = Counter()\n        self.generation_config_fields = (\n            self.model_config.try_get_generation_config())\n\n        self.input_preprocessor = InputPreprocessor(self.model_config,\n                                                    self.tokenizer,\n                                                    mm_registry)\n\n        self.model_executor = executor_class(vllm_config=vllm_config)\n\n        if self.model_config.runner_type != \"pooling\":\n            self._initialize_kv_caches()\n\n        # If usage stat is enabled, collect relevant info.\n        if is_usage_stats_enabled():\n            from vllm.model_executor.model_loader import (\n                get_architecture_class_name)\n            usage_message.report_usage(\n                get_architecture_class_name(self.model_config),\n                usage_context,\n                extra_kvs={\n                    # Common configuration\n                    \"dtype\":\n                    str(self.model_config.dtype),\n                    \"tensor_parallel_size\":\n                    self.parallel_config.tensor_parallel_size,\n                    \"block_size\":\n                    self.cache_config.block_size,\n                    \"gpu_memory_utilization\":\n                    self.cache_config.gpu_memory_utilization,\n\n                    # Quantization\n                    \"quantization\":\n                    self.model_config.quantization,\n                    \"kv_cache_dtype\":\n                    str(self.cache_config.cache_dtype),\n\n                    # Feature flags\n                    \"enable_lora\":\n                    bool(self.lora_config),\n                    \"enable_prompt_adapter\":\n                    bool(self.prompt_adapter_config),\n                    \"enable_prefix_caching\":\n                    self.cache_config.enable_prefix_caching,\n                    \"enforce_eager\":\n                    self.model_config.enforce_eager,\n                    \"disable_custom_all_reduce\":\n                    self.parallel_config.disable_custom_all_reduce,\n                })\n\n        self.cached_scheduler_outputs = [\n            SchedulerOutputState()\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        self.scheduler_contexts = [\n            SchedulerContext(multi_step_stream_outputs=self.scheduler_config.\n                             multi_step_stream_outputs)\n            for _ in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        if self.model_config.use_async_output_proc:\n            process_model_outputs = weak_bind(self._process_model_outputs)\n\n            self.async_callbacks = [\n                partial(process_model_outputs,\n                        ctx=self.scheduler_contexts[v_id])\n                for v_id in range(self.parallel_config.pipeline_parallel_size)\n            ]\n        else:\n            self.async_callbacks = []\n\n        # Currently used by AsyncLLMEngine to ensure quick append\n        # of request outputs to asyncio queues\n        self.process_request_outputs_callback: Optional[Callable] = None\n\n        # Create the scheduler.\n        # NOTE: the cache_config here have been updated with the numbers of\n        # GPU and CPU blocks, which are profiled in the distributed executor.\n        if isinstance(self.vllm_config.scheduler_config.scheduler_cls, str):\n            Scheduler = resolve_obj_by_qualname(\n                self.vllm_config.scheduler_config.scheduler_cls)\n        else:\n            Scheduler = self.vllm_config.scheduler_config.scheduler_cls\n        self.scheduler = [\n            Scheduler(\n                self.scheduler_config, self.cache_config, self.lora_config,\n                self.parallel_config.pipeline_parallel_size,\n                self.async_callbacks[v_id]\n                if self.model_config.use_async_output_proc else None)\n            for v_id in range(self.parallel_config.pipeline_parallel_size)\n        ]\n\n        # Metric Logging.\n        if self.log_stats:\n            if stat_loggers is not None:\n                self.stat_loggers = stat_loggers\n            else:\n                # Lazy import for prometheus multiprocessing.\n                # We need to set PROMETHEUS_MULTIPROC_DIR environment variable\n                # before prometheus_client is imported.\n                # See https://prometheus.github.io/client_python/multiprocess/\n                from vllm.engine.metrics import (LoggingStatLogger,\n                                                 PrometheusStatLogger)\n\n                self.stat_loggers = {\n                    \"logging\":\n                    LoggingStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        vllm_config=vllm_config),\n                    \"prometheus\":\n                    PrometheusStatLogger(\n                        local_interval=_LOCAL_LOGGING_INTERVAL_SEC,\n                        labels=dict(\n                            model_name=self.model_config.served_model_name),\n                        vllm_config=vllm_config),\n                }\n                self.stat_loggers[\"prometheus\"].info(\"cache_config\",\n                                                     self.cache_config)\n\n        self.tracer = None\n        if self.observability_config.otlp_traces_endpoint:\n            self.tracer = init_tracer(\n                \"vllm.llm_engine\",\n                self.observability_config.otlp_traces_endpoint)\n\n        # Create sequence output processor, e.g. for beam search or\n        # speculative decoding.\n        self.output_processor = (\n            SequenceGroupOutputProcessor.create_output_processor(\n                self.scheduler_config,\n                self.detokenizer,\n                self.scheduler,\n                self.seq_counter,\n                get_tokenizer_for_seq,\n                stop_checker=StopChecker(self.scheduler_config.max_model_len,\n                                         get_tokenizer_for_seq),\n            ))\n\n        self.seq_id_to_seq_group: Dict[str, SequenceGroupBase] = {}\n\n        # Flag to set when an input fails to process and the engine should run\n        # the next step without re-scheduling.\n        self._skip_scheduling_next_step = False\n\n        # Don't keep the dummy data in memory\n        self.reset_mm_cache()\n\n    def _initialize_kv_caches(self) -> None:\n        \"\"\"Initialize the KV cache in the worker(s).\n\n        The workers will determine the number of blocks in both the GPU cache\n        and the swap CPU cache.\n        \"\"\"\n        start = time.time()\n        num_gpu_blocks, num_cpu_blocks = (\n            self.model_executor.determine_num_available_blocks())\n\n        if self.cache_config.num_gpu_blocks_override is not None:\n            num_gpu_blocks_override = self.cache_config.num_gpu_blocks_override\n            logger.info(\n                \"Overriding num_gpu_blocks=%d with \"\n                \"num_gpu_blocks_override=%d\", num_gpu_blocks,\n                num_gpu_blocks_override)\n            num_gpu_blocks = num_gpu_blocks_override\n\n        self.cache_config.num_gpu_blocks = num_gpu_blocks\n        self.cache_config.num_cpu_blocks = num_cpu_blocks\n\n        self.model_executor.initialize_cache(num_gpu_blocks, num_cpu_blocks)\n        elapsed = time.time() - start\n        logger.info((\"init engine (profile, create kv cache, \"\n                     \"warmup model) took %.2f seconds\"), elapsed)\n\n    @classmethod\n    def _get_executor_cls(cls,\n                          engine_config: VllmConfig) -> Type[ExecutorBase]:\n        # distributed_executor_backend must be set in VllmConfig.__post_init__\n        distributed_executor_backend = (\n            engine_config.parallel_config.distributed_executor_backend)\n        # Initialize the cluster and specify the executor class.\n        if isinstance(distributed_executor_backend, type):\n            if not issubclass(distributed_executor_backend, ExecutorBase):\n                raise TypeError(\n                    \"distributed_executor_backend must be a subclass of \"\n                    f\"ExecutorBase. Got {distributed_executor_backend}.\")\n            executor_class = distributed_executor_backend\n        elif distributed_executor_backend == \"ray\":\n            from vllm.executor.ray_distributed_executor import (\n                RayDistributedExecutor)\n            executor_class = RayDistributedExecutor\n        elif distributed_executor_backend == \"mp\":\n            from vllm.executor.mp_distributed_executor import (\n                MultiprocessingDistributedExecutor)\n            assert not envs.VLLM_USE_RAY_SPMD_WORKER, (\n                \"multiprocessing distributed executor backend does not \"\n                \"support VLLM_USE_RAY_SPMD_WORKER=1\")\n            executor_class = MultiprocessingDistributedExecutor\n        elif distributed_executor_backend == \"uni\":\n            # JAX-style, single-process, multi-device executor.\n            from vllm.executor.uniproc_executor import UniProcExecutor\n            executor_class = UniProcExecutor\n        elif distributed_executor_backend == \"external_launcher\":\n            # executor with external launcher\n            from vllm.executor.uniproc_executor import (  # noqa\n                ExecutorWithExternalLauncher)\n            executor_class = ExecutorWithExternalLauncher\n        else:\n            raise ValueError(\"unrecognized distributed_executor_backend: \"\n                             f\"{distributed_executor_backend}\")\n        return executor_class\n\n    @classmethod\n    def from_vllm_config(\n        cls,\n        vllm_config: VllmConfig,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n        disable_log_stats: bool = False,\n    ) -> \"LLMEngine\":\n        return cls(\n            vllm_config=vllm_config,\n            executor_class=cls._get_executor_cls(vllm_config),\n            log_stats=(not disable_log_stats),\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n        )\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[Dict[str, StatLoggerBase]] = None,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n        # Create the engine configs.\n        vllm_config = engine_args.create_engine_config(usage_context)\n\n        engine_cls = cls\n        if envs.VLLM_USE_V1:\n            from vllm.v1.engine.llm_engine import LLMEngine as V1LLMEngine\n            engine_cls = V1LLMEngine\n\n        return engine_cls.from_vllm_config(\n            vllm_config=vllm_config,\n            usage_context=usage_context,\n            stat_loggers=stat_loggers,\n            disable_log_stats=engine_args.disable_log_stats,\n        )\n\n    def __reduce__(self):\n        # This is to ensure that the LLMEngine is not referenced in\n        # the closure used to initialize Ray worker actors\n        raise RuntimeError(\"LLMEngine should not be pickled!\")\n\n    def __del__(self):\n        # Shutdown model executor when engine is garbage collected\n        # Use getattr since __init__ can fail before the field is set\n        if model_executor := getattr(self, \"model_executor\", None):\n            model_executor.shutdown()\n\n    def get_tokenizer_group(self) -> TokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(\"Unable to get tokenizer because \"\n                             \"skip_tokenizer_init is True\")\n\n        return self.tokenizer\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.get_tokenizer_group().get_lora_tokenizer(lora_request)\n\n    def _init_tokenizer(self) -> TokenizerGroup:\n        return init_tokenizer_from_configs(\n            model_config=self.model_config,\n            scheduler_config=self.scheduler_config,\n            lora_config=self.lora_config)\n\n    def _verify_args(self) -> None:\n        self.model_config.verify_with_parallel_config(self.parallel_config)\n        self.cache_config.verify_with_parallel_config(self.parallel_config)\n        if self.lora_config:\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_with_scheduler_config(\n                self.scheduler_config)\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n    def _add_processed_request(\n        self,\n        request_id: str,\n        processed_inputs: ProcessorInputs,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        priority: int = 0,\n    ) -> Optional[SequenceGroup]:\n        \"\"\"Add a processed request to the engine's request pool.\n        return the created sequence group.\n        \"\"\"\n        if isinstance(params, SamplingParams) and params.n > 1:\n            ParallelSampleSequenceGroup.add_request(\n                request_id,\n                self,\n                params,\n                processed_inputs=processed_inputs,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                priority=priority,\n            )\n            return None\n\n        self._validate_model_inputs(processed_inputs, lora_request)\n        # Create the sequences.\n        block_size = self.cache_config.block_size\n        seq_id = next(self.seq_counter)\n        eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)\n\n        encoder_inputs, decoder_inputs = split_enc_dec_inputs(processed_inputs)\n\n        seq = Sequence(seq_id, decoder_inputs, block_size, eos_token_id,\n                       lora_request, prompt_adapter_request)\n\n        encoder_seq = (None if encoder_inputs is None else Sequence(\n            seq_id, encoder_inputs, block_size, eos_token_id, lora_request,\n            prompt_adapter_request))\n\n        # Create a SequenceGroup based on SamplingParams or PoolingParams\n        if isinstance(params, SamplingParams):\n            seq_group = self._create_sequence_group_with_sampling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                trace_headers=trace_headers,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq,\n                priority=priority)\n        elif isinstance(params, PoolingParams):\n            seq_group = self._create_sequence_group_with_pooling(\n                request_id,\n                seq,\n                params,\n                arrival_time=arrival_time,\n                lora_request=lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                encoder_seq=encoder_seq,\n                priority=priority)\n        else:\n            raise ValueError(\n                \"Either SamplingParams or PoolingParams must be provided.\")\n\n        # Add the sequence group to the scheduler with least unfinished seqs.\n        costs = [\n            scheduler.get_num_unfinished_seq_groups()\n            for scheduler in self.scheduler\n        ]\n        min_cost_scheduler = self.scheduler[costs.index(min(costs))]\n        min_cost_scheduler.add_seq_group(seq_group)\n\n        return seq_group\n\n    def stop_remote_worker_execution_loop(self) -> None:\n        self.model_executor.stop_remote_worker_execution_loop()\n\n    @overload\n    def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @overload\n    @deprecated(\"'inputs' will be renamed to 'prompt\")\n    def add_request(\n        self,\n        request_id: str,\n        *,\n        inputs: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        ...\n\n    @deprecate_kwargs(\n        \"inputs\",\n        additional_message=\"Please use the 'prompt' parameter instead.\",\n    )\n    def add_request(\n            self,\n            request_id: str,\n            prompt: Optional[PromptType] = None,\n            params: Optional[Union[SamplingParams, PoolingParams]] = None,\n            arrival_time: Optional[float] = None,\n            lora_request: Optional[LoRARequest] = None,\n            tokenization_kwargs: Optional[dict[str, Any]] = None,\n            trace_headers: Optional[Mapping[str, str]] = None,\n            prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n            priority: int = 0,\n            *,\n            inputs: Optional[PromptType] = None,  # DEPRECATED\n    ) -> None:\n        \"\"\"Add a request to the engine's request pool.\n\n        The request is added to the request pool and will be processed by the\n        scheduler as `engine.step()` is called. The exact scheduling policy is\n        determined by the scheduler.\n\n        Args:\n            request_id: The unique ID of the request.\n            prompt: The prompt to the LLM. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each input.\n            params: Parameters for sampling or pooling.\n                {class}`~vllm.SamplingParams` for text generation.\n                {class}`~vllm.PoolingParams` for pooling.\n            arrival_time: The arrival time of the request. If None, we use\n                the current monotonic time.\n            lora_request: The LoRA request to add.\n            trace_headers: OpenTelemetry trace headers.\n            prompt_adapter_request: The prompt adapter request to add.\n            priority: The priority of the request.\n                Only applicable with priority scheduling.\n\n        Details:\n            - Set arrival_time to the current time if it is None.\n            - Set prompt_token_ids to the encoded prompt if it is None.\n            - Create `n` number of {class}`~vllm.Sequence` objects.\n            - Create a {class}`~vllm.SequenceGroup` object\n              from the list of {class}`~vllm.Sequence`.\n            - Add the {class}`~vllm.SequenceGroup` object to the scheduler.\n\n        Example:\n            >>> # initialize engine\n            >>> engine = LLMEngine.from_engine_args(engine_args)\n            >>> # set request arguments\n            >>> example_prompt = \"Who is the president of the United States?\"\n            >>> sampling_params = SamplingParams(temperature=0.0)\n            >>> request_id = 0\n            >>>\n            >>> # add the request to the engine\n            >>> engine.add_request(\n            >>>    str(request_id),\n            >>>    example_prompt,\n            >>>    SamplingParams(temperature=0.0))\n            >>> # continue the request processing\n            >>> ...\n        \"\"\"\n        if inputs is not None:\n            prompt = inputs\n        assert prompt is not None and params is not None\n\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n\n        if priority != 0 and not self.scheduler_config.policy == \"priority\":\n            raise ValueError(f\"Got priority {priority} but \"\n                             \"Priority scheduling is not enabled.\")\n\n        if isinstance(params, SamplingParams) \\\n            and (params.guided_decoding or params.logits_processors) \\\n            and self.scheduler_config.num_scheduler_steps > 1:\n            raise ValueError(\n                \"Guided decoding and logits processors are not supported \"\n                \"in multi-step decoding\")\n\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        if (isinstance(prompt, dict)\n                and prompt.get(\"prompt_embeds\", None) is not None\n                and not prompt.get(\"prompt_token_ids\", None)):\n            seq_len = prompt[\"prompt_embeds\"].shape[0]\n            prompt[\"prompt_token_ids\"] = [0] * seq_len\n\n        processed_inputs = self.input_preprocessor.preprocess(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        self._add_processed_request(\n            request_id=request_id,\n            processed_inputs=processed_inputs,\n            params=params,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            trace_headers=trace_headers,\n            priority=priority,\n        )\n\n    def _create_sequence_group_with_sampling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        sampling_params: SamplingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        encoder_seq: Optional[Sequence] = None,\n        priority: int = 0,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with SamplingParams.\"\"\"\n        max_logprobs = self.get_model_config().max_logprobs\n        if (sampling_params.logprobs\n                and sampling_params.logprobs > max_logprobs) or (\n                    sampling_params.prompt_logprobs\n                    and sampling_params.prompt_logprobs > max_logprobs):\n            raise ValueError(f\"Cannot request more than \"\n                             f\"{max_logprobs} logprobs.\")\n\n        sampling_params = self._build_logits_processors(\n            sampling_params, lora_request)\n\n        # Defensive copy of SamplingParams, which are used by the sampler,\n        # this doesn't deep-copy LogitsProcessor objects\n        sampling_params = sampling_params.clone()\n\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields, seq.eos_token_id)\n\n        # Create the sequence group.\n        draft_size = 1\n        if self.vllm_config.speculative_config is not None:\n            draft_size = \\\n                self.vllm_config.speculative_config.num_speculative_tokens + 1\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            sampling_params=sampling_params,\n            lora_request=lora_request,\n            trace_headers=trace_headers,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq,\n            priority=priority,\n            draft_size=draft_size)\n\n        return seq_group\n\n    def _create_sequence_group_with_pooling(\n        self,\n        request_id: str,\n        seq: Sequence,\n        pooling_params: PoolingParams,\n        arrival_time: float,\n        lora_request: Optional[LoRARequest],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        encoder_seq: Optional[Sequence] = None,\n        priority: int = 0,\n    ) -> SequenceGroup:\n        \"\"\"Creates a SequenceGroup with PoolingParams.\"\"\"\n        # Defensive copy of PoolingParams, which are used by the pooler\n        pooling_params = pooling_params.clone()\n        # Create the sequence group.\n        seq_group = SequenceGroup(\n            request_id=request_id,\n            seqs=[seq],\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            pooling_params=pooling_params,\n            prompt_adapter_request=prompt_adapter_request,\n            encoder_seq=encoder_seq,\n            priority=priority)\n        return seq_group\n\n    def abort_request(self, request_id: Union[str, Iterable[str]]) -> None:\n        \"\"\"Aborts a request(s) with the given ID.\n\n        Args:\n            request_id: The ID(s) of the request to abort.\n\n        Details:\n            - Refer to the\n              {meth}`~vllm.core.scheduler.Scheduler.abort_seq_group`\n              from class {class}`~vllm.core.scheduler.Scheduler`.\n\n        Example:\n            >>> # initialize engine and add a request with request_id\n            >>> request_id = str(0)\n            >>> # abort the request\n            >>> engine.abort_request(request_id)\n        \"\"\"\n        for scheduler in self.scheduler:\n            scheduler.abort_seq_group(\n                request_id, seq_id_to_seq_group=self.seq_id_to_seq_group)\n\n    def get_vllm_config(self) -> VllmConfig:\n        \"\"\"Gets the vllm configuration.\"\"\"\n        return self.vllm_config\n\n    def get_model_config(self) -> ModelConfig:\n        \"\"\"Gets the model configuration.\"\"\"\n        return self.model_config\n\n    def get_parallel_config(self) -> ParallelConfig:\n        \"\"\"Gets the parallel configuration.\"\"\"\n        return self.parallel_config\n\n    def get_decoding_config(self) -> DecodingConfig:\n        \"\"\"Gets the decoding configuration.\"\"\"\n        return self.decoding_config\n\n    def get_scheduler_config(self) -> SchedulerConfig:\n        \"\"\"Gets the scheduler configuration.\"\"\"\n        return self.scheduler_config\n\n    def get_lora_config(self) -> LoRAConfig:\n        \"\"\"Gets the LoRA configuration.\"\"\"\n        return self.lora_config\n\n    def get_num_unfinished_requests(self) -> int:\n        \"\"\"Gets the number of unfinished requests.\"\"\"\n        return sum(scheduler.get_num_unfinished_seq_groups()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests(self) -> bool:\n        \"\"\"Returns True if there are unfinished requests.\"\"\"\n        return any(scheduler.has_unfinished_seqs()\n                   for scheduler in self.scheduler)\n\n    def has_unfinished_requests_for_virtual_engine(\n            self, virtual_engine: int) -> bool:\n        \"\"\"\n        Returns True if there are unfinished requests for the virtual engine.\n        \"\"\"\n        return self.scheduler[virtual_engine].has_unfinished_seqs()\n\n    def reset_mm_cache(self) -> bool:\n        \"\"\"Reset the multi-modal cache.\"\"\"\n        return self.input_preprocessor.mm_registry.reset_processor_cache()\n\n    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:\n        \"\"\"Reset prefix cache for all devices.\"\"\"\n\n        success = True\n        for scheduler in self.scheduler:\n            success = success and scheduler.reset_prefix_cache(device)\n        return success\n\n    @staticmethod\n    def _process_sequence_group_outputs(\n        seq_group: SequenceGroup,\n        outputs: List[PoolingSequenceGroupOutput],\n    ) -> None:\n        seq_group.pooled_data = outputs[0].data\n\n        for seq in seq_group.get_seqs():\n            seq.status = SequenceStatus.FINISHED_STOPPED\n\n        return\n\n    def _update_num_computed_tokens_for_multi_step_prefill(\n            self, seq_group: SequenceGroup,\n            seq_group_meta: SequenceGroupMetadata,\n            is_first_step_output: Optional[bool]):\n        \"\"\"\n        This function updates num_computed_tokens for prompt sequences\n        when Multi-Step is enabled.\n\n        seq_group: SequenceGroup to update the num_computed_tokens for.\n        seq_group_meta: Metadata of the given SequenceGroup.\n        is_first_step_output: Optional[bool] -\n            When available, is_first_step_output indicates if the appended\n            output token is the output of the first-step in multi-step.\n            A value of None indicates that outputs from all steps in\n            in multi-step are submitted in a single burst.\n        \"\"\"\n\n        assert self.scheduler_config.is_multi_step\n\n        if not seq_group_meta.is_prompt:\n            # num_computed_token updates for multi-step decodes happen after\n            # the tokens are appended to the sequence.\n            return\n\n        do_update: bool = False\n        if self.scheduler_config.chunked_prefill_enabled:\n            # In multi-step + chunked-prefill case, the prompt sequences\n            # that are scheduled are fully processed in the first step.\n            do_update = is_first_step_output is None or is_first_step_output\n        else:\n            # Normal multi-step decoding case. In this case prompt-sequences\n            # are actually single-stepped. Always update in this case.\n            assert seq_group.state.num_steps == 1\n            do_update = True\n\n        if do_update:\n            seq_group.update_num_computed_tokens(\n                seq_group_meta.token_chunk_size)\n\n    def _process_model_outputs(self,\n                               ctx: SchedulerContext,\n                               request_id: Optional[str] = None) -> None:\n        \"\"\"Apply the model output to the sequences in the scheduled seq groups\n        and return responses.\n\n        ctx: The virtual engine context to work on\n        request_id: If provided, then only this request is going to be processed\n        \"\"\"\n\n        now = time.time()\n\n        if len(ctx.output_queue) == 0:\n            return None\n\n        # Get pending async postprocessor\n        if request_id:\n            # When we process only one request, no pop is required\n            # (since later we will process all of the rest)\n            (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n             is_last_step, is_first_step_output, skip) = ctx.output_queue[0]\n        else:\n            (outputs, seq_group_metadata_list, scheduler_outputs, is_async,\n             is_last_step, is_first_step_output,\n             skip) = ctx.output_queue.popleft()\n\n        # Sanity check\n        assert len(seq_group_metadata_list) == len(\n            scheduler_outputs.scheduled_seq_groups)\n\n        has_multiple_outputs: bool = len(outputs) > 1\n        outputs_by_sequence_group: List[List[SequenceGroupOutput]]\n        if has_multiple_outputs:\n            assert self.scheduler_config.is_multi_step or \\\n                     self.speculative_config\n            # Organize outputs by [step][sequence group] instead of\n            # [sequence group][step].\n            if self.scheduler_config.is_multi_step:\n                outputs_by_sequence_group = create_output_by_sequence_group(\n                    outputs, len(seq_group_metadata_list))\n            elif self.speculative_config:\n                # Decodes are multi-steps while prefills are not, outputting at\n                # most 1 token. Separate them so that we can trigger chunk\n                # processing without having to pad or copy over prompts K times\n                # to match decodes structure (costly with prompt_logprobs).\n                num_prefills = sum(sg.is_prompt\n                                   for sg in seq_group_metadata_list)\n                prefills, decodes = outputs[:num_prefills], outputs[\n                    num_prefills:]\n                outputs_by_sequence_group = create_output_by_sequence_group(\n                    decodes,\n                    num_seq_groups=len(seq_group_metadata_list) - num_prefills)\n                outputs_by_sequence_group = [p.outputs for p in prefills\n                                             ] + outputs_by_sequence_group\n            # We have outputs for multiple steps submitted in a single burst,\n            # so invalidate is_first_step_output.\n            is_first_step_output = None\n        else:\n            outputs_by_sequence_group = outputs\n\n        # Determine the requests we need to operate on\n        if request_id:\n            indices = []\n            for i, seq_group_meta in enumerate(seq_group_metadata_list):\n                if seq_group_meta.request_id == request_id:\n                    assert i not in skip  # Cannot be called twice\n                    indices.append(i)\n                    break\n\n            # If the request_id was not found, then it means that\n            # this is a new request that has no pending async\n            # postprocessor\n            if not indices:\n                return\n        else:\n            indices = range(len(seq_group_metadata_list))  # type: ignore\n\n        finished_before: List[int] = []\n        finished_now: List[int] = []\n        for i in indices:\n            if i in skip:\n                continue\n\n            seq_group_meta = seq_group_metadata_list[i]\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group: SequenceGroup = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                finished_before.append(i)\n                continue\n\n            output: List[SequenceGroupOutput]\n            if has_multiple_outputs:\n                output = outputs_by_sequence_group[i]\n            else:\n                output = [outputs_by_sequence_group[0][i]]\n\n            if not is_async:\n                if self.scheduler_config.is_multi_step:\n                    # Updates happen only if the sequence is prefill\n                    self._update_num_computed_tokens_for_multi_step_prefill(\n                        seq_group, seq_group_meta, is_first_step_output)\n                else:\n                    seq_group.update_num_computed_tokens(\n                        seq_group_meta.token_chunk_size or 0)\n\n            if outputs:\n                for o in outputs:\n                    if (isinstance(o, SamplerOutput)\n                            and seq_group.metrics is not None):\n                        if seq_group.metrics.model_forward_time is not None:\n                            seq_group.metrics.model_forward_time += (\n                                o.model_forward_time or 0)\n                        else:\n                            seq_group.metrics.model_forward_time = (\n                                o.model_forward_time)\n                        if seq_group.metrics.model_execute_time is not None:\n                            seq_group.metrics.model_execute_time += (\n                                o.model_execute_time or 0)\n                        else:\n                            seq_group.metrics.model_execute_time = (\n                                o.model_execute_time)\n\n            if self.model_config.runner_type == \"pooling\":\n                self._process_sequence_group_outputs(seq_group, output)\n            else:\n                self.output_processor.process_prompt_logprob(seq_group, output)\n                if seq_group_meta.do_sample:\n                    self.output_processor.process_outputs(\n                        seq_group, output, is_async)\n\n            if seq_group.is_finished():\n                finished_now.append(i)\n\n        # Generate outputs for the requests that finished this iteration\n        for i in finished_now:\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            if not seq_group.is_prefill():\n                seq_group.set_last_token_time(now)\n            request_output = RequestOutputFactory.create(\n                seq_group,\n                self.seq_id_to_seq_group,\n                use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # When we process a single request, we skip it for the next time,\n        # and invoke the request output callback (if there was final output)\n        if request_id:\n            assert len(indices) == 1\n            skip.append(indices[0])\n\n            if (finished_now\n                    and self.process_request_outputs_callback is not None):\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        # Free currently finished requests\n        if finished_now:\n            for scheduler in self.scheduler:\n                scheduler.free_finished_seq_groups()\n\n        # For multi-step without streaming, don't create outputs each iteration\n        if not is_last_step and not ctx.multi_step_stream_outputs:\n            # Immediately process request outputs here (if callback is given)\n            if (finished_now\n                    and self.process_request_outputs_callback is not None):\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        # Create the outputs\n        for i in indices:\n            if i in skip or i in finished_before or i in finished_now:\n                continue  # Avoids double processing\n\n            scheduled_seq_group = scheduler_outputs.scheduled_seq_groups[i]\n\n            seq_group = scheduled_seq_group.seq_group\n            seq_group.maybe_set_first_token_time(now)\n            if not seq_group.is_prefill():\n                seq_group.set_last_token_time(now)\n            request_output = RequestOutputFactory.create(\n                seq_group,\n                self.seq_id_to_seq_group,\n                use_cache=self.use_cached_outputs)\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # For multi-step with streaming, create outputs each iteration\n        if not is_last_step and ctx.multi_step_stream_outputs:\n            # Immediately process request outputs here (if callback is given)\n            if self.process_request_outputs_callback is not None:\n                self.process_request_outputs_callback(ctx.request_outputs)\n                ctx.request_outputs.clear()\n            return\n\n        for seq_group in scheduler_outputs.ignored_seq_groups:\n            params = seq_group.sampling_params\n            if params is not None and params.output_kind == (\n                    RequestOutputKind.DELTA) and not seq_group.is_finished():\n                continue\n\n            request_output = RequestOutputFactory.create(\n                seq_group,\n                self.seq_id_to_seq_group,\n                use_cache=self.use_cached_outputs,\n            )\n            if request_output:\n                ctx.request_outputs.append(request_output)\n\n        # Immediately process request outputs here (if callback is given)\n        if (ctx.request_outputs\n                and self.process_request_outputs_callback is not None):\n            self.process_request_outputs_callback(ctx.request_outputs)\n            ctx.request_outputs.clear()\n\n        # For async case, we need to record the stats here.\n        # For non-async case, the stats are done in the\n        # LLMEngine/AsyncLLMEngine directly\n        if is_async:\n            # Log stats.\n            self.do_log_stats(scheduler_outputs, outputs, finished_before,\n                              skip)\n\n            # Tracing\n            self.do_tracing(scheduler_outputs, finished_before)\n\n        return None\n\n    def _advance_to_next_step(\n            self, output: SamplerOutput,\n            seq_group_metadata_list: List[SequenceGroupMetadata],\n            scheduled_seq_groups: List[ScheduledSequenceGroup]) -> None:\n        \"\"\"Given model output from a single run, append the tokens to the\n        sequences. This is normally done inside output processor, but it is\n        required if the worker is to perform async forward pass to next step.\n        \"\"\"\n        for seq_group_metadata, sequence_group_outputs, scheduled_seq_group in \\\n            zip(seq_group_metadata_list, output, scheduled_seq_groups):\n            seq_group = scheduled_seq_group.seq_group\n\n            if seq_group.is_finished():\n                continue\n\n            if self.scheduler_config.is_multi_step:\n                # Updates happen only if the sequence is prefill\n                self._update_num_computed_tokens_for_multi_step_prefill(\n                    seq_group, seq_group_metadata,\n                    seq_group.state.num_steps == 1)\n            else:\n                token_chunk_size = (seq_group_metadata.token_chunk_size\n                                    if seq_group_metadata.token_chunk_size\n                                    is not None else 0)\n                seq_group.update_num_computed_tokens(token_chunk_size)\n\n            if seq_group_metadata.do_sample:\n                assert len(sequence_group_outputs.samples) == 1, (\n                    \"Async output processor expects a single sample\"\n                    \" (i.e sampling_params.n == 1)\")\n                sample = sequence_group_outputs.samples[0]\n\n                assert len(seq_group.seqs) == 1\n                seq = seq_group.seqs[0]\n\n                if self.scheduler_config.is_multi_step:\n                    is_prefill_append = seq.data.get_num_uncomputed_tokens(\n                    ) == 0\n                    seq.append_token_id(sample.output_token, sample.logprobs,\n                                        sample.output_embed)\n                    if not is_prefill_append:\n                        seq_group.update_num_computed_tokens(1)\n                else:\n                    seq.append_token_id(sample.output_token, sample.logprobs,\n                                        sample.output_embed)\n\n    def step(self) -> List[Union[RequestOutput, PoolingRequestOutput]]:\n        \"\"\"Performs one decoding iteration and returns newly generated results.\n\n        :::{figure} https://i.imgur.com/sv2HssD.png\n        :alt: Overview of the step function\n        :align: center\n\n        Overview of the step function.\n        :::\n\n        Details:\n        - Step 1: Schedules the sequences to be executed in the next\n            iteration and the token blocks to be swapped in/out/copy.\n\n            - Depending on the scheduling policy,\n                sequences may be `preempted/reordered`.\n            - A Sequence Group (SG) refer to a group of sequences\n                that are generated from the same prompt.\n\n        - Step 2: Calls the distributed executor to execute the model.\n        - Step 3: Processes the model output. This mainly includes:\n\n            - Decodes the relevant outputs.\n            - Updates the scheduled sequence groups with model outputs\n                based on its `sampling parameters` (`use_beam_search` or not).\n            - Frees the finished sequence groups.\n\n        - Finally, it creates and returns the newly generated results.\n\n        Example:\n        ```\n        # Please see the example/ folder for more detailed examples.\n\n        # initialize engine and request arguments\n        engine = LLMEngine.from_engine_args(engine_args)\n        example_inputs = [(0, \"What is LLM?\",\n        SamplingParams(temperature=0.0))]\n    \n        # Start the engine with an event loop\n        while True:\n            if example_inputs:\n                req_id, prompt, sampling_params = example_inputs.pop(0)\n                engine.add_request(str(req_id),prompt,sampling_params)\n\n            # continue the request processing\n            request_outputs = engine.step()\n            for request_output in request_outputs:\n                if request_output.finished:\n                    # return or show the request output\n\n            if not (engine.has_unfinished_requests() or example_inputs):\n                break\n        ```\n        \"\"\"\n        if self.parallel_config.pipeline_parallel_size > 1:\n            raise NotImplementedError(\n                \"Pipeline parallelism is only supported through AsyncLLMEngine \"\n                \"as performance will be severely degraded otherwise.\")\n\n        # For llm_engine, there is no pipeline parallel support, so the engine\n        # used is always 0.\n        virtual_engine = 0\n\n        # These are cached outputs from previous iterations. None if on first\n        # iteration\n        cached_outputs = self.cached_scheduler_outputs[virtual_engine]\n        seq_group_metadata_list = cached_outputs.seq_group_metadata_list\n        scheduler_outputs = cached_outputs.scheduler_outputs\n        allow_async_output_proc = cached_outputs.allow_async_output_proc\n\n        ctx = self.scheduler_contexts[virtual_engine]\n\n        # Clear outputs for each new scheduler iteration\n        ctx.request_outputs.clear()\n\n        # Skip the scheduler if there are any remaining steps in the seq groups.\n        # This ensures that the scheduler is only called again when the current\n        # batch has completed.\n        # The scheduler is also skipped if a single request caused the last\n        # engine step to fail, and the previous schedule needs to be rerun.\n        if not self._has_remaining_steps(\n                seq_group_metadata_list\n        ) and not self._skip_scheduling_next_step:\n            # Schedule iteration\n            (seq_group_metadata_list, scheduler_outputs,\n             allow_async_output_proc\n             ) = self.scheduler[virtual_engine].schedule()\n\n            ctx.seq_group_metadata_list = seq_group_metadata_list\n            ctx.scheduler_outputs = scheduler_outputs\n\n            finished_requests_ids = self.scheduler[\n                virtual_engine].get_and_reset_finished_requests_ids()\n            # When n>1, elements in self.seq_id_to_seq_group should be deleted\n            # here, otherwise memory leaks.\n            for finished_request_id in finished_requests_ids:\n                if finished_request_id in self.seq_id_to_seq_group:\n                    del self.seq_id_to_seq_group[finished_request_id]\n\n            # Maybe switch from async mode to sync mode\n            if not allow_async_output_proc and len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n\n            if (self.scheduler_config.is_multi_step\n                    and scheduler_outputs.num_lookahead_slots > 0):\n                # cache the scheduler outputs for the next iteration if we have\n                # lookahead slots\n                self._cache_scheduler_outputs_for_multi_step(\n                    virtual_engine, seq_group_metadata_list, scheduler_outputs,\n                    allow_async_output_proc)\n        else:\n            finished_requests_ids = list()\n\n        assert seq_group_metadata_list is not None\n        assert scheduler_outputs is not None\n\n        if not scheduler_outputs.is_empty():\n\n            # Check if we have a cached last_output from the previous iteration.\n            # For supporting PP this is probably the best way to pass the\n            # sampled_token_ids, as a separate broadcast over all the PP stages\n            # will cause one virtual engine's microbatch to block the pipeline.\n            last_sampled_token_ids = \\\n                self._get_last_sampled_token_ids(virtual_engine)\n\n            execute_model_req = ExecuteModelRequest(\n                seq_group_metadata_list=seq_group_metadata_list,\n                blocks_to_swap_in=scheduler_outputs.blocks_to_swap_in,\n                blocks_to_swap_out=scheduler_outputs.blocks_to_swap_out,\n                blocks_to_copy=scheduler_outputs.blocks_to_copy,\n                num_lookahead_slots=scheduler_outputs.num_lookahead_slots,\n                running_queue_size=scheduler_outputs.running_queue_size,\n                finished_requests_ids=finished_requests_ids,\n                # We use ExecuteModelRequest to pass the last sampled_token_ids\n                # to each of the non-last PP stages for in-place prepare_input.\n                last_sampled_token_ids=last_sampled_token_ids)\n\n            if allow_async_output_proc:\n                execute_model_req.async_callback = self.async_callbacks[\n                    virtual_engine]\n\n            try:\n                outputs = self.model_executor.execute_model(\n                    execute_model_req=execute_model_req)\n                self._skip_scheduling_next_step = False\n            except InputProcessingError as e:\n                # The input for this request cannot be processed, so we must\n                # abort it. If there are remaining requests in the batch that\n                # have been scheduled, they will be retried on the next step.\n                invalid_request_id = e.request_id\n                self._abort_and_cache_schedule(\n                    request_id=invalid_request_id,\n                    virtual_engine=virtual_engine,\n                    seq_group_metadata_list=seq_group_metadata_list,\n                    scheduler_outputs=scheduler_outputs,\n                    allow_async_output_proc=allow_async_output_proc)\n                # Raise so the caller is notified that this request failed\n                raise\n\n            # We need to do this here so that last step's sampled_token_ids can\n            # be passed to the next iteration for PP.\n            if self.scheduler_config.is_multi_step:\n                self._update_cached_scheduler_output(virtual_engine, outputs)\n        else:\n            # Nothing scheduled => If there is pending async postprocessor,\n            # then finish it here.\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            # No outputs in this case\n            outputs = []\n\n        # Finish the current step for all the sequence groups.\n        if self.scheduler_config.is_multi_step:\n            for seq_group in seq_group_metadata_list:\n                seq_group.finish_step()\n\n        if not self._has_remaining_steps(seq_group_metadata_list):\n            # clear the cache if we have finished all the steps.\n            if self.scheduler_config.is_multi_step:\n                self.cached_scheduler_outputs[0] = SchedulerOutputState()\n\n            # is_first_step_output is True only when the num_steps of all\n            # the sequences are 1. When the num_steps > 1,\n            # multi_step_model_runner does the first-step output append.\n            is_first_step_output: bool = False if not seq_group_metadata_list \\\n                else seq_group_metadata_list[0].state.num_steps == 1\n\n            # Add results to the output_queue\n            ctx.append_output(outputs=outputs,\n                              seq_group_metadata_list=seq_group_metadata_list,\n                              scheduler_outputs=scheduler_outputs,\n                              is_async=allow_async_output_proc,\n                              is_last_step=True,\n                              is_first_step_output=is_first_step_output)\n\n            if outputs and allow_async_output_proc:\n                assert len(outputs) == 1, (\n                    \"Async postprocessor expects only a single output set\")\n\n                self._advance_to_next_step(\n                    outputs[0], seq_group_metadata_list,\n                    scheduler_outputs.scheduled_seq_groups)\n\n            # Check if need to run the usual non-async path\n            if not allow_async_output_proc:\n                self._process_model_outputs(ctx=ctx)\n\n                # Log stats.\n                self.do_log_stats(scheduler_outputs, outputs)\n\n                # Tracing\n                self.do_tracing(scheduler_outputs)\n        else:\n            # Multi-step case\n            return ctx.request_outputs\n\n        if not self.has_unfinished_requests():\n            # Drain async postprocessor (if exists)\n            if len(ctx.output_queue) > 0:\n                self._process_model_outputs(ctx=ctx)\n            assert len(ctx.output_queue) == 0\n\n            # Stop the execute model loop in parallel workers until there are\n            # more requests to process. This avoids waiting indefinitely in\n            # torch.distributed ops which may otherwise timeout, and unblocks\n            # the RPC thread in the workers so that they can process any other\n            # queued control plane messages, such as add/remove lora adapters.\n            logger.debug(\"Stopping remote worker execution loop.\")\n            self.model_executor.stop_remote_worker_execution_loop()\n\n        return ctx.request_outputs\n\n    def _abort_and_cache_schedule(\n            self, request_id: str, virtual_engine: int,\n            seq_group_metadata_list: List[SequenceGroupMetadata],\n            scheduler_outputs: SchedulerOutputs,\n            allow_async_output_proc: bool) -> None:\n        \"\"\"Aborts a single request, and caches the scheduler outputs minus that\n        request. This allows the next step to continue processing the remaining\n        requests without having to re-run the scheduler.\"\"\"\n\n        # Abort the request and remove its sequence group from the current\n        # schedule\n        self.abort_request(request_id)\n        for i, metadata in enumerate(seq_group_metadata_list):\n            if metadata.request_id == request_id:\n                del seq_group_metadata_list[i]\n                break\n        for i, group in enumerate(scheduler_outputs.scheduled_seq_groups):\n            if group.seq_group.request_id == request_id:\n                del scheduler_outputs.scheduled_seq_groups[i]\n                break\n\n        # If there are still other sequence groups left in the schedule, cache\n        # them and flag the engine to reuse the schedule.\n        if len(seq_group_metadata_list) > 0:\n            self._skip_scheduling_next_step = True\n            # Reuse multi-step caching logic\n            self._cache_scheduler_outputs_for_multi_step(\n                virtual_engine=virtual_engine,\n                scheduler_outputs=scheduler_outputs,\n                seq_group_metadata_list=seq_group_metadata_list,\n                allow_async_output_proc=allow_async_output_proc)\n\n    def _has_remaining_steps(\n        self, seq_group_metadata_list: Optional[List[SequenceGroupMetadata]]\n    ) -> bool:\n        if (not self.scheduler_config.is_multi_step\n                or not seq_group_metadata_list):\n            return False\n\n        # TODO(will) this is a sanity check for nowto make sure that all the\n        # seqs are on the same steps. Eventually we will want to do some sort of\n        # dynamic scheduling when doing multi-step decoding.\n        ref_remaining_steps = seq_group_metadata_list[0].state.remaining_steps\n        if any([\n                seq_group.state.remaining_steps != ref_remaining_steps\n                for seq_group in seq_group_metadata_list[1:]\n        ]):\n            raise AssertionError(\"All running sequence groups should \"\n                                 \"have the same remaining steps.\")\n\n        return ref_remaining_steps > 0\n\n    def _cache_scheduler_outputs_for_multi_step(\n            self, virtual_engine: int,\n            seq_group_metadata_list: Optional[List[SequenceGroupMetadata]],\n            scheduler_outputs: SchedulerOutputs,\n            allow_async_output_proc: bool) -> None:\n        co = self.cached_scheduler_outputs[virtual_engine]\n\n        co.seq_group_metadata_list = seq_group_metadata_list\n        co.scheduler_outputs = scheduler_outputs\n        co.allow_async_output_proc = allow_async_output_proc\n        co.last_output = None\n\n    def _update_cached_scheduler_output(\n            self, virtual_engine: int,\n            output: List[Optional[SamplerOutput]]) -> None:\n        if (self.parallel_config.pipeline_parallel_size > 1 and len(output) > 0\n                and output[0] is not None):\n            last_output = output[-1]\n            assert last_output is not None\n            assert last_output.sampled_token_ids_cpu is not None\n            assert last_output.sampled_token_ids is None\n            assert last_output.sampled_token_probs is None\n            self.cached_scheduler_outputs[\n                virtual_engine].last_output = last_output\n\n    def _get_last_sampled_token_ids(\n            self, virtual_engine: int) -> Optional[torch.Tensor]:\n        cached_last_output = self.cached_scheduler_outputs[\n            virtual_engine].last_output\n        if (self.scheduler_config.is_multi_step\n                and self.parallel_config.pipeline_parallel_size > 1\n                and cached_last_output is not None\n                and cached_last_output.sampled_token_ids_cpu is not None):\n            return cached_last_output.sampled_token_ids_cpu\n        return None\n\n    def add_logger(self, logger_name: str, logger: StatLoggerBase) -> None:\n        if not self.log_stats:\n            raise RuntimeError(\n                \"Stat logging is disabled. Set `disable_log_stats=False` \"\n                \"argument to enable.\")\n        if logger_name in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} already exists.\")\n        self.stat_loggers[logger_name] = logger\n\n    def remove_logger(self, logger_name: str) -> None:\n        if not self.log_stats:\n            raise RuntimeError(\n                \"Stat logging is disabled. Set `disable_log_stats=False` \"\n                \"argument to enable.\")\n        if logger_name not in self.stat_loggers:\n            raise KeyError(f\"Logger with name {logger_name} does not exist.\")\n        del self.stat_loggers[logger_name]\n\n    def do_log_stats(self,\n                     scheduler_outputs: Optional[SchedulerOutputs] = None,\n                     model_output: Optional[List[SamplerOutput]] = None,\n                     finished_before: Optional[List[int]] = None,\n                     skip: Optional[List[int]] = None) -> None:\n        \"\"\"Forced log when no requests active.\"\"\"\n        if self.log_stats:\n            stats = self._get_stats(scheduler_outputs, model_output,\n                                    finished_before, skip)\n            for logger in self.stat_loggers.values():\n                logger.log(stats)\n\n    def _get_stats(self,\n                   scheduler_outputs: Optional[SchedulerOutputs],\n                   model_output: Optional[List[SamplerOutput]] = None,\n                   finished_before: Optional[List[int]] = None,\n                   skip: Optional[List[int]] = None) -> Stats:\n        \"\"\"Get Stats to be Logged to Prometheus.\n\n        Args:\n            scheduler_outputs: Optional, used to populate metrics related to\n                the scheduled batch,\n            model_output: Optional, used to emit speculative decoding metrics\n                which are created by the workers.\n            finished_before: Optional, indices of sequences that were finished\n                before. These sequences will be ignored.\n            skip: Optional, indices of sequences that were preempted. These\n                sequences will be ignored.\n        \"\"\"\n        now = time.time()\n\n        # System State\n        #   Scheduler State\n        num_running_sys = sum(\n            len(scheduler.running) for scheduler in self.scheduler)\n        num_swapped_sys = sum(\n            len(scheduler.swapped) for scheduler in self.scheduler)\n        num_waiting_sys = sum(\n            len(scheduler.waiting) for scheduler in self.scheduler)\n\n        # KV Cache Usage in %\n        num_total_gpu = self.cache_config.num_gpu_blocks\n        gpu_cache_usage_sys = 0.\n        if num_total_gpu:  # Guard against both None and 0\n            num_free_gpu = sum(\n                scheduler.block_manager.get_num_free_gpu_blocks()\n                for scheduler in self.scheduler)\n            gpu_cache_usage_sys = 1.0 - (num_free_gpu / num_total_gpu)\n\n        num_total_cpu = self.cache_config.num_cpu_blocks\n        cpu_cache_usage_sys = 0.\n        if num_total_cpu:  # Guard against both None and 0\n            num_free_cpu = sum(\n                scheduler.block_manager.get_num_free_cpu_blocks()\n                for scheduler in self.scheduler)\n            cpu_cache_usage_sys = 1.0 - (num_free_cpu / num_total_cpu)\n\n        # Prefix Cache Hit Rate. Note that we always use\n        # the cache hit rate of the first virtual engine.\n        cpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.CPU)\n        gpu_prefix_cache_hit_rate = self.scheduler[\n            0].get_prefix_cache_hit_rate(Device.GPU)\n\n        # Iteration stats\n        num_prompt_tokens_iter = 0\n        num_generation_tokens_iter = 0\n        num_tokens_iter = 0\n        time_to_first_tokens_iter: List[float] = []\n        time_per_output_tokens_iter: List[float] = []\n        num_preemption_iter = (0 if scheduler_outputs is None else\n                               scheduler_outputs.preempted)\n\n        # Request stats\n        #   Latency\n        time_e2e_requests: List[float] = []\n        time_queue_requests: List[float] = []\n        time_inference_requests: List[float] = []\n        time_prefill_requests: List[float] = []\n        time_decode_requests: List[float] = []\n        time_in_queue_requests: List[float] = []\n        model_forward_time_requests: List[float] = []\n        model_execute_time_requests: List[float] = []\n        #   Metadata\n        num_prompt_tokens_requests: List[int] = []\n        num_generation_tokens_requests: List[int] = []\n        n_requests: List[int] = []\n        max_num_generation_tokens_requests: List[int] = []\n        max_tokens_requests: List[int] = []\n        finished_reason_requests: List[str] = []\n\n        # LoRA requests\n        running_lora_adapters = dict(\n            collectionsCounter([\n                running_request.lora_request.lora_name\n                for scheduler in self.scheduler\n                for running_request in scheduler.running\n                if running_request.lora_request\n            ]))\n        waiting_lora_adapters = dict(\n            collectionsCounter([\n                waiting_request.lora_request.lora_name\n                for scheduler in self.scheduler\n                for waiting_request in scheduler.waiting\n                if waiting_request.lora_request\n            ]))\n        max_lora_stat = \"0\"\n        if self.lora_config:\n            max_lora_stat = str(self.lora_config.max_loras)\n\n        # NOTE: This loop assumes prefill seq_groups are before\n        # decode seq_groups in scheduled_seq_groups.\n        if scheduler_outputs is not None:\n            # For async postprocessor, already finished sequences need to be\n            # not counted (to avoid double counting)\n            actual_num_batched_tokens = scheduler_outputs.num_batched_tokens  # type: ignore\n\n            num_generation_tokens_from_prefill_groups = 0\n            # NOTE: if scheduler_outputs.num_prefill_groups > 0 and\n            # the len of scheduler_outputs.scheduled_seq_groups is !=\n            # scheduler_outputs.num_prefill_groups, this means that\n            # chunked prefills have been detected.\n\n            for idx, scheduled_seq_group in enumerate(\n                    scheduler_outputs.scheduled_seq_groups):\n                # Skip double logging when using async output proc\n                if finished_before and idx in finished_before:\n                    actual_num_batched_tokens -= 1\n                    continue\n\n                # Currently, skip == preempted sequences, so we need to skip\n                # their log stats\n                if skip and idx in skip:\n                    continue\n\n                group_was_prefill = idx < scheduler_outputs.num_prefill_groups\n                seq_group = scheduled_seq_group.seq_group\n\n                # NOTE: a seq_group that completed all of its prefill tokens\n                # in the last iteration will have seq_group.is_prefill() = False\n                # with group_was_prefill = True\n                if group_was_prefill:\n                    # Number of prompt tokens.\n                    num_prompt_tokens_iter += (\n                        scheduled_seq_group.token_chunk_size)\n\n                    # If the seq_group just finished the prefill state\n                    # get TTFT.\n                    if not seq_group.is_prefill():\n                        latency = seq_group.get_last_token_latency()\n                        time_to_first_tokens_iter.append(latency)\n\n                        # One generation token per finished prefill.\n                        num_generation_tokens_from_prefill_groups += (\n                            seq_group.num_seqs())\n                else:\n                    # TPOTs.\n                    latency = seq_group.get_last_token_latency()\n                    time_per_output_tokens_iter.append(latency)\n                    if seq_group.state.current_step == 0:\n                        # For async_output_proc, the do_log_stats()\n                        # is called following init_multi_step(), which\n                        # sets the current_step to zero.\n                        actual_num_batched_tokens +=\\\n                            seq_group.state.num_steps - 1\n                    else:\n                        actual_num_batched_tokens +=\\\n                            seq_group.state.current_step - 1\n\n                # Because of chunked prefill, we can have a single sequence\n                # group that does multiple prompt_runs. To prevent logging\n                # the same metadata more than once per request, we standardize\n                # on logging request level information for finished requests,\n                # which can only happen once.\n                if seq_group.is_finished():\n                    # Latency timings\n                    time_e2e_requests.append(now -\n                                             seq_group.metrics.arrival_time)\n                    if (seq_group.metrics.first_scheduled_time is not None and\n                            seq_group.metrics.first_token_time is not None):\n                        time_queue_requests.append(\n                            seq_group.metrics.first_scheduled_time -\n                            seq_group.metrics.arrival_time)\n                        time_prefill_requests.append(\n                            seq_group.metrics.first_token_time -\n                            seq_group.metrics.first_scheduled_time)\n                        time_decode_requests.append(\n                            now - seq_group.metrics.first_token_time)\n                        time_inference_requests.append(\n                            now - seq_group.metrics.first_scheduled_time)\n                    if seq_group.metrics.time_in_queue is not None:\n                        time_in_queue_requests.append(\n                            seq_group.metrics.time_in_queue)\n                    if seq_group.metrics.model_forward_time is not None:\n                        model_forward_time_requests.append(\n                            seq_group.metrics.model_forward_time)\n                    if seq_group.metrics.model_execute_time is not None:\n                        model_execute_time_requests.append(\n                            seq_group.metrics.model_execute_time * 1000)\n                    # Metadata\n                    num_prompt_tokens_requests.append(\n                        len(seq_group.prompt_token_ids))\n                    num_generation_tokens_requests.extend([\n                        seq.get_output_len()\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n                    max_num_generation_tokens_requests.append(\n                        max(seq.get_output_len()\n                            for seq in seq_group.get_seqs()))\n                    if seq_group.sampling_params is not None:\n                        n_requests.append(seq_group.sampling_params.n)\n                        max_tokens_requests.append(\n                            seq_group.sampling_params.max_tokens)\n                    finished_reason_requests.extend([\n                        SequenceStatus.get_finished_reason(seq.status)\n                        for seq in seq_group.get_finished_seqs()\n                    ])\n\n            # Number of generation tokens.\n            #   num_batched_tokens equals the number of prompt_tokens plus the\n            #   number of decode_tokens in a single iteration. So,\n            #   num_generation_tokens = num_batched_tokens - num_prompt_tokens\n            #   + num_generation_tokens_from_prefill_groups (since we generate\n            #   one token on prefills on iters where the prefill finishes).\n            num_generation_tokens_iter = (\n                actual_num_batched_tokens - num_prompt_tokens_iter +\n                num_generation_tokens_from_prefill_groups)\n            num_tokens_iter = (num_generation_tokens_iter +\n                               num_prompt_tokens_iter)\n        # Spec decode, if enabled, emits specialized metrics from the worker in\n        # sampler output.\n        if model_output and isinstance(model_output[0], SamplerOutput) and (\n                model_output[0].spec_decode_worker_metrics is not None):\n            spec_decode_metrics = model_output[0].spec_decode_worker_metrics\n        else:\n            spec_decode_metrics = None\n\n        return Stats(\n            now=now,\n            # System stats\n            #   Scheduler State\n            num_running_sys=num_running_sys,\n            num_swapped_sys=num_swapped_sys,\n            num_waiting_sys=num_waiting_sys,\n            #   KV Cache Usage in %\n            gpu_cache_usage_sys=gpu_cache_usage_sys,\n            cpu_cache_usage_sys=cpu_cache_usage_sys,\n            #   Prefix Cache Hit Rate\n            cpu_prefix_cache_hit_rate=cpu_prefix_cache_hit_rate,\n            gpu_prefix_cache_hit_rate=gpu_prefix_cache_hit_rate,\n\n            # Iteration stats\n            num_prompt_tokens_iter=num_prompt_tokens_iter,\n            num_generation_tokens_iter=num_generation_tokens_iter,\n            num_tokens_iter=num_tokens_iter,\n            time_to_first_tokens_iter=time_to_first_tokens_iter,\n            time_per_output_tokens_iter=time_per_output_tokens_iter,\n            spec_decode_metrics=spec_decode_metrics,\n            num_preemption_iter=num_preemption_iter,\n\n            # Request stats\n            #   Latency\n            time_e2e_requests=time_e2e_requests,\n            time_queue_requests=time_queue_requests,\n            time_inference_requests=time_inference_requests,\n            time_prefill_requests=time_prefill_requests,\n            time_decode_requests=time_decode_requests,\n            time_in_queue_requests=time_in_queue_requests,\n            model_forward_time_requests=model_forward_time_requests,\n            model_execute_time_requests=model_execute_time_requests,\n            #   Metadata\n            num_prompt_tokens_requests=num_prompt_tokens_requests,\n            num_generation_tokens_requests=num_generation_tokens_requests,\n            max_num_generation_tokens_requests=\n            max_num_generation_tokens_requests,\n            n_requests=n_requests,\n            max_tokens_requests=max_tokens_requests,\n            finished_reason_requests=finished_reason_requests,\n            max_lora=str(max_lora_stat),\n            waiting_lora_adapters=list(waiting_lora_adapters.keys()),\n            running_lora_adapters=list(running_lora_adapters.keys()))\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.model_executor.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.model_executor.remove_lora(lora_id)\n\n    def list_loras(self) -> Set[int]:\n        return self.model_executor.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.model_executor.pin_lora(lora_id)\n\n    def add_prompt_adapter(\n            self, prompt_adapter_request: PromptAdapterRequest) -> bool:\n        return self.model_executor.add_prompt_adapter(prompt_adapter_request)\n\n    def remove_prompt_adapter(self, prompt_adapter_id: int) -> bool:\n        return self.model_executor.remove_prompt_adapter(prompt_adapter_id)\n\n    def list_prompt_adapters(self) -> List[int]:\n        return self.model_executor.list_prompt_adapters()\n\n    def start_profile(self) -> None:\n        self.model_executor.start_profile()\n\n    def stop_profile(self) -> None:\n        self.model_executor.stop_profile()\n\n    def sleep(self, level: int = 1) -> None:\n        assert self.vllm_config.model_config.enable_sleep_mode, (\n            \"Sleep mode is not enabled in the model config\")\n        self.model_executor.sleep(level=level)\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        assert self.vllm_config.model_config.enable_sleep_mode, (\n            \"Sleep mode is not enabled in the model config\")\n        self.model_executor.wake_up(tags)\n\n    def is_sleeping(self) -> bool:\n        return self.model_executor.is_sleeping\n\n    def check_health(self) -> None:\n        self.model_executor.check_health()\n\n    def is_tracing_enabled(self) -> bool:\n        return self.tracer is not None\n\n    def do_tracing(self,\n                   scheduler_outputs: SchedulerOutputs,\n                   finished_before: Optional[List[int]] = None) -> None:\n        if self.tracer is None:\n            return\n\n        for idx, scheduled_seq_group in enumerate(\n                scheduler_outputs.scheduled_seq_groups):\n            # Skip double tracing when using async output proc\n            if finished_before and idx in finished_before:\n                continue\n\n            seq_group = scheduled_seq_group.seq_group\n            if seq_group.is_finished():\n                self.create_trace_span(seq_group)\n\n    def create_trace_span(self, seq_group: SequenceGroup) -> None:\n        if self.tracer is None or seq_group.sampling_params is None:\n            return\n        arrival_time_nano_seconds = int(seq_group.metrics.arrival_time * 1e9)\n\n        trace_context = extract_trace_context(seq_group.trace_headers)\n\n        with self.tracer.start_as_current_span(\n                \"llm_request\",\n                kind=SpanKind.SERVER,\n                context=trace_context,\n                start_time=arrival_time_nano_seconds) as seq_span:\n            metrics = seq_group.metrics\n            ttft = metrics.first_token_time - metrics.arrival_time\n            e2e_time = metrics.finished_time - metrics.arrival_time\n            seq_span.set_attribute(SpanAttributes.GEN_AI_RESPONSE_MODEL,\n                                   self.model_config.model)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_REQUEST_ID,\n                                   seq_group.request_id)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_REQUEST_TEMPERATURE,\n                                   seq_group.sampling_params.temperature)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_REQUEST_TOP_P,\n                                   seq_group.sampling_params.top_p)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_REQUEST_MAX_TOKENS,\n                                   seq_group.sampling_params.max_tokens)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_REQUEST_N,\n                                   seq_group.sampling_params.n)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_USAGE_NUM_SEQUENCES,\n                                   seq_group.num_seqs())\n            seq_span.set_attribute(SpanAttributes.GEN_AI_USAGE_PROMPT_TOKENS,\n                                   len(seq_group.prompt_token_ids))\n            seq_span.set_attribute(\n                SpanAttributes.GEN_AI_USAGE_COMPLETION_TOKENS,\n                sum([\n                    seq.get_output_len()\n                    for seq in seq_group.get_finished_seqs()\n                ]))\n            seq_span.set_attribute(SpanAttributes.GEN_AI_LATENCY_TIME_IN_QUEUE,\n                                   metrics.time_in_queue)\n            seq_span.set_attribute(\n                SpanAttributes.GEN_AI_LATENCY_TIME_TO_FIRST_TOKEN, ttft)\n            seq_span.set_attribute(SpanAttributes.GEN_AI_LATENCY_E2E, e2e_time)\n            if metrics.scheduler_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.GEN_AI_LATENCY_TIME_IN_SCHEDULER,\n                    metrics.scheduler_time)\n            if metrics.model_forward_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.GEN_AI_LATENCY_TIME_IN_MODEL_FORWARD,\n                    metrics.model_forward_time / 1000.0)\n            if metrics.model_execute_time is not None:\n                seq_span.set_attribute(\n                    SpanAttributes.GEN_AI_LATENCY_TIME_IN_MODEL_EXECUTE,\n                    metrics.model_execute_time)\n\n    def _validate_model_inputs(self, inputs: ProcessorInputs,\n                               lora_request: Optional[LoRARequest]):\n        encoder_inputs, decoder_inputs = split_enc_dec_inputs(inputs)\n\n        if encoder_inputs is not None:\n            self._validate_model_input(encoder_inputs,\n                                       lora_request,\n                                       prompt_type=\"encoder\")\n\n        self._validate_model_input(decoder_inputs,\n                                   lora_request,\n                                   prompt_type=\"decoder\")\n\n    def _validate_model_input(\n        self,\n        prompt_inputs: SingletonInputs,\n        lora_request: Optional[LoRARequest],\n        *,\n        prompt_type: Literal[\"encoder\", \"decoder\"],\n    ):\n        model_config = self.model_config\n        tokenizer = (None if self.tokenizer is None else\n                     self.tokenizer.get_lora_tokenizer(lora_request))\n\n        prompt_ids = prompt_inputs.get(\"prompt_token_ids\", [])\n        if not prompt_ids:\n            if prompt_type == \"encoder\" and model_config.is_multimodal_model:\n                pass  # Mllama may have empty encoder inputs for text-only data\n            elif prompt_inputs[\"type\"] == \"embeds\":\n                pass\n            else:\n                raise ValueError(f\"The {prompt_type} prompt cannot be empty\")\n\n        if tokenizer is not None:\n            max_input_id = max(prompt_ids, default=0)\n            if max_input_id > tokenizer.max_token_id:\n                raise ValueError(\n                    f\"Token id {max_input_id} is out of vocabulary\")\n\n        max_prompt_len = self.model_config.max_model_len\n        if len(prompt_ids) > max_prompt_len:\n            if prompt_type == \"encoder\" and model_config.is_multimodal_model:\n                mm_registry = self.input_preprocessor.mm_registry\n                mm_processor = mm_registry.create_processor(\n                    model_config,\n                    tokenizer=tokenizer or object(),  # Dummy if no tokenizer\n                )\n                assert isinstance(mm_processor, EncDecMultiModalProcessor)\n\n                if mm_processor.pad_dummy_encoder_prompt:\n                    return  # Skip encoder length check for Whisper\n\n            if model_config.is_multimodal_model:\n                suggestion = (\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens plus multimodal tokens. For image \"\n                    \"inputs, the number of image tokens depends on the number \"\n                    \"of images, and possibly their aspect ratios as well.\")\n            else:\n                suggestion = (\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens.\")\n\n            raise ValueError(\n                f\"The {prompt_type} prompt (length {len(prompt_ids)}) is \"\n                f\"longer than the maximum model length of {max_prompt_len}. \"\n                f\"{suggestion}\")\n\n            # TODO: Find out how many placeholder tokens are there so we can\n            # check that chunked prefill does not truncate them\n            # max_batch_len = self.scheduler_config.max_num_batched_tokens\n\n    def _build_logits_processors(\n            self, sampling_params: SamplingParams,\n            lora_request: Optional[LoRARequest]) -> SamplingParams:\n        \"\"\"Constructs logits processors based on the guided_decoding,\n        logits_bias, and allowed_token_ids fields in sampling_params. Deletes\n        those fields and adds the constructed logits processors to the\n        logits_processors field. Returns the modified sampling params.\"\"\"\n\n        logits_processors = []\n\n        if sampling_params.guided_decoding is not None:\n            # Defensively copy sampling params since guided decoding logits\n            # processors can have different state for each request\n            sampling_params = copy.copy(sampling_params)\n            guided_decoding = sampling_params.guided_decoding\n\n            logger.debug(\n                \"Building guided decoding logits processor in \"\n                \"LLMEngine. Params: %s\", guided_decoding)\n\n            tokenizer = self.get_tokenizer(lora_request=lora_request)\n            guided_decoding.backend = guided_decoding.backend or \\\n                self.decoding_config.backend\n\n            if self.decoding_config.reasoning_backend:\n                logger.debug(\"Building with reasoning backend %s\",\n                             self.decoding_config.reasoning_backend)\n\n            processor = get_local_guided_decoding_logits_processor(\n                guided_params=guided_decoding,\n                tokenizer=tokenizer,\n                model_config=self.model_config,\n                reasoning_backend=self.decoding_config.reasoning_backend,\n            )\n            if processor:\n                logits_processors.append(processor)\n\n            # Unset so this doesn't get passed down to the model\n            sampling_params.guided_decoding = None\n\n        if (sampling_params.logit_bias or sampling_params.allowed_token_ids):\n            tokenizer = self.get_tokenizer(lora_request=lora_request)\n\n            processors = get_openai_logits_processors(\n                logit_bias=sampling_params.logit_bias,\n                allowed_token_ids=sampling_params.allowed_token_ids,\n                tokenizer=tokenizer)\n            logits_processors.extend(processors)\n\n            # Unset so these don't get passed down to the model\n            sampling_params.logit_bias = None\n            sampling_params.allowed_token_ids = None\n\n        if len(sampling_params.bad_words) > 0:\n            tokenizer = self.get_tokenizer(lora_request)\n            processors = get_bad_words_logits_processors(\n                bad_words=sampling_params.bad_words, tokenizer=tokenizer)\n            logits_processors.extend(processors)\n\n        if logits_processors:\n            if sampling_params.logits_processors is None:\n                sampling_params.logits_processors = logits_processors\n            else:\n                sampling_params.logits_processors.extend(logits_processors)\n\n        return sampling_params\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.model_executor.collective_rpc(method, timeout, args,\n                                                  kwargs)\n\n\nif envs.is_set(\"VLLM_USE_V1\") and envs.VLLM_USE_V1:\n    from vllm.v1.engine.llm_engine import LLMEngine as V1LLMEngine\n    LLMEngine = V1LLMEngine  # type: ignore\n", 2151]}, "functions": {"LLM.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:160)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 160], "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)": ["/home/jeromeku/vllm/vllm/platforms/__init__.py", 264], "Platform.pre_register_and_update (/home/jeromeku/vllm/vllm/platforms/interface.py:268)": ["/home/jeromeku/vllm/vllm/platforms/interface.py", 268], "check_gguf_file (/home/jeromeku/vllm/vllm/transformers_utils/utils.py:19)": ["/home/jeromeku/vllm/vllm/transformers_utils/utils.py", 19], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:750)": ["/home/jeromeku/vllm/vllm/envs.py", 750], "__getattr__ (/home/jeromeku/vllm/vllm/envs.py:819)": ["/home/jeromeku/vllm/vllm/envs.py", 819], "EngineArgs.create_model_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:861)": ["/home/jeromeku/vllm/vllm/engine/arg_utils.py", 861], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:589)": ["/home/jeromeku/vllm/vllm/envs.py", 589], "Platform.is_cuda (/home/jeromeku/vllm/vllm/platforms/interface.py:135)": ["/home/jeromeku/vllm/vllm/platforms/interface.py", 135], "_LoadNvmlLibrary (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2394)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2394], "_nvmlGetFunctionPointer (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1051)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 1051], "_nvmlCheckReturn (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1044)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 1044], "nvmlInitWithFlags (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2373)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2373], "nvmlInit (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2390)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2390], "Platform.device_id_to_physical_device_id (/home/jeromeku/vllm/vllm/platforms/interface.py:166)": ["/home/jeromeku/vllm/vllm/platforms/interface.py", 166], "nvmlDeviceGetHandleByIndex (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2603)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2603], "nvmlDeviceGetCudaComputeCapability (/home/jeromeku/vllm/vllm/third_party/pynvml.py:3217)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 3217], "NvmlCudaPlatform.get_device_capability (/home/jeromeku/vllm/vllm/platforms/cuda.py:321)": ["/home/jeromeku/vllm/vllm/platforms/cuda.py", 321], "nvmlShutdown (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2428)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2428], "with_nvml_context.<locals>.wrapper (/home/jeromeku/vllm/vllm/platforms/cuda.py:38)": ["/home/jeromeku/vllm/vllm/platforms/cuda.py", 38], "_ModelRegistry._normalize_archs.<locals>.<lambda> (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:452)": ["/home/jeromeku/vllm/vllm/model_executor/models/registry.py", 452], "_ModelRegistry._normalize_archs (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:441)": ["/home/jeromeku/vllm/vllm/model_executor/models/registry.py", 441], "_ModelRegistry._try_inspect_model_cls (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:435)": ["/home/jeromeku/vllm/vllm/model_executor/models/registry.py", 435], "_ModelRegistry.inspect_model_cls (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:459)": ["/home/jeromeku/vllm/vllm/model_executor/models/registry.py", 459], "_ModelRegistry.is_v1_compatible (/home/jeromeku/vllm/vllm/model_executor/models/registry.py:555)": ["/home/jeromeku/vllm/vllm/model_executor/models/registry.py", 555], "ModelConfig.is_v1_compatible (/home/jeromeku/vllm/vllm/config.py:1369)": ["/home/jeromeku/vllm/vllm/config.py", 1369], "is_set (/home/jeromeku/vllm/vllm/envs.py:830)": ["/home/jeromeku/vllm/vllm/envs.py", 830], "CudaPlatformBase.supports_v1 (/home/jeromeku/vllm/vllm/platforms/cuda.py:306)": ["/home/jeromeku/vllm/vllm/platforms/cuda.py", 306], "Platform.is_tpu (/home/jeromeku/vllm/vllm/platforms/interface.py:141)": ["/home/jeromeku/vllm/vllm/platforms/interface.py", 141], "EngineArgs._is_v1_supported_oracle (/home/jeromeku/vllm/vllm/engine/arg_utils.py:1190)": ["/home/jeromeku/vllm/vllm/engine/arg_utils.py", 1190], "set_vllm_use_v1 (/home/jeromeku/vllm/vllm/envs.py:837)": ["/home/jeromeku/vllm/vllm/envs.py", 837], "nvmlDeviceGetMemoryInfo (/home/jeromeku/vllm/vllm/third_party/pynvml.py:3191)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 3191], "_PrintableStructure.__getattribute__ (/home/jeromeku/vllm/vllm/third_party/pynvml.py:1142)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 1142], "NvmlCudaPlatform.get_device_total_memory (/home/jeromeku/vllm/vllm/platforms/cuda.py:359)": ["/home/jeromeku/vllm/vllm/platforms/cuda.py", 359], "convertStrBytes.<locals>.wrapper.<locals>.<listcomp> (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2345)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2345], "nvmlDeviceGetName (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2638)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2638], "convertStrBytes.<locals>.wrapper (/home/jeromeku/vllm/vllm/third_party/pynvml.py:2342)": ["/home/jeromeku/vllm/vllm/third_party/pynvml.py", 2342], "NvmlCudaPlatform._get_physical_device_name (/home/jeromeku/vllm/vllm/platforms/cuda.py:393)": ["/home/jeromeku/vllm/vllm/platforms/cuda.py", 393], "NvmlCudaPlatform.get_device_name (/home/jeromeku/vllm/vllm/platforms/cuda.py:346)": ["/home/jeromeku/vllm/vllm/platforms/cuda.py", 346], "EngineArgs._set_default_args_v1 (/home/jeromeku/vllm/vllm/engine/arg_utils.py:1467)": ["/home/jeromeku/vllm/vllm/engine/arg_utils.py", 1467], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:387)": ["/home/jeromeku/vllm/vllm/envs.py", 387], "ModelConfig.get_hf_config_sliding_window (/home/jeromeku/vllm/vllm/config.py:1043)": ["/home/jeromeku/vllm/vllm/config.py", 1043], "ModelConfig.get_sliding_window (/home/jeromeku/vllm/vllm/config.py:1055)": ["/home/jeromeku/vllm/vllm/config.py", 1055], "is_in_ray_actor (/home/jeromeku/vllm/vllm/utils.py:2494)": ["/home/jeromeku/vllm/vllm/utils.py", 2494], "EngineArgs.create_speculative_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:928)": ["/home/jeromeku/vllm/vllm/engine/arg_utils.py", 928], "ModelConfig.runner_type (/home/jeromeku/vllm/vllm/config.py:1365)": ["/home/jeromeku/vllm/vllm/config.py", 1365], "ModelConfig.is_multimodal_model (/home/jeromeku/vllm/vllm/config.py:1349)": ["/home/jeromeku/vllm/vllm/config.py", 1349], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:424)": ["/home/jeromeku/vllm/vllm/envs.py", 424], "EngineArgs.create_load_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:914)": ["/home/jeromeku/vllm/vllm/engine/arg_utils.py", 914], "EngineArgs.create_engine_config (/home/jeromeku/vllm/vllm/engine/arg_utils.py:960)": ["/home/jeromeku/vllm/vllm/engine/arg_utils.py", 960], "Executor.get_class (/home/jeromeku/vllm/vllm/v1/executor/abstract.py:26)": ["/home/jeromeku/vllm/vllm/v1/executor/abstract.py", 26], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:664)": ["/home/jeromeku/vllm/vllm/envs.py", 664], "get_cached_tokenizer.<locals>.CachedTokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:107)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py", 107], "get_cached_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:83)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py", 83], "get_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:160)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py", 160], "LRUCache.__init__ (/home/jeromeku/vllm/vllm/utils.py:258)": ["/home/jeromeku/vllm/vllm/utils.py", 258], "TokenizerGroup.__init__ (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:17)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py", 17], "init_tokenizer_from_configs (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:107)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py", 107], "try_get_generation_config (/home/jeromeku/vllm/vllm/transformers_utils/config.py:801)": ["/home/jeromeku/vllm/vllm/transformers_utils/config.py", 801], "ModelConfig.try_get_generation_config (/home/jeromeku/vllm/vllm/config.py:1275)": ["/home/jeromeku/vllm/vllm/config.py", 1275], "InputPreprocessor.__init__ (/home/jeromeku/vllm/vllm/inputs/preprocess.py:30)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 30], "ProcessingCache.get_lru_cache (/home/jeromeku/vllm/vllm/multimodal/processing.py:894)": ["/home/jeromeku/vllm/vllm/multimodal/processing.py", 894], "MirroredProcessingCache.__init__ (/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py:35)": ["/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py", 35], "Processor.__init__ (/home/jeromeku/vllm/vllm/v1/engine/processor.py:31)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 31], "LoRARequestStates.__init__ (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:185)": ["/home/jeromeku/vllm/vllm/v1/metrics/stats.py", 185], "OutputProcessor.__init__ (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:231)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 231], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:795)": ["/home/jeromeku/vllm/vllm/envs.py", 795], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:801)": ["/home/jeromeku/vllm/vllm/envs.py", 801], "MsgpackEncoder.__init__ (/home/jeromeku/vllm/vllm/v1/serial_utils.py:58)": ["/home/jeromeku/vllm/vllm/v1/serial_utils.py", 58], "MsgpackDecoder.__init__ (/home/jeromeku/vllm/vllm/v1/serial_utils.py:203)": ["/home/jeromeku/vllm/vllm/v1/serial_utils.py", 203], "CoreEngine.__init__ (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:286)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 286], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:261)": ["/home/jeromeku/vllm/vllm/envs.py", 261], "get_open_zmq_ipc_path (/home/jeromeku/vllm/vllm/utils.py:626)": ["/home/jeromeku/vllm/vllm/utils.py", 626], "MPClient._get_zmq_addresses (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:441)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 441], "split_zmq_path (/home/jeromeku/vllm/vllm/utils.py:2378)": ["/home/jeromeku/vllm/vllm/utils.py", 2378], "make_zmq_socket (/home/jeromeku/vllm/vllm/utils.py:2418)": ["/home/jeromeku/vllm/vllm/utils.py", 2418], "_maybe_force_spawn (/home/jeromeku/vllm/vllm/utils.py:2505)": ["/home/jeromeku/vllm/vllm/utils.py", 2505], "<lambda> (/home/jeromeku/vllm/vllm/envs.py:456)": ["/home/jeromeku/vllm/vllm/envs.py", 456], "get_mp_context (/home/jeromeku/vllm/vllm/utils.py:2533)": ["/home/jeromeku/vllm/vllm/utils.py", 2533], "CoreEngineProcManager.finished_procs.<locals>.<dictcomp> (/home/jeromeku/vllm/vllm/v1/utils.py:161)": ["/home/jeromeku/vllm/vllm/v1/utils.py", 161], "CoreEngineProcManager.finished_procs (/home/jeromeku/vllm/vllm/v1/utils.py:159)": ["/home/jeromeku/vllm/vllm/v1/utils.py", 159], "CoreEngineProcManager.__init__ (/home/jeromeku/vllm/vllm/v1/utils.py:104)": ["/home/jeromeku/vllm/vllm/v1/utils.py", 104], "CoreEngineProcManager.sentinels.<locals>.<listcomp> (/home/jeromeku/vllm/vllm/v1/utils.py:157)": ["/home/jeromeku/vllm/vllm/v1/utils.py", 157], "CoreEngineProcManager.sentinels (/home/jeromeku/vllm/vllm/v1/utils.py:156)": ["/home/jeromeku/vllm/vllm/v1/utils.py", 156], "MPClient._wait_for_engine_startup.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:501)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 501], "MsgpackEncoder.encode (/home/jeromeku/vllm/vllm/v1/serial_utils.py:70)": ["/home/jeromeku/vllm/vllm/v1/serial_utils.py", 70], "MPClient._wait_for_engine_startup (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:460)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 460], "MPClient.__init__ (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:355)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 355], "get_open_zmq_inproc_path (/home/jeromeku/vllm/vllm/utils.py:631)": ["/home/jeromeku/vllm/vllm/utils.py", 631], "SyncMPClient.__init__ (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:587)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 587], "EngineCoreClient.make_client (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:62)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 62], "Processor.mm_registry (/home/jeromeku/vllm/vllm/v1/engine/processor.py:57)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 57], "LRUCache.clear (/home/jeromeku/vllm/vllm/utils.py:427)": ["/home/jeromeku/vllm/vllm/utils.py", 427], "ProcessingCache.reset (/home/jeromeku/vllm/vllm/multimodal/processing.py:1032)": ["/home/jeromeku/vllm/vllm/multimodal/processing.py", 1032], "MultiModalRegistry.reset_processor_cache (/home/jeromeku/vllm/vllm/multimodal/registry.py:91)": ["/home/jeromeku/vllm/vllm/multimodal/registry.py", 91], "MirroredProcessingCache.reset (/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py:87)": ["/home/jeromeku/vllm/vllm/v1/engine/mm_input_cache.py", 87], "MPClient.ensure_alive (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:561)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 561], "MPClient.free_pending_messages (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:569)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 569], "SyncMPClient._send_input (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:659)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 659], "SyncMPClient.call_utility (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:674)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 674], "SyncMPClient.reset_mm_cache (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:693)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 693], "LLMEngine.reset_mm_cache (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:246)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 246], "LLMEngine.__init__ (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:40)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 40], "LLMEngine.from_vllm_config (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:107)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 107], "LLMEngine.from_engine_args (/home/jeromeku/vllm/vllm/engine/llm_engine.py:495)": ["/home/jeromeku/vllm/vllm/engine/llm_engine.py", 495], "Counter.__init__ (/home/jeromeku/vllm/vllm/utils.py:213)": ["/home/jeromeku/vllm/vllm/utils.py", 213], "LLM.__init__ (/home/jeromeku/vllm/vllm/entrypoints/llm.py:158)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 158], "deprecate_args.<locals>.wrapper.<locals>.inner (/home/jeromeku/vllm/vllm/utils.py:1161)": ["/home/jeromeku/vllm/vllm/utils.py", 1161]}}}