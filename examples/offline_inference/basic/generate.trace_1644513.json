{"traceEvents": [{"ph": "M", "pid": 1644513, "tid": 1644513, "name": "process_name", "args": {"name": "MainProcess"}}, {"ph": "M", "pid": 1644513, "tid": 1644857, "name": "thread_name", "args": {"name": "Thread-1"}}, {"ph": "M", "pid": 1644513, "tid": 1644853, "name": "thread_name", "args": {"name": "EngineCoreOutputQueueThread"}}, {"ph": "M", "pid": 1644513, "tid": 1644513, "name": "thread_name", "args": {"name": "MainThread"}}, {"pid": 1644513, "tid": 1644513, "ts": 13815205531774.44, "ph": "X", "dur": 1.2837963207279668, "name": "LLM.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:382)", "args": {"func_args": {}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205531804.566, "ph": "X", "dur": 34.7472616927002, "name": "ModelConfig.runner_type (/home/jeromeku/vllm/vllm/config.py:1365)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "'generate'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205531948.314, "ph": "X", "dur": 5.2494698218490745, "name": "LLM._add_guided_params (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1422)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "guided_options": "None"}, "return_value": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533741.385, "ph": "X", "dur": 2.1056926299180523, "name": "Counter.__next__ (/home/jeromeku/vllm/vllm/utils.py:216)", "args": {"func_args": {"self": "<vllm.utils.Counter object at 0x7f05b9d63f10>"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533780.512, "ph": "X", "dur": 1.0247513658036294, "name": "Processor._validate_lora (/home/jeromeku/vllm/vllm/v1/engine/processor.py:145)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533791.922, "ph": "X", "dur": 5.778035814433661, "name": "Processor._validate_logprobs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:61)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533808.89, "ph": "X", "dur": 3.6628194729375076, "name": "Processor._validate_structured_output (/home/jeromeku/vllm/vllm/v1/engine/processor.py:150)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533814.934, "ph": "X", "dur": 4.519001143808756, "name": "Processor._validate_logit_bias (/home/jeromeku/vllm/vllm/v1/engine/processor.py:96)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533800.43, "ph": "X", "dur": 19.563608323734346, "name": "Processor._validate_sampling_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:78)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533821.635, "ph": "X", "dur": 4.104719690161378, "name": "Processor._validate_supported_sampling_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:116)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533783.453, "ph": "X", "dur": 42.595752404212945, "name": "Processor._validate_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:128)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533859.123, "ph": "X", "dur": 424.5289673054143, "name": "is_encoder_decoder (/home/jeromeku/vllm/vllm/transformers_utils/config.py:254)", "args": {"func_args": {"config": "OPTConfig {\n  \"_remove_final_layer_norm\": false,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"relu\",\n  \"architectures\": [\n    \"OPTForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"do_layer_norm_before\": true,\n  \"dropout\": 0.1,\n  \"enable_bias\": true,\n  \"eos_token_id\": 2,\n  \"ffn_dim\": 3072,\n  \"hidden_size\": 768,\n  \"init_std\": 0.02,\n  \"layer_norm_elementwise_affine\": true,\n  \"layerdrop\": 0.0,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"opt\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"prefix\": \"</s>\",\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50272,\n  \"word_embed_proj_dim\": 768\n}\n"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533836.469, "ph": "X", "dur": 447.3944464332762, "name": "ModelConfig.is_encoder_decoder (/home/jeromeku/vllm/vllm/config.py:1340)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534285.215, "ph": "X", "dur": 1.523793852496103, "name": "is_explicit_encoder_decoder_prompt (/home/jeromeku/vllm/vllm/inputs/parse.py:135)", "args": {"func_args": {"prompt": "'Hello, my name is'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534298.223, "ph": "X", "dur": 3.059968530043737, "name": "parse_singleton_prompt (/home/jeromeku/vllm/vllm/inputs/parse.py:117)", "args": {"func_args": {"prompt": "'Hello, my name is'"}, "return_value": "{'type': 'str', 'content': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534315.275, "ph": "X", "dur": 1.2437967320999441, "name": "InputPreprocessor.get_tokenizer_group (/home/jeromeku/vllm/vllm/inputs/preprocess.py:42)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>"}, "return_value": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534319.676, "ph": "X", "dur": 6.367553561118091, "name": "InputPreprocessor._get_tokenization_kw (/home/jeromeku/vllm/vllm/inputs/preprocess.py:182)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "overrides": "None"}, "return_value": "{}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534333.998, "ph": "X", "dur": 1.2895105476748272, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534398.502, "ph": "X", "dur": 787.000477596767, "name": "encode_tokens (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:54)", "args": {"func_args": {"tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "text": "'Hello, my name is'", "truncation": "None", "max_length": "None", "add_special_tokens": "None"}, "return_value": "[2, 31414, 6, 127, 766, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535190.316, "ph": "X", "dur": 6.627550887200238, "name": "TokenizerGroup._raise_if_input_too_long (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:34)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "encoded_tokens": "[2, 31414, 6, 127, 766, 16]", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534329.668, "ph": "X", "dur": 867.4967925971894, "name": "TokenizerGroup.encode (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:46)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "prompt": "'Hello, my name is'", "max_length": "None", "truncation": "None", "lora_request": "None", "add_special_tokens": "None"}, "return_value": "[2, 31414, 6, 127, 766, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534313.178, "ph": "X", "dur": 885.1118495320443, "name": "InputPreprocessor._tokenize_prompt (/home/jeromeku/vllm/vllm/inputs/preprocess.py:199)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'Hello, my name is'", "lora_request": "None", "tokenization_kwargs": "None"}, "return_value": "[2, 31414, 6, 127, 766, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535204.1, "ph": "X", "dur": 4.353288562349804, "name": "token_inputs (/home/jeromeku/vllm/vllm/inputs/data.py:186)", "args": {"func_args": {"prompt_token_ids": "[2, 31414, 6, 127, 766, 16]", "token_type_ids": "None", "prompt": "'Hello, my name is'", "cache_salt": "None"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534307.627, "ph": "X", "dur": 903.5830881377705, "name": "InputPreprocessor._process_text (/home/jeromeku/vllm/vllm/inputs/preprocess.py:389)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "parsed_content": "{'prompt': 'Hello, my name is'}", "tokenization_kwargs": "None", "lora_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534294.414, "ph": "X", "dur": 918.4096023225575, "name": "InputPreprocessor._prompt_to_llm_inputs (/home/jeromeku/vllm/vllm/inputs/preprocess.py:457)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'Hello, my name is'", "tokenization_kwargs": "None", "lora_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535266.98, "ph": "X", "dur": 1.579983750806897, "name": "InputPreprocessor._apply_prompt_adapter (/home/jeromeku/vllm/vllm/inputs/preprocess.py:170)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt_token_ids": "[2, 31414, 6, 127, 766, 16]", "prompt_adapter_request": "None"}, "return_value": "[2, 31414, 6, 127, 766, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535216.795, "ph": "X", "dur": 52.61564935553263, "name": "InputPreprocessor._build_decoder_only_llm_inputs (/home/jeromeku/vllm/vllm/inputs/preprocess.py:756)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt_inputs": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}", "prompt_adapter_request": "None"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205534288.77, "ph": "X", "dur": 981.7156179245081, "name": "InputPreprocessor._process_decoder_only_prompt (/home/jeromeku/vllm/vllm/inputs/preprocess.py:771)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'Hello, my name is'", "tokenization_kwargs": "None", "lora_request": "None", "prompt_adapter_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533831.516, "ph": "X", "dur": 1440.1756648403796, "name": "InputPreprocessor.preprocess (/home/jeromeku/vllm/vllm/inputs/preprocess.py:828)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'Hello, my name is'", "tokenization_kwargs": "None", "lora_request": "None", "prompt_adapter_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535287.332, "ph": "X", "dur": 1.686649320481624, "name": "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)", "args": {"func_args": {"name": "'current_platform'"}, "return_value": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535298.818, "ph": "X", "dur": 12.041780919350455, "name": "Platform.validate_request (/home/jeromeku/vllm/vllm/platforms/interface.py:456)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "prompt": "'Hello, my name is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "processed_inputs": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535315.047, "ph": "X", "dur": 1.1657022971595188, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535313.18, "ph": "X", "dur": 50.644241058865795, "name": "InputPreprocessor.get_eos_token_id (/home/jeromeku/vllm/vllm/inputs/preprocess.py:59)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "lora_request": "None"}, "return_value": "2"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535370.38, "ph": "X", "dur": 1.9142660271982295, "name": "split_enc_dec_inputs (/home/jeromeku/vllm/vllm/inputs/parse.py:140)", "args": {"func_args": {"inputs": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "return_value": "(None, {'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535382.762, "ph": "X", "dur": 0.8409437323462868, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535405.094, "ph": "X", "dur": 21.017879081710312, "name": "get_cached_tokenizer.<locals>.CachedTokenizer.max_token_id (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:121)", "args": {"func_args": {"self": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "return_value": "50265"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535379.354, "ph": "X", "dur": 48.10998140793321, "name": "Processor._validate_model_input (/home/jeromeku/vllm/vllm/v1/engine/processor.py:346)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "prompt_inputs": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}", "lora_request": "None", "prompt_type": "'decoder'"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535366.13, "ph": "X", "dur": 61.732698449248375, "name": "Processor._validate_model_inputs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:332)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "inputs": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535428.43, "ph": "X", "dur": 1.8066480863656922, "name": "split_enc_dec_inputs (/home/jeromeku/vllm/vllm/inputs/parse.py:140)", "args": {"func_args": {"inputs": "{'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'}"}, "return_value": "(None, {'type': 'token', 'prompt_token_ids': [2, 31414, 6, 127, 766, 16], 'prompt': 'Hello, my name is'})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535438.73, "ph": "X", "dur": 113.4483570606505, "name": "SamplingParams.clone (/home/jeromeku/vllm/vllm/sampling_params.py:536)", "args": {"func_args": {"self": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535560.879, "ph": "X", "dur": 9.221809921074854, "name": "SamplingParams.update_from_generation_config (/home/jeromeku/vllm/vllm/sampling_params.py:457)", "args": {"func_args": {"self": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "generation_config": "{'pad_token_id': 1, 'bos_token_id': 2, 'eos_token_id': 2, '_from_model_config': True, 'transformers_version': '4.51.3'}", "model_eos_token_id": "2"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535571.162, "ph": "X", "dur": 1.1723688952641893, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535598.643, "ph": "X", "dur": 22.461673756950372, "name": "SamplingParams.update_from_tokenizer (/home/jeromeku/vllm/vllm/sampling_params.py:483)", "args": {"func_args": {"self": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533768.576, "ph": "X", "dur": 1855.5142504729265, "name": "Processor.process_inputs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:203)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "request_id": "'0'", "prompt": "'Hello, my name is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "arrival_time": "None", "lora_request": "None", "tokenization_kwargs": "None", "trace_headers": "None", "prompt_adapter_request": "None", "priority": "0"}, "return_value": "('Hello, my name is', EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0))"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535652.328, "ph": "X", "dur": 0.9066573422351814, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535699.865, "ph": "X", "dur": 27.619715947649677, "name": "LogprobsProcessor.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:34)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.logprobs.LogprobsProcessor'>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535887.344, "ph": "X", "dur": 1.03617981969735, "name": "IncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:22)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535879.963, "ph": "X", "dur": 10.693223359891403, "name": "BaseIncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:60)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535850.814, "ph": "X", "dur": 85.79245100900403, "name": "FastIncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:154)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535750.928, "ph": "X", "dur": 186.17713289797388, "name": "IncrementalDetokenizer.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:37)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.detokenizer.IncrementalDetokenizer'>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535942.225, "ph": "X", "dur": 35.17487634255692, "name": "RequestState.__init__ (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:74)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "request_id": "'0'", "parent_req": "None", "request_index": "0", "lora_name": "None", "output_kind": "<RequestOutputKind.FINAL_ONLY: 2>", "prompt": "'Hello, my name is'", "prompt_token_ids": "[2, 31414, 6, 127, 766, 16]", "logprobs_processor": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "detokenizer": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "max_tokens_param": "16", "arrival_time": "1747773181.6719882", "queue": "None", "log_stats": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535673.025, "ph": "X", "dur": 305.0282915368809, "name": "RequestState.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:107)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.output_processor.RequestState'>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)", "prompt": "'Hello, my name is'", "parent_req": "None", "request_index": "0", "queue": "None", "log_stats": "False"}, "return_value": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535984.268, "ph": "X", "dur": 1.1133218834799652, "name": "LoRARequestStates.get_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:188)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535981.188, "ph": "X", "dur": 4.532334340018097, "name": "LoRARequestStates.add_request (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:195)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535639.723, "ph": "X", "dur": 346.2897719490019, "name": "OutputProcessor.add_request (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:272)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)", "prompt": "'Hello, my name is'", "parent_req": "None", "request_index": "0", "queue": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536016.23, "ph": "X", "dur": 1.5752218950178465, "name": "MPClient.ensure_alive (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:561)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536018.244, "ph": "X", "dur": 1.5228414813382931, "name": "MPClient.free_pending_messages (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:569)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536025.809, "ph": "X", "dur": 13.651288176049464, "name": "MsgpackEncoder.encode (/home/jeromeku/vllm/vllm/v1/serial_utils.py:70)", "args": {"func_args": {"self": "<vllm.v1.serial_utils.MsgpackEncoder object at 0x7f05b8792890>", "obj": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "[b'\\x9b\\xa10\\x96\\x02\\xcdz\\xb6\\x06\\x7f\\xcd\\x02\\xfe\\x10\\xc0\\xc0\\xc0\\xde\\x00\\x07\\xabtemperature\\xcb?\\xe9\\x99\\x99\\x99\\x99\\x99\\x9a\\xa5top_p\\xcb?\\xeeffffff\\xa4stop\\x90\\xaestop_token_ids\\x90\\xaboutput_kind\\x02\\xb3_all_stop_token_ids\\x91\\x02\\xa9bad_words\\x90\\x02\\xcbA\\xda\\x0b9\\xbfk\\x01\\xdb\\xc0\\xc0\\x00']"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535997.844, "ph": "X", "dur": 81.95153812955606, "name": "SyncMPClient._send_input (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:659)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_type": "<EngineCoreRequestType.ADD: b'\\x00'>", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205535988.186, "ph": "X", "dur": 91.93334023456332, "name": "SyncMPClient.add_request (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:683)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request": "EngineCoreRequest(request_id='0', prompt_token_ids=[2, 31414, 6, 127, 766, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6719882, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533749.295, "ph": "X", "dur": 2331.0998355485385, "name": "LLMEngine.add_request (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:174)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>", "request_id": "'0'", "prompt": "'Hello, my name is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "arrival_time": "None", "lora_request": "None", "tokenization_kwargs": "None", "trace_headers": "None", "prompt_adapter_request": "None", "priority": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205533723.252, "ph": "X", "dur": 2359.1147855266795, "name": "LLM._add_request (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1402)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "prompt": "'Hello, my name is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "tokenization_kwargs": "None", "lora_request": "None", "prompt_adapter_request": "None", "priority": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536097.123, "ph": "X", "dur": 1.832362107626564, "name": "Counter.__next__ (/home/jeromeku/vllm/vllm/utils.py:216)", "args": {"func_args": {"self": "<vllm.utils.Counter object at 0x7f05b9d63f10>"}, "return_value": "1"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536113.781, "ph": "X", "dur": 0.7828490917198729, "name": "Processor._validate_lora (/home/jeromeku/vllm/vllm/v1/engine/processor.py:145)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536122.549, "ph": "X", "dur": 6.461838305741288, "name": "Processor._validate_logprobs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:61)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536136.521, "ph": "X", "dur": 5.820892516535114, "name": "Processor._validate_structured_output (/home/jeromeku/vllm/vllm/v1/engine/processor.py:150)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536143.01, "ph": "X", "dur": 5.608513748343469, "name": "Processor._validate_logit_bias (/home/jeromeku/vllm/vllm/v1/engine/processor.py:96)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536130.096, "ph": "X", "dur": 19.175993262505646, "name": "Processor._validate_sampling_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:78)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536150.213, "ph": "X", "dur": 6.018033346201797, "name": "Processor._validate_supported_sampling_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:116)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536115.371, "ph": "X", "dur": 41.15195772897288, "name": "Processor._validate_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:128)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536183.502, "ph": "X", "dur": 397.779718596003, "name": "is_encoder_decoder (/home/jeromeku/vllm/vllm/transformers_utils/config.py:254)", "args": {"func_args": {"config": "OPTConfig {\n  \"_remove_final_layer_norm\": false,\n  \"activation_dropout\": 0.0,\n  \"activation_function\": \"relu\",\n  \"architectures\": [\n    \"OPTForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 2,\n  \"do_layer_norm_before\": true,\n  \"dropout\": 0.1,\n  \"enable_bias\": true,\n  \"eos_token_id\": 2,\n  \"ffn_dim\": 3072,\n  \"hidden_size\": 768,\n  \"init_std\": 0.02,\n  \"layer_norm_elementwise_affine\": true,\n  \"layerdrop\": 0.0,\n  \"max_position_embeddings\": 2048,\n  \"model_type\": \"opt\",\n  \"num_attention_heads\": 12,\n  \"num_hidden_layers\": 12,\n  \"pad_token_id\": 1,\n  \"prefix\": \"</s>\",\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.51.3\",\n  \"use_cache\": true,\n  \"vocab_size\": 50272,\n  \"word_embed_proj_dim\": 768\n}\n"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536161.994, "ph": "X", "dur": 419.561399346277, "name": "ModelConfig.is_encoder_decoder (/home/jeromeku/vllm/vllm/config.py:1340)", "args": {"func_args": {"self": "ModelConfig(model='facebook/opt-125m', task='generate', tokenizer='facebook/opt-125m', tokenizer_mode='auto', trust_remote_code=False, dtype=torch.float16, seed=0, hf_config_path=None, allowed_local_media_path='', revision=None, code_revision=None, rope_scaling={}, rope_theta=None, tokenizer_revision=None, max_model_len=2048, spec_target_max_model_len=None, quantization=None, enforce_eager=False, max_seq_len_to_capture=2048, max_logprobs=20, disable_sliding_window=False, disable_cascade_attn=False, skip_tokenizer_init=False, enable_prompt_embeds=False, served_model_name='facebook/opt-125m', limit_mm_per_prompt={}, use_async_output_proc=True, config_format=<ConfigFormat.AUTO: 'auto'>, hf_token=None, hf_overrides=None, mm_processor_kwargs=None, disable_mm_preprocessor_cache=False, override_neuron_config={}, pooler_config=None, override_pooler_config=None, logits_processor_pattern=None, generation_config='auto', override_generation_config={}, enable_sleep_mode=False, model_impl='auto')"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536582.3, "ph": "X", "dur": 1.2152255973656423, "name": "is_explicit_encoder_decoder_prompt (/home/jeromeku/vllm/vllm/inputs/parse.py:135)", "args": {"func_args": {"prompt": "'The president of the United States is'"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536589.314, "ph": "X", "dur": 2.2818812941129143, "name": "parse_singleton_prompt (/home/jeromeku/vllm/vllm/inputs/parse.py:117)", "args": {"func_args": {"prompt": "'The president of the United States is'"}, "return_value": "{'type': 'str', 'content': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536598.902, "ph": "X", "dur": 0.8838004344477398, "name": "InputPreprocessor.get_tokenizer_group (/home/jeromeku/vllm/vllm/inputs/preprocess.py:42)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>"}, "return_value": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536600.812, "ph": "X", "dur": 4.361859902770095, "name": "InputPreprocessor._get_tokenization_kw (/home/jeromeku/vllm/vllm/inputs/preprocess.py:182)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "overrides": "None"}, "return_value": "{}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536609.13, "ph": "X", "dur": 0.8933241460258404, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536639.559, "ph": "X", "dur": 160.88120257538074, "name": "encode_tokens (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:54)", "args": {"func_args": {"tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "text": "'The president of the United States is'", "truncation": "None", "max_length": "None", "add_special_tokens": "None"}, "return_value": "[2, 133, 394, 9, 5, 315, 532, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536802.46, "ph": "X", "dur": 2.852351617641143, "name": "TokenizerGroup._raise_if_input_too_long (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:34)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "encoded_tokens": "[2, 133, 394, 9, 5, 315, 532, 16]", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536607.12, "ph": "X", "dur": 198.6522426941279, "name": "TokenizerGroup.encode (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:46)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "prompt": "'The president of the United States is'", "max_length": "None", "truncation": "None", "lora_request": "None", "add_special_tokens": "None"}, "return_value": "[2, 133, 394, 9, 5, 315, 532, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536597.582, "ph": "X", "dur": 209.27499058834138, "name": "InputPreprocessor._tokenize_prompt (/home/jeromeku/vllm/vllm/inputs/preprocess.py:199)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'The president of the United States is'", "lora_request": "None", "tokenization_kwargs": "None"}, "return_value": "[2, 133, 394, 9, 5, 315, 532, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536808.35, "ph": "X", "dur": 2.9694932700517813, "name": "token_inputs (/home/jeromeku/vllm/vllm/inputs/data.py:186)", "args": {"func_args": {"prompt_token_ids": "[2, 133, 394, 9, 5, 315, 532, 16]", "token_type_ids": "None", "prompt": "'The president of the United States is'", "cache_salt": "None"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536594.588, "ph": "X", "dur": 218.87774897254025, "name": "InputPreprocessor._process_text (/home/jeromeku/vllm/vllm/inputs/preprocess.py:389)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "parsed_content": "{'prompt': 'The president of the United States is'}", "tokenization_kwargs": "None", "lora_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536587.406, "ph": "X", "dur": 227.44147042356835, "name": "InputPreprocessor._prompt_to_llm_inputs (/home/jeromeku/vllm/vllm/inputs/preprocess.py:457)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'The president of the United States is'", "tokenization_kwargs": "None", "lora_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536821.73, "ph": "X", "dur": 1.3218911670403695, "name": "InputPreprocessor._apply_prompt_adapter (/home/jeromeku/vllm/vllm/inputs/preprocess.py:170)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt_token_ids": "[2, 133, 394, 9, 5, 315, 532, 16]", "prompt_adapter_request": "None"}, "return_value": "[2, 133, 394, 9, 5, 315, 532, 16]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536816.527, "ph": "X", "dur": 7.298972553456334, "name": "InputPreprocessor._build_decoder_only_llm_inputs (/home/jeromeku/vllm/vllm/inputs/preprocess.py:756)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt_inputs": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}", "prompt_adapter_request": "None"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536584.455, "ph": "X", "dur": 240.54704992619264, "name": "InputPreprocessor._process_decoder_only_prompt (/home/jeromeku/vllm/vllm/inputs/preprocess.py:771)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'The president of the United States is'", "tokenization_kwargs": "None", "lora_request": "None", "prompt_adapter_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536159.0, "ph": "X", "dur": 667.1883764596297, "name": "InputPreprocessor.preprocess (/home/jeromeku/vllm/vllm/inputs/preprocess.py:828)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "prompt": "'The president of the United States is'", "tokenization_kwargs": "None", "lora_request": "None", "prompt_adapter_request": "None", "return_mm_hashes": "True"}, "return_value": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536833.143, "ph": "X", "dur": 0.8161820822432252, "name": "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)", "args": {"func_args": {"name": "'current_platform'"}, "return_value": "<vllm.platforms.cuda.NvmlCudaPlatform object at 0x7f05deac9050>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536835.764, "ph": "X", "dur": 8.909432181313152, "name": "Platform.validate_request (/home/jeromeku/vllm/vllm/platforms/interface.py:456)", "args": {"func_args": {"cls": "<class 'vllm.platforms.cuda.NvmlCudaPlatform'>", "prompt": "'The president of the United States is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "processed_inputs": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536846.414, "ph": "X", "dur": 0.8485627016087673, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536845.29, "ph": "X", "dur": 35.76725120271478, "name": "InputPreprocessor.get_eos_token_id (/home/jeromeku/vllm/vllm/inputs/preprocess.py:59)", "args": {"func_args": {"self": "<vllm.inputs.preprocess.InputPreprocessor object at 0x7f05b9d740d0>", "lora_request": "None"}, "return_value": "2"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536884.72, "ph": "X", "dur": 1.9275992234075705, "name": "split_enc_dec_inputs (/home/jeromeku/vllm/vllm/inputs/parse.py:140)", "args": {"func_args": {"inputs": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "return_value": "(None, {'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536892.43, "ph": "X", "dur": 1.3923666327183142, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536912.582, "ph": "X", "dur": 15.132225326444113, "name": "get_cached_tokenizer.<locals>.CachedTokenizer.max_token_id (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:121)", "args": {"func_args": {"self": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "return_value": "50265"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536889.426, "ph": "X", "dur": 39.451022841124114, "name": "Processor._validate_model_input (/home/jeromeku/vllm/vllm/v1/engine/processor.py:346)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "prompt_inputs": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}", "lora_request": "None", "prompt_type": "'decoder'"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536881.678, "ph": "X", "dur": 47.535701599773745, "name": "Processor._validate_model_inputs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:332)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "inputs": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}", "lora_request": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536929.662, "ph": "X", "dur": 1.6209357105927298, "name": "split_enc_dec_inputs (/home/jeromeku/vllm/vllm/inputs/parse.py:140)", "args": {"func_args": {"inputs": "{'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'}"}, "return_value": "(None, {'type': 'token', 'prompt_token_ids': [2, 133, 394, 9, 5, 315, 532, 16], 'prompt': 'The president of the United States is'})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536933.592, "ph": "X", "dur": 68.08882355647275, "name": "SamplingParams.clone (/home/jeromeku/vllm/vllm/sampling_params.py:536)", "args": {"func_args": {"self": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "return_value": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537006.543, "ph": "X", "dur": 6.626598516042428, "name": "SamplingParams.update_from_generation_config (/home/jeromeku/vllm/vllm/sampling_params.py:457)", "args": {"func_args": {"self": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "generation_config": "{'pad_token_id': 1, 'bos_token_id': 2, 'eos_token_id': 2, '_from_model_config': True, 'transformers_version': '4.51.3'}", "model_eos_token_id": "2"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537013.996, "ph": "X", "dur": 1.051417758222311, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537032.54, "ph": "X", "dur": 20.894070831195005, "name": "SamplingParams.update_from_tokenizer (/home/jeromeku/vllm/vllm/sampling_params.py:483)", "args": {"func_args": {"self": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536106.93, "ph": "X", "dur": 948.4340554436776, "name": "Processor.process_inputs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:203)", "args": {"func_args": {"self": "<vllm.v1.engine.processor.Processor object at 0x7f05b8790310>", "request_id": "'1'", "prompt": "'The president of the United States is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "arrival_time": "None", "lora_request": "None", "tokenization_kwargs": "None", "trace_headers": "None", "prompt_adapter_request": "None", "priority": "0"}, "return_value": "('The president of the United States is', EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0))"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537074.113, "ph": "X", "dur": 0.7904680609823534, "name": "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)", "args": {"func_args": {"self": "<vllm.transformers_utils.tokenizer_group.TokenizerGroup object at 0x7f05b89e7710>", "lora_request": "None"}, "return_value": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537114.275, "ph": "X", "dur": 24.160703902483526, "name": "LogprobsProcessor.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:34)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.logprobs.LogprobsProcessor'>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537242.72, "ph": "X", "dur": 0.8314200207681862, "name": "IncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:22)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537236.617, "ph": "X", "dur": 7.977060817817099, "name": "BaseIncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:60)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537212.602, "ph": "X", "dur": 49.76996433599616, "name": "FastIncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:154)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537157.59, "ph": "X", "dur": 105.3208215998994, "name": "IncrementalDetokenizer.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:37)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.detokenizer.IncrementalDetokenizer'>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537265.19, "ph": "X", "dur": 29.38731681654516, "name": "RequestState.__init__ (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:74)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "request_id": "'1'", "parent_req": "None", "request_index": "0", "lora_name": "None", "output_kind": "<RequestOutputKind.FINAL_ONLY: 2>", "prompt": "'The president of the United States is'", "prompt_token_ids": "[2, 133, 394, 9, 5, 315, 532, 16]", "logprobs_processor": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "detokenizer": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "max_tokens_param": "16", "arrival_time": "1747773181.6743186", "queue": "None", "log_stats": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537092.295, "ph": "X", "dur": 202.94934135816692, "name": "RequestState.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:107)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.output_processor.RequestState'>", "tokenizer": "CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n)", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)", "prompt": "'The president of the United States is'", "parent_req": "None", "request_index": "0", "queue": "None", "log_stats": "False"}, "return_value": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537297.893, "ph": "X", "dur": 1.2237969377859328, "name": "LoRARequestStates.get_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:188)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537296.365, "ph": "X", "dur": 3.074254097410888, "name": "LoRARequestStates.add_request (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:195)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537065.377, "ph": "X", "dur": 234.40330358715994, "name": "OutputProcessor.add_request (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:272)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)", "prompt": "'The president of the United States is'", "parent_req": "None", "request_index": "0", "queue": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537314.658, "ph": "X", "dur": 1.1761783798954295, "name": "MPClient.ensure_alive (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:561)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537316.26, "ph": "X", "dur": 1.0380845620129704, "name": "MPClient.free_pending_messages (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:569)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537319.885, "ph": "X", "dur": 9.92275509322306, "name": "MsgpackEncoder.encode (/home/jeromeku/vllm/vllm/v1/serial_utils.py:70)", "args": {"func_args": {"self": "<vllm.v1.serial_utils.MsgpackEncoder object at 0x7f05b8792890>", "obj": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "[b'\\x9b\\xa11\\x98\\x02\\xcc\\x85\\xcd\\x01\\x8a\\t\\x05\\xcd\\x01;\\xcd\\x02\\x14\\x10\\xc0\\xc0\\xc0\\xde\\x00\\x07\\xabtemperature\\xcb?\\xe9\\x99\\x99\\x99\\x99\\x99\\x9a\\xa5top_p\\xcb?\\xeeffffff\\xa4stop\\x90\\xaestop_token_ids\\x90\\xaboutput_kind\\x02\\xb3_all_stop_token_ids\\x91\\x02\\xa9bad_words\\x90\\x02\\xcbA\\xda\\x0b9\\xbfk(\\t\\xc0\\xc0\\x00']"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537307.193, "ph": "X", "dur": 52.69374379047306, "name": "SyncMPClient._send_input (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:659)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_type": "<EngineCoreRequestType.ADD: b'\\x00'>", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537300.398, "ph": "X", "dur": 59.80319448352518, "name": "SyncMPClient.add_request (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:683)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request": "EngineCoreRequest(request_id='1', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], mm_inputs=None, mm_hashes=None, mm_placeholders=None, sampling_params=SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), eos_token_id=2, arrival_time=1747773181.6743186, lora_request=None, cache_salt=None, current_wave=0)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536100.307, "ph": "X", "dur": 1260.123230858282, "name": "LLMEngine.add_request (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:174)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>", "request_id": "'1'", "prompt": "'The president of the United States is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "arrival_time": "None", "lora_request": "None", "tokenization_kwargs": "None", "trace_headers": "None", "prompt_adapter_request": "None", "priority": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205536090.373, "ph": "X", "dur": 1271.0602612345729, "name": "LLM._add_request (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1402)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "prompt": "'The president of the United States is'", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "tokenization_kwargs": "None", "lora_request": "None", "prompt_adapter_request": "None", "priority": "0"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205531933.377, "ph": "X", "dur": 5594.752937484272, "name": "LLM._validate_and_add_requests (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1345)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "prompts": "['Hello, my name is', 'The president of the United States is']", "params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "use_tqdm": "True", "lora_request": "None", "prompt_adapter_request": "None", "tokenization_kwargs": "None", "guided_options": "None", "priority": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537547.094, "ph": "X", "dur": 1.3399862190387608, "name": "OutputProcessor.get_num_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:242)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "2"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537545.115, "ph": "X", "dur": 3.665676586410938, "name": "LLMEngine.get_num_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:148)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "2"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537731.53, "ph": "X", "dur": 1.0180847676989588, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537729.307, "ph": "X", "dur": 3.660914730621888, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537738.445, "ph": "X", "dur": 11324.429249266652, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[8966], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.548315257, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549087.22, "ph": "X", "dur": 3.1209202841435815, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[8966], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549103.742, "ph": "X", "dur": 8.019917519918552, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "8966"}, "return_value": "' Joel'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549098.48, "ph": "X", "dur": 14.283662624835346, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[8966]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549114.67, "ph": "X", "dur": 38.82912447507414, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[8966], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549156.758, "ph": "X", "dur": 2.7494955325976562, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[8966]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549162.719, "ph": "X", "dur": 1.2218921954703126, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549078.89, "ph": "X", "dur": 88.96670407798497, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[8966], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549172.824, "ph": "X", "dur": 1.1599880702126584, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537735.482, "ph": "X", "dur": 11438.73188325586, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549176.072, "ph": "X", "dur": 1.0457035312754508, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549174.998, "ph": "X", "dur": 2.352356759790859, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549178.7, "ph": "X", "dur": 26219.336064009553, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[6], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2288], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.57495169, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575414.371, "ph": "X", "dur": 3.225681111502688, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[6], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575422.14, "ph": "X", "dur": 4.778998469890903, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "6"}, "return_value": "','"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575419.494, "ph": "X", "dur": 8.304676496103763, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[6]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575428.576, "ph": "X", "dur": 32.29871544597052, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[6], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575461.467, "ph": "X", "dur": 2.318071398109697, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[6]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575465.084, "ph": "X", "dur": 2.481879237253028, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[2288], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575470.684, "ph": "X", "dur": 4.176147526997132, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "2288"}, "return_value": "' reportedly'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575468.527, "ph": "X", "dur": 6.965642648222811, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[2288]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575476.033, "ph": "X", "dur": 20.70169185731737, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[2288], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575497.117, "ph": "X", "dur": 1.5542697295460253, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[2288]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575499.352, "ph": "X", "dur": 0.88760991907898, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575407.639, "ph": "X", "dur": 94.1171272994218, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[6], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2288], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575503.586, "ph": "X", "dur": 0.901895486446131, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205549177.76, "ph": "X", "dur": 26326.975909378718, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575506.303, "ph": "X", "dur": 0.9142763114976619, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575505.47, "ph": "X", "dur": 1.979027265929314, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575508.805, "ph": "X", "dur": 2455.9014091814424, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[145], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.577527056, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205577976.797, "ph": "X", "dur": 2.2409293343270815, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205577981.912, "ph": "X", "dur": 3.2266334826604983, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "127"}, "return_value": "' my'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205577979.957, "ph": "X", "dur": 7.281829872615753, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[127]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205577987.656, "ph": "X", "dur": 22.999763461113055, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578011.035, "ph": "X", "dur": 1.614269112488059, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[127]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578013.514, "ph": "X", "dur": 5.057090847971442, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[145], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578021.096, "ph": "X", "dur": 3.677105040304659, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "145"}, "return_value": "' being'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578019.244, "ph": "X", "dur": 6.037080769357998, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[145]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578025.633, "ph": "X", "dur": 18.34457324173746, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[145], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578044.332, "ph": "X", "dur": 1.6276023086974, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[145]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578048.223, "ph": "X", "dur": 0.9123715691820418, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205577971.422, "ph": "X", "dur": 78.80109433952035, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[145], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578051.758, "ph": "X", "dur": 0.7057070279372577, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205575507.846, "ph": "X", "dur": 2544.7881140821714, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578053.955, "ph": "X", "dur": 0.73904001846061, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578053.174, "ph": "X", "dur": 1.7447439611080382, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578055.97, "ph": "X", "dur": 2169.289118723135, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[4252], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[6807], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.579822104, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580237.283, "ph": "X", "dur": 2.154263558966366, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[4252], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580242.39, "ph": "X", "dur": 3.609486688100144, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "4252"}, "return_value": "' dad'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580240.447, "ph": "X", "dur": 6.032318913568949, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[4252]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580246.861, "ph": "X", "dur": 19.62075059320295, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[4252], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580267.03, "ph": "X", "dur": 1.4466517887134878, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[4252]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580269.268, "ph": "X", "dur": 1.8399810768890446, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[6807], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580273.26, "ph": "X", "dur": 3.474249983691115, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "6807"}, "return_value": "' investigated'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580271.805, "ph": "X", "dur": 5.404706320572116, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[6807]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580277.592, "ph": "X", "dur": 18.02457653271328, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[6807], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580295.92, "ph": "X", "dur": 1.4990322023930414, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[6807]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580297.879, "ph": "X", "dur": 0.9657043540194052, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580232.35, "ph": "X", "dur": 67.34692642453871, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[4252], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[6807], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580300.809, "ph": "X", "dur": 0.5866606332109997, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205578055.234, "ph": "X", "dur": 2246.329278791865, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580302.732, "ph": "X", "dur": 0.7676111531949119, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580302.104, "ph": "X", "dur": 1.5999835451209083, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580304.559, "ph": "X", "dur": 2173.423361919188, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[16], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[30], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.582089083, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582489.4, "ph": "X", "dur": 2.540926249037252, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[16], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582495.22, "ph": "X", "dur": 3.313299258021214, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "16"}, "return_value": "' is'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582493.129, "ph": "X", "dur": 5.983747984520634, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[16]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582499.566, "ph": "X", "dur": 24.08737132333215, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[16], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582524.03, "ph": "X", "dur": 2.420927483153184, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[16]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582527.29, "ph": "X", "dur": 2.382832636840781, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[30], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582532.072, "ph": "X", "dur": 3.3485369908601865, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "30"}, "return_value": "' by'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582530.318, "ph": "X", "dur": 5.717084060333817, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[30]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582536.346, "ph": "X", "dur": 18.345525612895273, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[30], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582555.064, "ph": "X", "dur": 1.5485555025991649, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[30]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582557.086, "ph": "X", "dur": 0.8095154841385547, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582484.707, "ph": "X", "dur": 73.99638184836857, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[16], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[30], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582559.756, "ph": "X", "dur": 0.5847558908953796, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205580303.908, "ph": "X", "dur": 2256.556792655587, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582561.564, "ph": "X", "dur": 0.7342781626715597, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582560.902, "ph": "X", "dur": 1.539031791021064, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582563.396, "ph": "X", "dur": 2194.0164834645157, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.584374428, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584769.107, "ph": "X", "dur": 2.1495017031773154, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584773.863, "ph": "X", "dur": 2.473307896832737, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "127"}, "return_value": "' my'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584772.107, "ph": "X", "dur": 4.6570949616912145, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[127]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584777.082, "ph": "X", "dur": 19.82074853634306, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584797.316, "ph": "X", "dur": 1.390461890402694, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[127]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584799.34, "ph": "X", "dur": 1.8923614905685981, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584803.312, "ph": "X", "dur": 2.437117792835955, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "5"}, "return_value": "' the'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584801.855, "ph": "X", "dur": 4.351383820034184, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[5]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584806.545, "ph": "X", "dur": 17.220775275521586, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584824.13, "ph": "X", "dur": 1.388557148087074, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[5]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584825.963, "ph": "X", "dur": 0.8047536283495045, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584764.516, "ph": "X", "dur": 63.01173291418729, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[127], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584828.73, "ph": "X", "dur": 0.7618969262480515, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205582562.703, "ph": "X", "dur": 2266.9376382757164, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584830.834, "ph": "X", "dur": 0.6771358932029558, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584830.082, "ph": "X", "dur": 1.6028406585943384, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584832.666, "ph": "X", "dur": 2171.5281433151467, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[1441], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2448], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.586605076, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587015.396, "ph": "X", "dur": 2.0695025259212696, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[1441], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587020.582, "ph": "X", "dur": 3.1313963668794917, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "1441"}, "return_value": "' friend'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587018.69, "ph": "X", "dur": 5.571371273188877, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[1441]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587024.713, "ph": "X", "dur": 19.04742315620129, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[1441], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587044.217, "ph": "X", "dur": 1.420937767452616, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[1441]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587046.217, "ph": "X", "dur": 1.788553034367301, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[2448], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587050.361, "ph": "X", "dur": 2.879018010059825, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "2448"}, "return_value": "' FBI'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587048.785, "ph": "X", "dur": 4.8723308433562895, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[2448]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587053.97, "ph": "X", "dur": 17.638866213800203, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[2448], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587077.14, "ph": "X", "dur": 1.348557559459051, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[2448]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587078.873, "ph": "X", "dur": 0.7752301224573924, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587010.668, "ph": "X", "dur": 69.64690277065, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[1441], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2448], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587081.305, "ph": "X", "dur": 0.5990414582625305, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205584831.979, "ph": "X", "dur": 2250.061621359322, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587083.344, "ph": "X", "dur": 1.164749926001709, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587082.434, "ph": "X", "dur": 2.2142629419083995, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587086.23, "ph": "X", "dur": 2212.4791507298214, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[8], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[81], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.588920485, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589310.428, "ph": "X", "dur": 1.9380753061434812, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[8], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589315.148, "ph": "X", "dur": 3.559011016736211, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "8"}, "return_value": "' and'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589313.527, "ph": "X", "dur": 5.676132100547984, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[8]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589319.525, "ph": "X", "dur": 19.127422333457332, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[8], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589338.883, "ph": "X", "dur": 1.2942724034638775, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[8]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589340.773, "ph": "X", "dur": 2.0666454124478397, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[81], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589345.021, "ph": "X", "dur": 3.150443790035693, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "81"}, "return_value": "' over'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589343.434, "ph": "X", "dur": 5.082804869232314, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[81]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589348.809, "ph": "X", "dur": 17.540771984545763, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[81], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589366.707, "ph": "X", "dur": 1.3657002402996326, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[81]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589368.443, "ph": "X", "dur": 1.300939001568548, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589305.664, "ph": "X", "dur": 65.13361585378811, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[8], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[81], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589372.73, "ph": "X", "dur": 0.9133239403398518, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205587084.846, "ph": "X", "dur": 2288.9555070731276, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589375.05, "ph": "X", "dur": 0.7342781626715597, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589374.307, "ph": "X", "dur": 1.609507256699009, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589376.98, "ph": "X", "dur": 2166.090104004051, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[52], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.5911154, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591554.465, "ph": "X", "dur": 2.7552097595445164, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[52], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591560.248, "ph": "X", "dur": 3.3199658561258847, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "52"}, "return_value": "' we'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591558.426, "ph": "X", "dur": 5.722798287280677, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[52]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591564.467, "ph": "X", "dur": 22.6445290192499, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[52], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591587.406, "ph": "X", "dur": 1.5047464293399018, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[52]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591589.64, "ph": "X", "dur": 2.6218777974511074, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591594.562, "ph": "X", "dur": 2.709495943969633, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "5"}, "return_value": "' the'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591592.893, "ph": "X", "dur": 4.818045687361115, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[5]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591597.988, "ph": "X", "dur": 17.74838889694836, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591616.12, "ph": "X", "dur": 1.4076045712432752, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[5]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591617.895, "ph": "X", "dur": 0.8552292997134379, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591549.51, "ph": "X", "dur": 70.04689865693024, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[52], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[5], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591620.736, "ph": "X", "dur": 0.7342781626715597, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205589376.115, "ph": "X", "dur": 2245.5350012462513, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591622.84, "ph": "X", "dur": 0.679040635518576, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591622.021, "ph": "X", "dur": 1.596174060489668, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591624.574, "ph": "X", "dur": 2260.96531874509, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[32], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[1857], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.593423365, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593896.832, "ph": "X", "dur": 2.2409293343270815, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[32], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593902.168, "ph": "X", "dur": 3.175205440138755, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "32"}, "return_value": "' are'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593900.191, "ph": "X", "dur": 5.628513542657481, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[32]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593906.146, "ph": "X", "dur": 18.80742562443315, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[32], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593925.312, "ph": "X", "dur": 1.581888493122517, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[32]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593929.11, "ph": "X", "dur": 2.2285485092755506, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[1857], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593933.58, "ph": "X", "dur": 3.4961545203207467, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "1857"}, "return_value": "' allegations'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593931.957, "ph": "X", "dur": 5.5418477672967645, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[1857]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593937.799, "ph": "X", "dur": 18.680760260444416, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[1857], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593956.916, "ph": "X", "dur": 1.4752229234477898, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[1857]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593958.736, "ph": "X", "dur": 0.8066583706651246, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593892.053, "ph": "X", "dur": 68.22025077625054, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[32], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[1857], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593961.38, "ph": "X", "dur": 0.7657064108792917, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205591623.8, "ph": "X", "dur": 2338.495949960092, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593963.574, "ph": "X", "dur": 0.8238010515057057, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593962.799, "ph": "X", "dur": 1.7476010745814683, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593965.49, "ph": "X", "dur": 2202.6830610005873, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[11], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[14], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.595725317, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596179.66, "ph": "X", "dur": 2.5428309913528717, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[11], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596185.191, "ph": "X", "dur": 3.2028242037152466, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "11"}, "return_value": "' in'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596183.168, "ph": "X", "dur": 5.75803602011965, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[11]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596189.262, "ph": "X", "dur": 19.97503266390829, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[11], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596209.66, "ph": "X", "dur": 1.492365604288371, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[11]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596211.723, "ph": "X", "dur": 1.8656950981499163, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[14], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596215.824, "ph": "X", "dur": 3.309489773389974, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "14"}, "return_value": "' that'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596214.21, "ph": "X", "dur": 5.350421164576942, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[14]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596219.863, "ph": "X", "dur": 16.794112996822676, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[14], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596236.998, "ph": "X", "dur": 1.282843949570157, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[14]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596238.676, "ph": "X", "dur": 0.8999907441305108, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596174.908, "ph": "X", "dur": 65.32028060071889, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[11], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[14], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596241.238, "ph": "X", "dur": 0.6533266142577042, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205593964.754, "ph": "X", "dur": 2277.261341626378, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596243.098, "ph": "X", "dur": 0.6914214605701068, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596242.37, "ph": "X", "dur": 1.5714124103866063, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596244.879, "ph": "X", "dur": 2178.8080684454467, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[10], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[37], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.598028349, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598434.617, "ph": "X", "dur": 2.0628359278165993, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[10], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598439.229, "ph": "X", "dur": 3.280918638655672, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "10"}, "return_value": "' a'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598437.662, "ph": "X", "dur": 5.304707349002059, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[10]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598443.281, "ph": "X", "dur": 18.557904381086914, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[10], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598462.088, "ph": "X", "dur": 1.7533153015283285, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[10]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598464.436, "ph": "X", "dur": 2.273309953692624, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[37], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598468.775, "ph": "X", "dur": 3.0218736837313345, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "37"}, "return_value": "' he'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598467.28, "ph": "X", "dur": 4.927568370509273, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[37]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598472.564, "ph": "X", "dur": 19.18932645871499, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[37], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598492.084, "ph": "X", "dur": 1.3666526114574424, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[37]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598493.898, "ph": "X", "dur": 0.8895146613946001, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598430.027, "ph": "X", "dur": 65.53265936891053, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[10], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[37], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598496.861, "ph": "X", "dur": 0.6704692950982853, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205596244.148, "ph": "X", "dur": 2253.5311094872245, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598498.771, "ph": "X", "dur": 0.6752311508873358, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598498.088, "ph": "X", "dur": 1.5133177697601923, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598500.482, "ph": "X", "dur": 2196.060271969176, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[1291], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2263], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.600267867, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600709.787, "ph": "X", "dur": 2.3018810884269256, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[1291], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600714.701, "ph": "X", "dur": 3.589486893786133, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "1291"}, "return_value": "' relationship'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600713.055, "ph": "X", "dur": 5.725655400754107, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[1291]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600719.014, "ph": "X", "dur": 19.473133063742388, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[1291], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600738.963, "ph": "X", "dur": 1.3380814767231404, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[1291]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600740.885, "ph": "X", "dur": 1.9533132446684423, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[2263], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600744.941, "ph": "X", "dur": 2.984731208576742, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "2263"}, "return_value": "' broke'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600743.467, "ph": "X", "dur": 4.859950018304758, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[2263]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600748.67, "ph": "X", "dur": 17.260774864149607, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[2263], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600766.264, "ph": "X", "dur": 1.2676060110451959, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[2263]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600767.906, "ph": "X", "dur": 0.8314200207681862, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600705.043, "ph": "X", "dur": 64.4031471757478, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[1291], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2263], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600770.549, "ph": "X", "dur": 0.5752321793172789, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205598499.814, "ph": "X", "dur": 2271.4452109656313, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600772.531, "ph": "X", "dur": 0.8609435266602983, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600771.668, "ph": "X", "dur": 1.8437905615202848, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600774.354, "ph": "X", "dur": 2171.060529076662, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[637], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.602548072, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602956.03, "ph": "X", "dur": 1.9018852021466988, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602960.28, "ph": "X", "dur": 3.6123438015735743, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "4"}, "return_value": "'.'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602958.846, "ph": "X", "dur": 5.578990242451358, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[4]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602964.715, "ph": "X", "dur": 18.22266973353777, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602983.256, "ph": "X", "dur": 1.256177557151475, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[4]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602985.076, "ph": "X", "dur": 1.9761701524558837, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[637], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602989.148, "ph": "X", "dur": 3.756151846402894, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "637"}, "return_value": "' campaign'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602987.592, "ph": "X", "dur": 5.779940556749281, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[637]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602993.62, "ph": "X", "dur": 17.391249712769586, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[637], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603011.35, "ph": "X", "dur": 1.3742715807199228, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[637]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603013.084, "ph": "X", "dur": 0.7295163068825093, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205602951.393, "ph": "X", "dur": 63.078398895234, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[637], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603015.451, "ph": "X", "dur": 0.5361849618470662, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205600773.705, "ph": "X", "dur": 2242.4026525082136, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603017.078, "ph": "X", "dur": 0.547613415740787, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603016.453, "ph": "X", "dur": 1.2866534342013969, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603018.592, "ph": "X", "dur": 2178.2461694623385, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[38], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2879], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.604794813, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605207.955, "ph": "X", "dur": 1.8685522116233464, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[38], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605219.62, "ph": "X", "dur": 3.0456829626765862, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "38"}, "return_value": "' I'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605210.74, "ph": "X", "dur": 12.41225329973857, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[38]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605223.414, "ph": "X", "dur": 18.62457036213362, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[38], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605242.371, "ph": "X", "dur": 1.4123664270323257, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[38]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605244.326, "ph": "X", "dur": 1.935218192670051, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[2879], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605248.494, "ph": "X", "dur": 3.4085363738022205, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "2879"}, "return_value": "' finance'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605246.8, "ph": "X", "dur": 5.577085500135738, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[2879]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605252.793, "ph": "X", "dur": 17.32553610288069, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[2879], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605270.496, "ph": "X", "dur": 1.4285567367150966, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[2879]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605272.262, "ph": "X", "dur": 0.7904680609823534, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605203.389, "ph": "X", "dur": 70.38118093332156, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[38], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2879], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605274.719, "ph": "X", "dur": 0.5866606332109997, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205603017.943, "ph": "X", "dur": 2259.258669630294, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605278.379, "ph": "X", "dur": 0.5961843447891003, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605277.605, "ph": "X", "dur": 1.493317975446181, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605281.756, "ph": "X", "dur": 2218.4114706718206, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='0', new_token_ids=[524], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2074], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.607119588, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607518.408, "ph": "X", "dur": 2.699019861233723, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "engine_core_output": "EngineCoreOutput(request_id='0', new_token_ids=[524], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607524.064, "ph": "X", "dur": 2.912351000583177, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "next_token_id": "524"}, "return_value": "' am'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607522.379, "ph": "X", "dur": 5.02566259976371, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "new_token_ids": "[524]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607527.701, "ph": "X", "dur": 20.378838034819758, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='0', new_token_ids=[524], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607561.79, "ph": "X", "dur": 1.609507256699009, "name": "BaseIncrementalDetokenizer.get_next_output_text (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:135)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>", "finished": "True", "delta": "False"}, "return_value": "' Joel, my dad is my friend and we are in a relationship. I am'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607564.87, "ph": "X", "dur": 0.6514218719420841, "name": "IncrementalDetokenizer.output_token_ids (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:25)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b88cb750>"}, "return_value": "[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607570.322, "ph": "X", "dur": 5.657084677391783, "name": "FinishReason.__str__ (/home/jeromeku/vllm/vllm/v1/engine/__init__.py:37)", "args": {"func_args": {"self": "<FinishReason.LENGTH: 1>"}, "return_value": "'length'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607554.203, "ph": "X", "dur": 28.854941339329333, "name": "RequestState._new_completion_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:198)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "token_ids": "[524]", "finish_reason": "<FinishReason.LENGTH: 1>", "stop_reason": "None"}, "return_value": "CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607598.814, "ph": "X", "dur": 11.055124399859228, "name": "RequestOutput.__init__ (/home/jeromeku/vllm/vllm/outputs.py:109)", "args": {"func_args": {"self": "Not Displayable", "request_id": "'0'", "prompt": "'Hello, my name is'", "prompt_token_ids": "[2, 31414, 6, 127, 766, 16]", "prompt_logprobs": "None", "outputs": "[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]", "finished": "True", "metrics": "None", "lora_request": "None", "encoder_prompt": "None", "encoder_prompt_token_ids": "None", "num_cached_tokens": "None", "multi_modal_placeholders": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607589.84, "ph": "X", "dur": 20.600740514589504, "name": "RequestState._new_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:174)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "request_id": "'0'", "outputs": "[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]", "finished": "True", "kv_transfer_params": "None"}, "return_value": "RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607548.69, "ph": "X", "dur": 67.2231181740234, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "new_token_ids": "[524]", "finish_reason": "<FinishReason.LENGTH: 1>", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607622.574, "ph": "X", "dur": 2.9313984237393784, "name": "EngineCoreOutput.finished (/home/jeromeku/vllm/vllm/v1/engine/__init__.py:110)", "args": {"func_args": {"self": "EngineCoreOutput(request_id='0', new_token_ids=[524], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607628.09, "ph": "X", "dur": 2.0599788143431694, "name": "OutputProcessor._update_stats_from_finished (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:407)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8784bd0>", "finish_reason": "<FinishReason.LENGTH: 1>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607631.924, "ph": "X", "dur": 2.636163364818258, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[2074], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607637.191, "ph": "X", "dur": 3.8037704042933975, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "2074"}, "return_value": "' laws'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607635.295, "ph": "X", "dur": 6.263745104916794, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[2074]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607641.896, "ph": "X", "dur": 21.648348788180574, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[2074], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607663.871, "ph": "X", "dur": 1.597126431647478, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[2074]", "finish_reason": "None", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607665.979, "ph": "X", "dur": 0.8923717748680303, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607511.854, "ph": "X", "dur": 155.93268203939965, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='0', new_token_ids=[524], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None), EngineCoreOutput(request_id='1', new_token_ids=[2074], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=None, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607676.527, "ph": "X", "dur": 0.7180878529887885, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205605279.295, "ph": "X", "dur": 2398.1400984602624, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607716.824, "ph": "X", "dur": 2.146644589703885, "name": "LLM._run_engine.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1475)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b85c2e00>"}, "return_value": "16"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607719.207, "ph": "X", "dur": 1.1923686895782006, "name": "LLM._run_engine.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1475)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b85c2e00>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607749.691, "ph": "X", "dur": 0.9809422925443664, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607748.396, "ph": "X", "dur": 2.5142598566185703, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607752.09, "ph": "X", "dur": 2142.8084386802266, "name": "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>"}, "return_value": "EngineCoreOutputs(engine_index=0, outputs=[EngineCoreOutput(request_id='1', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)], scheduler_stats=None, timestamp=13815205.609504262, utility_output=None, finished_requests=None, wave_complete=None, start_wave=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609909.215, "ph": "X", "dur": 3.2428237923432692, "name": "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "engine_core_output": "EngineCoreOutput(request_id='1', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609915.842, "ph": "X", "dur": 3.969482985752349, "name": "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "next_token_id": "4"}, "return_value": "'.'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609913.47, "ph": "X", "dur": 6.977071102116532, "name": "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "new_token_ids": "[4]", "stop_terminated": "False"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609920.963, "ph": "X", "dur": 26.989246241179416, "name": "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)", "args": {"func_args": {"self": "LogprobsProcessor(tokenizer=CachedGPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='left', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n\t1: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n}\n), logprobs=None, prompt_logprobs=None, cumulative_logprob=None, num_logprobs=None, num_prompt_logprobs=None)", "output": "EngineCoreOutput(request_id='1', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609953.893, "ph": "X", "dur": 3.1142536860389103, "name": "BaseIncrementalDetokenizer.get_next_output_text (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:135)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>", "finished": "True", "delta": "False"}, "return_value": "' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609958.127, "ph": "X", "dur": 0.7238020799356489, "name": "IncrementalDetokenizer.output_token_ids (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:25)", "args": {"func_args": {"self": "<vllm.v1.engine.detokenizer.FastIncrementalDetokenizer object at 0x7f05b89399d0>"}, "return_value": "[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609961.684, "ph": "X", "dur": 2.2552149016942327, "name": "FinishReason.__str__ (/home/jeromeku/vllm/vllm/v1/engine/__init__.py:37)", "args": {"func_args": {"self": "<FinishReason.LENGTH: 1>"}, "return_value": "'length'"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609951.195, "ph": "X", "dur": 15.165558316967466, "name": "RequestState._new_completion_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:198)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "token_ids": "[4]", "finish_reason": "<FinishReason.LENGTH: 1>", "stop_reason": "None"}, "return_value": "CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609976.299, "ph": "X", "dur": 11.24369388910562, "name": "RequestOutput.__init__ (/home/jeromeku/vllm/vllm/outputs.py:109)", "args": {"func_args": {"self": "Not Displayable", "request_id": "'1'", "prompt": "'The president of the United States is'", "prompt_token_ids": "[2, 133, 394, 9, 5, 315, 532, 16]", "prompt_logprobs": "None", "outputs": "[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]", "finished": "True", "metrics": "None", "lora_request": "None", "encoder_prompt": "None", "encoder_prompt_token_ids": "None", "num_cached_tokens": "None", "multi_modal_placeholders": "None", "kv_transfer_params": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609971.06, "ph": "X", "dur": 16.929349701231704, "name": "RequestState._new_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:174)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "request_id": "'1'", "outputs": "[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)]", "finished": "True", "kv_transfer_params": "None"}, "return_value": "RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609948.596, "ph": "X", "dur": 44.79192029412295, "name": "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "new_token_ids": "[4]", "finish_reason": "<FinishReason.LENGTH: 1>", "stop_reason": "None", "kv_transfer_params": "None"}, "return_value": "RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609999.467, "ph": "X", "dur": 2.159025414755416, "name": "EngineCoreOutput.finished (/home/jeromeku/vllm/vllm/v1/engine/__init__.py:110)", "args": {"func_args": {"self": "EngineCoreOutput(request_id='1', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)"}, "return_value": "True"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610002.2, "ph": "X", "dur": 2.099026031813382, "name": "OutputProcessor._update_stats_from_finished (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:407)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "req_state": "<vllm.v1.engine.output_processor.RequestState object at 0x7f05b8939790>", "finish_reason": "<FinishReason.LENGTH: 1>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610004.89, "ph": "X", "dur": 0.8923717748680303, "name": "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)", "args": {"func_args": {"self": "<vllm.v1.metrics.stats.LoRARequestStates object at 0x7f05ba94a190>", "iteration_stats": "None"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205609903.592, "ph": "X", "dur": 103.18751020640485, "name": "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>", "engine_core_outputs": "[EngineCoreOutput(request_id='1', new_token_ids=[4], new_logprobs=None, new_prompt_logprobs_tensors=None, finish_reason=<FinishReason.LENGTH: 1>, stop_reason=None, events=None, kv_transfer_params=None)]", "engine_core_timestamp": "None", "iteration_stats": "None"}, "return_value": "OutputProcessorOutput(request_outputs=[RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})], reqs_to_abort=[])"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610016.72, "ph": "X", "dur": 0.9847517771756067, "name": "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)", "args": {"func_args": {"self": "<vllm.v1.engine.core_client.SyncMPClient object at 0x7f05b9dedb10>", "request_ids": "[]"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205607751.234, "ph": "X", "dur": 2266.7385927037344, "name": "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "[RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610040.137, "ph": "X", "dur": 1.2799868360967266, "name": "LLM._run_engine.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1475)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b85c2e00>"}, "return_value": "16"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610041.604, "ph": "X", "dur": 0.8799909498164995, "name": "LLM._run_engine.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1475)", "args": {"func_args": {".0": "<list_iterator object at 0x7f05b85c2e00>"}, "return_value": "None"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610056.902, "ph": "X", "dur": 0.8485627016087673, "name": "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)", "args": {"func_args": {"self": "<vllm.v1.engine.output_processor.OutputProcessor object at 0x7f05ba94a410>"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610055.836, "ph": "X", "dur": 2.155215930124176, "name": "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)", "args": {"func_args": {"self": "<vllm.v1.engine.llm_engine.LLMEngine object at 0x7f05b88cac10>"}, "return_value": "False"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610184.766, "ph": "X", "dur": 7.546589054486951, "name": "LLM._run_engine.<locals>.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1491)", "args": {"func_args": {"x": "RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})"}, "return_value": "0"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610192.768, "ph": "X", "dur": 6.768501818556127, "name": "LLM._run_engine.<locals>.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1491)", "args": {"func_args": {"x": "RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})"}, "return_value": "1"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205537541.479, "ph": "X", "dur": 72658.90322258833, "name": "LLM._run_engine (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1445)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "use_tqdm": "True"}, "return_value": "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={}), RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205610214.822, "ph": "X", "dur": 11.772259881690207, "name": "LLMEngine.validate_outputs (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:164)", "args": {"func_args": {"cls": "<class 'vllm.v1.engine.llm_engine.LLMEngine'>", "outputs": "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={}), RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]", "output_type": "<class 'vllm.outputs.RequestOutput'>"}, "return_value": "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={}), RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205531789.293, "ph": "X", "dur": 78445.2094281564, "name": "LLM.generate (/home/jeromeku/vllm/vllm/entrypoints/llm.py:380)", "args": {"func_args": {"self": "<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>", "prompts": "['Hello, my name is', 'The president of the United States is']", "sampling_params": "SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None)", "prompt_token_ids": "None", "use_tqdm": "True", "lora_request": "None", "prompt_adapter_request": "None", "guided_options_request": "None", "priority": "None"}, "return_value": "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={}), RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"}, "cat": "FEE"}, {"pid": 1644513, "tid": 1644513, "ts": 13815205531730.45, "ph": "X", "dur": 78510.99637062446, "name": "deprecate_kwargs.<locals>.wrapper.<locals>.inner (/home/jeromeku/vllm/vllm/utils.py:1196)", "args": {"func_args": {"args": "(<vllm.entrypoints.llm.LLM object at 0x7f06d3154850>, ['Hello, my name is', 'The president of the United States is'], SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.8, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=16, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None))", "kwargs": "{}"}, "return_value": "[RequestOutput(request_id=0, prompt='Hello, my name is', prompt_token_ids=[2, 31414, 6, 127, 766, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' Joel, my dad is my friend and we are in a relationship. I am', token_ids=[8966, 6, 127, 4252, 16, 127, 1441, 8, 52, 32, 11, 10, 1291, 4, 38, 524], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={}), RequestOutput(request_id=1, prompt='The president of the United States is', prompt_token_ids=[2, 133, 394, 9, 5, 315, 532, 16], encoder_prompt=None, encoder_prompt_token_ids=None, prompt_logprobs=None, outputs=[CompletionOutput(index=0, text=' reportedly being investigated by the FBI over the allegations that he broke campaign finance laws.', token_ids=[2288, 145, 6807, 30, 5, 2448, 81, 5, 1857, 14, 37, 2263, 637, 2879, 2074, 4], cumulative_logprob=None, logprobs=None, finish_reason=length, stop_reason=None)], finished=True, metrics=None, lora_request=None, num_cached_tokens=None, multi_modal_placeholders={})]"}, "cat": "FEE"}], "viztracer_metadata": {"version": "1.0.4", "overflow": false, "baseTimeNanoseconds": 1733957976138160729}, "file_info": {"files": {"/home/jeromeku/vllm/vllm/entrypoints/llm.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nimport warnings\nfrom collections.abc import Sequence\nfrom contextlib import contextmanager\nfrom typing import Any, Callable, ClassVar, Optional, Union, cast, overload\n\nimport cloudpickle\nimport torch.nn as nn\nfrom tqdm.auto import tqdm\nfrom typing_extensions import TypeVar, deprecated\n\nfrom vllm.beam_search import (BeamSearchInstance, BeamSearchOutput,\n                              BeamSearchSequence, get_beam_search_score)\nfrom vllm.config import (CompilationConfig, ModelDType, TokenizerMode,\n                         is_init_field)\nfrom vllm.engine.arg_utils import (EngineArgs, HfOverrides, PoolerConfig,\n                                   TaskOption)\nfrom vllm.engine.llm_engine import LLMEngine\nfrom vllm.entrypoints.chat_utils import (ChatCompletionMessageParam,\n                                         ChatTemplateContentFormatOption,\n                                         apply_hf_chat_template,\n                                         apply_mistral_chat_template,\n                                         parse_chat_messages,\n                                         resolve_chat_template_content_format)\nfrom vllm.entrypoints.score_utils import (_cosine_similarity,\n                                          _validate_score_input_lens)\nfrom vllm.entrypoints.utils import _validate_truncation_size\nfrom vllm.inputs import PromptType, SingletonPrompt, TextPrompt, TokensPrompt\nfrom vllm.inputs.parse import parse_and_batch_prompt\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.model_executor.guided_decoding.guided_fields import (\n    GuidedDecodingRequest, LLMGuidedOptions)\nfrom vllm.model_executor.layers.quantization import QuantizationMethods\nfrom vllm.outputs import (ClassificationRequestOutput, EmbeddingRequestOutput,\n                          PoolingRequestOutput, RequestOutput,\n                          ScoringRequestOutput)\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import (BeamSearchParams, GuidedDecodingParams,\n                                  RequestOutputKind, SamplingParams)\nfrom vllm.transformers_utils.tokenizer import (AnyTokenizer, MistralTokenizer,\n                                               get_cached_tokenizer)\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import (Counter, Device, deprecate_args, deprecate_kwargs,\n                        is_list_of)\n\nlogger = init_logger(__name__)\n\n_R = TypeVar(\"_R\", default=Any)\n\n\nclass LLM:\n    \"\"\"An LLM for generating texts from given prompts and sampling parameters.\n\n    This class includes a tokenizer, a language model (possibly distributed\n    across multiple GPUs), and GPU memory space allocated for intermediate\n    states (aka KV cache). Given a batch of prompts and sampling parameters,\n    this class generates texts from the model, using an intelligent batching\n    mechanism and efficient memory management.\n\n    Args:\n        model: The name or path of a HuggingFace Transformers model.\n        tokenizer: The name or path of a HuggingFace Transformers tokenizer.\n        tokenizer_mode: The tokenizer mode. \"auto\" will use the fast tokenizer\n            if available, and \"slow\" will always use the slow tokenizer.\n        skip_tokenizer_init: If true, skip initialization of tokenizer and\n            detokenizer. Expect valid prompt_token_ids and None for prompt\n            from the input.\n        trust_remote_code: Trust remote code (e.g., from HuggingFace) when\n            downloading the model and tokenizer.\n        allowed_local_media_path: Allowing API requests to read local images\n            or videos from directories specified by the server file system.\n            This is a security risk. Should only be enabled in trusted\n            environments.\n        tensor_parallel_size: The number of GPUs to use for distributed\n            execution with tensor parallelism.\n        dtype: The data type for the model weights and activations. Currently,\n            we support `float32`, `float16`, and `bfloat16`. If `auto`, we use\n            the `torch_dtype` attribute specified in the model config file.\n            However, if the `torch_dtype` in the config is `float32`, we will\n            use `float16` instead.\n        quantization: The method used to quantize the model weights. Currently,\n            we support \"awq\", \"gptq\", and \"fp8\" (experimental).\n            If None, we first check the `quantization_config` attribute in the\n            model config file. If that is None, we assume the model weights are\n            not quantized and use `dtype` to determine the data type of\n            the weights.\n        revision: The specific model version to use. It can be a branch name,\n            a tag name, or a commit id.\n        tokenizer_revision: The specific tokenizer version to use. It can be a\n            branch name, a tag name, or a commit id.\n        seed: The seed to initialize the random number generator for sampling.\n        gpu_memory_utilization: The ratio (between 0 and 1) of GPU memory to\n            reserve for the model weights, activations, and KV cache. Higher\n            values will increase the KV cache size and thus improve the model's\n            throughput. However, if the value is too high, it may cause out-of-\n            memory (OOM) errors.\n        swap_space: The size (GiB) of CPU memory per GPU to use as swap space.\n            This can be used for temporarily storing the states of the requests\n            when their `best_of` sampling parameters are larger than 1. If all\n            requests will have `best_of=1`, you can safely set this to 0.\n            Noting that `best_of` is only supported in V0. Otherwise, too small\n            values may cause out-of-memory (OOM) errors.\n        cpu_offload_gb: The size (GiB) of CPU memory to use for offloading\n            the model weights. This virtually increases the GPU memory space\n            you can use to hold the model weights, at the cost of CPU-GPU data\n            transfer for every forward pass.\n        enforce_eager: Whether to enforce eager execution. If True, we will\n            disable CUDA graph and always execute the model in eager mode.\n            If False, we will use CUDA graph and eager execution in hybrid.\n        max_seq_len_to_capture: Maximum sequence len covered by CUDA graphs.\n            When a sequence has context length larger than this, we fall back\n            to eager mode. Additionally for encoder-decoder models, if the\n            sequence length of the encoder input is larger than this, we fall\n            back to the eager mode.\n        disable_custom_all_reduce: See {class}`~vllm.config.ParallelConfig`\n        disable_async_output_proc: Disable async output processing.\n            This may result in lower performance.\n        hf_token: The token to use as HTTP bearer authorization for remote files\n            . If `True`, will use the token generated when running\n            `huggingface-cli login` (stored in `~/.huggingface`).\n        hf_overrides: If a dictionary, contains arguments to be forwarded to the\n            HuggingFace config. If a callable, it is called to update the\n            HuggingFace config.\n        compilation_config: Either an integer or a dictionary. If it is an\n            integer, it is used as the level of compilation optimization. If it\n            is a dictionary, it can specify the full compilation configuration.\n        **kwargs: Arguments for {class}`~vllm.EngineArgs`. (See\n            {ref}`engine-args`)\n\n    :::{note}\n    This class is intended to be used for offline inference. For online\n    serving, use the {class}`~vllm.AsyncLLMEngine` class instead.\n    :::\n    \"\"\"\n\n    DEPRECATE_LEGACY: ClassVar[bool] = True\n    \"\"\"A flag to toggle whether to deprecate the legacy generate/encode API.\"\"\"\n\n    DEPRECATE_INIT_POSARGS: ClassVar[bool] = True\n    \"\"\"\n    A flag to toggle whether to deprecate positional arguments in\n    {meth}`LLM.__init__`.\n    \"\"\"\n\n    @classmethod\n    @contextmanager\n    def deprecate_legacy_api(cls):\n        cls.DEPRECATE_LEGACY = True\n\n        yield\n\n        cls.DEPRECATE_LEGACY = False\n\n    @deprecate_args(\n        start_index=2,  # Ignore self and model\n        is_deprecated=lambda: LLM.DEPRECATE_INIT_POSARGS,\n        additional_message=(\n            \"All positional arguments other than `model` will be \"\n            \"replaced with keyword arguments in an upcoming version.\"),\n    )\n    def __init__(\n        self,\n        model: str,\n        tokenizer: Optional[str] = None,\n        tokenizer_mode: TokenizerMode = \"auto\",\n        skip_tokenizer_init: bool = False,\n        trust_remote_code: bool = False,\n        allowed_local_media_path: str = \"\",\n        tensor_parallel_size: int = 1,\n        dtype: ModelDType = \"auto\",\n        quantization: Optional[QuantizationMethods] = None,\n        revision: Optional[str] = None,\n        tokenizer_revision: Optional[str] = None,\n        seed: Optional[int] = None,\n        gpu_memory_utilization: float = 0.9,\n        swap_space: float = 4,\n        cpu_offload_gb: float = 0,\n        enforce_eager: bool = False,\n        max_seq_len_to_capture: int = 8192,\n        disable_custom_all_reduce: bool = False,\n        disable_async_output_proc: bool = False,\n        hf_token: Optional[Union[bool, str]] = None,\n        hf_overrides: Optional[HfOverrides] = None,\n        mm_processor_kwargs: Optional[dict[str, Any]] = None,\n        # After positional args are removed, move this right below `model`\n        task: TaskOption = \"auto\",\n        override_pooler_config: Optional[PoolerConfig] = None,\n        compilation_config: Optional[Union[int, dict[str, Any]]] = None,\n        **kwargs,\n    ) -> None:\n        \"\"\"LLM constructor.\"\"\"\n\n        if \"disable_log_stats\" not in kwargs:\n            kwargs[\"disable_log_stats\"] = True\n\n        if \"worker_cls\" in kwargs:\n            worker_cls = kwargs[\"worker_cls\"]\n            # if the worker_cls is not qualified string name,\n            # we serialize it using cloudpickle to avoid pickling issues\n            if isinstance(worker_cls, type):\n                kwargs[\"worker_cls\"] = cloudpickle.dumps(worker_cls)\n\n        if compilation_config is not None:\n            if isinstance(compilation_config, int):\n                compilation_config_instance = CompilationConfig(\n                    level=compilation_config)\n            elif isinstance(compilation_config, dict):\n                predicate = lambda x: is_init_field(CompilationConfig, x[0])\n                compilation_config_instance = CompilationConfig(\n                    **dict(filter(predicate, compilation_config.items())))\n            else:\n                compilation_config_instance = compilation_config\n        else:\n            compilation_config_instance = None\n\n        engine_args = EngineArgs(\n            model=model,\n            task=task,\n            tokenizer=tokenizer,\n            tokenizer_mode=tokenizer_mode,\n            skip_tokenizer_init=skip_tokenizer_init,\n            trust_remote_code=trust_remote_code,\n            allowed_local_media_path=allowed_local_media_path,\n            tensor_parallel_size=tensor_parallel_size,\n            dtype=dtype,\n            quantization=quantization,\n            revision=revision,\n            tokenizer_revision=tokenizer_revision,\n            seed=seed,\n            gpu_memory_utilization=gpu_memory_utilization,\n            swap_space=swap_space,\n            cpu_offload_gb=cpu_offload_gb,\n            enforce_eager=enforce_eager,\n            max_seq_len_to_capture=max_seq_len_to_capture,\n            disable_custom_all_reduce=disable_custom_all_reduce,\n            disable_async_output_proc=disable_async_output_proc,\n            hf_token=hf_token,\n            hf_overrides=hf_overrides,\n            mm_processor_kwargs=mm_processor_kwargs,\n            override_pooler_config=override_pooler_config,\n            compilation_config=compilation_config_instance,\n            **kwargs,\n        )\n\n        # Create the Engine (autoselects V0 vs V1)\n        self.llm_engine = LLMEngine.from_engine_args(\n            engine_args=engine_args, usage_context=UsageContext.LLM_CLASS)\n        self.engine_class = type(self.llm_engine)\n\n        self.request_counter = Counter()\n        self.default_sampling_params: Union[dict[str, Any], None] = None\n\n    def get_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        return self.llm_engine.get_tokenizer_group().get_lora_tokenizer(\n            lora_request)\n\n    def set_tokenizer(self, tokenizer: AnyTokenizer) -> None:\n        tokenizer_group = self.llm_engine.get_tokenizer_group()\n\n        # While CachedTokenizer is dynamic, have no choice but\n        # compare class name. Misjudgment will arise from\n        # user-defined tokenizer started with 'Cached'\n        if tokenizer.__class__.__name__.startswith(\"Cached\"):\n            tokenizer_group.tokenizer = tokenizer\n        else:\n            tokenizer_group.tokenizer = get_cached_tokenizer(tokenizer)\n\n    def get_default_sampling_params(self) -> SamplingParams:\n        if self.default_sampling_params is None:\n            self.default_sampling_params = (\n                self.llm_engine.model_config.get_diff_sampling_param())\n        if self.default_sampling_params:\n            return SamplingParams.from_optional(**self.default_sampling_params)\n        return SamplingParams()\n\n    @overload\n    def generate(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        sampling_params: Optional[Union[SamplingParams,\n                                        Sequence[SamplingParams]]] = None,\n        *,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: str,\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        prompt_token_ids: Optional[list[int]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: list[str],\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        prompt_token_ids: Optional[list[list[int]]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: Optional[str] = None,\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        *,\n        prompt_token_ids: list[int],\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: Optional[list[str]] = None,\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        *,\n        prompt_token_ids: list[list[int]],\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @overload  # LEGACY: single or multi token ids [pos-only]\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def generate(\n        self,\n        prompts: None,\n        sampling_params: None,\n        prompt_token_ids: Union[list[int], list[list[int]]],\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n    ) -> list[RequestOutput]:\n        ...\n\n    @deprecate_kwargs(\n        \"prompt_token_ids\",\n        is_deprecated=lambda: LLM.DEPRECATE_LEGACY,\n        additional_message=\"Please use the 'prompts' parameter instead.\",\n    )\n    def generate(\n        self,\n        prompts: Union[Union[PromptType, Sequence[PromptType]],\n                       Optional[Union[str, list[str]]]] = None,\n        sampling_params: Optional[Union[SamplingParams,\n                                        Sequence[SamplingParams]]] = None,\n        prompt_token_ids: Optional[Union[list[int], list[list[int]]]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        guided_options_request: Optional[Union[LLMGuidedOptions,\n                                               GuidedDecodingRequest]] = None,\n        priority: Optional[list[int]] = None,\n    ) -> list[RequestOutput]:\n        \"\"\"Generates the completions for the input prompts.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            sampling_params: The sampling parameters for text generation. If\n                None, we use the default sampling parameters.\n                When it is a single value, it is applied to every prompt.\n                When it is a list, the list must have the same length as the\n                prompts and it is paired one by one with the prompt.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n            priority: The priority of the requests, if any.\n                Only applicable when priority scheduling policy is enabled.\n\n        Returns:\n            A list of `RequestOutput` objects containing the\n            generated completions in the same order as the input prompts.\n\n        :::{note}\n        Using `prompts` and `prompt_token_ids` as keyword parameters is\n        considered legacy and may be deprecated in the future. You should\n        instead pass them via the `inputs` parameter.\n        :::\n        \"\"\"\n        runner_type = self.llm_engine.model_config.runner_type\n        if runner_type not in [\"generate\", \"transcription\"]:\n            messages = [\n                \"LLM.generate() is only supported for (conditional) generation \"\n                \"models (XForCausalLM, XForConditionalGeneration).\",\n            ]\n\n            supported_runner_types = self.llm_engine.model_config \\\n                .supported_runner_types\n            if \"generate\" in supported_runner_types:\n                messages.append(\n                    \"Your model supports the 'generate' runner, but is \"\n                    f\"currently initialized for the '{runner_type}' runner. \"\n                    \"Please initialize vLLM using `--task generate`.\")\n\n            raise ValueError(\" \".join(messages))\n\n        if prompt_token_ids is not None:\n            parsed_prompts = self._convert_v1_inputs(\n                prompts=cast(Optional[Union[str, list[str]]], prompts),\n                prompt_token_ids=prompt_token_ids,\n            )\n        else:\n            parsed_prompts = cast(Union[PromptType, Sequence[PromptType]],\n                                  prompts)\n\n        if isinstance(guided_options_request, dict):\n            if len(guided_options_request) > 1:\n                raise ValueError(\n                    \"You can only use one guided decoding but multiple is \"\n                    f\"specified: {guided_options_request}\")\n            guided_options_request = GuidedDecodingRequest(\n                **guided_options_request)\n\n        if sampling_params is None:\n            # Use default sampling params.\n            sampling_params = self.get_default_sampling_params()\n\n        self._validate_and_add_requests(\n            prompts=parsed_prompts,\n            params=sampling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            guided_options=guided_options_request,\n            priority=priority,\n        )\n\n        outputs = self._run_engine(use_tqdm=use_tqdm)\n        return self.engine_class.validate_outputs(outputs, RequestOutput)\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        \"\"\"\n        Execute an RPC call on all workers.\n\n        Args:\n            method: Name of the worker method to execute, or a callable that\n                is serialized and sent to all workers to execute.\n\n                If the method is a callable, it should accept an additional\n                `self` argument, in addition to the arguments passed in `args`\n                and `kwargs`. The `self` argument will be the worker object.\n            timeout: Maximum time in seconds to wait for execution. Raises a\n                {exc}`TimeoutError` on timeout. `None` means wait indefinitely.\n            args: Positional arguments to pass to the worker method.\n            kwargs: Keyword arguments to pass to the worker method.\n\n        Returns:\n            A list containing the results from each worker.\n\n        :::{note}\n        It is recommended to use this API to only pass control messages,\n        and set up data-plane communication to pass data.\n        :::\n        \"\"\"\n\n        return self.llm_engine.collective_rpc(method, timeout, args, kwargs)\n\n    def apply_model(self, func: Callable[[nn.Module], _R]) -> list[_R]:\n        \"\"\"\n        Run a function directly on the model inside each worker,\n        returning the result for each of them.\n        \"\"\"\n        executor = self.llm_engine.model_executor\n        return executor.apply_model(func)\n\n    def beam_search(\n        self,\n        prompts: list[Union[TokensPrompt, TextPrompt]],\n        params: BeamSearchParams,\n    ) -> list[BeamSearchOutput]:\n        \"\"\"\n        Generate sequences using beam search.\n\n        Args:\n            prompts: A list of prompts. Each prompt can be a string or a list\n                of token IDs.\n            params: The beam search parameters.\n        \"\"\"\n        # TODO: how does beam search work together with length penalty,\n        # frequency, penalty, and stopping criteria, etc.?\n        beam_width = params.beam_width\n        max_tokens = params.max_tokens\n        temperature = params.temperature\n        ignore_eos = params.ignore_eos\n        length_penalty = params.length_penalty\n\n        def sort_beams_key(x: BeamSearchSequence) -> float:\n            return get_beam_search_score(x.tokens, x.cum_logprob,\n                                         tokenizer.eos_token_id,\n                                         length_penalty)\n\n        def create_tokens_prompt_from_beam(\n                beam: BeamSearchSequence) -> TokensPrompt:\n            token_prompt_kwargs: TokensPrompt = {\n                \"prompt_token_ids\": beam.tokens\n            }\n            if beam.multi_modal_data is not None:\n                token_prompt_kwargs[\"multi_modal_data\"] = beam.multi_modal_data\n\n            if beam.mm_processor_kwargs is not None:\n                token_prompt_kwargs[\n                    \"mm_processor_kwargs\"] = beam.mm_processor_kwargs\n            return TokensPrompt(**token_prompt_kwargs)\n\n        tokenizer = self.get_tokenizer()\n        # generate 2 * beam_width candidates at each step\n        # following the huggingface transformers implementation\n        # at https://github.com/huggingface/transformers/blob/e15687fffe5c9d20598a19aeab721ae0a7580f8a/src/transformers/generation/beam_search.py#L534 # noqa\n        beam_search_params = SamplingParams(logprobs=2 * beam_width,\n                                            max_tokens=1,\n                                            temperature=temperature)\n        instances: list[BeamSearchInstance] = []\n\n        for prompt in prompts:\n            # Add multimodal processor kwargs & data\n            mm_kwargs = {}\n            if \"multi_modal_data\" in prompt:\n                mm_kwargs[\"multi_modal_data\"] = prompt[\"multi_modal_data\"]\n            if \"mm_processor_kwargs\" in prompt:\n                mm_kwargs[\"mm_processor_kwargs\"] = prompt[\n                    \"mm_processor_kwargs\"]\n\n            if \"prompt_token_ids\" in prompt:\n                prompt = cast(TokensPrompt, prompt)  # Needed for mypy\n                prompt_tokens = prompt[\"prompt_token_ids\"]\n            else:\n                prompt_tokens = tokenizer.encode(prompt[\"prompt\"])\n\n            instances.append(\n                BeamSearchInstance(prompt_tokens, logprobs=None, **mm_kwargs))\n\n        for _ in range(max_tokens):\n            all_beams: list[BeamSearchSequence] = list(\n                sum((instance.beams for instance in instances), []))\n            pos = [0] + list(\n                itertools.accumulate(\n                    len(instance.beams) for instance in instances))\n            instance_start_and_end: list[tuple[int, int]] = list(\n                zip(pos[:-1], pos[1:]))\n\n            if len(all_beams) == 0:\n                break\n\n            prompts_batch = [\n                create_tokens_prompt_from_beam(beam) for beam in all_beams\n            ]\n\n            # only runs for one step\n            # we don't need to use tqdm here\n            output = self.generate(prompts_batch,\n                                   sampling_params=beam_search_params,\n                                   use_tqdm=False)\n\n            for (start, end), instance in zip(instance_start_and_end,\n                                              instances):\n                instance_new_beams = []\n                for i in range(start, end):\n                    current_beam = all_beams[i]\n                    result = output[i]\n\n                    if result.outputs[0].logprobs is not None:\n                        # if `result.outputs[0].logprobs` is None, it means\n                        # the sequence is completed because of the max-model-len\n                        # or abortion. we don't need to add it to the new beams.\n                        logprobs = result.outputs[0].logprobs[0]\n                        for token_id, logprob_obj in logprobs.items():\n                            new_beam = BeamSearchSequence(\n                                tokens=current_beam.tokens + [token_id],\n                                logprobs=current_beam.logprobs + [logprobs],\n                                cum_logprob=current_beam.cum_logprob +\n                                logprob_obj.logprob,\n                                multi_modal_data=current_beam.multi_modal_data,\n                                mm_processor_kwargs=current_beam.\n                                mm_processor_kwargs)\n\n                            if token_id == tokenizer.eos_token_id and \\\n                                not ignore_eos:\n                                instance.completed.append(new_beam)\n                            else:\n                                instance_new_beams.append(new_beam)\n                sorted_beams = sorted(instance_new_beams,\n                                      key=sort_beams_key,\n                                      reverse=True)\n                instance.beams = sorted_beams[:beam_width]\n\n        outputs = []\n        for instance in instances:\n            instance.completed.extend(instance.beams)\n            sorted_completed = sorted(instance.completed,\n                                      key=sort_beams_key,\n                                      reverse=True)\n            best_beams = sorted_completed[:beam_width]\n\n            for beam in best_beams:\n                beam.text = tokenizer.decode(beam.tokens)\n            outputs.append(BeamSearchOutput(sequences=best_beams))\n\n        return outputs\n\n    def chat(\n        self,\n        messages: Union[list[ChatCompletionMessageParam],\n                        list[list[ChatCompletionMessageParam]]],\n        sampling_params: Optional[Union[SamplingParams,\n                                        list[SamplingParams]]] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[LoRARequest] = None,\n        chat_template: Optional[str] = None,\n        chat_template_content_format: ChatTemplateContentFormatOption = \"auto\",\n        add_generation_prompt: bool = True,\n        continue_final_message: bool = False,\n        tools: Optional[list[dict[str, Any]]] = None,\n        chat_template_kwargs: Optional[dict[str, Any]] = None,\n        mm_processor_kwargs: Optional[dict[str, Any]] = None,\n    ) -> list[RequestOutput]:\n        \"\"\"\n        Generate responses for a chat conversation.\n\n        The chat conversation is converted into a text prompt using the\n        tokenizer and calls the {meth}`generate` method to generate the\n        responses.\n\n        Multi-modal inputs can be passed in the same way you would pass them\n        to the OpenAI API.\n\n        Args:\n            messages: A list of conversations or a single conversation.\n\n              - Each conversation is represented as a list of messages.\n              - Each message is a dictionary with 'role' and 'content' keys.\n\n            sampling_params: The sampling parameters for text generation.\n                If None, we use the default sampling parameters. When it\n                is a single value, it is applied to every prompt. When it\n                is a list, the list must have the same length as the\n                prompts and it is paired one by one with the prompt.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            chat_template: The template to use for structuring the chat.\n              If not provided, the model's default chat template will be used.\n            chat_template_content_format: The format to render message content.\n\n              - \"string\" will render the content as a string.\n                Example: ``\"Who are you?\"``\n              - \"openai\" will render the content as a list of dictionaries,\n                similar to OpenAI schema.\n                Example: ``[{\"type\": \"text\", \"text\": \"Who are you?\"}]``\n\n            add_generation_prompt: If True, adds a generation template\n                to each message.\n            continue_final_message: If True, continues the final message in\n                the conversation instead of starting a new one. Cannot be\n                ``True`` if ``add_generation_prompt`` is also ``True``.\n            chat_template_kwargs: Additional kwargs to pass to the chat\n                template.\n            mm_processor_kwargs: Multimodal processor kwarg overrides for this\n                chat request. Only used for offline requests.\n\n        Returns:\n            A list of ``RequestOutput`` objects containing the generated\n            responses in the same order as the input messages.\n        \"\"\"\n        list_of_messages: list[list[ChatCompletionMessageParam]]\n\n        # Handle multi and single conversations\n        if is_list_of(messages, list):\n            # messages is list[list[...]]\n            list_of_messages = cast(list[list[ChatCompletionMessageParam]],\n                                    messages)\n        else:\n            # messages is list[...]\n            list_of_messages = [\n                cast(list[ChatCompletionMessageParam], messages)\n            ]\n\n        tokenizer = self.get_tokenizer(lora_request)\n        model_config = self.llm_engine.get_model_config()\n        resolved_content_format = resolve_chat_template_content_format(\n            chat_template,\n            tools,\n            chat_template_content_format,\n            tokenizer,\n            model_config=model_config,\n        )\n\n        _chat_template_kwargs: dict[str, Any] = dict(\n            chat_template=chat_template,\n            add_generation_prompt=add_generation_prompt,\n            continue_final_message=continue_final_message,\n            tools=tools,\n        )\n        _chat_template_kwargs.update(chat_template_kwargs or {})\n\n        prompts: list[Union[TokensPrompt, TextPrompt]] = []\n\n        for msgs in list_of_messages:\n            # NOTE: _parse_chat_message_content_parts() currently doesn't\n            # handle mm_processor_kwargs, since there is no implementation in\n            # the chat message parsing for it.\n            conversation, mm_data = parse_chat_messages(\n                msgs,\n                model_config,\n                tokenizer,\n                content_format=resolved_content_format,\n            )\n\n            if isinstance(tokenizer, MistralTokenizer):\n                prompt_token_ids = apply_mistral_chat_template(\n                    tokenizer,\n                    messages=msgs,\n                    **_chat_template_kwargs,\n                )\n            else:\n                prompt_str = apply_hf_chat_template(\n                    tokenizer=tokenizer,\n                    conversation=conversation,\n                    model_config=model_config,\n                    **_chat_template_kwargs,\n                )\n                # Special tokens are already included in chat templates so\n                # should not be added by the tokenizer in this case.\n                prompt_token_ids = tokenizer.encode(prompt_str,\n                                                    add_special_tokens=False)\n\n            prompt = TokensPrompt(prompt_token_ids=prompt_token_ids)\n\n            if mm_data is not None:\n                prompt[\"multi_modal_data\"] = mm_data\n\n            if mm_processor_kwargs is not None:\n                prompt[\"mm_processor_kwargs\"] = mm_processor_kwargs\n\n            prompts.append(prompt)\n\n        return self.generate(\n            prompts,\n            sampling_params=sampling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n        )\n\n    @overload\n    def encode(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        *,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: str,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        prompt_token_ids: Optional[list[int]] = None,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (prompt + optional token ids)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: list[str],\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        prompt_token_ids: Optional[list[list[int]]] = None,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: single (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: Optional[str] = None,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        *,\n        prompt_token_ids: list[int],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: multi (token ids + optional prompt)\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: Optional[list[str]] = None,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        *,\n        prompt_token_ids: list[list[int]],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @overload  # LEGACY: single or multi token ids [pos-only]\n    @deprecated(\"'prompt_token_ids' will become part of 'prompts'\")\n    def encode(\n        self,\n        prompts: None,\n        pooling_params: None,\n        prompt_token_ids: Union[list[int], list[list[int]]],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        ...\n\n    @deprecate_kwargs(\n        \"prompt_token_ids\",\n        is_deprecated=lambda: LLM.DEPRECATE_LEGACY,\n        additional_message=\"Please use the 'prompts' parameter instead.\",\n    )\n    def encode(\n        self,\n        prompts: Union[Union[PromptType, Sequence[PromptType]],\n                       Optional[Union[str, list[str]]]] = None,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        prompt_token_ids: Optional[Union[list[int], list[list[int]]]] = None,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[PoolingRequestOutput]:\n        \"\"\"Apply pooling to the hidden states corresponding to the input\n        prompts.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            pooling_params: The pooling parameters for pooling. If None, we\n                use the default pooling parameters.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of `PoolingRequestOutput` objects containing the\n            pooled hidden states in the same order as the input prompts.\n\n        :::{note}\n        Using `prompts` and `prompt_token_ids` as keyword parameters is\n        considered legacy and may be deprecated in the future. You should\n        instead pass them via the `inputs` parameter.\n        :::\n        \"\"\"\n        runner_type = self.llm_engine.model_config.runner_type\n        if runner_type != \"pooling\":\n            messages = [\"LLM.encode() is only supported for pooling models.\"]\n\n            supported_runner_types = self.llm_engine.model_config \\\n                .supported_runner_types\n            if \"pooling\" in supported_runner_types:\n                messages.append(\n                    \"Your model supports the 'pooling' runner, but is \"\n                    f\"currently initialized for the '{runner_type}' runner. \"\n                    \"Please initialize vLLM using `--task embed`, \"\n                    \"`--task classify`, `--task score` etc.\")\n\n            raise ValueError(\" \".join(messages))\n\n        if prompt_token_ids is not None:\n            parsed_prompts = self._convert_v1_inputs(\n                prompts=cast(Optional[Union[str, list[str]]], prompts),\n                prompt_token_ids=prompt_token_ids,\n            )\n        else:\n            parsed_prompts = cast(Union[PromptType, Sequence[PromptType]],\n                                  prompts)\n\n        if pooling_params is None:\n            # Use default pooling params.\n            pooling_params = PoolingParams()\n        elif isinstance(pooling_params, PoolingParams):\n            pooling_params.verify(self.llm_engine.model_config)\n        else:\n            for pooling_param in pooling_params:\n                pooling_param.verify(self.llm_engine.model_config)\n\n        tokenization_kwargs: dict[str, Any] = {}\n        _validate_truncation_size(self.llm_engine.model_config.max_model_len,\n                                  truncate_prompt_tokens, tokenization_kwargs)\n\n        self._validate_and_add_requests(\n            prompts=parsed_prompts,\n            params=pooling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            tokenization_kwargs=tokenization_kwargs,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        outputs = self._run_engine(use_tqdm=use_tqdm)\n        return self.engine_class.validate_outputs(outputs,\n                                                  PoolingRequestOutput)\n\n    def embed(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        *,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        pooling_params: Optional[Union[PoolingParams,\n                                       Sequence[PoolingParams]]] = None,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[EmbeddingRequestOutput]:\n        \"\"\"\n        Generate an embedding vector for each prompt.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            pooling_params: The pooling parameters for pooling. If None, we\n                use the default pooling parameters.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of ``EmbeddingRequestOutput`` objects containing the\n            embedding vectors in the same order as the input prompts.\n        \"\"\"\n        if self.llm_engine.model_config.task != \"embed\":\n            raise ValueError(\n                \"Embedding API is only enabled for `--task embed`\")\n\n        items = self.encode(prompts,\n                            truncate_prompt_tokens=truncate_prompt_tokens,\n                            use_tqdm=use_tqdm,\n                            pooling_params=pooling_params,\n                            lora_request=lora_request,\n                            prompt_adapter_request=prompt_adapter_request)\n\n        return [EmbeddingRequestOutput.from_base(item) for item in items]\n\n    def classify(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        /,\n        *,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ClassificationRequestOutput]:\n        \"\"\"\n        Generate class logits for each prompt.\n\n        This class automatically batches the given prompts, considering\n        the memory constraint. For the best performance, put all of your prompts\n        into a single list and pass it to this method.\n\n        Args:\n            prompts: The prompts to the LLM. You may pass a sequence of prompts\n                for batch inference. See {class}`~vllm.inputs.PromptType`\n                for more details about the format of each prompts.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of ``ClassificationRequestOutput`` objects containing the\n            embedding vectors in the same order as the input prompts.\n        \"\"\"\n        if self.llm_engine.model_config.task != \"classify\":\n            raise ValueError(\n                \"Classification API is only enabled for `--task classify`\")\n\n        items = self.encode(prompts,\n                            use_tqdm=use_tqdm,\n                            lora_request=lora_request,\n                            prompt_adapter_request=prompt_adapter_request)\n\n        return [ClassificationRequestOutput.from_base(item) for item in items]\n\n    def _embedding_score(\n        self,\n        tokenizer: AnyTokenizer,\n        text_1: list[Union[str, TextPrompt, TokensPrompt]],\n        text_2: list[Union[str, TextPrompt, TokensPrompt]],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ScoringRequestOutput]:\n\n        encoded_output: list[PoolingRequestOutput] = self.encode(\n            text_1 + text_2,\n            truncate_prompt_tokens=truncate_prompt_tokens,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request)\n\n        encoded_output_1: list[PoolingRequestOutput] = encoded_output[\n            0:len(text_1)]\n        encoded_output_2: list[PoolingRequestOutput] = encoded_output[\n            len(text_1):]\n\n        if len(encoded_output_1) == 1:\n            encoded_output_1 = encoded_output_1 * len(encoded_output_2)\n\n        scores = _cosine_similarity(tokenizer=tokenizer,\n                                    embed_1=encoded_output_1,\n                                    embed_2=encoded_output_2)\n\n        items = self.engine_class.validate_outputs(scores,\n                                                   PoolingRequestOutput)\n        return [ScoringRequestOutput.from_base(item) for item in items]\n\n    def _cross_encoding_score(\n        self,\n        tokenizer: AnyTokenizer,\n        text_1: list[str],\n        text_2: list[str],\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ScoringRequestOutput]:\n\n        if isinstance(tokenizer, MistralTokenizer):\n            raise ValueError(\n                \"Score API is only enabled for `--task embed or score`\")\n\n        if len(text_1) == 1:\n            text_1 = text_1 * len(text_2)\n\n        input_pairs = [(t1, t2) for t1, t2 in zip(text_1, text_2)]\n\n        pooling_params = PoolingParams()\n\n        tokenization_kwargs: dict[str, Any] = {}\n        _validate_truncation_size(self.llm_engine.model_config.max_model_len,\n                                  truncate_prompt_tokens, tokenization_kwargs)\n\n        parsed_prompts = []\n\n        for q, t in input_pairs:\n            prompt_inputs = tokenizer(text=q,\n                                      text_pair=t,\n                                      **tokenization_kwargs)\n            engine_prompt = TokensPrompt(\n                prompt_token_ids=prompt_inputs[\"input_ids\"],\n                token_type_ids=prompt_inputs.get(\"token_type_ids\"))\n            parsed_prompts.append(engine_prompt)\n\n        self._validate_and_add_requests(\n            prompts=parsed_prompts,\n            params=pooling_params,\n            use_tqdm=use_tqdm,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n        outputs = self._run_engine(use_tqdm=use_tqdm)\n        items = self.engine_class.validate_outputs(outputs,\n                                                   PoolingRequestOutput)\n\n        return [ScoringRequestOutput.from_base(item) for item in items]\n\n    def score(\n        self,\n        text_1: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n        text_2: Union[SingletonPrompt, Sequence[SingletonPrompt]],\n        /,\n        *,\n        truncate_prompt_tokens: Optional[int] = None,\n        use_tqdm: bool = True,\n        lora_request: Optional[Union[list[LoRARequest], LoRARequest]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n    ) -> list[ScoringRequestOutput]:\n        \"\"\"Generate similarity scores for all pairs ``<text,text_pair>``.\n\n        The inputs can be ``1 -> 1``, ``1 -> N`` or ``N -> N``.\n        In the ``1 - N`` case the ``text_1`` sentence will be replicated ``N``\n        times to pair with the ``text_2`` sentences.\n        The input pairs are used to build a list of prompts for the\n        cross encoder model. This class automatically batches the prompts,\n        considering the memory constraint. For the best performance, put all\n        of your texts into a single list and pass it to this method.\n\n        Args:\n            text_1: can be a single prompt or a list of prompts, in which\n                case it has to have the same length as the ``text_2`` list\n            text_2: The texts to pair with the query to form the input\n                to the LLM. See {class}`~vllm.inputs.PromptType` for\n                more details about the format of each prompts.\n            use_tqdm: Whether to use tqdm to display the progress bar.\n            lora_request: LoRA request to use for generation, if any.\n            prompt_adapter_request: Prompt Adapter request to use for\n                generation, if any.\n\n        Returns:\n            A list of ``ScoringRequestOutput`` objects containing the\n            generated scores in the same order as the input prompts.\n        \"\"\"\n        runner_type = self.llm_engine.model_config.runner_type\n        if runner_type != \"pooling\":\n            messages = [\"LLM.score() is only supported for pooling models.\"]\n\n            supported_runner_types = self.llm_engine.model_config \\\n                .supported_runner_types\n            if \"pooling\" in supported_runner_types:\n                messages.append(\n                    \"Your model supports the 'pooling' runner, but is \"\n                    f\"currently initialized for the '{runner_type}' runner. \"\n                    \"Please initialize vLLM using `--task embed`, \"\n                    \"`--task classify`, `--task score` etc.\")\n\n            raise ValueError(\" \".join(messages))\n\n        if self.llm_engine.model_config.task not in (\"embed\", \"score\"):\n            raise ValueError(\n                \"Score API is only enabled for `--task embed or --task score`\")\n\n        # the tokenizer for models such as\n        # \"cross-encoder/ms-marco-MiniLM-L-6-v2\" doesn't support passing\n        # lists of tokens to the `text` and `text_pair` kwargs\n        tokenizer = self.llm_engine.get_tokenizer()\n\n        def ensure_str(prompt: SingletonPrompt):\n            if isinstance(prompt, dict):\n                if \"multi_modal_data\" in prompt:\n                    raise ValueError(\"Multi-modal prompt is not \"\n                                     \"supported for scoring\")\n                elif \"prompt_token_ids\" in prompt:\n                    prompt = tokenizer.decode(\n                        cast(TokensPrompt, prompt)[\"prompt_token_ids\"])\n                elif \"prompt\" in prompt:\n                    prompt = cast(TextPrompt, prompt)[\"prompt\"]\n            assert type(prompt) is str\n            return prompt\n\n        if isinstance(text_1, (str, dict)):\n            # Convert a single prompt to a list.\n            text_1 = [text_1]\n        input_text_1: list[str] = [ensure_str(t) for t in text_1]\n\n        if isinstance(text_2, (str, dict)):\n            # Convert a single prompt to a list.\n            text_2 = [text_2]\n        input_text_2: list[str] = [ensure_str(t) for t in text_2]\n\n        _validate_score_input_lens(input_text_1, input_text_2)\n\n        if self.llm_engine.model_config.is_cross_encoder:\n            return self._cross_encoding_score(tokenizer, input_text_1,\n                                              input_text_2,\n                                              truncate_prompt_tokens, use_tqdm,\n                                              lora_request,\n                                              prompt_adapter_request)\n        else:\n            return self._embedding_score(\n                tokenizer,\n                input_text_1,  # type: ignore[arg-type]\n                input_text_2,  # type: ignore[arg-type]\n                truncate_prompt_tokens,\n                use_tqdm,\n                lora_request,\n                prompt_adapter_request)\n\n    def start_profile(self) -> None:\n        self.llm_engine.start_profile()\n\n    def stop_profile(self) -> None:\n        self.llm_engine.stop_profile()\n\n    def reset_prefix_cache(self, device: Optional[Device] = None) -> bool:\n        return self.llm_engine.reset_prefix_cache(device)\n\n    def sleep(self, level: int = 1):\n        \"\"\"\n        Put the engine to sleep. The engine should not process any requests.\n        The caller should guarantee that no requests are being processed\n        during the sleep period, before `wake_up` is called.\n\n        Args:\n            level: The sleep level. Level 1 sleep will offload the model \n                weights and discard the kv cache. The content of kv cache \n                is forgotten. Level 1 sleep is good for sleeping and waking\n                up the engine to run the same model again. The model weights \n                are backed up in CPU memory. Please make sure there's enough \n                CPU memory to store the model weights. Level 2 sleep will \n                discard both the model weights and the kv cache. The content \n                of both the model weights and kv cache is forgotten. Level 2 \n                sleep is good for sleeping and waking up the engine to run a\n                different model or update the model, where previous model \n                weights are not needed. It reduces CPU memory pressure.\n        \"\"\"\n        self.reset_prefix_cache()\n        self.llm_engine.sleep(level=level)\n\n    def wake_up(self, tags: Optional[list[str]] = None):\n        \"\"\"\n        Wake up the engine from sleep mode. See the {meth}`sleep` method\n        for more details.\n        \n        Args:\n            tags: An optional list of tags to reallocate the engine memory \n                for specific memory allocations. Values must be in \n                (\"weights\", \"kv_cache\",). If None, all memory is reallocated.\n                wake_up should be called with all tags (or None) before the \n                engine is used again.\n        \"\"\"\n        self.llm_engine.wake_up(tags)\n\n    # LEGACY\n    def _convert_v1_inputs(\n        self,\n        prompts: Optional[Union[str, list[str]]],\n        prompt_token_ids: Optional[Union[list[int], list[list[int]]]],\n    ):\n        # skip_tokenizer_init is now checked in engine\n\n        if prompts is not None:\n            prompts = [p[\"content\"] for p in parse_and_batch_prompt(prompts)]\n        if prompt_token_ids is not None:\n            prompt_token_ids = [\n                p[\"content\"] for p in parse_and_batch_prompt(prompt_token_ids)\n            ]\n\n        num_requests = None\n        if prompts is not None:\n            num_requests = len(prompts)\n        if prompt_token_ids is not None:\n            if (num_requests is not None\n                    and num_requests != len(prompt_token_ids)):\n                raise ValueError(\"The lengths of prompts and prompt_token_ids \"\n                                 \"must be the same.\")\n\n            num_requests = len(prompt_token_ids)\n        if num_requests is None:\n            raise ValueError(\"Either prompts or prompt_token_ids must be \"\n                             \"provided.\")\n\n        parsed_prompts: list[PromptType] = []\n        for i in range(num_requests):\n            item: PromptType\n\n            if prompts is not None:\n                item = TextPrompt(prompt=prompts[i])\n            elif prompt_token_ids is not None:\n                item = TokensPrompt(prompt_token_ids=prompt_token_ids[i])\n            else:\n                raise AssertionError\n\n            parsed_prompts.append(item)\n\n        return parsed_prompts\n\n    def _validate_and_add_requests(\n        self,\n        prompts: Union[PromptType, Sequence[PromptType]],\n        params: Union[SamplingParams, Sequence[SamplingParams], PoolingParams,\n                      Sequence[PoolingParams]],\n        *,\n        use_tqdm: bool,\n        lora_request: Optional[Union[Sequence[LoRARequest], LoRARequest]],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        guided_options: Optional[GuidedDecodingRequest] = None,\n        priority: Optional[list[int]] = None,\n    ) -> None:\n        if guided_options is not None:\n            warnings.warn(\n                \"guided_options_request is deprecated, use \"\n                \"SamplingParams.guided_decoding instead\",\n                DeprecationWarning,\n                stacklevel=2,\n            )\n\n        if isinstance(prompts, (str, dict)):\n            # Convert a single prompt to a list.\n            prompts = [prompts]\n\n        num_requests = len(prompts)\n        if isinstance(params, list) and len(params) != num_requests:\n            raise ValueError(\"The lengths of prompts and params \"\n                             \"must be the same.\")\n        if isinstance(lora_request,\n                      list) and len(lora_request) != num_requests:\n            raise ValueError(\"The lengths of prompts and lora_request \"\n                             \"must be the same.\")\n\n        for sp in params if isinstance(params, list) else (params, ):\n            if isinstance(sp, SamplingParams):\n                self._add_guided_params(sp, guided_options)\n\n                # We only care about the final output\n                sp.output_kind = RequestOutputKind.FINAL_ONLY\n\n        # Add requests to the engine.\n        it = prompts\n        if use_tqdm:\n            it = tqdm(it, desc=\"Adding requests\")\n\n        for i, prompt in enumerate(it):\n            self._add_request(\n                prompt,\n                params[i] if isinstance(params, Sequence) else params,\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request[i] if isinstance(\n                    lora_request, Sequence) else lora_request,\n                prompt_adapter_request=prompt_adapter_request,\n                priority=priority[i] if priority else 0,\n            )\n\n    def _add_request(\n        self,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        request_id = str(next(self.request_counter))\n        self.llm_engine.add_request(\n            request_id,\n            prompt,\n            params,\n            lora_request=lora_request,\n            tokenization_kwargs=tokenization_kwargs,\n            prompt_adapter_request=prompt_adapter_request,\n            priority=priority,\n        )\n\n    def _add_guided_params(\n            self,\n            params: SamplingParams,\n            guided_options: Optional[GuidedDecodingRequest] = None):\n        if guided_options is None:\n            return params\n\n        if params.guided_decoding is not None:\n            raise ValueError(\"Cannot set both guided_options_request and \"\n                             \"params.guided_decoding.\")\n\n        params.guided_decoding = GuidedDecodingParams(\n            json=guided_options.guided_json,\n            regex=guided_options.guided_regex,\n            choice=guided_options.guided_choice,\n            grammar=guided_options.guided_grammar,\n            json_object=guided_options.guided_json_object,\n            backend=guided_options.guided_decoding_backend,\n            whitespace_pattern=guided_options.guided_whitespace_pattern,\n            structural_tag=guided_options.structural_tag,\n        )\n        return params\n\n    def _run_engine(\n            self, *, use_tqdm: bool\n    ) -> list[Union[RequestOutput, PoolingRequestOutput]]:\n        # Initialize tqdm.\n        if use_tqdm:\n            num_requests = self.llm_engine.get_num_unfinished_requests()\n            pbar = tqdm(\n                total=num_requests,\n                desc=\"Processed prompts\",\n                dynamic_ncols=True,\n                postfix=(f\"est. speed input: {0:.2f} toks/s, \"\n                         f\"output: {0:.2f} toks/s\"),\n            )\n\n        # Run the engine.\n        outputs: list[Union[RequestOutput, PoolingRequestOutput]] = []\n        total_in_toks = 0\n        total_out_toks = 0\n        while self.llm_engine.has_unfinished_requests():\n            step_outputs = self.llm_engine.step()\n            for output in step_outputs:\n                if output.finished:\n                    outputs.append(output)\n                    if use_tqdm:\n                        if isinstance(output, RequestOutput):\n                            # Calculate tokens only for RequestOutput\n                            n = len(output.outputs)\n                            assert output.prompt_token_ids is not None\n                            total_in_toks += len(output.prompt_token_ids) * n\n                            in_spd = total_in_toks / pbar.format_dict[\"elapsed\"]\n                            total_out_toks += sum(\n                                len(stp.token_ids) for stp in output.outputs)\n                            out_spd = (total_out_toks /\n                                       pbar.format_dict[\"elapsed\"])\n                            pbar.postfix = (\n                                f\"est. speed input: {in_spd:.2f} toks/s, \"\n                                f\"output: {out_spd:.2f} toks/s\")\n                            pbar.update(n)\n                        else:\n                            pbar.update(1)\n\n        if use_tqdm:\n            pbar.close()\n        # Sort the outputs by request ID.\n        # This is necessary because some requests may be finished earlier than\n        # its previous requests.\n        return sorted(outputs, key=lambda x: int(x.request_id))\n", 1491], "/home/jeromeku/vllm/vllm/config.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport ast\nimport copy\nimport enum\nimport hashlib\nimport inspect\nimport json\nimport re\nimport textwrap\nimport uuid\nimport warnings\nfrom collections import Counter\nfrom contextlib import contextmanager\nfrom dataclasses import (MISSING, Field, asdict, dataclass, field, fields,\n                         is_dataclass, replace)\nfrom functools import cached_property\nfrom importlib.util import find_spec\nfrom pathlib import Path\nfrom typing import (TYPE_CHECKING, Any, Callable, ClassVar, Literal, Optional,\n                    Protocol, TypeVar, Union, cast, get_args, get_origin)\n\nimport torch\nfrom torch.distributed import ProcessGroup, ReduceOp\nfrom transformers import PretrainedConfig\nfrom typing_extensions import deprecated\n\nimport vllm.envs as envs\nfrom vllm import version\nfrom vllm.compilation.inductor_pass import CallableInductorPass, InductorPass\nfrom vllm.logger import init_logger\nfrom vllm.model_executor.layers.quantization import (QUANTIZATION_METHODS,\n                                                     QuantizationMethods,\n                                                     get_quantization_config)\nfrom vllm.model_executor.models import ModelRegistry\nfrom vllm.platforms import current_platform\nfrom vllm.tracing import is_otel_available, otel_import_error_traceback\nfrom vllm.transformers_utils.config import (\n    ConfigFormat, get_config, get_hf_image_processor_config,\n    get_hf_text_config, get_pooling_config,\n    get_sentence_transformer_tokenizer_config, is_encoder_decoder,\n    try_get_generation_config, uses_mrope)\nfrom vllm.transformers_utils.s3_utils import S3Model\nfrom vllm.transformers_utils.utils import is_s3, maybe_model_redirect\nfrom vllm.utils import (GiB_bytes, LayerBlockType, cuda_device_count_stateless,\n                        get_cpu_memory, get_open_port, is_torch_equal_or_newer,\n                        random_uuid, resolve_obj_by_qualname)\n\nif TYPE_CHECKING:\n    from _typeshed import DataclassInstance\n    from ray.util.placement_group import PlacementGroup\n\n    from vllm.executor.executor_base import ExecutorBase\n    from vllm.model_executor.layers.quantization.base_config import (\n        QuantizationConfig)\n    from vllm.model_executor.model_loader import BaseModelLoader\n\n    ConfigType = type[DataclassInstance]\nelse:\n    QuantizationConfig = Any\n    ConfigType = type\n\nlogger = init_logger(__name__)\n\nConfigT = TypeVar(\"ConfigT\", bound=ConfigType)\n\n# This value is chosen to have a balance between ITL and TTFT. Note it is\n# not optimized for throughput.\n_DEFAULT_MAX_NUM_BATCHED_TOKENS = 2048\n_POOLING_MODEL_MAX_NUM_BATCHED_TOKENS = 32768\n_MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS = 5120\n\nTaskOption = Literal[\"auto\", \"generate\", \"embedding\", \"embed\", \"classify\",\n                     \"score\", \"reward\", \"transcription\"]\n\n_ResolvedTask = Literal[\"generate\", \"embed\", \"classify\", \"score\", \"reward\",\n                        \"draft\", \"transcription\"]\n\nRunnerType = Literal[\"generate\", \"pooling\", \"draft\", \"transcription\"]\n\n_RUNNER_TASKS: dict[RunnerType, list[_ResolvedTask]] = {\n    \"generate\": [\"generate\"],\n    \"pooling\": [\"embed\", \"classify\", \"score\", \"reward\"],\n    \"draft\": [\"draft\"],\n    \"transcription\": [\"transcription\"],\n}\n\n_TASK_RUNNER: dict[_ResolvedTask, RunnerType] = {\n    task: runner\n    for runner, tasks in _RUNNER_TASKS.items()\n    for task in tasks\n}\n\nHfOverrides = Union[dict[str, Any], Callable[[PretrainedConfig],\n                                             PretrainedConfig]]\n\n\nclass SupportsHash(Protocol):\n\n    def compute_hash(self) -> str:\n        ...\n\n\nclass SupportsMetricsInfo(Protocol):\n\n    def metrics_info(self) -> dict[str, str]:\n        ...\n\n\nclass ModelImpl(str, enum.Enum):\n    AUTO = \"auto\"\n    VLLM = \"vllm\"\n    TRANSFORMERS = \"transformers\"\n\n\ndef get_attr_docs(cls: type[Any]) -> dict[str, str]:\n    \"\"\"\n    Get any docstrings placed after attribute assignments in a class body.\n\n    https://davidism.com/mit-license/\n    \"\"\"\n\n    def pairwise(iterable):\n        \"\"\"\n        Manually implement https://docs.python.org/3/library/itertools.html#itertools.pairwise\n\n        Can be removed when Python 3.9 support is dropped.\n        \"\"\"\n        iterator = iter(iterable)\n        a = next(iterator, None)\n\n        for b in iterator:\n            yield a, b\n            a = b\n\n    cls_node = ast.parse(textwrap.dedent(inspect.getsource(cls))).body[0]\n\n    if not isinstance(cls_node, ast.ClassDef):\n        raise TypeError(\"Given object was not a class.\")\n\n    out = {}\n\n    # Consider each pair of nodes.\n    for a, b in pairwise(cls_node.body):\n        # Must be an assignment then a constant string.\n        if (not isinstance(a, (ast.Assign, ast.AnnAssign))\n                or not isinstance(b, ast.Expr)\n                or not isinstance(b.value, ast.Constant)\n                or not isinstance(b.value.value, str)):\n            continue\n\n        doc = inspect.cleandoc(b.value.value)\n\n        # An assignment can have multiple targets (a = b = v), but an\n        # annotated assignment only has one target.\n        targets = a.targets if isinstance(a, ast.Assign) else [a.target]\n\n        for target in targets:\n            # Must be assigning to a plain name.\n            if not isinstance(target, ast.Name):\n                continue\n\n            out[target.id] = doc\n\n    return out\n\n\ndef config(cls: ConfigT) -> ConfigT:\n    \"\"\"\n    A decorator that ensures all fields in a dataclass have default values\n    and that each field has a docstring.\n\n    If a `ConfigT` is used as a CLI argument itself, the default value provided\n    by `get_kwargs` will be the result parsing a JSON string as the kwargs\n    (i.e. `ConfigT(**json.loads(cli_arg))`). However, if a particular `ConfigT`\n    requires custom construction from CLI (i.e. `CompilationConfig`), it can\n    have a `from_cli` method, which will be called instead.\n    \"\"\"\n    if not is_dataclass(cls):\n        raise TypeError(\"The decorated class must be a dataclass.\")\n    attr_docs = get_attr_docs(cls)\n    for f in fields(cls):\n        if f.init and f.default is MISSING and f.default_factory is MISSING:\n            raise ValueError(\n                f\"Field '{f.name}' in {cls.__name__} must have a default value.\"\n            )\n\n        if f.name not in attr_docs:\n            raise ValueError(\n                f\"Field '{f.name}' in {cls.__name__} must have a docstring.\")\n\n        if get_origin(f.type) is Union:\n            args = get_args(f.type)\n            literal_args = [arg for arg in args if get_origin(arg) is Literal]\n            if len(literal_args) > 1:\n                raise ValueError(\n                    f\"Field '{f.name}' in {cls.__name__} must use a single \"\n                    \"Literal type. Please use 'Literal[Literal1, Literal2]' \"\n                    \"instead of 'Union[Literal1, Literal2]'.\")\n    return cls\n\n\ndef get_field(cls: ConfigType, name: str) -> Field:\n    \"\"\"Get the default factory field of a dataclass by name. Used for getting\n    default factory fields in `EngineArgs`.\"\"\"\n    if not is_dataclass(cls):\n        raise TypeError(\"The given class is not a dataclass.\")\n    cls_fields = {f.name: f for f in fields(cls)}\n    if name not in cls_fields:\n        raise ValueError(f\"Field '{name}' not found in {cls.__name__}.\")\n    named_field: Field = cls_fields[name]\n    if (default_factory := named_field.default_factory) is not MISSING:\n        return field(default_factory=default_factory)\n    if (default := named_field.default) is not MISSING:\n        return field(default=default)\n    raise ValueError(\n        f\"{cls.__name__}.{name} must have a default value or default factory.\")\n\n\ndef is_init_field(cls: ConfigType, name: str) -> bool:\n    return next(f for f in fields(cls) if f.name == name).init\n\n\nTokenizerMode = Literal[\"auto\", \"slow\", \"mistral\", \"custom\"]\nModelDType = Literal[\"auto\", \"half\", \"float16\", \"bfloat16\", \"float\", \"float32\"]\n\n\n@config\n@dataclass\nclass ModelConfig:\n    \"\"\"Configuration for the model.\"\"\"\n\n    model: str = \"facebook/opt-125m\"\n    \"\"\"Name or path of the Hugging Face model to use. It is also used as the\n    content for `model_name` tag in metrics output when `served_model_name` is\n    not specified.\"\"\"\n    task: Literal[TaskOption, Literal[\"draft\"]] = \"auto\"\n    \"\"\"The task to use the model for. Each vLLM instance only supports one\n    task, even if the same model can be used for multiple tasks. When the model\n    only supports one task, \"auto\" can be used to select it; otherwise, you\n    must specify explicitly which task to use.\"\"\"\n    tokenizer: str = None  # type: ignore\n    \"\"\"Name or path of the Hugging Face tokenizer to use. If unspecified, model\n    name or path will be used.\"\"\"\n    tokenizer_mode: TokenizerMode = \"auto\"\n    \"\"\"Tokenizer mode:\\n\n    - \"auto\" will use the fast tokenizer if available.\\n\n    - \"slow\" will always use the slow tokenizer.\\n\n    - \"mistral\" will always use the tokenizer from `mistral_common`.\\n\n    - \"custom\" will use --tokenizer to select the preregistered tokenizer.\"\"\"\n    trust_remote_code: bool = False\n    \"\"\"Trust remote code (e.g., from HuggingFace) when downloading the model\n    and tokenizer.\"\"\"\n    dtype: Union[ModelDType, torch.dtype] = \"auto\"\n    \"\"\"Data type for model weights and activations:\\n\n    - \"auto\" will use FP16 precision for FP32 and FP16 models, and BF16\n    precision for BF16 models.\\n\n    - \"half\" for FP16. Recommended for AWQ quantization.\\n\n    - \"float16\" is the same as \"half\".\\n\n    - \"bfloat16\" for a balance between precision and range.\\n\n    - \"float\" is shorthand for FP32 precision.\\n\n    - \"float32\" for FP32 precision.\"\"\"\n    seed: Optional[int] = None\n    \"\"\"Random seed for reproducibility. Initialized to None in V0, but\n    initialized to 0 in V1.\"\"\"\n    hf_config_path: Optional[str] = None\n    \"\"\"Name or path of the Hugging Face config to use. If unspecified, model\n    name or path will be used.\"\"\"\n    allowed_local_media_path: str = \"\"\n    \"\"\"Allowing API requests to read local images or videos from directories\n    specified by the server file system. This is a security risk. Should only\n    be enabled in trusted environments.\"\"\"\n    revision: Optional[str] = None\n    \"\"\"The specific model version to use. It can be a branch name, a tag name,\n    or a commit id. If unspecified, will use the default version.\"\"\"\n    code_revision: Optional[str] = None\n    \"\"\"The specific revision to use for the model code on the Hugging Face Hub.\n    It can be a branch name, a tag name, or a commit id. If unspecified, will\n    use the default version.\"\"\"\n    rope_scaling: dict[str, Any] = field(default_factory=dict)\n    \"\"\"RoPE scaling configuration. For example,\n    `{\"rope_type\":\"dynamic\",\"factor\":2.0}`.\"\"\"\n    rope_theta: Optional[float] = None\n    \"\"\"RoPE theta. Use with `rope_scaling`. In some cases, changing the RoPE\n    theta improves the performance of the scaled model.\"\"\"\n    tokenizer_revision: Optional[str] = None\n    \"\"\"The specific revision to use for the tokenizer on the Hugging Face Hub.\n    It can be a branch name, a tag name, or a commit id. If unspecified, will\n    use the default version.\"\"\"\n    max_model_len: int = None  # type: ignore\n    \"\"\"Model context length (prompt and output). If unspecified, will be\n    automatically derived from the model config.\n\n    When passing via `--max-model-len`, supports k/m/g/K/M/G in human-readable\n    format. Examples:\\n\n    - 1k -> 1000\\n\n    - 1K -> 1024\\n\n    - 25.6k -> 25,600\"\"\"\n    spec_target_max_model_len: Optional[int] = None\n    \"\"\"Specify the maximum length for spec decoding draft models.\"\"\"\n    quantization: Optional[QuantizationMethods] = None\n    \"\"\"Method used to quantize the weights. If `None`, we first check the\n    `quantization_config` attribute in the model config file. If that is\n    `None`, we assume the model weights are not quantized and use `dtype` to\n    determine the data type of the weights.\"\"\"\n    enforce_eager: bool = False\n    \"\"\"Whether to always use eager-mode PyTorch. If True, we will disable CUDA\n    graph and always execute the model in eager mode. If False, we will use\n    CUDA graph and eager execution in hybrid for maximal performance and\n    flexibility.\"\"\"\n    max_seq_len_to_capture: int = 8192\n    \"\"\"Maximum sequence len covered by CUDA graphs. When a sequence has context\n    length larger than this, we fall back to eager mode. Additionally for\n    encoder-decoder models, if the sequence length of the encoder input is\n    larger than this, we fall back to the eager mode.\"\"\"\n    max_logprobs: int = 20\n    \"\"\"Maximum number of log probabilities to return when `logprobs` is\n    specified in `SamplingParams`. The default value comes the default for the\n    OpenAI Chat Completions API.\"\"\"\n    disable_sliding_window: bool = False\n    \"\"\"Whether to disable sliding window. If True, we will disable the sliding\n    window functionality of the model, capping to sliding window size. If the\n    model does not support sliding window, this argument is ignored.\"\"\"\n    disable_cascade_attn: bool = False\n    \"\"\"Disable cascade attention for V1. While cascade attention does not\n    change the mathematical correctness, disabling it could be useful for\n    preventing potential numerical issues. Note that even if this is set to\n    False, cascade attention will be only used when the heuristic tells that\n    it's beneficial.\"\"\"\n    skip_tokenizer_init: bool = False\n    \"\"\"Skip initialization of tokenizer and detokenizer. Expects valid\n    `prompt_token_ids` and `None` for prompt from the input. The generated\n    output will contain token ids.\"\"\"\n    enable_prompt_embeds: bool = False\n    \"\"\"If `True`, enables passing text embeddings as inputs via the\n    `prompt_embeds` key. Note that enabling this will double the time required\n    for graph compilation.\"\"\"\n    served_model_name: Optional[Union[str, list[str]]] = None\n    \"\"\"The model name(s) used in the API. If multiple names are provided, the\n    server will respond to any of the provided names. The model name in the\n    model field of a response will be the first name in this list. If not\n    specified, the model name will be the same as the `--model` argument. Noted\n    that this name(s) will also be used in `model_name` tag content of\n    prometheus metrics, if multiple names provided, metrics tag will take the\n    first one.\"\"\"\n    limit_mm_per_prompt: dict[str, int] = field(default_factory=dict)\n    \"\"\"Maximum number of data items per modality per prompt. Only applicable\n    for multimodal models.\"\"\"\n    use_async_output_proc: bool = True\n    \"\"\"Whether to use async output processor.\"\"\"\n    config_format: Union[str, ConfigFormat] = ConfigFormat.AUTO.value\n    \"\"\"The format of the model config to load:\\n\n    - \"auto\" will try to load the config in hf format if available else it\n    will try to load in mistral format.\\n\n    - \"hf\" will load the config in hf format.\\n\n    - \"mistral\" will load the config in mistral format.\"\"\"\n    hf_token: Optional[Union[bool, str]] = None\n    \"\"\"The token to use as HTTP bearer authorization for remote files . If\n    `True`, will use the token generated when running `huggingface-cli login`\n    (stored in `~/.huggingface`).\"\"\"\n    hf_overrides: HfOverrides = field(default_factory=dict)\n    \"\"\"If a dictionary, contains arguments to be forwarded to the Hugging Face\n    config. If a callable, it is called to update the HuggingFace config.\"\"\"\n    mm_processor_kwargs: Optional[dict[str, Any]] = None\n    \"\"\"Arguments to be forwarded to the model's processor for multi-modal data,\n    e.g., image processor. Overrides for the multi-modal processor obtained\n    from `AutoProcessor.from_pretrained`. The available overrides depend on the\n    model that is being run. For example, for Phi-3-Vision: `{\"num_crops\": 4}`.\n    \"\"\"\n    disable_mm_preprocessor_cache: bool = False\n    \"\"\"If `True`, disable caching of the multi-modal preprocessor/mapper (not\n    recommended).\"\"\"\n    override_neuron_config: dict[str, Any] = field(default_factory=dict)\n    \"\"\"Initialize non-default neuron config or override default neuron config\n    that are specific to Neuron devices, this argument will be used to\n    configure the neuron config that can not be gathered from the vllm\n    arguments. e.g. `{\"cast_logits_dtype\": \"bloat16\"}`.\"\"\"\n    pooler_config: Optional[\"PoolerConfig\"] = field(init=False)\n    \"\"\"Pooler config which controls the behaviour of output pooling in pooling\n    models.\"\"\"\n    override_pooler_config: Optional[Union[dict, \"PoolerConfig\"]] = None\n    \"\"\"Initialize non-default pooling config or override default pooling config\n    for the pooling model. e.g. `{\"pooling_type\": \"mean\", \"normalize\": false}`.\n    \"\"\"\n    logits_processor_pattern: Optional[str] = None\n    \"\"\"Optional regex pattern specifying valid logits processor qualified names\n    that can be passed with the `logits_processors` extra completion argument.\n    Defaults to `None`, which allows no processors.\"\"\"\n    generation_config: str = \"auto\"\n    \"\"\"The folder path to the generation config. Defaults to `\"auto\"`, the\n    generation config will be loaded from model path. If set to `\"vllm\"`, no\n    generation config is loaded, vLLM defaults will be used. If set to a folder\n    path, the generation config will be loaded from the specified folder path.\n    If `max_new_tokens` is specified in generation config, then it sets a\n    server-wide limit on the number of output tokens for all requests.\"\"\"\n    override_generation_config: dict[str, Any] = field(default_factory=dict)\n    \"\"\"Overrides or sets generation config. e.g. `{\"temperature\": 0.5}`. If\n    used with `--generation-config auto`, the override parameters will be\n    merged with the default config from the model. If used with\n    `--generation-config vllm`, only the override parameters are used.\"\"\"\n    enable_sleep_mode: bool = False\n    \"\"\"Enable sleep mode for the engine (only cuda platform is supported).\"\"\"\n    model_impl: Union[str, ModelImpl] = ModelImpl.AUTO.value\n    \"\"\"Which implementation of the model to use:\\n\n    - \"auto\" will try to use the vLLM implementation, if it exists, and fall\n    back to the Transformers implementation if no vLLM implementation is\n    available.\\n\n    - \"vllm\" will use the vLLM model implementation.\\n\n    - \"transformers\" will use the Transformers model implementation.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.model)\n        factors.append(self.dtype)\n        factors.append(self.quantization)\n        factors.append(self.revision)\n        factors.append(self.code_revision)\n        factors.append(self.max_model_len)\n        factors.append(self.max_logprobs)\n        factors.append(self.disable_sliding_window)\n        factors.append(self.trust_remote_code)\n        factors.append(self.generation_config)\n        factors.append(self.model_impl)\n        factors.append(self.override_generation_config)\n        factors.append(self.rope_scaling)\n        factors.append(self.rope_theta)\n        # hf_config can control how the model looks!\n        factors.append(self.hf_config.to_json_string())\n        str_factors = str(factors)\n        assert_hashable(str_factors)\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __post_init__(self) -> None:\n        # Set the default seed to 0 in V1.\n        # NOTE(woosuk): In V0, we set the default seed to None because the\n        # driver worker shares the same process as the user process, and thus\n        # setting a seed affects the user process as well.\n        # In V1, we use separate processes for workers (unless\n        # VLLM_ENABLE_V1_MULTIPROCESSING=0), so setting a seed here\n        # doesn't affect the user process. However, without a consistent seed,\n        # different tensor parallel workers would sample different tokens,\n        # leading to inconsistent results.\n        if envs.VLLM_USE_V1 and self.seed is None:\n            self.seed = 0\n            if not envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n                logger.warning(\n                    \"The global random seed is set to %d. Since \"\n                    \"VLLM_ENABLE_V1_MULTIPROCESSING is set to False, this may \"\n                    \"affect the random state of the Python process that \"\n                    \"launched vLLM.\", self.seed)\n\n        self.model = maybe_model_redirect(self.model)\n        # The tokenizer is consistent with the model by default.\n        if self.tokenizer is None:\n            self.tokenizer = self.model\n        if self.tokenizer_revision is None:\n            self.tokenizer_revision = self.revision\n        self.tokenizer = maybe_model_redirect(self.tokenizer)\n\n        if isinstance(self.hf_config_path, str):\n            self.hf_config_path = maybe_model_redirect(self.hf_config_path)\n\n        if callable(self.hf_overrides):\n            hf_overrides_kw = {}\n            hf_overrides_fn = self.hf_overrides\n        else:\n            hf_overrides_kw = self.hf_overrides\n            hf_overrides_fn = None\n\n        if self.rope_scaling:\n            hf_override: dict[str, Any] = {\"rope_scaling\": self.rope_scaling}\n            hf_overrides_kw.update(hf_override)\n            hf_overrides_str = json.dumps(hf_overrides_kw)\n            msg = (\n                \"`--rope-scaling` will be removed in a future release. \"\n                f\"'Please instead use `--hf-overrides '{hf_overrides_str}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n        if self.rope_theta is not None:\n            hf_override = {\"rope_theta\": self.rope_theta}\n            hf_overrides_kw.update(hf_override)\n            hf_overrides_str = json.dumps(hf_overrides_kw)\n            msg = (\n                \"`--rope-theta` will be removed in a future release. \"\n                f\"'Please instead use `--hf-overrides '{hf_overrides_str}'`\")\n            warnings.warn(DeprecationWarning(msg), stacklevel=2)\n\n        self.maybe_pull_model_tokenizer_for_s3(self.model, self.tokenizer)\n\n        if (backend := envs.VLLM_ATTENTION_BACKEND\n            ) and backend == \"FLASHINFER\" and find_spec(\"flashinfer\") is None:\n            raise ValueError(\n                \"VLLM_ATTENTION_BACKEND is set to FLASHINFER, but flashinfer \"\n                \"module was not found. See \"\n                \"https://github.com/vllm-project/vllm/blob/main/docker/Dockerfile \"  # noqa: E501\n                \"for instructions on how to install it.\")\n\n        from vllm.platforms import current_platform\n\n        if (self.enable_sleep_mode\n                and not current_platform.is_sleep_mode_available()):\n            raise ValueError(\n                \"Sleep mode is not supported on current platform.\")\n\n        if isinstance(self.config_format, str):\n            self.config_format = ConfigFormat(self.config_format)\n\n        hf_config = get_config(self.hf_config_path or self.model,\n                               self.trust_remote_code, self.revision,\n                               self.code_revision, self.config_format)\n\n        if hf_overrides_kw:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_kw)\n            hf_config.update(hf_overrides_kw)\n        if hf_overrides_fn:\n            logger.info(\"Overriding HF config with %s\", hf_overrides_fn)\n            hf_config = hf_overrides_fn(hf_config)\n\n        self.hf_config = hf_config\n\n        self.hf_text_config = get_hf_text_config(self.hf_config)\n        self.attention_chunk_size = getattr(self.hf_text_config,\n                                            \"attention_chunk_size\", None)\n        self.encoder_config = self._get_encoder_config()\n        self.hf_image_processor_config = get_hf_image_processor_config(\n            self.model, hf_token=self.hf_token, revision=self.revision)\n        self.dtype = _get_and_verify_dtype(self.hf_config, self.dtype)\n\n        interleaved_attn_models = [\"gemma2\", \"gemma3_text\", \"cohere2\"]\n        sliding_window = getattr(self.hf_text_config, \"sliding_window\", None)\n        has_interleaved_attention = (sliding_window is not None) and (\n            isinstance(sliding_window, list) or\n            (self.hf_text_config.model_type in interleaved_attn_models))\n\n        if (not self.disable_sliding_window and has_interleaved_attention):\n            if (backend :=\n                    envs.VLLM_ATTENTION_BACKEND) in (\"XFORMERS\", \"FLASHINFER\"):\n                sliding_window_len_min = get_min_sliding_window(\n                    self.hf_text_config.sliding_window)\n\n                logger.warning_once(\n                    \"%s has interleaved attention, which is currently not supported by the %s backend. Disabling sliding window and capping the max length to the sliding window size (%d).\",  # noqa: E501\n                    self.hf_text_config.model_type,\n                    backend,\n                    sliding_window_len_min,\n                )\n                self.disable_sliding_window = True\n            else:\n                # for a model with interleaved attention,\n                # the scheduler and the model treat it as full attention\n                # (i.e., not dropping any tokens outside the window).\n                # only the attention layer itself is aware of the sliding\n                # window, and use the window size to compute the attention.\n                self.hf_text_config.interleaved_sliding_window = sliding_window\n                delattr(self.hf_text_config, \"sliding_window\")\n                sliding_window = None\n\n        self.max_model_len = _get_and_verify_max_len(\n            hf_config=self.hf_text_config,\n            max_model_len=self.max_model_len,\n            disable_sliding_window=self.disable_sliding_window,\n            sliding_window_len=self.get_hf_config_sliding_window(),\n            spec_target_max_model_len=self.spec_target_max_model_len,\n            encoder_config=self.encoder_config)\n        self.served_model_name = get_served_model_name(self.model,\n                                                       self.served_model_name)\n        self.multimodal_config = self._init_multimodal_config()\n        if not self.skip_tokenizer_init:\n            self._verify_tokenizer_mode()\n\n        self.is_attention_free = self._init_attention_free()\n        self.is_hybrid = self._init_is_hybrid()\n        self.has_noops = self._init_has_noops()\n        self.has_inner_state = self._init_has_inner_state()\n\n        if (not current_platform.is_neuron() and self.override_neuron_config):\n            raise ValueError(\n                \"`override_neuron_config` is only supported on Neuron.\")\n\n        supported_tasks, task = self._resolve_task(self.task)\n        self.supported_tasks = supported_tasks\n        self.task = task\n        if self.task in (\"draft\", \"generate\"):\n            self.truncation_side = \"left\"\n        else:\n            self.truncation_side = \"right\"\n\n        self.pooler_config = self._init_pooler_config()\n\n        self._verify_quantization()\n        self._verify_cuda_graph()\n        self._verify_bnb_config()\n\n    @property\n    def registry(self):\n        return ModelRegistry\n\n    @property\n    def architectures(self) -> list[str]:\n        return getattr(self.hf_config, \"architectures\", [])\n\n    def maybe_pull_model_tokenizer_for_s3(self, model: str,\n                                          tokenizer: str) -> None:\n        \"\"\"Pull model/tokenizer from S3 to temporary directory when needed.\n        \n        Args:\n            model: Model name or path\n            tokenizer: Tokenizer name or path\n        \"\"\"\n        if not (is_s3(model) or is_s3(tokenizer)):\n            return\n\n        if is_s3(model):\n            s3_model = S3Model()\n            s3_model.pull_files(model,\n                                allow_pattern=[\"*.model\", \"*.py\", \"*.json\"])\n            self.model_weights = model\n            self.model = s3_model.dir\n\n            # If tokenizer is same as model, download to same directory\n            if model == tokenizer:\n                s3_model.pull_files(\n                    model, ignore_pattern=[\"*.pt\", \"*.safetensors\", \"*.bin\"])\n                self.tokenizer = s3_model.dir\n                return\n\n        # Only download tokenizer if needed and not already handled\n        if is_s3(tokenizer):\n            s3_tokenizer = S3Model()\n            s3_tokenizer.pull_files(\n                model, ignore_pattern=[\"*.pt\", \"*.safetensors\", \"*.bin\"])\n            self.tokenizer = s3_tokenizer.dir\n\n    def _init_multimodal_config(self) -> Optional[\"MultiModalConfig\"]:\n        if self.registry.is_multimodal_model(self.architectures):\n            return MultiModalConfig(\n                limit_per_prompt=self.limit_mm_per_prompt,\n                mm_processor_kwargs=self.mm_processor_kwargs,\n                disable_mm_preprocessor_cache=self.\n                disable_mm_preprocessor_cache)\n\n        if self.limit_mm_per_prompt:\n            raise ValueError(\"`limit_mm_per_prompt` is only supported for \"\n                             \"multimodal models.\")\n        if self.mm_processor_kwargs:\n            raise ValueError(\"`mm_processor_kwargs` is only supported for \"\n                             \"multimodal models.\")\n        if self.disable_mm_preprocessor_cache:\n            raise ValueError(\"`disable_mm_preprocessor_cache` is only \"\n                             \"supported for multimodal models.\")\n\n        return None\n\n    def _get_encoder_config(self):\n        return get_sentence_transformer_tokenizer_config(\n            self.model, self.revision)\n\n    def _init_pooler_config(self) -> Optional[\"PoolerConfig\"]:\n\n        if self.runner_type == \"pooling\":\n            if isinstance(self.override_pooler_config, dict):\n                self.override_pooler_config = PoolerConfig(\n                    **self.override_pooler_config)\n\n            pooler_config = self.override_pooler_config or PoolerConfig()\n\n            base_config = get_pooling_config(self.model, self.revision)\n            if base_config is not None:\n                # Only set values that are not overridden by the user\n                for k, v in base_config.items():\n                    if getattr(pooler_config, k) is None:\n                        setattr(pooler_config, k, v)\n\n            if self.is_matryoshka:\n                if pooler_config.normalize is None:\n                    pooler_config.normalize = True\n                elif not pooler_config.normalize:\n                    raise ValueError(\n                        \"`normalize` must be enabled (set to True) \"\n                        \"for models that are compatible with \"\n                        \"Matryoshka Representation.\")\n\n            return pooler_config\n\n        return None\n\n    def _init_attention_free(self) -> bool:\n        return self.registry.is_attention_free_model(self.architectures)\n\n    def _init_is_hybrid(self) -> bool:\n        return self.registry.is_hybrid_model(self.architectures)\n\n    def _init_has_noops(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return self.registry.is_noops_model(architectures)\n\n    def _init_has_inner_state(self) -> bool:\n        return self.registry.model_has_inner_state(self.architectures)\n\n    def _verify_tokenizer_mode(self) -> None:\n        tokenizer_mode = cast(TokenizerMode, self.tokenizer_mode.lower())\n        if tokenizer_mode not in get_args(TokenizerMode):\n            raise ValueError(\n                f\"Unknown tokenizer mode: {self.tokenizer_mode}. Must be \"\n                f\"one of {get_args(TokenizerMode)}.\")\n        self.tokenizer_mode = tokenizer_mode\n\n    def _get_preferred_task(\n        self,\n        architectures: list[str],\n        supported_tasks: set[_ResolvedTask],\n    ) -> Optional[_ResolvedTask]:\n        model_id = self.model\n        if get_pooling_config(model_id, self.revision):\n            return \"embed\"\n        if self.registry.is_cross_encoder_model(architectures):\n            return \"score\"\n        if self.registry.is_transcription_model(architectures):\n            return \"transcription\"\n\n        suffix_to_preferred_task: list[tuple[str, _ResolvedTask]] = [\n            # Other models follow this pattern\n            (\"ForCausalLM\", \"generate\"),\n            (\"ForConditionalGeneration\", \"generate\"),\n            (\"ForSequenceClassification\", \"classify\"),\n            (\"ChatModel\", \"generate\"),\n            (\"LMHeadModel\", \"generate\"),\n            (\"EmbeddingModel\", \"embed\"),\n            (\"RewardModel\", \"reward\"),\n        ]\n        _, arch = self.registry.inspect_model_cls(architectures)\n\n        for suffix, pref_task in suffix_to_preferred_task:\n            if arch.endswith(suffix) and pref_task in supported_tasks:\n                return pref_task\n\n        return None\n\n    def _resolve_task(\n        self,\n        task_option: Literal[TaskOption, Literal[\"draft\"]],\n    ) -> tuple[set[_ResolvedTask], _ResolvedTask]:\n        if task_option == \"draft\":\n            return {\"draft\"}, \"draft\"\n\n        registry = self.registry\n        architectures = self.architectures\n\n        runner_support: dict[RunnerType, bool] = {\n            # NOTE: Listed from highest to lowest priority,\n            # in case the model supports multiple of them\n            \"transcription\": registry.is_transcription_model(architectures),\n            \"generate\": registry.is_text_generation_model(architectures),\n            \"pooling\": registry.is_pooling_model(architectures),\n        }\n        supported_runner_types_lst: list[RunnerType] = [\n            runner_type\n            for runner_type, is_supported in runner_support.items()\n            if is_supported\n        ]\n\n        supported_tasks_lst: list[_ResolvedTask] = [\n            task for runner_type in supported_runner_types_lst\n            for task in _RUNNER_TASKS[runner_type]\n        ]\n        supported_tasks = set(supported_tasks_lst)\n\n        if task_option == \"auto\":\n            selected_task = next(iter(supported_tasks_lst))\n\n            if len(supported_tasks_lst) > 1:\n                preferred_task = self._get_preferred_task(\n                    architectures, supported_tasks)\n                if preferred_task is not None:\n                    selected_task = preferred_task\n\n                logger.info(\n                    \"This model supports multiple tasks: %s. \"\n                    \"Defaulting to '%s'.\", supported_tasks, selected_task)\n        else:\n            # Aliases\n            if task_option == \"embedding\":\n                preferred_task = self._get_preferred_task(\n                    architectures, supported_tasks)\n                if preferred_task != \"embed\":\n                    msg = (\"The 'embedding' task will be restricted to \"\n                           \"embedding models in a future release. Please \"\n                           \"pass `--task classify`, `--task score`, or \"\n                           \"`--task reward` explicitly for other pooling \"\n                           \"models.\")\n                    warnings.warn(msg, DeprecationWarning, stacklevel=2)\n\n                task_option = preferred_task or \"embed\"\n\n            if task_option not in supported_tasks:\n                msg = (\n                    f\"This model does not support the '{task_option}' task. \"\n                    f\"Supported tasks: {supported_tasks}\")\n                raise ValueError(msg)\n\n            selected_task = task_option\n\n        return supported_tasks, selected_task\n\n    def _parse_quant_hf_config(self):\n        quant_cfg = getattr(self.hf_config, \"quantization_config\", None)\n        if quant_cfg is None:\n            # compressed-tensors uses a \"compression_config\" key\n            quant_cfg = getattr(self.hf_config, \"compression_config\", None)\n        return quant_cfg\n\n    def _verify_quantization(self) -> None:\n        supported_quantization = QUANTIZATION_METHODS\n        optimized_quantization_methods = [\n            \"fp8\", \"marlin\", \"modelopt\", \"gptq_marlin_24\", \"gptq_marlin\",\n            \"awq_marlin\", \"fbgemm_fp8\", \"compressed-tensors\", \"experts_int8\",\n            \"quark\", \"nvfp4\", \"bitblas\", \"gptq_bitblas\"\n        ]\n        if self.quantization is not None:\n            self.quantization = cast(QuantizationMethods,\n                                     self.quantization.lower())\n\n        # Parse quantization method from the HF model config, if available.\n        quant_cfg = self._parse_quant_hf_config()\n\n        if quant_cfg is not None:\n            quant_method = quant_cfg.get(\"quant_method\", \"\").lower()\n            quant_method = quant_method.replace(\"compressed_tensors\",\n                                                \"compressed-tensors\")\n            quant_cfg[\"quant_method\"] = quant_method\n\n            # Quantization methods which are overrides (i.e. they have a\n            # `override_quantization_method` method) must be checked in order\n            # of preference (this is particularly important for GPTQ).\n            overrides = [\n                \"marlin\",\n                \"bitblas\",\n                \"gptq_marlin_24\",\n                \"gptq_marlin\",\n                \"gptq_bitblas\",\n                \"awq_marlin\",\n                \"ipex\",\n                \"moe_wna16\",\n            ]\n            quantization_methods = [\n                q for q in supported_quantization if q not in overrides\n            ]\n            # Any custom overrides will be in quantization_methods so we place\n            # them at the start of the list so custom overrides have preference\n            # over the built in ones.\n            quantization_methods = quantization_methods + overrides\n\n            # Detect which checkpoint is it\n            for name in quantization_methods:\n                method = get_quantization_config(name)\n                quantization_override = method.override_quantization_method(\n                    quant_cfg, self.quantization)\n                if quantization_override is not None:\n                    # Raise error if the override is not custom (custom would\n                    # be in QUANTIZATION_METHODS but not QuantizationMethods)\n                    # and hasn't been added to the overrides list.\n                    if (name in get_args(QuantizationMethods)\n                            and name not in overrides):\n                        raise ValueError(\n                            f\"Quantization method {name} is an override but \"\n                            \"is has not been added to the `overrides` list \"\n                            \"above. This is necessary to ensure that the \"\n                            \"overrides are checked in order of preference.\")\n                    quant_method = quantization_override\n                    self.quantization = quantization_override\n                    break\n\n            # Verify quantization configurations.\n            if self.quantization is None:\n                self.quantization = quant_method\n            elif self.quantization != quant_method:\n                raise ValueError(\n                    \"Quantization method specified in the model config \"\n                    f\"({quant_method}) does not match the quantization \"\n                    f\"method specified in the `quantization` argument \"\n                    f\"({self.quantization}).\")\n\n        if self.quantization is not None:\n            if self.quantization not in supported_quantization:\n                raise ValueError(\n                    f\"Unknown quantization method: {self.quantization}. Must \"\n                    f\"be one of {supported_quantization}.\")\n            from vllm.platforms import current_platform\n            current_platform.verify_quantization(self.quantization)\n            if self.quantization not in optimized_quantization_methods:\n                logger.warning(\n                    \"%s quantization is not fully \"\n                    \"optimized yet. The speed can be slower than \"\n                    \"non-quantized models.\", self.quantization)\n\n    def _verify_cuda_graph(self) -> None:\n        self.max_seq_len_to_capture = min(self.max_seq_len_to_capture,\n                                          self.max_model_len)\n        # CUDAGraph capture not supported for enc-dec models and mllama on ROCm\n        ROCM_UNSUPPORTED_MODELS = ['mllama']\n        unsupported_rocm = (self.hf_config.model_type\n                            in ROCM_UNSUPPORTED_MODELS\n                            or self.is_encoder_decoder)\n\n        if (unsupported_rocm and not self.enforce_eager\n                and current_platform.is_rocm()):\n            logger.warning(\n                \"CUDA graph is not supported for %s on ROCm yet, fallback \"\n                \"to eager mode.\", self.hf_config.model_type)\n            self.enforce_eager = True\n\n    def _verify_bnb_config(self) -> None:\n        \"\"\"\n        The current version of bitsandbytes (0.45.3) with 8-bit models does not\n        yet support CUDA graph.\n        # TODO Remove this when bitsandbytes supports.\n        \"\"\"\n        is_bitsandbytes = self.quantization == \"bitsandbytes\"\n        has_quantization_config = (getattr(self.hf_config,\n                                           \"quantization_config\", None)\n                                   is not None)\n        is_8bit = (self.hf_config.quantization_config.get(\n            \"load_in_8bit\", False) if has_quantization_config else False)\n        if all([\n                is_bitsandbytes,\n                has_quantization_config,\n                is_8bit,\n                not self.enforce_eager,\n        ]):\n            logger.warning(\n                \"CUDA graph is not supported on BitsAndBytes 8bit yet, \"\n                \"fallback to the eager mode.\")\n\n            self.enforce_eager = True\n\n    def _verify_with_expert_parallelism(self) -> None:\n        num_expert_names = [\n            \"moe_num_experts\",  # Dbrx\n            \"num_experts\",  # Jamba\n            \"n_routed_experts\",  # DeepSeek\n            \"num_local_experts\",  # Mixtral\n        ]\n        num_experts = 0\n        for name in num_expert_names:\n            num_experts = getattr(self.hf_text_config, name, 0)\n            if num_experts > 0:\n                break\n        if num_experts < 1:\n            raise ValueError(\n                \"Number of experts in the model must be greater than 0 \"\n                \"when expert parallelism is enabled.\")\n\n    def verify_dual_chunk_attention_config(\n        self,\n        load_config: \"LoadConfig\",\n    ) -> None:\n        if hasattr(self.hf_config, \"dual_chunk_attention_config\"):\n            # Try loading the sparse attention config\n            from vllm.model_executor.model_loader.weight_utils import (\n                get_sparse_attention_config)\n            sparse_attn_config = get_sparse_attention_config(self, load_config)\n            if sparse_attn_config:\n                self.hf_config.dual_chunk_attention_config[\n                    \"sparse_attention_config\"] = sparse_attn_config\n                if \"sparse_attention_enabled\" not in \\\n                        self.hf_config.dual_chunk_attention_config:\n                    self.hf_config.dual_chunk_attention_config[\n                        \"sparse_attention_enabled\"] = True\n\n    def verify_async_output_proc(self, parallel_config, speculative_config,\n                                 device_config) -> None:\n        if not self.use_async_output_proc:\n            # Nothing to check\n            return\n\n        if parallel_config.pipeline_parallel_size > 1:\n            self.use_async_output_proc = False\n            return\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        from vllm.platforms import current_platform\n        if not current_platform.is_async_output_supported(self.enforce_eager):\n            self.use_async_output_proc = False\n            return\n\n        if envs.VLLM_USE_RAY_SPMD_WORKER:\n            self.use_async_output_proc = False\n            return\n\n        # Async postprocessor is not necessary for pooling models\n        # since there is no token generation\n        if self.runner_type == \"pooling\":\n            self.use_async_output_proc = False\n\n        # Reminder: Please update docs/source/features/compatibility_matrix.md\n        # If the feature combo become valid\n        if speculative_config:\n            self.use_async_output_proc = False\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n\n        if parallel_config.distributed_executor_backend == \"external_launcher\":\n            assert self.seed is not None, (\n                \"Seed must be set when using external launcher backend to \"\n                \"make sure sampling results are the same across workers.\")\n\n        total_num_attention_heads = getattr(self.hf_text_config,\n                                            \"num_attention_heads\", 0)\n        tensor_parallel_size = parallel_config.tensor_parallel_size\n        if total_num_attention_heads % tensor_parallel_size != 0:\n            raise ValueError(\n                f\"Total number of attention heads ({total_num_attention_heads})\"\n                \" must be divisible by tensor parallel size \"\n                f\"({tensor_parallel_size}).\")\n\n        if parallel_config.enable_expert_parallel:\n            self._verify_with_expert_parallelism()\n\n        pipeline_parallel_size = parallel_config.pipeline_parallel_size\n        if pipeline_parallel_size > 1:\n            if not self.registry.is_pp_supported_model(self.architectures):\n                raise NotImplementedError(\n                    \"Pipeline parallelism is not supported for this model. \"\n                    \"Supported models implement the `SupportsPP` interface.\")\n\n            if self.use_async_output_proc:\n                self.use_async_output_proc = False\n\n    def get_hf_config_sliding_window(\n            self) -> Union[Optional[int], list[Optional[int]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\"\"\"\n\n        # Some models, like Qwen2 and Qwen1.5, use `use_sliding_window` in\n        # addition to sliding window size. We check if that field is present\n        # and if it's False, return None.\n        if (hasattr(self.hf_text_config, \"use_sliding_window\")\n                and not self.hf_text_config.use_sliding_window):\n            return None\n        return getattr(self.hf_text_config, \"sliding_window\", None)\n\n    def get_sliding_window(self) -> Optional[Union[int, list[Optional[int]]]]:\n        \"\"\"Get the sliding window size, or None if disabled.\n        \"\"\"\n        # If user disables sliding window, return None.\n        if self.disable_sliding_window:\n            return None\n        # Otherwise get the value from the hf config.\n        return self.get_hf_config_sliding_window()\n\n    def get_vocab_size(self) -> int:\n        return self.hf_text_config.vocab_size\n\n    def get_hidden_size(self) -> int:\n        return self.hf_text_config.hidden_size\n\n    @property\n    def is_deepseek_mla(self) -> bool:\n        if not hasattr(self.hf_text_config, \"model_type\"):\n            return False\n        elif self.hf_text_config.model_type in \\\n            ('deepseek_v2', 'deepseek_v3', 'deepseek_mtp'):\n            return self.hf_text_config.kv_lora_rank is not None\n        elif self.hf_text_config.model_type == 'eagle':\n            # if the model is an EAGLE module, check for the\n            # underlying architecture\n            return self.hf_text_config.model.model_type in \\\n                    ('deepseek_v2', 'deepseek_v3') \\\n                and self.hf_text_config.kv_lora_rank is not None\n        return False\n\n    def get_head_size(self) -> int:\n        # TODO remove hard code\n        if self.is_deepseek_mla:\n            qk_rope_head_dim = getattr(self.hf_text_config, \"qk_rope_head_dim\",\n                                       0)\n            if self.use_mla:\n                return self.hf_text_config.kv_lora_rank + qk_rope_head_dim\n            else:\n                qk_nope_head_dim = getattr(self.hf_text_config,\n                                           \"qk_nope_head_dim\", 0)\n                if qk_rope_head_dim and qk_nope_head_dim:\n                    return qk_rope_head_dim + qk_nope_head_dim\n\n        if hasattr(self.hf_text_config,\n                   \"model_type\") and (self.hf_text_config.model_type\n                                      == \"zamba2\"):\n            return self.hf_text_config.attention_head_dim\n\n        if self.is_attention_free:\n            return 0\n\n        # NOTE: Some configs may set head_dim=None in the config\n        if getattr(self.hf_text_config, \"head_dim\", None) is not None:\n            return self.hf_text_config.head_dim\n\n        # FIXME(woosuk): This may not be true for all models.\n        return (self.hf_text_config.hidden_size //\n                self.hf_text_config.num_attention_heads)\n\n    def get_total_num_kv_heads(self) -> int:\n        \"\"\"Returns the total number of KV heads.\"\"\"\n        # For GPTBigCode & Falcon:\n        # NOTE: for falcon, when new_decoder_architecture is True, the\n        # multi_query flag is ignored and we use n_head_kv for the number of\n        # KV heads.\n        falcon_model_types = [\"falcon\", \"RefinedWeb\", \"RefinedWebModel\"]\n        new_decoder_arch_falcon = (\n            self.hf_config.model_type in falcon_model_types\n            and getattr(self.hf_config, \"new_decoder_architecture\", False))\n        if not new_decoder_arch_falcon and getattr(self.hf_text_config,\n                                                   \"multi_query\", False):\n            # Multi-query attention, only one KV head.\n            # Currently, tensor parallelism is not supported in this case.\n            return 1\n\n        # For DBRX and MPT\n        if self.hf_config.model_type == \"mpt\":\n            if \"kv_n_heads\" in self.hf_config.attn_config:\n                return self.hf_config.attn_config[\"kv_n_heads\"]\n            return self.hf_config.num_attention_heads\n        if self.hf_config.model_type == \"dbrx\":\n            return getattr(self.hf_config.attn_config, \"kv_n_heads\",\n                           self.hf_config.num_attention_heads)\n\n        if self.hf_config.model_type == \"nemotron-nas\":\n            for block in self.hf_config.block_configs:\n                if not block.attention.no_op:\n                    return self.hf_config.num_attention_heads \\\n                        // block.attention.n_heads_in_group\n\n            raise RuntimeError(\"Couldn't determine number of kv heads\")\n\n        if self.is_attention_free:\n            return 0\n\n        attributes = [\n            # For Falcon:\n            \"n_head_kv\",\n            \"num_kv_heads\",\n            # For LLaMA-2:\n            \"num_key_value_heads\",\n            # For ChatGLM:\n            \"multi_query_group_num\",\n        ]\n        for attr in attributes:\n            num_kv_heads = getattr(self.hf_text_config, attr, None)\n            if num_kv_heads is not None:\n                return num_kv_heads\n\n        # For non-grouped-query attention models, the number of KV heads is\n        # equal to the number of attention heads.\n        return self.hf_text_config.num_attention_heads\n\n    def get_num_kv_heads(self, parallel_config: \"ParallelConfig\") -> int:\n        \"\"\"Returns the number of KV heads per GPU.\"\"\"\n        if self.use_mla:\n            # When using MLA during decode it becomes MQA\n            return 1\n\n        total_num_kv_heads = self.get_total_num_kv_heads()\n        # If tensor parallelism is used, we divide the number of KV heads by\n        # the tensor parallel size. We will replicate the KV heads in the\n        # case where the number of KV heads is smaller than the tensor\n        # parallel size so each GPU has at least one KV head.\n        return max(1,\n                   total_num_kv_heads // parallel_config.tensor_parallel_size)\n\n    def get_num_attention_heads(self,\n                                parallel_config: \"ParallelConfig\") -> int:\n        num_heads = getattr(self.hf_text_config, \"num_attention_heads\", 0)\n        return num_heads // parallel_config.tensor_parallel_size\n\n    def get_layers_start_end_indices(\n            self, parallel_config: \"ParallelConfig\") -> tuple[int, int]:\n        from vllm.distributed.utils import get_pp_indices\n        if (self.hf_text_config.model_type == \"deepseek_mtp\"\n                or self.hf_config.model_type == \"mimo_mtp\"):\n            total_num_hidden_layers = getattr(self.hf_text_config,\n                                              \"num_nextn_predict_layers\", 0)\n        else:\n            total_num_hidden_layers = getattr(self.hf_text_config,\n                                              \"num_hidden_layers\", 0)\n        # the layout order is: DP x PP x TP\n        pp_rank = (parallel_config.rank // parallel_config.tensor_parallel_size\n                   ) % parallel_config.pipeline_parallel_size\n        pp_size = parallel_config.pipeline_parallel_size\n        start, end = get_pp_indices(total_num_hidden_layers, pp_rank, pp_size)\n        return start, end\n\n    def get_num_layers(self, parallel_config: \"ParallelConfig\") -> int:\n        start, end = self.get_layers_start_end_indices(parallel_config)\n        return end - start\n\n    def get_num_layers_by_block_type(\n        self,\n        parallel_config: \"ParallelConfig\",\n        block_type: LayerBlockType = LayerBlockType.attention,\n    ) -> int:\n        # This function relies on 'layers_block_type' in hf_config,\n        # for w/o this attribute, we will need to have workarounds like so\n        attn_block_type = block_type == LayerBlockType.attention\n        is_transformer = not self.is_hybrid and \\\n                            not self.has_noops and \\\n                            not self.is_attention_free\n        start, end = self.get_layers_start_end_indices(parallel_config)\n\n        if is_transformer:\n            # Handle the basic case first\n            return end - start if attn_block_type else 0\n        elif self.is_attention_free:\n            # Attention free\n            # Note that this code assumes there\n            # is only one type of attention-free block type.\n            return 0 if attn_block_type else end - start\n        elif self.has_noops:\n            block_configs = self.hf_config.block_configs\n            return sum(not bc.attention.no_op\n                       for bc in block_configs[start:end])\n        else:\n            # Hybrid model Jamba\n            layers_block_type_value = getattr(self.hf_config,\n                                              \"layers_block_type\", None)\n            if layers_block_type_value is not None:\n                if hasattr(self.hf_text_config,\n                           \"model_type\") and (self.hf_text_config.model_type\n                                              == \"zamba2\"):\n                    if attn_block_type:\n                        return sum(t == \"hybrid\"\n                                   for t in layers_block_type_value[start:end])\n                    else:\n                        return self.get_num_layers(parallel_config)\n                return sum(t == block_type.value\n                           for t in layers_block_type_value[start:end])\n\n            # Hybrid model Minimax\n            attn_type_list = getattr(self.hf_config, \"attn_type_list\", None)\n            if attn_type_list:\n                return sum(t == 1 for t in attn_type_list[start:end])\n\n            if layers_block_type_value is None and attn_type_list is None:\n                raise ValueError(\n                    \"The model is an hybrid without a\"\n                    \"layers_block_type or an attn_type_list in the hf_config,\"\n                    \"cannot determine the num of \"\n                    f\"{block_type.value} layers\")\n\n            return sum(t == 1 for t in attn_type_list[start:end])\n\n    def get_multimodal_config(self) -> \"MultiModalConfig\":\n        \"\"\"\n        Get the multimodal configuration of the model.\n\n        Raises:\n            ValueError: If the model is not multimodal.\n        \"\"\"\n        if self.multimodal_config is None:\n            raise ValueError(\"The model is not multimodal.\")\n\n        return self.multimodal_config\n\n    def try_get_generation_config(self) -> dict[str, Any]:\n        if self.generation_config in (\"auto\", \"vllm\"):\n            config = try_get_generation_config(\n                self.hf_config_path or self.model,\n                trust_remote_code=self.trust_remote_code,\n                revision=self.revision,\n            )\n        else:\n            config = try_get_generation_config(\n                self.generation_config,\n                trust_remote_code=self.trust_remote_code,\n            )\n\n        if config is None:\n            return {}\n\n        return config.to_diff_dict()\n\n    def get_diff_sampling_param(self) -> dict[str, Any]:\n        \"\"\"\n        This method returns a dictionary containing the parameters\n        that differ from the default sampling parameters. If\n        `generation_config` is `\"vllm\"`, an empty dictionary is returned.\n\n        Returns:\n            dict[str, Any]: A dictionary with the differing sampling\n            parameters, if `generation_config` is `\"vllm\"` an empty dictionary.\n        \"\"\"\n        if self.generation_config == \"vllm\":\n            config = {}\n        else:\n            config = self.try_get_generation_config()\n\n        # Overriding with given generation config\n        config.update(self.override_generation_config)\n\n        available_params = [\n            \"repetition_penalty\",\n            \"temperature\",\n            \"top_k\",\n            \"top_p\",\n            \"min_p\",\n            \"max_new_tokens\",\n        ]\n        if any(p in config for p in available_params):\n            diff_sampling_param = {\n                p: config.get(p)\n                for p in available_params if config.get(p) is not None\n            }\n            # Huggingface definition of max_new_tokens is equivalent\n            # to vLLM's max_tokens\n            if \"max_new_tokens\" in diff_sampling_param:\n                diff_sampling_param[\"max_tokens\"] = diff_sampling_param.pop(\n                    \"max_new_tokens\")\n        else:\n            diff_sampling_param = {}\n\n        if diff_sampling_param:\n            logger.warning_once(\n                \"Default sampling parameters have been overridden by the \"\n                \"model's Hugging Face generation config recommended from the \"\n                \"model creator. If this is not intended, please relaunch \"\n                \"vLLM instance with `--generation-config vllm`.\")\n        return diff_sampling_param\n\n    @property\n    def is_encoder_decoder(self) -> bool:\n        \"\"\"Extract the HF encoder/decoder model flag.\"\"\"\n        return is_encoder_decoder(self.hf_config)\n\n    @property\n    def uses_mrope(self) -> bool:\n        return uses_mrope(self.hf_config)\n\n    @property\n    def is_multimodal_model(self) -> bool:\n        return self.multimodal_config is not None\n\n    @property\n    def is_cross_encoder(self) -> bool:\n        return self.registry.is_cross_encoder_model(self.architectures)\n\n    @property\n    def use_mla(self) -> bool:\n        return self.is_deepseek_mla and not envs.VLLM_MLA_DISABLE\n\n    @property\n    def supported_runner_types(self) -> set[RunnerType]:\n        return {_TASK_RUNNER[task] for task in self.supported_tasks}\n\n    @property\n    def runner_type(self) -> RunnerType:\n        return _TASK_RUNNER[cast(_ResolvedTask, self.task)]\n\n    @property\n    def is_v1_compatible(self) -> bool:\n        architectures = getattr(self.hf_config, \"architectures\", [])\n        return ModelRegistry.is_v1_compatible(architectures)\n\n    @property\n    def is_matryoshka(self) -> bool:\n        return (hasattr(self.hf_config, \"matryoshka_dimensions\")\n                or getattr(self.hf_config, \"is_matryoshka\", False))\n\n    @property\n    def matryoshka_dimensions(self):\n        return getattr(self.hf_config, \"matryoshka_dimensions\", None)\n\n\nBlockSize = Literal[1, 8, 16, 32, 64, 128]\nCacheDType = Literal[\"auto\", \"fp8\", \"fp8_e4m3\", \"fp8_e5m2\"]\nPrefixCachingHashAlgo = Literal[\"builtin\", \"sha256\"]\n\n\n@config\n@dataclass\nclass CacheConfig:\n    \"\"\"Configuration for the KV cache.\"\"\"\n\n    block_size: BlockSize = None  # type: ignore\n    \"\"\"Size of a contiguous cache block in number of tokens. This is ignored on\n    neuron devices and set to `--max-model-len`. On CUDA devices, only block\n    sizes up to 32 are supported. On HPU devices, block size defaults to 128.\n\n    This config has no static default. If left unspecified by the user, it will\n    be set in `Platform.check_and_update_configs()` based on the current\n    platform.\"\"\"\n    gpu_memory_utilization: float = 0.9\n    \"\"\"The fraction of GPU memory to be used for the model executor, which can\n    range from 0 to 1. For example, a value of 0.5 would imply 50% GPU memory\n    utilization. If unspecified, will use the default value of 0.9. This is a\n    per-instance limit, and only applies to the current vLLM instance. It does\n    not matter if you have another vLLM instance running on the same GPU. For\n    example, if you have two vLLM instances running on the same GPU, you can\n    set the GPU memory utilization to 0.5 for each instance.\"\"\"\n    swap_space: float = 4\n    \"\"\"Size of the CPU swap space per GPU (in GiB).\"\"\"\n    cache_dtype: CacheDType = \"auto\"\n    \"\"\"Data type for kv cache storage. If \"auto\", will use model data type.\n    CUDA 11.8+ supports fp8 (=fp8_e4m3) and fp8_e5m2. ROCm (AMD GPU) supports\n    fp8 (=fp8_e4m3).\"\"\"\n    is_attention_free: bool = False\n    \"\"\"Whether the model is attention-free. This is primarily set in\n    `ModelConfig` and that value should be manually duplicated here.\"\"\"\n    num_gpu_blocks_override: Optional[int] = None\n    \"\"\"Number of GPU blocks to use. This overrides the profiled `num_gpu_blocks`\n    if specified. Does nothing if `None`. Used for testing preemption.\"\"\"\n    sliding_window: Optional[int] = None\n    \"\"\"Sliding window size for the KV cache. This is primarily set in\n    `ModelConfig` and that value should be manually duplicated here.\"\"\"\n    enable_prefix_caching: Optional[bool] = None\n    \"\"\"Whether to enable prefix caching. Disabled by default for V0. Enabled by\n    default for V1.\"\"\"\n    prefix_caching_hash_algo: PrefixCachingHashAlgo = \"builtin\"\n    \"\"\"Set the hash algorithm for prefix caching:\\n\n    - \"builtin\" is Python's built-in hash.\\n\n    - \"sha256\" is collision resistant but with certain overheads.\"\"\"\n    cpu_offload_gb: float = 0\n    \"\"\"The space in GiB to offload to CPU, per GPU. Default is 0, which means\n    no offloading. Intuitively, this argument can be seen as a virtual way to\n    increase the GPU memory size. For example, if you have one 24 GB GPU and\n    set this to 10, virtually you can think of it as a 34 GB GPU. Then you can\n    load a 13B model with BF16 weight, which requires at least 26GB GPU memory.\n    Note that this requires fast CPU-GPU interconnect, as part of the model is\n    loaded from CPU memory to GPU memory on the fly in each model forward pass.\n    \"\"\"\n    calculate_kv_scales: bool = False\n    \"\"\"This enables dynamic calculation of `k_scale` and `v_scale` when\n    kv_cache_dtype is fp8. If `False`, the scales will be loaded from the model\n    checkpoint if available. Otherwise, the scales will default to 1.0.\"\"\"\n\n    # Will be set after profiling.\n    num_gpu_blocks: Optional[int] = field(default=None, init=False)\n    \"\"\"The number of blocks to allocate for GPU memory.\"\"\"\n    num_cpu_blocks: Optional[int] = field(default=None, init=False)\n    \"\"\"The number of blocks to allocate for CPU memory.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.cache_dtype)\n        # `cpu_offload_gb` does not use `torch.compile` yet.\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        self.swap_space_bytes = self.swap_space * GiB_bytes\n\n        self._verify_args()\n        self._verify_cache_dtype()\n        self._verify_prefix_caching()\n\n    def metrics_info(self):\n        # convert cache_config to dict(key: str, value: str) for prometheus\n        # metrics info\n        return {key: str(value) for key, value in self.__dict__.items()}\n\n    def _verify_args(self) -> None:\n        if self.cpu_offload_gb < 0:\n            raise ValueError(\"CPU offload space must be non-negative\"\n                             f\", but got {self.cpu_offload_gb}\")\n\n        if self.gpu_memory_utilization > 1.0:\n            raise ValueError(\n                \"GPU memory utilization must be less than 1.0. Got \"\n                f\"{self.gpu_memory_utilization}.\")\n\n    def _verify_cache_dtype(self) -> None:\n        if self.cache_dtype == \"auto\":\n            pass\n        elif self.cache_dtype in get_args(CacheDType):\n            logger.info(\n                \"Using fp8 data type to store kv cache. It reduces the GPU \"\n                \"memory footprint and boosts the performance. \"\n                \"Meanwhile, it may cause accuracy drop without a proper \"\n                \"scaling factor\")\n        else:\n            raise ValueError(f\"Unknown kv cache dtype: {self.cache_dtype}\")\n\n    def _verify_prefix_caching(self) -> None:\n        if not self.enable_prefix_caching:\n            return\n\n        if self.sliding_window is not None and not envs.VLLM_USE_V1:\n            raise NotImplementedError(\n                \"Prefix caching is not supported with sliding window. \"\n                \"Run with --disable-sliding-window to use prefix caching.\")\n\n        if (self.enable_prefix_caching and self.prefix_caching_hash_algo\n                not in get_args(PrefixCachingHashAlgo)):\n            raise ValueError(\n                \"Unknown prefix caching hash algorithm: \"\n                f\"{self.prefix_caching_hash_algo}. Must be one of \"\n                f\"{get_args(PrefixCachingHashAlgo)}.\")\n\n    def verify_with_parallel_config(\n        self,\n        parallel_config: \"ParallelConfig\",\n    ) -> None:\n        total_cpu_memory = get_cpu_memory()\n        # FIXME(woosuk): Here, it is assumed that the GPUs in a tensor parallel\n        # group are in the same node. However, the GPUs may span multiple nodes.\n        num_gpus_per_node = parallel_config.tensor_parallel_size\n        cpu_memory_usage = self.swap_space_bytes * num_gpus_per_node\n\n        msg = (f\"{cpu_memory_usage / GiB_bytes:.2f} GiB out of the \"\n               f\"{total_cpu_memory / GiB_bytes:.2f} GiB total CPU memory \"\n               \"is allocated for the swap space.\")\n        if cpu_memory_usage > 0.7 * total_cpu_memory:\n            raise ValueError(\"Too large swap space. \" + msg)\n        elif cpu_memory_usage > 0.4 * total_cpu_memory:\n            logger.warning(\"Possibly too large swap space. %s\", msg)\n\n\n@config\n@dataclass\nclass TokenizerPoolConfig:\n    \"\"\"This config is deprecated and will be removed in a future release.\n\n    Passing these parameters will have no effect. Please remove them from your\n    configurations.\n    \"\"\"\n\n    pool_size: int = 0\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Passing this parameter will have no effect. Please remove it from your\n    configurations.\"\"\"\n    pool_type: str = \"ray\"\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Passing this parameter will have no effect. Please remove it from your\n    configurations.\"\"\"\n    extra_config: dict = field(default_factory=dict)\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Passing this parameter will have no effect. Please remove it from your\n    configurations.\"\"\"\n\n    def __post_init__(self) -> None:\n        logger.warning_once(\n            \"TokenizerPoolConfig is deprecated and will be removed in a \"\n            \"future release. Passing this parameter will have no effect. \"\n            \"Please remove it from your configurations.\")\n\n\nclass LoadFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    PT = \"pt\"\n    SAFETENSORS = \"safetensors\"\n    NPCACHE = \"npcache\"\n    DUMMY = \"dummy\"\n    TENSORIZER = \"tensorizer\"\n    SHARDED_STATE = \"sharded_state\"\n    GGUF = \"gguf\"\n    BITSANDBYTES = \"bitsandbytes\"\n    MISTRAL = \"mistral\"\n    RUNAI_STREAMER = \"runai_streamer\"\n    RUNAI_STREAMER_SHARDED = \"runai_streamer_sharded\"\n    FASTSAFETENSORS = \"fastsafetensors\"\n\n\n@config\n@dataclass\nclass LoadConfig:\n    \"\"\"Configuration for loading the model weights.\"\"\"\n\n    load_format: Union[str, LoadFormat,\n                       \"BaseModelLoader\"] = LoadFormat.AUTO.value\n    \"\"\"The format of the model weights to load:\\n\n    - \"auto\" will try to load the weights in the safetensors format and fall\n    back to the pytorch bin format if safetensors format is not available.\\n\n    - \"pt\" will load the weights in the pytorch bin format.\\n\n    - \"safetensors\" will load the weights in the safetensors format.\\n\n    - \"npcache\" will load the weights in pytorch format and store a numpy cache\n    to speed up the loading.\\n\n    - \"dummy\" will initialize the weights with random values, which is mainly\n    for profiling.\\n\n    - \"tensorizer\" will use CoreWeave's tensorizer library for fast weight\n    loading. See the Tensorize vLLM Model script in the Examples section for\n    more information.\\n\n    - \"runai_streamer\" will load the Safetensors weights using Run:ai Model\n    Streamer.\\n\n    - \"bitsandbytes\" will load the weights using bitsandbytes quantization.\\n\n    - \"sharded_state\" will load weights from pre-sharded checkpoint files,\n    supporting efficient loading of tensor-parallel models.\\n\n    - \"gguf\" will load weights from GGUF format files (details specified in\n    https://github.com/ggml-org/ggml/blob/master/docs/gguf.md).\\n\n    - \"mistral\" will load weights from consolidated safetensors files used by\n    Mistral models.\"\"\"\n    download_dir: Optional[str] = None\n    \"\"\"Directory to download and load the weights, default to the default\n    cache directory of Hugging Face.\"\"\"\n    model_loader_extra_config: dict = field(default_factory=dict)\n    \"\"\"Extra config for model loader. This will be passed to the model loader\n    corresponding to the chosen load_format.\"\"\"\n    ignore_patterns: Optional[Union[list[str], str]] = None\n    \"\"\"The list of patterns to ignore when loading the model. Default to\n    \"original/**/*\" to avoid repeated loading of llama's checkpoints.\"\"\"\n    use_tqdm_on_load: bool = True\n    \"\"\"Whether to enable tqdm for showing progress bar when loading model\n    weights.\"\"\"\n    pt_load_map_location: Union[str, dict[str, str]] = \"cpu\"\n    \"\"\"\n    pt_load_map_location: the map location for loading pytorch checkpoint, to\n    support loading checkpoints can only be loaded on certain devices like\n    \"cuda\", this is equivalent to {\"\": \"cuda\"}. Another supported format is\n    mapping from different devices like from GPU 1 to GPU 0:\n    {\"cuda:1\": \"cuda:0\"}. Note that when passed from command line, the strings\n    in dictionary needs to be double quoted for json parsing. For more details,\n    see original doc for `map_location` in https://pytorch.org/docs/stable/generated/torch.load.html\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if isinstance(self.load_format, str):\n            load_format = self.load_format.lower()\n            self.load_format = LoadFormat(load_format)\n\n        if self.ignore_patterns is not None and len(self.ignore_patterns) > 0:\n            logger.info(\n                \"Ignoring the following patterns when downloading weights: %s\",\n                self.ignore_patterns)\n        else:\n            self.ignore_patterns = [\"original/**/*\"]\n\n\nDistributedExecutorBackend = Literal[\"ray\", \"mp\", \"uni\", \"external_launcher\"]\n\n\n@config\n@dataclass\nclass ParallelConfig:\n    \"\"\"Configuration for the distributed execution.\"\"\"\n\n    pipeline_parallel_size: int = 1\n    \"\"\"Number of pipeline parallel groups.\"\"\"\n    tensor_parallel_size: int = 1\n    \"\"\"Number of tensor parallel groups.\"\"\"\n    data_parallel_size: int = 1\n    \"\"\"Number of data parallel groups. MoE layers will be sharded according to\n    the product of the tensor parallel size and data parallel size.\"\"\"\n    data_parallel_size_local: int = 1\n    \"\"\"Number of local data parallel groups.\"\"\"\n    data_parallel_rank: int = 0\n    \"\"\"Rank of the data parallel group.\"\"\"\n    data_parallel_rank_local: Optional[int] = None\n    \"\"\"Local rank of the data parallel group,\n    set only in SPMD mode.\"\"\"\n    data_parallel_master_ip: str = \"127.0.0.1\"\n    \"\"\"IP of the data parallel master.\"\"\"\n    data_parallel_rpc_port: int = 29550\n    \"\"\"Port for data parallel messaging.\"\"\"\n    data_parallel_master_port: int = 29500\n    \"\"\"Port of the data parallel master.\"\"\"\n    enable_expert_parallel: bool = False\n    \"\"\"Use expert parallelism instead of tensor parallelism for MoE layers.\"\"\"\n    max_parallel_loading_workers: Optional[int] = None\n    \"\"\"Maximum number of parallel loading workers when loading model\n    sequentially in multiple batches. To avoid RAM OOM when using tensor\n    parallel and large models.\"\"\"\n\n    disable_custom_all_reduce: bool = False\n    \"\"\"Disable the custom all-reduce kernel and fall back to NCCL.\"\"\"\n\n    tokenizer_pool_config: Optional[TokenizerPoolConfig] = None\n    \"\"\"This parameter is deprecated and will be removed in a future release.\n    Please remove it from your configs\"\"\"\n\n    ray_workers_use_nsight: bool = False\n    \"\"\"Whether to profile Ray workers with nsight, see https://docs.ray.io/en/latest/ray-observability/user-guides/profiling.html#profiling-nsight-profiler.\"\"\"\n\n    placement_group: Optional[\"PlacementGroup\"] = None\n    \"\"\"ray distributed model workers placement group.\"\"\"\n\n    distributed_executor_backend: Optional[Union[DistributedExecutorBackend,\n                                                 type[\"ExecutorBase\"]]] = None\n    \"\"\"Backend to use for distributed model\n    workers, either \"ray\" or \"mp\" (multiprocessing). If the product\n    of pipeline_parallel_size and tensor_parallel_size is less than\n    or equal to the number of GPUs available, \"mp\" will be used to\n    keep processing on a single host. Otherwise, this will default\n    to \"ray\" if Ray is installed and fail otherwise. Note that tpu\n    and hpu only support Ray for distributed inference.\"\"\"\n\n    worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use. If \"auto\", the worker class\n    will be determined based on the platform.\"\"\"\n    sd_worker_cls: str = \"auto\"\n    \"\"\"The full name of the worker class to use for speculative decofing.\n    If \"auto\", the worker class will be determined based on the platform.\"\"\"\n    worker_extension_cls: str = \"\"\n    \"\"\"The full name of the worker extension class to use. The worker extension\n    class is dynamically inherited by the worker class. This is used to inject\n    new attributes and methods to the worker class for use in collective_rpc\n    calls.\"\"\"\n\n    world_size: int = field(init=False)\n    \"\"\"world_size is TPxPP, it affects the number of workers we create.\"\"\"\n\n    rank: int = 0\n    \"\"\"Global rank in distributed setup.\"\"\"\n\n    @property\n    def world_size_across_dp(self) -> int:\n        \"\"\"world_size_across_dp is TPxPPxDP, it is the size of the world\n        including data parallelism.\"\"\"\n        return self.world_size * self.data_parallel_size\n\n    def get_next_dp_init_port(self) -> int:\n        \"\"\"\n        We might need to initialize process groups in multiple\n        processes that is related to data parallelism,\n        e.g. both in the worker and in the engine, which\n        can live in different processes. To avoid port conflicts, we\n        increment the port number each time we need to initialize a\n        new process group related to data parallelism.\n        \"\"\"\n        answer = self.data_parallel_master_port\n        self.data_parallel_master_port += 1\n        return answer\n\n    def stateless_init_dp_group(self) -> \"ProcessGroup\":\n        from vllm.distributed.utils import (\n            stateless_init_torch_distributed_process_group)\n\n        # use gloo since the engine process might not have cuda device\n        dp_group = stateless_init_torch_distributed_process_group(\n            self.data_parallel_master_ip,\n            self.get_next_dp_init_port(),\n            self.data_parallel_rank,\n            self.data_parallel_size,\n            backend=\"gloo\")\n\n        return dp_group\n\n    @staticmethod\n    def has_unfinished_dp(dp_group: \"ProcessGroup\",\n                          has_unfinished: bool) -> bool:\n        tensor = torch.tensor([has_unfinished],\n                              dtype=torch.int32,\n                              device=\"cpu\")\n        # dp rank 0: has_unfinished_seqs=True\n        # dp rank 1: has_unfinished_seqs=False\n        # aggregated: has_unfinished_seqs=True\n        # so this is an OR operation, i.e. MAX in integers\n        torch.distributed.all_reduce(tensor, op=ReduceOp.MAX, group=dp_group)\n        aggregated_has_unfinished = bool(tensor.item())\n        return aggregated_has_unfinished\n\n    def compute_hash(self):\n        \"\"\"\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.pipeline_parallel_size)\n        factors.append(self.tensor_parallel_size)\n        factors.append(self.enable_expert_parallel)\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __post_init__(self) -> None:\n        self.world_size = self.pipeline_parallel_size * \\\n            self.tensor_parallel_size\n\n        if self.data_parallel_size_local > self.data_parallel_size:\n            raise ValueError(\n                f\"data_parallel_size_local ({self.data_parallel_size_local}) \"\n                f\"must be <= data_parallel_size ({self.data_parallel_size})\")\n\n        if self.data_parallel_size > 1 or self.data_parallel_size_local == 0:\n            # Data parallel was specified in the engine args.\n            self.data_parallel_master_port = get_open_port()\n        else:\n            # Otherwise fall back to env vars (e.g. for offline SPMD case).\n            self.data_parallel_size = envs.VLLM_DP_SIZE\n            self.data_parallel_rank = envs.VLLM_DP_RANK\n            self.data_parallel_rank_local = envs.VLLM_DP_RANK_LOCAL\n            self.data_parallel_master_ip = envs.VLLM_DP_MASTER_IP\n            self.data_parallel_master_port = envs.VLLM_DP_MASTER_PORT\n\n        if self.distributed_executor_backend == \"external_launcher\":\n            import os\n            os.environ[\"VLLM_ENABLE_V1_MULTIPROCESSING\"] = \"0\"\n            logger.info(\"Disabling V1 multiprocessing for external launcher.\")\n\n        ray_only_devices: list[str] = []\n        from vllm.platforms import current_platform\n        if (current_platform.device_type in ray_only_devices\n                and self.world_size > 1):\n            if self.distributed_executor_backend is None:\n                self.distributed_executor_backend = \"ray\"\n            if self.distributed_executor_backend != \"ray\":\n                raise ValueError(\n                    f\"{current_platform.device_type.upper()} backend only \"\n                    \"supports Ray for distributed inference.\")\n\n        if self.distributed_executor_backend is None and self.world_size > 1:\n            # We use multiprocessing by default if world_size fits on the\n            # current node and we aren't in a ray placement group.\n\n            from vllm.executor import ray_utils\n            backend: DistributedExecutorBackend = \"mp\"\n            ray_found = ray_utils.ray_is_available()\n            if current_platform.is_neuron():\n                # neuron uses single process to control multiple devices\n                backend = \"uni\"\n            elif (current_platform.is_cuda()\n                  and cuda_device_count_stateless() < self.world_size):\n                if not ray_found:\n                    raise ValueError(\"Unable to load Ray which is \"\n                                     \"required for multi-node inference, \"\n                                     \"please install Ray with `pip install \"\n                                     \"ray`.\") from ray_utils.ray_import_err\n                backend = \"ray\"\n            elif ray_found:\n                if self.placement_group:\n                    backend = \"ray\"\n                else:\n                    from ray import is_initialized as ray_is_initialized\n                    if ray_is_initialized():\n                        from ray.util import get_current_placement_group\n                        if get_current_placement_group():\n                            backend = \"ray\"\n            self.distributed_executor_backend = backend\n            logger.info(\"Defaulting to use %s for distributed inference\",\n                        backend)\n\n        if self.distributed_executor_backend is None and self.world_size == 1:\n            self.distributed_executor_backend = \"uni\"\n\n        self._verify_args()\n\n    @property\n    def use_ray(self) -> bool:\n        return self.distributed_executor_backend == \"ray\" or (\n            isinstance(self.distributed_executor_backend, type)\n            and self.distributed_executor_backend.uses_ray)\n\n    def _verify_args(self) -> None:\n        # Lazy import to avoid circular import\n        from vllm.executor.executor_base import ExecutorBase\n        from vllm.platforms import current_platform\n        if self.distributed_executor_backend not in (\n                \"ray\", \"mp\", \"uni\",\n                \"external_launcher\", None) and not (isinstance(\n                    self.distributed_executor_backend, type) and issubclass(\n                        self.distributed_executor_backend, ExecutorBase)):\n            raise ValueError(\n                \"Unrecognized distributed executor backend \"\n                f\"{self.distributed_executor_backend}. Supported \"\n                \"values are 'ray', 'mp' 'uni', 'external_launcher' or\"\n                \" custom ExecutorBase subclass.\")\n        if self.use_ray:\n            from vllm.executor import ray_utils\n            ray_utils.assert_ray_available()\n\n        if not current_platform.use_custom_allreduce():\n            self.disable_custom_all_reduce = True\n            logger.info(\n                \"Disabled the custom all-reduce kernel because it is not \"\n                \"supported on current platform.\")\n        if self.ray_workers_use_nsight and not self.use_ray:\n            raise ValueError(\"Unable to use nsight profiling unless workers \"\n                             \"run with Ray.\")\n\n        assert isinstance(self.worker_extension_cls, str), (\n            \"worker_extension_cls must be a string (qualified class name).\")\n\n\nPreemptionMode = Literal[\"swap\", \"recompute\"]\nSchedulerPolicy = Literal[\"fcfs\", \"priority\"]\n\n\n@config\n@dataclass\nclass SchedulerConfig:\n    \"\"\"Scheduler configuration.\"\"\"\n\n    runner_type: RunnerType = \"generate\"\n    \"\"\"The runner type to launch for the model.\"\"\"\n\n    max_num_batched_tokens: int = None  # type: ignore\n    \"\"\"Maximum number of tokens to be processed in a single iteration.\n\n    This config has no static default. If left unspecified by the user, it will\n    be set in `EngineArgs.create_engine_config` based on the usage context.\"\"\"\n\n    max_num_seqs: int = None  # type: ignore\n    \"\"\"Maximum number of sequences to be processed in a single iteration.\n\n    This config has no static default. If left unspecified by the user, it will\n    be set in `EngineArgs.create_engine_config` based on the usage context.\"\"\"\n\n    max_model_len: int = None  # type: ignore\n    \"\"\"Maximum length of a sequence (including prompt and generated text). This\n    is primarily set in `ModelConfig` and that value should be manually\n    duplicated here.\"\"\"\n\n    max_num_partial_prefills: int = 1\n    \"\"\"For chunked prefill, the maximum number of sequences that can be\n    partially prefilled concurrently.\"\"\"\n\n    max_long_partial_prefills: int = 1\n    \"\"\"For chunked prefill, the maximum number of prompts longer than\n    long_prefill_token_threshold that will be prefilled concurrently. Setting\n    this less than max_num_partial_prefills will allow shorter prompts to jump\n    the queue in front of longer prompts in some cases, improving latency.\"\"\"\n\n    long_prefill_token_threshold: int = 0\n    \"\"\"For chunked prefill, a request is considered long if the prompt is\n    longer than this number of tokens.\"\"\"\n\n    num_lookahead_slots: int = 0\n    \"\"\"The number of slots to allocate per sequence per\n    step, beyond the known token ids. This is used in speculative\n    decoding to store KV activations of tokens which may or may not be\n    accepted.\n\n    NOTE: This will be replaced by speculative config in the future; it is\n    present to enable correctness tests until then.\"\"\"\n\n    cuda_graph_sizes: list[int] = field(default_factory=lambda: [512])\n    \"\"\"Cuda graph capture sizes, default is 512.\n    1. if one value is provided, then the capture list would follow the\n    pattern: [1, 2, 4] + [i for i in range(8, cuda_graph_sizes + 1, 8)]\n    2. more than one value (e.g. 1 2 128) is provided, then the capture list\n    will follow the provided list.\"\"\"\n\n    delay_factor: float = 0.0\n    \"\"\"Apply a delay (of delay factor multiplied by previous\n    prompt latency) before scheduling next prompt.\"\"\"\n\n    enable_chunked_prefill: bool = None  # type: ignore\n    \"\"\"If True, prefill requests can be chunked based\n    on the remaining max_num_batched_tokens.\"\"\"\n\n    is_multimodal_model: bool = False\n    \"\"\"True if the model is multimodal.\"\"\"\n\n    # TODO (ywang96): Make this configurable.\n    max_num_encoder_input_tokens: int = field(init=False)\n    \"\"\"Multimodal encoder compute budget, only used in V1.\n\n    NOTE: This is not currently configurable. It will be overridden by\n    max_num_batched_tokens in case max multimodal embedding size is larger.\"\"\"\n\n    # TODO (ywang96): Make this configurable.\n    encoder_cache_size: int = field(init=False)\n    \"\"\"Multimodal encoder cache size, only used in V1.\n\n    NOTE: This is not currently configurable. It will be overridden by\n    max_num_batched_tokens in case max multimodal embedding size is larger.\"\"\"\n\n    preemption_mode: Optional[PreemptionMode] = None\n    \"\"\"Whether to perform preemption by swapping or\n    recomputation. If not specified, we determine the mode as follows:\n    We use recomputation by default since it incurs lower overhead than\n    swapping. However, when the sequence group has multiple sequences\n    (e.g., beam search), recomputation is not currently supported. In\n    such a case, we use swapping instead.\"\"\"\n\n    num_scheduler_steps: int = 1\n    \"\"\"Maximum number of forward steps per scheduler call.\"\"\"\n\n    multi_step_stream_outputs: bool = True\n    \"\"\"If False, then multi-step will stream outputs at the end of all steps\"\"\"\n\n    send_delta_data: bool = False\n    \"\"\"Private API. If used, scheduler sends delta data to\n    workers instead of an entire data. It should be enabled only\n    when SPMD worker architecture is enabled. I.e.,\n    VLLM_USE_RAY_SPMD_WORKER=1\"\"\"\n\n    policy: SchedulerPolicy = \"fcfs\"\n    \"\"\"The scheduling policy to use:\\n\n    - \"fcfs\" means first come first served, i.e. requests are handled in order\n    of arrival.\\n\n    - \"priority\" means requests are handled based on given priority (lower\n    value means earlier handling) and time of arrival deciding any ties).\"\"\"\n\n    chunked_prefill_enabled: bool = field(init=False)\n    \"\"\"True if chunked prefill is enabled.\"\"\"\n\n    disable_chunked_mm_input: bool = False\n    \"\"\"If set to true and chunked prefill is enabled, we do not want to\n    partially schedule a multimodal item. Only used in V1\n    This ensures that if a request has a mixed prompt\n    (like text tokens TTTT followed by image tokens IIIIIIIIII) where only\n    some image tokens can be scheduled (like TTTTIIIII, leaving IIIII),\n    it will be scheduled as TTTT in one step and IIIIIIIIII in the next.\"\"\"\n\n    # scheduler class or path. \"vllm.core.scheduler.Scheduler\" (default)\n    # or \"mod.custom_class\".\n    scheduler_cls: Union[str, type[object]] = \"vllm.core.scheduler.Scheduler\"\n    \"\"\"The scheduler class to use. \"vllm.core.scheduler.Scheduler\" is the\n    default scheduler. Can be a class directly or the path to a class of form\n    \"mod.custom_class\".\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        if self.max_model_len is None:\n            self.max_model_len = 8192\n\n        if self.max_num_seqs is None:\n            self.max_num_seqs = 128\n\n        if self.max_num_batched_tokens is None:\n            if self.enable_chunked_prefill:\n                if self.num_scheduler_steps > 1:\n                    # Multi-step Chunked-Prefill doesn't allow prompt-chunking\n                    # for now. Have max_num_batched_tokens set to max_model_len\n                    # so we don't reject sequences on account of a short\n                    # max_num_batched_tokens.\n                    self.max_num_batched_tokens = max(\n                        self.max_model_len, _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n                else:\n                    self.max_num_batched_tokens = (\n                        _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n            else:\n                # If max_model_len is too short, use\n                # _DEFAULT_MAX_NUM_BATCHED_TOKENS as the default value\n                # for higher throughput.\n                self.max_num_batched_tokens = max(\n                    self.max_model_len, _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n\n            if self.runner_type == \"pooling\":\n                # Choose specific value for higher throughput\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _POOLING_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n            if self.is_multimodal_model:\n                # The value needs to be at least the number of multimodal tokens\n                self.max_num_batched_tokens = max(\n                    self.max_num_batched_tokens,\n                    _MULTIMODAL_MODEL_MAX_NUM_BATCHED_TOKENS,\n                )\n\n            # When using default settings,\n            # Ensure max_num_batched_tokens does not exceed model limit.\n            # Some models (e.g., Whisper) have embeddings tied to max length.\n            self.max_num_batched_tokens = min(\n                self.max_num_seqs * self.max_model_len,\n                self.max_num_batched_tokens)\n\n        self.max_num_encoder_input_tokens = self.max_num_batched_tokens\n        self.encoder_cache_size = self.max_num_batched_tokens\n\n        if self.enable_chunked_prefill:\n            logger.info(\n                \"Chunked prefill is enabled with max_num_batched_tokens=%d.\",\n                self.max_num_batched_tokens)\n\n        self.chunked_prefill_enabled = self.enable_chunked_prefill\n        if self.max_num_partial_prefills > 1:\n            if self.long_prefill_token_threshold == 0:\n                self.long_prefill_token_threshold = int(self.max_model_len *\n                                                        0.04)\n\n            logger.info(\n                \"Concurrent partial prefills enabled with \"\n                \"max_num_partial_prefills=%d, max_long_partial_prefills=%d, \"\n                \"long_prefill_token_threshold=%d\",\n                self.max_num_partial_prefills, self.max_long_partial_prefills,\n                self.long_prefill_token_threshold)\n\n        self._verify_args()\n\n    def _verify_args(self) -> None:\n        if (self.max_num_batched_tokens < self.max_model_len\n                and not self.chunked_prefill_enabled):\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) is \"\n                f\"smaller than max_model_len ({self.max_model_len}). \"\n                \"This effectively limits the maximum sequence length to \"\n                \"max_num_batched_tokens and makes vLLM reject longer \"\n                \"sequences. Please increase max_num_batched_tokens or \"\n                \"decrease max_model_len.\")\n\n        if self.max_num_batched_tokens < self.max_num_seqs:\n            raise ValueError(\n                f\"max_num_batched_tokens ({self.max_num_batched_tokens}) must \"\n                \"be greater than or equal to max_num_seqs \"\n                f\"({self.max_num_seqs}).\")\n\n        if self.max_num_batched_tokens > self.max_num_seqs * self.max_model_len:\n            logger.warning(\n                \"max_num_batched_tokens (%d) exceeds max_num_seqs\"\n                \"* max_model_len (%d). This may lead to unexpected behavior.\",\n                self.max_num_batched_tokens,\n                self.max_num_seqs * self.max_model_len)\n\n        if self.num_lookahead_slots < 0:\n            raise ValueError(\n                \"num_lookahead_slots \"\n                f\"({self.num_lookahead_slots}) must be greater than or \"\n                \"equal to 0.\")\n\n        if self.num_scheduler_steps < 1:\n            raise ValueError(\n                \"num_scheduler_steps \"\n                f\"({self.num_scheduler_steps}) must be greater than or \"\n                \"equal to 1.\")\n\n        if self.max_num_partial_prefills < 1:\n            raise ValueError(\n                f\"max_num_partial_prefills ({self.max_num_partial_prefills}) \"\n                \"must be greater than or equal to 1.\")\n        elif self.max_num_partial_prefills > 1:\n            if not self.chunked_prefill_enabled:\n                raise ValueError(\"Chunked prefill must be enabled to set \"\n                                 \"max_num_partial_prefills > 1.\")\n\n            if self.long_prefill_token_threshold > self.max_model_len:\n                raise ValueError(\n                    \"long_prefill_token_threshold \"\n                    f\"({self.long_prefill_token_threshold}) cannot be greater \"\n                    f\"than the max_model_len ({self.max_model_len}).\")\n\n        if (self.max_long_partial_prefills\n                < 1) or (self.max_long_partial_prefills\n                         > self.max_num_partial_prefills):\n            raise ValueError(\n                f\"max_long_partial_prefills ({self.max_long_partial_prefills}) \"\n                \"must be greater than or equal to 1 and less than or equal to \"\n                f\"max_num_partial_prefills ({self.max_num_partial_prefills}).\")\n\n    @property\n    def is_multi_step(self) -> bool:\n        return self.num_scheduler_steps > 1\n\n\nDevice = Literal[\"auto\", \"cuda\", \"neuron\", \"cpu\", \"tpu\", \"xpu\", \"hpu\"]\n\n\n@config\n@dataclass\nclass DeviceConfig:\n    \"\"\"Configuration for the device to use for vLLM execution.\"\"\"\n\n    device: Union[Device, torch.device] = \"auto\"\n    \"\"\"Device type for vLLM execution.\"\"\"\n    device_type: str = field(init=False)\n    \"\"\"Device type from the current platform. This is set in\n    `__post_init__`.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # the device/platform information will be summarized\n        # by torch/vllm automatically.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if self.device == \"auto\":\n            # Automated device type detection\n            from vllm.platforms import current_platform\n            self.device_type = current_platform.device_type\n            if not self.device_type:\n                raise RuntimeError(\n                    \"Failed to infer device type, please set \"\n                    \"the environment variable `VLLM_LOGGING_LEVEL=DEBUG` \"\n                    \"to turn on verbose logging to help debug the issue.\")\n        else:\n            # Device type is assigned explicitly\n            self.device_type = self.device\n\n        # Some device types require processing inputs on CPU\n        if self.device_type in [\"neuron\"]:\n            self.device = torch.device(\"cpu\")\n        elif self.device_type in [\"tpu\"]:\n            self.device = None\n        else:\n            # Set device with device type\n            self.device = torch.device(self.device_type)\n\n\nSpeculativeMethod = Literal[\"ngram\", \"eagle\", \"medusa\", \"mlp_speculator\",\n                            \"draft_model\"]\nSpeculativeAcceptanceMethod = Literal[\"rejection_sampler\",\n                                      \"typical_acceptance_sampler\"]\n\n\n@config\n@dataclass\nclass SpeculativeConfig:\n    \"\"\"Configuration for speculative decoding.\"\"\"\n\n    # General speculative decoding control\n    num_speculative_tokens: int = field(default=None,\n                                        init=True)  # type: ignore\n    \"\"\"The number of speculative tokens, if provided. It will default to the\n    number in the draft model config if present, otherwise, it is required.\"\"\"\n    model: Optional[str] = None\n    \"\"\"The name of the draft model, eagle head, or additional weights, if\n    provided.\"\"\"\n    method: Optional[SpeculativeMethod] = None\n    \"\"\"The name of the speculative method to use. If users provide and set the\n    `model` param, the speculative method type will be detected automatically\n    if possible, if `model` param is not provided, the method name must be\n    provided.\n\n    If using `ngram` method, the related configuration `prompt_lookup_max` and\n    `prompt_lookup_min` should be considered.\"\"\"\n    acceptance_method: SpeculativeAcceptanceMethod = \"rejection_sampler\"\n    \"\"\"The method to use for accepting draft tokens:\\n\n    - \"rejection_sampler\" maps to `RejectionSampler`.\\n\n    - \"typical_acceptance_sampler\" maps to `TypicalAcceptanceSampler`.\n\n    If using `typical_acceptance_sampler`, the related configuration\n    `posterior_threshold` and `posterior_alpha` should be considered.\"\"\"\n    draft_tensor_parallel_size: Optional[int] = None\n    \"\"\"The degree of the tensor parallelism for the draft model. Can only be 1\n    or the same as the target model's tensor parallel size.\"\"\"\n    disable_logprobs: bool = True\n    \"\"\"If set to True, token log probabilities are not returned during\n    speculative decoding. If set to False, token log probabilities are returned\n    according to the log probability settings in SamplingParams.\"\"\"\n\n    # Draft model configuration\n    quantization: Optional[QuantizationMethods] = None\n    \"\"\"Quantization method that was used to quantize the draft model weights.\n    If `None`, we assume the model weights are not quantized. Note that it only\n    takes effect when using the draft model-based speculative method.\"\"\"\n    max_model_len: Optional[int] = None\n    \"\"\"The maximum model length of the draft model. Used when testing the\n    ability to skip speculation for some sequences.\"\"\"\n    revision: Optional[str] = None\n    \"\"\"The specific model version to use for the draft model. It can be a\n    branch name, a tag name, or a commit id. If unspecified, will use the\n    default version.\"\"\"\n    code_revision: Optional[str] = None\n    \"\"\"The specific revision to use for the draft model code on Hugging Face\n    Hub. It can be a branch name, a tag name, or a commit id. If unspecified,\n    will use the default version.\"\"\"\n\n    # Advanced control\n    disable_mqa_scorer: bool = False\n    \"\"\"Disable the MQA scorer and fall back to batch expansion for scoring\n    proposals.\"\"\"\n    disable_by_batch_size: Optional[int] = None\n    \"\"\"Disable speculative decoding for new incoming requests when the number\n    of enqueued requests is larger than this value, if provided.\"\"\"\n\n    # Ngram proposer configuration\n    prompt_lookup_max: Optional[int] = None\n    \"\"\"Maximum size of ngram token window when using Ngram proposer, required\n    when method is set to ngram.\"\"\"\n    prompt_lookup_min: Optional[int] = None\n    \"\"\"Minimum size of ngram token window when using Ngram proposer, if\n    provided. Defaults to 1.\"\"\"\n\n    # Typical acceptance sampler configuration\n    posterior_threshold: Optional[float] = None\n    \"\"\"A threshold value that sets a lower bound on the posterior probability\n    of a token in the target model for it to be accepted. This threshold is\n    used only when we use the `TypicalAcceptanceSampler` for token acceptance.\n    \"\"\"\n    posterior_alpha: Optional[float] = None\n    \"\"\"Scaling factor for entropy-based threshold, applied when using\n    `TypicalAcceptanceSampler`.\"\"\"\n\n    speculative_token_tree: Optional[str] = None\n    \"\"\"Specifies the tree structure for speculative token generation.\n    \"\"\"\n    # required configuration params passed from engine\n    target_model_config: ModelConfig = field(default=None,\n                                             init=True)  # type: ignore\n    \"\"\"The configuration of the target model.\"\"\"\n    target_parallel_config: ParallelConfig = field(default=None,\n                                                   init=True)  # type: ignore\n    \"\"\"The parallel configuration for the target model.\"\"\"\n    enable_chunked_prefill: bool = field(default=None,\n                                         init=True)  # type: ignore\n    \"\"\"Whether vLLM is configured to use chunked prefill or not. Used for\n    raising an error since it's not yet compatible with speculative decode.\"\"\"\n    disable_log_stats: bool = field(default=None, init=True)  # type: ignore\n    \"\"\"Whether to disable the periodic printing of stage times in speculative\n    decoding.\"\"\"\n\n    # params generated in the post-init stage\n    draft_model_config: ModelConfig = field(default=None,\n                                            init=True)  # type: ignore\n    \"\"\"The configuration of the draft model initialized internal.\"\"\"\n    draft_parallel_config: ParallelConfig = field(default=None,\n                                                  init=True)  # type: ignore\n    \"\"\"The parallel configuration for the draft model initialized internal.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        # Eagle3 affects the computation graph because it returns intermediate\n        # hidden states in addition to the final hidden state.\n        factors.append(self.method == \"eagle3\")\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    @classmethod\n    def from_dict(cls, dict_value: dict) -> \"SpeculativeConfig\":\n        \"\"\"Parse the CLI value for the speculative config.\"\"\"\n        return cls(**dict_value)\n\n    @staticmethod\n    def hf_config_override(hf_config: PretrainedConfig) -> PretrainedConfig:\n        if hf_config.model_type == \"deepseek_v3\":\n            hf_config.model_type = \"deepseek_mtp\"\n        if hf_config.model_type == \"deepseek_mtp\":\n            n_predict = getattr(hf_config, \"num_nextn_predict_layers\", None)\n            hf_config.update({\n                \"n_predict\": n_predict,\n                \"architectures\": [\"DeepSeekMTPModel\"]\n            })\n\n        if hf_config.architectures[0] == \"MiMoForCausalLM\":\n            hf_config.model_type = \"mimo_mtp\"\n            n_predict = getattr(hf_config, \"num_nextn_predict_layers\", None)\n            hf_config.update({\n                \"num_hidden_layers\": 0,\n                \"n_predict\": n_predict,\n                \"architectures\": [\"MiMoMTPModel\"]\n            })\n            return hf_config\n\n        return hf_config\n\n    def __post_init__(self):\n\n        # Note: \"method\" is a new parameter that helps to extend the\n        # configuration of non-model-based proposers, and the \"model\" parameter\n        # will be used to set the draft model, eagle head, or additional weight\n        # when needed. If users do not specify \"method\", the speculative method\n        # will be detected automatically if possible. If the speculative method\n        # can not be detected, it will be considered as the \"draft_model\" by\n        # default.\n\n        if self.model is None and self.num_speculative_tokens is not None:\n            # TODO(Shangming): Refactor mtp configuration logic when supporting\n            # mtp acceleration for more models besides deepseek_v3\n            if self.target_model_config and \\\n                (self.target_model_config.hf_text_config.model_type \\\n                        == \"deepseek_v3\" or\n                    self.target_model_config.hf_text_config.model_type \\\n                        == \"mimo\"):\n                # use the draft model from the same model:\n                self.model = self.target_model_config.model\n            elif self.method in (\"ngram\", \"[ngram]\"):\n                self.model = \"ngram\"\n            else:\n                raise ValueError(\"num_speculative_tokens was provided without \"\n                                 \"speculative model.\")\n\n        # Automatically configure the method for ngram when \"model\" is used\n        # instead of \"method\"\n        if self.method is None and (self.model is not None\n                                    and self.model in (\"ngram\", \"[ngram]\")):\n            self.method = \"ngram\"\n\n        if self.method in (\"ngram\", \"[ngram]\"):\n            # Unified to \"ngram\" internally\n            self.method = \"ngram\"\n            # Set default values if not provided\n            if (self.prompt_lookup_min is None\n                    and self.prompt_lookup_max is None):\n                # TODO(woosuk): Tune these values. They are arbitrarily chosen.\n                self.prompt_lookup_min = 5\n                self.prompt_lookup_max = 5\n            elif self.prompt_lookup_min is None:\n                assert self.prompt_lookup_max is not None\n                self.prompt_lookup_min = self.prompt_lookup_max\n            elif self.prompt_lookup_max is None:\n                assert self.prompt_lookup_min is not None\n                self.prompt_lookup_max = self.prompt_lookup_min\n\n            # Validate values\n            if self.prompt_lookup_min < 1:\n                raise ValueError(\n                    f\"prompt_lookup_min={self.prompt_lookup_min} must be > 0\")\n            if self.prompt_lookup_max < 1:\n                raise ValueError(\n                    f\"prompt_lookup_max={self.prompt_lookup_max} must be > 0\")\n            if self.prompt_lookup_min > self.prompt_lookup_max:\n                raise ValueError(\n                    f\"prompt_lookup_min={self.prompt_lookup_min} must \"\n                    f\"be <= prompt_lookup_max={self.prompt_lookup_max}\")\n\n            # TODO: current we still need extract vocab_size from target model\n            # config, in future, we may try refactor it out, and set\n            # draft related config as None here.\n            self.draft_model_config = self.target_model_config\n            self.draft_parallel_config = self.target_parallel_config\n        else:\n            self.prompt_lookup_max = 0\n            self.prompt_lookup_min = 0\n\n            if self.model is not None:\n                self.draft_model_config = ModelConfig(\n                    model=self.model,\n                    task=\"draft\",\n                    tokenizer=self.target_model_config.tokenizer,\n                    tokenizer_mode=self.target_model_config.tokenizer_mode,\n                    trust_remote_code=self.target_model_config.\n                    trust_remote_code,\n                    allowed_local_media_path=self.target_model_config.\n                    allowed_local_media_path,\n                    dtype=self.target_model_config.dtype,\n                    seed=self.target_model_config.seed,\n                    revision=self.revision,\n                    code_revision=self.code_revision,\n                    tokenizer_revision=self.target_model_config.\n                    tokenizer_revision,\n                    spec_target_max_model_len=self.target_model_config.\n                    max_model_len,\n                    quantization=self.quantization,\n                    enforce_eager=self.target_model_config.enforce_eager,\n                    max_seq_len_to_capture=self.target_model_config.\n                    max_seq_len_to_capture,\n                    max_logprobs=self.target_model_config.max_logprobs,\n                    hf_overrides=SpeculativeConfig.hf_config_override,\n                )\n\n                # Automatically detect the method\n                if self.method in ('eagle', 'eagle3'):\n                    pass\n                elif \"eagle-\" in self.draft_model_config.model.lower() or \\\n                        \"eagle3-\" in self.draft_model_config.model.lower():\n                    self.method = \"eagle\"\n                elif self.draft_model_config.hf_config.model_type == \"medusa\":\n                    self.method = \"medusa\"\n                elif (self.draft_model_config.hf_config.model_type ==\n                      \"mlp_speculator\"):\n                    self.method = \"mlp_speculator\"\n                else:\n                    self.method = \"draft_model\"\n\n                # Replace hf_config for EAGLE draft_model\n                if self.method in (\"eagle\", \"eagle3\"):\n                    if self.enable_chunked_prefill and not envs.VLLM_USE_V1:\n                        raise ValueError(\n                            \"Chunked prefill and EAGLE are not compatible \"\n                            \"when using V0.\")\n\n                    from vllm.platforms import current_platform\n                    from vllm.transformers_utils.configs.eagle import (\n                        EAGLEConfig)\n                    if isinstance(self.draft_model_config.hf_config,\n                                  EAGLEConfig) or current_platform.is_neuron():\n                        pass\n                    else:\n                        eagle_config = EAGLEConfig(\n                            self.draft_model_config.hf_config,\n                            method=self.method)\n                        self.draft_model_config.hf_config = eagle_config\n\n                if (self.num_speculative_tokens is not None\n                        and hasattr(self.draft_model_config.hf_config,\n                                    \"num_lookahead_tokens\")):\n                    self.draft_model_config.hf_config.num_lookahead_tokens = \\\n                    self.num_speculative_tokens\n\n                n_predict = getattr(self.draft_model_config.hf_config,\n                                    \"n_predict\", None)\n                if n_predict is not None:\n                    if self.num_speculative_tokens is None:\n                        # Default to max value defined in draft model config.\n                        self.num_speculative_tokens = n_predict\n                    elif self.num_speculative_tokens > n_predict and \\\n                            self.num_speculative_tokens % n_predict != 0:\n                        # Ensure divisibility for MTP module reuse.\n                        raise ValueError(\n                            f\"num_speculative_tokens:{self.num_speculative_tokens}\"\n                            f\" must be divisible by {n_predict=}\")\n\n                self.draft_tensor_parallel_size = \\\n                    SpeculativeConfig._verify_and_get_draft_tp(\n                        self.target_parallel_config,\n                        self.draft_tensor_parallel_size,\n                        self.draft_model_config.hf_config\n                )\n\n                self.draft_model_config.max_model_len = (\n                    SpeculativeConfig._maybe_override_draft_max_model_len(\n                        self.max_model_len,\n                        self.draft_model_config.max_model_len,\n                        self.target_model_config.max_model_len,\n                    ))\n\n                self.draft_parallel_config = (\n                    SpeculativeConfig.create_draft_parallel_config(\n                        self.target_parallel_config,\n                        self.draft_tensor_parallel_size))\n\n        if self.acceptance_method == \"typical_acceptance_sampler\":\n            if self.posterior_threshold is None:\n                self.posterior_threshold = 0.09\n            if self.posterior_alpha is None:\n                self.posterior_alpha = 0.3\n\n        self._verify_args()\n\n    @staticmethod\n    def _maybe_override_draft_max_model_len(\n        speculative_max_model_len: Optional[int],\n        draft_max_model_len: int,\n        target_max_model_len: int,\n    ) -> int:\n        \"\"\"Determine the max sequence len for the draft model. This is usually\n        the draft_max_model_len, but may be the target_max_model_len if it is\n        less than the draft_max_model_len, or may be speculative_max_model_len\n        if it is specified.\n\n        This is necessary so that sequences do not exceed the capacity of the\n        draft model or the target model.\n\n        speculative_max_model_len is mainly used for testing that sequences can\n        skip speculation.\n        \"\"\"\n\n        if speculative_max_model_len is not None:\n\n            if speculative_max_model_len > draft_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {draft_max_model_len=}\")\n\n            if speculative_max_model_len > target_max_model_len:\n                raise ValueError(f\"{speculative_max_model_len=} cannot be \"\n                                 f\"larger than {target_max_model_len=}\")\n\n            return speculative_max_model_len\n\n        return min(\n            draft_max_model_len,\n            target_max_model_len,\n        )\n\n    @staticmethod\n    def _verify_and_get_draft_tp(\n            target_parallel_config: ParallelConfig,\n            speculative_draft_tensor_parallel_size: Optional[int],\n            draft_hf_config: PretrainedConfig) -> int:\n        \"\"\"\n        Verifies and adjusts the tensor parallel size for a draft model\n        specified using speculative_draft_tensor_parallel_size.\n        \"\"\"\n        # If speculative_draft_tensor_parallel_size is unset then set it\n        # appropriately else verify that it is set correctly.\n        if speculative_draft_tensor_parallel_size is None:\n            if draft_hf_config.model_type == \"mlp_speculator\":\n                speculative_draft_tensor_parallel_size = 1\n                if target_parallel_config.tensor_parallel_size > 1:\n                    logger.warning(\n                        \"%s cannot currently be run with tp>1; \"\n                        \"setting speculative_draft_tensor_parallel_size=1\",\n                        draft_hf_config.model_type)\n            else:\n                speculative_draft_tensor_parallel_size = \\\n                    target_parallel_config.tensor_parallel_size\n        elif speculative_draft_tensor_parallel_size not in (\n                1, target_parallel_config.tensor_parallel_size):\n            raise ValueError(\n                f\"{speculative_draft_tensor_parallel_size=} cannot be \"\n                f\"other value than 1 or target model tensor_parallel_size\")\n        return speculative_draft_tensor_parallel_size\n\n    @staticmethod\n    def create_draft_parallel_config(\n        target_parallel_config: ParallelConfig,\n        speculative_draft_tensor_parallel_size: int,\n    ) -> ParallelConfig:\n        \"\"\"Create a parallel config for use by the draft worker.\n\n        This is mostly a copy of the target parallel config, except the tp_size.\n        \"\"\"\n        draft_parallel_config = ParallelConfig(\n            pipeline_parallel_size=target_parallel_config.\n            pipeline_parallel_size,\n            tensor_parallel_size=speculative_draft_tensor_parallel_size,\n            distributed_executor_backend=target_parallel_config.\n            distributed_executor_backend,\n            max_parallel_loading_workers=target_parallel_config.\n            max_parallel_loading_workers,\n            disable_custom_all_reduce=target_parallel_config.\n            disable_custom_all_reduce,\n            ray_workers_use_nsight=target_parallel_config.\n            ray_workers_use_nsight,\n            placement_group=target_parallel_config.placement_group,\n        )\n\n        return draft_parallel_config\n\n    def _verify_args(self) -> None:\n        if self.num_speculative_tokens is None:\n            raise ValueError(\n                \"num_speculative_tokens must be provided with \"\n                \"speculative model unless the draft model config contains an \"\n                \"n_predict parameter.\")\n\n        if self.num_speculative_tokens <= 0:\n            raise ValueError(\"Expected num_speculative_tokens to be greater \"\n                             f\"than zero ({self.num_speculative_tokens}).\")\n\n        if self.draft_model_config:\n            self.draft_model_config.verify_with_parallel_config(\n                self.draft_parallel_config)\n            # Validate and set draft token acceptance related settings.\n\n        if self.acceptance_method is None:\n            raise ValueError(\"acceptance_method is not set. \"\n                             \"Expected values are rejection_sampler or \"\n                             \"typical_acceptance_sampler.\")\n\n        if (self.acceptance_method != 'rejection_sampler'\n                and self.acceptance_method != 'typical_acceptance_sampler'):\n            raise ValueError(\n                \"Expected acceptance_method to be either \"\n                \"rejection_sampler or typical_acceptance_sampler. Instead it \"\n                f\"is {self.acceptance_method}\")\n\n        if self.acceptance_method == \"typical_acceptance_sampler\" and (\n            (self.posterior_threshold is not None\n             and self.posterior_threshold < 0) or\n            (self.posterior_alpha is not None and self.posterior_alpha < 0)):\n            raise ValueError(\n                \"Expected the posterior_threshold and posterior_alpha of \"\n                \"typical_acceptance_sampler to be > 0. \"\n                \"Instead found posterior_threshold = \"\n                f\"{self.posterior_threshold} and posterior_alpha = \"\n                f\"{self.posterior_alpha}\")\n\n        if (self.disable_by_batch_size is not None\n                and self.disable_by_batch_size < 2):\n            raise ValueError(\"Expect the batch size threshold of disabling \"\n                             \"speculative decoding is > 1, but got \"\n                             f\"{self.disable_by_batch_size=}\")\n\n        if self.method == \"eagle3\" and self.target_model_config and \\\n            \"llama\" not in self.target_model_config.hf_text_config.model_type:\n            raise ValueError(\n                \"Eagle3 is only supported for Llama models. \"\n                f\"Got {self.target_model_config.hf_text_config.model_type=}\")\n\n    @property\n    def num_lookahead_slots(self) -> int:\n        \"\"\"The number of additional slots the scheduler should allocate per\n        step, in addition to the slots allocated for each known token.\n\n        This is equal to the number of speculative tokens, as each speculative\n        token must be scored.\n        \"\"\"\n        return self.num_speculative_tokens\n\n    def use_eagle(self) -> bool:\n        return self.method in (\"eagle\", \"eagle3\")\n\n    def __repr__(self) -> str:\n        method = self.method\n        model = None if method == \"ngram\" else self.draft_model_config.model\n        num_spec_tokens = self.num_speculative_tokens\n        return f\"SpeculativeConfig({method=}, {model=}, {num_spec_tokens=})\"\n\n\nLoRADType = Literal[\"auto\", \"float16\", \"bfloat16\"]\n\n\n@config\n@dataclass\nclass LoRAConfig:\n    \"\"\"Configuration for LoRA.\"\"\"\n\n    max_lora_rank: int = 16\n    \"\"\"Max LoRA rank.\"\"\"\n    max_loras: int = 1\n    \"\"\"Max number of LoRAs in a single batch.\"\"\"\n    fully_sharded_loras: bool = False\n    \"\"\"By default, only half of the LoRA computation is sharded with tensor\n    parallelism. Enabling this will use the fully sharded layers. At high\n    sequence length, max rank or tensor parallel size, this is likely faster.\n    \"\"\"\n    max_cpu_loras: Optional[int] = None\n    \"\"\"Maximum number of LoRAs to store in CPU memory. Must be >= than\n    `max_loras`.\"\"\"\n    lora_dtype: Union[torch.dtype, LoRADType] = \"auto\"\n    \"\"\"Data type for LoRA. If auto, will default to base model dtype.\"\"\"\n    lora_extra_vocab_size: int = 256\n    \"\"\"Maximum size of extra vocabulary that can be present in a LoRA adapter\n    (added to the base model vocabulary).\"\"\"\n    lora_vocab_padding_size: ClassVar[int] = current_platform\\\n        .get_lora_vocab_padding_size()\n    long_lora_scaling_factors: Optional[tuple[float, ...]] = None\n    \"\"\"Specify multiple scaling factors (which can be different from base model\n    scaling factor - see eg. Long LoRA) to allow for multiple LoRA adapters\n    trained with those scaling factors to be used at the same time. If not\n    specified, only adapters trained with the base model scaling factor are\n    allowed.\"\"\"\n    bias_enabled: bool = False\n    \"\"\"Enable bias for LoRA adapters.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.max_lora_rank)\n        factors.append(self.max_loras)\n        factors.append(self.fully_sharded_loras)\n        factors.append(self.lora_dtype)\n        factors.append(self.lora_extra_vocab_size)\n        factors.append(self.lora_vocab_padding_size)\n        factors.append(self.long_lora_scaling_factors)\n        factors.append(self.bias_enabled)\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        # Setting the maximum rank to 512 should be able to satisfy the vast\n        # majority of applications.\n        possible_max_ranks = (8, 16, 32, 64, 128, 256, 320, 512)\n        possible_lora_extra_vocab_size = (256, 512)\n        if self.max_lora_rank not in possible_max_ranks:\n            raise ValueError(\n                f\"max_lora_rank ({self.max_lora_rank}) must be one of \"\n                f\"{possible_max_ranks}.\")\n        if self.lora_extra_vocab_size not in possible_lora_extra_vocab_size:\n            raise ValueError(\n                f\"lora_extra_vocab_size ({self.lora_extra_vocab_size}) \"\n                f\"must be one of {possible_lora_extra_vocab_size}.\")\n        if self.max_loras < 1:\n            raise ValueError(f\"max_loras ({self.max_loras}) must be >= 1.\")\n        if self.max_cpu_loras is None:\n            self.max_cpu_loras = self.max_loras\n        elif self.max_cpu_loras < self.max_loras:\n            raise ValueError(\n                f\"max_cpu_loras ({self.max_cpu_loras}) must be >= \"\n                f\"max_loras ({self.max_loras})\")\n\n    def verify_with_cache_config(self, cache_config: CacheConfig):\n        if cache_config.cpu_offload_gb > 0 and not envs.VLLM_USE_V1:\n            raise ValueError(\n                \"V0 LoRA does not support CPU offload, please use V1.\")\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.lora_dtype in (None, \"auto\"):\n            self.lora_dtype = model_config.dtype\n        elif isinstance(self.lora_dtype, str):\n            self.lora_dtype = getattr(torch, self.lora_dtype)\n\n    def verify_lora_support(self):\n        if self.long_lora_scaling_factors is not None and envs.VLLM_USE_V1:\n            raise ValueError(\n                \"V1 LoRA does not support long LoRA, please use V0.\")\n\n\n@config\n@dataclass\nclass PromptAdapterConfig:\n    \"\"\"Configuration for PromptAdapters.\"\"\"\n\n    max_prompt_adapters: int = 1\n    \"\"\"Max number of PromptAdapters in a batch.\"\"\"\n    max_prompt_adapter_token: int = 0\n    \"\"\"Max number of PromptAdapters tokens.\"\"\"\n    max_cpu_prompt_adapters: Optional[int] = None\n    \"\"\"Maximum number of PromptAdapters to store in CPU memory. Must be >= than\n    `max_prompt_adapters`.\"\"\"\n    prompt_adapter_dtype: Union[torch.dtype, str] = \"auto\"\n    \"\"\"Data type for PromptAdapter. If auto, will default to base model dtype.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n\n        if self.max_prompt_adapters < 1:\n            raise ValueError(f\"max_prompt_adapters \"\n                             f\"({self.max_prompt_adapters}) must be >= 1.\")\n        if self.max_prompt_adapter_token == 0:\n            raise ValueError(\"max_prompt_adapter_token must be set.\")\n        if self.max_cpu_prompt_adapters is None:\n            self.max_cpu_prompt_adapters = self.max_prompt_adapters\n\n    def verify_with_model_config(self, model_config: ModelConfig):\n        if self.prompt_adapter_dtype == \"auto\":\n            self.prompt_adapter_dtype = model_config.dtype\n        elif isinstance(self.prompt_adapter_dtype, str):\n            self.prompt_adapter_dtype = getattr(torch,\n                                                self.prompt_adapter_dtype)\n\n\n@config\n@dataclass\nclass MultiModalConfig:\n    \"\"\"Controls the behavior of multimodal models.\"\"\"\n\n    limit_per_prompt: dict[str, int] = \\\n        cast(dict[str, int], get_field(ModelConfig, \"limit_mm_per_prompt\"))\n    \"\"\"\n    The maximum number of input items allowed per prompt for each modality.\n    Defaults to 1 (V0) or 999 (V1) for each modality.\n\n    For example, to allow up to 16 images and 2 videos per prompt:\n    `{\"images\": 16, \"videos\": 2}`\n    \"\"\"\n\n    mm_processor_kwargs: Optional[dict[str, object]] = None\n    \"\"\"\n    Overrides for the multi-modal processor obtained from\n    `transformers.AutoProcessor.from_pretrained`.\n\n    The available overrides depend on the model that is being run.\n\n    For example, for Phi-3-Vision:\n    `{\"num_crops\": 4}`.\n    \"\"\"\n\n    disable_mm_preprocessor_cache: bool = False\n    \"\"\"\n    If `True`, disable caching of the processed multi-modal inputs.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def get_limit_per_prompt(self, modality: str) -> int:\n        \"\"\"\n        Get the maximum number of input items allowed per prompt\n        for the given modality.\n        \"\"\"\n        return self.limit_per_prompt.get(\n            modality,\n            999 if envs.VLLM_USE_V1 else 1,\n        )\n\n    # TODO: Add configs to init vision tower or not.\n\n\n@config\n@dataclass\nclass PoolerConfig:\n    \"\"\"Controls the behavior of output pooling in pooling models.\"\"\"\n\n    pooling_type: Optional[str] = None\n    \"\"\"\n    The pooling method of the pooling model. This should be a key in\n    {class}`vllm.model_executor.layers.pooler.PoolingType`.\n    \"\"\"\n\n    normalize: Optional[bool] = None\n    \"\"\"\n    Whether to normalize the pooled outputs. Usually, this should be set to\n    ``True`` for embedding outputs.\n    \"\"\"\n\n    softmax: Optional[bool] = None\n    \"\"\"\n    Whether to apply softmax to the pooled outputs. Usually, this should be set\n    to ``True`` for classification outputs.\n    \"\"\"\n\n    step_tag_id: Optional[int] = None\n    \"\"\"\n    If set, only the score corresponding to the ``step_tag_id`` in the\n    generated sentence should be returned. Otherwise, the scores for all tokens\n    are returned.\n    \"\"\"\n\n    returned_token_ids: Optional[list[int]] = None\n    \"\"\"\n    A list of indices for the vocabulary dimensions to be extracted,\n    such as the token IDs of ``good_token`` and ``bad_token`` in the\n    ``math-shepherd-mistral-7b-prm`` model.\n    \"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n\n_STR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.float16,\n    \"float16\": torch.float16,\n    \"float\": torch.float32,\n    \"float32\": torch.float32,\n    \"bfloat16\": torch.bfloat16,\n}\n\n_ROCM_NOT_SUPPORTED_DTYPE: list[str] = []  #\n\n\ndef _get_and_verify_dtype(\n    config: PretrainedConfig,\n    dtype: Union[str, torch.dtype],\n) -> torch.dtype:\n    # NOTE: getattr(config, \"torch_dtype\", torch.float32) is not correct\n    # because config.torch_dtype can be None.\n    config_dtype = getattr(config, \"torch_dtype\", None)\n\n    # Fallbacks for multi-modal models if the root config\n    # does not define torch_dtype\n    if config_dtype is None:\n        config_dtype = getattr(config.get_text_config(), \"torch_dtype\", None)\n    if config_dtype is None and hasattr(config, \"vision_config\"):\n        config_dtype = getattr(config.vision_config, \"torch_dtype\", None)\n\n    if config_dtype is None:\n        config_dtype = torch.float32\n\n    if isinstance(dtype, str):\n        dtype = dtype.lower()\n        if dtype == \"auto\":\n            # Set default dtype from model config\n            if config_dtype == torch.float32:\n                # Following common practice, we use float16 for float32 models\n                torch_dtype = torch.float16\n            else:\n                torch_dtype = config_dtype\n\n            if config.model_type == \"plamo2\":\n                logger.warning(\n                    \"For PLaMo2, we cast models to bfloat16 instead of using \"\n                    \"float16 by default. This is because float16 does not work.\"\n                )\n                torch_dtype = torch.bfloat16\n\n            # Deal with torch dtype fallback for device compatibility.\n            from vllm.platforms import current_platform\n            if torch_dtype not in current_platform.supported_dtypes:\n                device_name = current_platform.get_device_name()\n\n                if ((capability := current_platform.get_device_capability())\n                        is None):\n                    compute_str = \"\"\n                else:\n                    version_str = capability.as_version_str()\n                    compute_str = f\" (with compute capability {version_str})\"\n                fallback_dtype = current_platform.supported_dtypes[0]\n                logger.warning(\n                    \"Your %s device%s doesn't support %s. \" \\\n                    \"Falling back to %s for compatibility.\",\n                    device_name, compute_str, torch_dtype, fallback_dtype\n                    )\n                torch_dtype = fallback_dtype\n\n            if current_platform.is_hpu() and torch_dtype == torch.float16:\n                logger.warning(\n                    \"For HPU, we cast models to bfloat16 instead of \"\n                    \"using float16 by default. Please specify `dtype` if you \"\n                    \"want to use float16.\")\n                torch_dtype = torch.bfloat16\n        elif dtype == \"float16\" and config.model_type == \"plamo2\":\n            logger.warning(\n                \"For PLaMo2, using float16 is unstable and might cause \"\n                \"unexpected behavior. Please use bfloat16 or float32 instead.\")\n            torch_dtype = torch.float16\n        else:\n            if dtype not in _STR_DTYPE_TO_TORCH_DTYPE:\n                raise ValueError(f\"Unknown dtype: {dtype}\")\n            torch_dtype = _STR_DTYPE_TO_TORCH_DTYPE[dtype]\n    elif isinstance(dtype, torch.dtype):\n        torch_dtype = dtype\n    else:\n        raise ValueError(f\"Unknown dtype: {dtype}\")\n\n    # Verify the dtype.\n    if torch_dtype != config_dtype:\n        if torch_dtype == torch.float32:\n            # Upcasting to float32 is allowed.\n            logger.info(\"Upcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        elif config_dtype == torch.float32:\n            # Downcasting from float32 to float16 or bfloat16 is allowed.\n            logger.info(\"Downcasting %s to %s.\", config_dtype, torch_dtype)\n            pass\n        else:\n            # Casting between float16 and bfloat16 is allowed with a warning.\n            logger.warning(\"Casting %s to %s.\", config_dtype, torch_dtype)\n\n    return torch_dtype\n\n\ndef _get_and_verify_max_len(\n    hf_config: PretrainedConfig,\n    max_model_len: Optional[int],\n    disable_sliding_window: bool,\n    sliding_window_len: Optional[Union[int, list[Optional[int]]]],\n    spec_target_max_model_len: Optional[int] = None,\n    encoder_config: Optional[Any] = None,\n) -> int:\n    \"\"\"Get and verify the model's maximum length.\"\"\"\n    derived_max_model_len = float(\"inf\")\n    possible_keys = [\n        # OPT\n        \"max_position_embeddings\",\n        # GPT-2\n        \"n_positions\",\n        # MPT\n        \"max_seq_len\",\n        # ChatGLM2\n        \"seq_length\",\n        # Command-R\n        \"model_max_length\",\n        # Whisper\n        \"max_target_positions\",\n        # Others\n        \"max_sequence_length\",\n        \"max_seq_length\",\n        \"seq_len\",\n    ]\n    # Choose the smallest \"max_length\" from the possible keys.\n    max_len_key = None\n    for key in possible_keys:\n        max_len = getattr(hf_config, key, None)\n        if max_len is not None:\n            max_len_key = key if max_len < derived_max_model_len \\\n                else max_len_key\n            derived_max_model_len = min(derived_max_model_len, max_len)\n    # For Command-R / Cohere, Cohere2 / Aya Vision models\n    if tmp_max_len := getattr(hf_config, \"model_max_length\", None):\n        max_len_key = \"model_max_length\"\n        derived_max_model_len = tmp_max_len\n\n    # If sliding window is manually disabled, max_length should be less\n    # than the sliding window length in the model config.\n    if disable_sliding_window and sliding_window_len is not None:\n\n        sliding_window_len_min = get_min_sliding_window(sliding_window_len)\n        max_len_key = \"sliding_window\" \\\n            if sliding_window_len_min < derived_max_model_len else max_len_key\n        derived_max_model_len = min(derived_max_model_len,\n                                    sliding_window_len_min)\n\n    # If none of the keys were found in the config, use a default and\n    # log a warning.\n    if derived_max_model_len == float(\"inf\"):\n        if max_model_len is not None:\n            # If max_model_len is specified, we use it.\n            return max_model_len\n\n        if spec_target_max_model_len is not None:\n            # If this is a speculative draft model, we use the max model len\n            # from the target model.\n            return spec_target_max_model_len\n\n        default_max_len = 2048\n        logger.warning(\n            \"The model's config.json does not contain any of the following \"\n            \"keys to determine the original maximum length of the model: \"\n            \"%s. Assuming the model's maximum length is %d.\", possible_keys,\n            default_max_len)\n        derived_max_model_len = default_max_len\n\n    rope_scaling = getattr(hf_config, \"rope_scaling\", None)\n    # NOTE(woosuk): Gemma3's max_model_len (128K) is already scaled by RoPE\n    # scaling, so we skip applying the scaling factor again.\n    if rope_scaling is not None and \"gemma3\" not in hf_config.model_type:\n        # No need to consider \"type\" key because of patch_rope_scaling when\n        # loading HF config\n        rope_type = rope_scaling[\"rope_type\"]\n\n        if rope_type not in (\"su\", \"longrope\", \"llama3\"):\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that supports rope_scaling\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"with rope_scaling. Please raise an issue so we can \"\n                    \"investigate.\")\n\n            # NOTE: rope_type == \"default\" does not define factor\n            # https://github.com/huggingface/transformers/blob/v4.45.2/src/transformers/modeling_rope_utils.py\n            scaling_factor = rope_scaling.get(\"factor\", 1.0)\n\n            if rope_type == \"yarn\":\n                derived_max_model_len = rope_scaling[\n                    \"original_max_position_embeddings\"]\n            derived_max_model_len *= scaling_factor\n\n    if encoder_config and \"max_seq_length\" in encoder_config:\n        derived_max_model_len = encoder_config[\"max_seq_length\"]\n\n    # If the user specified a max length, make sure it is smaller than the\n    # derived length from the HF model config.\n    if max_model_len is None:\n        max_model_len = int(derived_max_model_len)\n        if current_platform.is_tpu():\n            logger.warning(\n                \"--max-model-len is not specified, \"\n                \"it's currently using model's default length %s, \"\n                \"which might be too large.\"\n                \"Please input with --max-model-len based on your \"\n                \"request input length and output length, to avoid \"\n                \"unnecessary degradation.\", max_model_len)\n    elif max_model_len > derived_max_model_len:\n        # Some models might have a separate key for specifying model_max_length\n        # that will be bigger than derived_max_model_len. We compare user input\n        # with model_max_length and allow this override when it's smaller.\n        model_max_length = getattr(hf_config, \"model_max_length\", None)\n        if model_max_length is not None and max_model_len <= model_max_length:\n            if disable_sliding_window:\n                # TODO(robertgshaw): Find a model that has model_max_length\n                # with sliding window to see if this case should be allowed.\n                raise NotImplementedError(\n                    \"Disabling sliding window is not supported for models \"\n                    \"model_max_length in the config. Please raise an issue \"\n                    \"so we can investigate.\")\n        else:\n            msg = (\n                f\"User-specified max_model_len ({max_model_len}) is greater \"\n                f\"than the derived max_model_len ({max_len_key}=\"\n                f\"{derived_max_model_len} or model_max_length=\"\n                f\"{model_max_length} in model's config.json). This may lead \"\n                \"to incorrect model outputs or CUDA errors.\")\n            if envs.VLLM_ALLOW_LONG_MAX_MODEL_LEN:\n                logger.warning(\n                    \"%s Make sure the value is correct and within the \"\n                    \"model context size.\", msg)\n            else:\n                raise ValueError(\n                    f\"{msg} To allow overriding this maximum, set \"\n                    \"the env var VLLM_ALLOW_LONG_MAX_MODEL_LEN=1\")\n    return int(max_model_len)\n\n\ndef get_min_sliding_window(\n        sliding_window: Union[int, list[Optional[int]]]) -> int:\n    if isinstance(sliding_window, list):\n        return min(s for s in sliding_window if s is not None)\n\n    return sliding_window\n\n\ndef get_served_model_name(model: str,\n                          served_model_name: Optional[Union[str, list[str]]]):\n    \"\"\"\n    If the input is a non-empty list, the first model_name in\n    `served_model_name` is taken.\n    If the input is a non-empty string, it is used directly.\n    For cases where the input is either an empty string or an\n    empty list, the fallback is to use `self.model`.\n    \"\"\"\n    if not served_model_name:\n        return model\n    if isinstance(served_model_name, list):\n        return served_model_name[0]\n    return served_model_name\n\n\nGuidedDecodingBackendV0 = Literal[\"auto\", \"outlines\", \"lm-format-enforcer\",\n                                  \"xgrammar\", \"guidance\"]\nGuidedDecodingBackendV1 = Literal[\"auto\", \"xgrammar\", \"guidance\"]\nGuidedDecodingBackend = Literal[GuidedDecodingBackendV0,\n                                GuidedDecodingBackendV1]\n\n\n@config\n@dataclass\nclass DecodingConfig:\n    \"\"\"Dataclass which contains the decoding strategy of the engine.\"\"\"\n\n    @property\n    @deprecated(\n        \"`guided_decoding_backend` is deprecated and has been renamed to \"\n        \"`backend`. This will be removed in v0.10.0. Please use the \"\n        \"`backend` argument instead.\")\n    def guided_decoding_backend(self) -> GuidedDecodingBackend:\n        return self.backend\n\n    @guided_decoding_backend.setter\n    def guided_decoding_backend(self, value: GuidedDecodingBackend):\n        self.backend = value\n\n    backend: GuidedDecodingBackend = \"auto\" if envs.VLLM_USE_V1 else \"xgrammar\"\n    \"\"\"Which engine will be used for guided decoding (JSON schema / regex etc)\n    by default. With \"auto\", we will make opinionated choices based on request\n    contents and what the backend libraries currently support, so the behavior\n    is subject to change in each release.\"\"\"\n\n    disable_fallback: bool = False\n    \"\"\"If `True`, vLLM will not fallback to a different backend on error.\"\"\"\n\n    disable_any_whitespace: bool = False\n    \"\"\"If `True`, the model will not generate any whitespace during guided\n    decoding. This is only supported for xgrammar and guidance backends.\"\"\"\n\n    disable_additional_properties: bool = False\n    \"\"\"If `True`, the `guidance` backend will not use `additionalProperties`\n    in the JSON schema. This is only supported for the `guidance` backend and\n    is used to better align its behaviour with `outlines` and `xgrammar`.\"\"\"\n\n    reasoning_backend: str = \"\"\n    \"\"\"Select the reasoning parser depending on the model that you're using.\n    This is used to parse the reasoning content into OpenAI API format.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if \":\" in self.backend:\n            self._extract_backend_options()\n\n        if envs.VLLM_USE_V1:\n            valid_guided_backends = get_args(GuidedDecodingBackendV1)\n        else:\n            valid_guided_backends = get_args(GuidedDecodingBackendV0)\n        if self.backend not in valid_guided_backends:\n            raise ValueError(f\"Invalid backend '{self.backend}',\"\n                             f\" must be one of {valid_guided_backends}\")\n        if (self.disable_any_whitespace\n                and self.backend not in (\"xgrammar\", \"guidance\")):\n            raise ValueError(\"disable_any_whitespace is only supported for \"\n                             \"xgrammar and guidance backends.\")\n        if (self.disable_additional_properties and self.backend != \"guidance\"):\n            raise ValueError(\"disable_additional_properties is only supported \"\n                             \"for the guidance backend.\")\n\n    @deprecated(\n        \"Passing guided decoding backend options inside backend in the format \"\n        \"'backend:...' is deprecated. This will be removed in v0.10.0. Please \"\n        \"use the dedicated arguments '--disable-fallback', \"\n        \"'--disable-any-whitespace' and '--disable-additional-properties' \"\n        \"instead.\")\n    def _extract_backend_options(self):\n        \"\"\"Extract backend options from the backend string.\"\"\"\n        backend, options = self.backend.split(\":\")\n        self.backend = cast(GuidedDecodingBackend, backend)\n        options_set = set(options.strip().split(\",\"))\n        if \"no-fallback\" in options_set:\n            self.disable_fallback = True\n        if \"disable-any-whitespace\" in options_set:\n            self.disable_any_whitespace = True\n        if \"no-additional-properties\" in options_set:\n            self.disable_additional_properties = True\n\n\nDetailedTraceModules = Literal[\"model\", \"worker\", \"all\"]\n\n\n@config\n@dataclass\nclass ObservabilityConfig:\n    \"\"\"Configuration for observability - metrics and tracing.\"\"\"\n\n    show_hidden_metrics_for_version: Optional[str] = None\n    \"\"\"Enable deprecated Prometheus metrics that have been hidden since the\n    specified version. For example, if a previously deprecated metric has been\n    hidden since the v0.7.0 release, you use\n    `--show-hidden-metrics-for-version=0.7` as a temporary escape hatch while\n    you migrate to new metrics. The metric is likely to be removed completely\n    in an upcoming release.\"\"\"\n\n    @cached_property\n    def show_hidden_metrics(self) -> bool:\n        \"\"\"Check if the hidden metrics should be shown.\"\"\"\n        if self.show_hidden_metrics_for_version is None:\n            return False\n        return version._prev_minor_version_was(\n            self.show_hidden_metrics_for_version)\n\n    otlp_traces_endpoint: Optional[str] = None\n    \"\"\"Target URL to which OpenTelemetry traces will be sent.\"\"\"\n\n    collect_detailed_traces: Optional[list[DetailedTraceModules]] = None\n    \"\"\"It makes sense to set this only if `--otlp-traces-endpoint` is set. If\n    set, it will collect detailed traces for the specified modules. This\n    involves use of possibly costly and or blocking operations and hence might\n    have a performance impact.\n\n    Note that collecting detailed timing information for each request can be\n    expensive.\"\"\"\n\n    @cached_property\n    def collect_model_forward_time(self) -> bool:\n        \"\"\"Whether to collect model forward time for the request.\"\"\"\n        return (self.collect_detailed_traces is not None\n                and (\"model\" in self.collect_detailed_traces\n                     or \"all\" in self.collect_detailed_traces))\n\n    @cached_property\n    def collect_model_execute_time(self) -> bool:\n        \"\"\"Whether to collect model execute time for the request.\"\"\"\n        return (self.collect_detailed_traces is not None\n                and (\"worker\" in self.collect_detailed_traces\n                     or \"all\" in self.collect_detailed_traces))\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self):\n        if (self.collect_detailed_traces is not None\n                and len(self.collect_detailed_traces) == 1\n                and \",\" in self.collect_detailed_traces[0]):\n            self._parse_collect_detailed_traces()\n\n        if not is_otel_available() and self.otlp_traces_endpoint is not None:\n            raise ValueError(\n                \"OpenTelemetry is not available. Unable to configure \"\n                \"'otlp_traces_endpoint'. Ensure OpenTelemetry packages are \"\n                f\"installed. Original error:\\n{otel_import_error_traceback}\")\n\n    def _parse_collect_detailed_traces(self):\n        assert isinstance(self.collect_detailed_traces, list)\n        self.collect_detailed_traces = cast(\n            list[DetailedTraceModules],\n            self.collect_detailed_traces[0].split(\",\"))\n\n\nKVProducer = Literal[\"kv_producer\", \"kv_both\"]\nKVConsumer = Literal[\"kv_consumer\", \"kv_both\"]\nKVRole = Literal[KVProducer, KVConsumer]\n\n\n@config\n@dataclass\nclass KVTransferConfig:\n    \"\"\"Configuration for distributed KV cache transfer.\"\"\"\n\n    kv_connector: Optional[str] = None\n    \"\"\"The KV connector for vLLM to transmit KV caches between vLLM instances.\n    \"\"\"\n\n    engine_id: str = str(uuid.uuid4())\n    \"\"\"The engine id for KV transfers.\"\"\"\n\n    kv_buffer_device: Optional[str] = \"cuda\"\n    \"\"\"The device used by kv connector to buffer the KV cache.\n    Currently only support 'cuda'.\"\"\"\n\n    kv_buffer_size: float = 1e9\n    \"\"\"The buffer size for TorchDistributedConnector. Measured in number of\n    bytes. Recommended value: 1e9 (about 1GB).\"\"\"\n\n    kv_role: Optional[KVRole] = None\n    \"\"\"Whether this vLLM instance produces, consumes KV cache, or both. Choices\n    are 'kv_producer', 'kv_consumer', and 'kv_both'.\"\"\"\n\n    kv_rank: Optional[int] = None\n    \"\"\"The rank of this vLLM instance in the KV cache transfer. Typical value:\n    0 for prefill instance, 1 for decode instance.\n    Currently only 1P1D is supported.\"\"\"\n\n    kv_parallel_size: int = 1\n    \"\"\"The number of parallel instances for KV cache transfer. For\n    PyNcclConnector, this should be 2.\"\"\"\n\n    kv_ip: str = \"127.0.0.1\"\n    \"\"\"The KV connector ip, used to build distributed connection.\"\"\"\n\n    kv_port: int = 14579\n    \"\"\"The KV connector port, used to build distributed connection.\"\"\"\n\n    kv_connector_extra_config: dict[str, Any] = field(default_factory=dict)\n    \"\"\"any extra config that the connector may need.\"\"\"\n\n    kv_connector_module_path: Optional[str] = None\n    \"\"\"The Python module path to dynamically load the KV connector from.\n    Only supported in V1.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        # no factors to consider.\n        # this config will not affect the computation graph.\n        factors: list[Any] = []\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()\n        return hash_str\n\n    def __post_init__(self) -> None:\n        if self.kv_role is not None and self.kv_role not in get_args(KVRole):\n            raise ValueError(f\"Unsupported kv_role: {self.kv_role}. \"\n                             f\"Supported roles are {get_args(KVRole)}\")\n\n        if self.kv_connector is not None and self.kv_role is None:\n            raise ValueError(\"Please specify kv_disagg_role when kv_connector \"\n                             f\"is set, supported roles are {get_args(KVRole)}\")\n\n    @property\n    def is_kv_transfer_instance(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in get_args(KVRole)\n\n    @property\n    def is_kv_producer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in get_args(KVProducer)\n\n    @property\n    def is_kv_consumer(self) -> bool:\n        return self.kv_connector is not None and \\\n            self.kv_role in get_args(KVConsumer)\n\n    def get_from_extra_config(self, key, default) -> Any:\n        return self.kv_connector_extra_config.get(key, default)\n\n\n@config\n@dataclass\nclass KVEventsConfig:\n    \"\"\"Configuration for KV event publishing.\"\"\"\n\n    enable_kv_cache_events: bool = False\n    \"\"\"If True, enable KV cache events for tracking block storage and removal.\n    Events can be published externally by zmq using the event publisher config.\n    \"\"\"\n\n    publisher: str = \"null\"\n    \"\"\"The publisher to use for publishing kv events. Can be \"null\", \"zmq\".\n    \"\"\"\n\n    endpoint: str = \"tcp://*:5557\"\n    \"\"\"The zmq endpoint to use for publishing kv events.\n    \"\"\"\n\n    replay_endpoint: Optional[str] = None\n    \"\"\"The zmq endpoint to use for replaying kv events.\n    \"\"\"\n\n    buffer_steps: int = 10_000\n    \"\"\"The number of steps to cache for replay endpoint. Will only save\n    events from the last N steps for the replay endpoint.\n    \"\"\"\n\n    hwm: int = 100_000\n    \"\"\"The zmq high water mark for the event publisher. After queueing N events,\n    events will start dropping if the consumer is not keeping up.\n    \"\"\"\n\n    max_queue_size: int = 100_000\n    \"\"\"The maximum number of events to queue while waiting for publishing.\n    \"\"\"\n\n    topic: str = \"\"\n    \"\"\"The topic to use for the event publisher. Consumers can subscribe to\n    this topic to receive events.\n    \"\"\"\n\n\nclass CompilationLevel:\n    # constants for the levels of the compilation process\n    NO_COMPILATION = 0\n    DYNAMO_AS_IS = 1\n    DYNAMO_ONCE = 2\n    PIECEWISE = 3\n\n\n@config\n@dataclass\nclass PassConfig:\n    \"\"\"Configuration for custom Inductor passes.\n\n    This is separate from general `CompilationConfig` so that inductor passes\n    don't all have access to full configuration - that would create a cycle as\n    the `PassManager` is set as a property of config.\"\"\"\n\n    dump_graph_stages: list[str] = field(default_factory=list)\n    \"\"\"List of stages for which we want to dump the graph. Each pass defines\n    its own stages (before, after, maybe in-between).\"\"\"\n    dump_graph_dir: Path = Path(\".\")\n    \"\"\"Directory to dump the graphs.\"\"\"\n    # TODO(luka) better pass enabling system.\n    enable_fusion: bool = True\n    \"\"\"Whether to enable the custom fusion pass.\"\"\"\n    enable_noop: bool = True\n    \"\"\"Whether to enable the custom no-op elimination pass.\"\"\"\n    enable_sequence_parallelism: bool = False\n    \"\"\"Whether to enable sequence parallelism.\"\"\"\n\n    def uuid(self):\n        \"\"\"\n        Produces a hash unique to the pass configuration.\n        Any new fields that affect compilation should be added to the hash.\n        Do not include dump_graph_* in the hash - they don't affect\n        compilation.\n        \"\"\"\n        include = {\n            \"enable_fusion\", \"enable_noop\", \"enable_sequence_parallelism\"\n        }\n        dict_ = {k: v for k, v in asdict(self).items() if k in include}\n        return InductorPass.hash_dict(dict_)\n\n    def __post_init__(self) -> None:\n        if not self.enable_noop and self.enable_fusion:\n            logger.warning_once(\n                \"Fusion enabled but reshape elimination disabled. \"\n                \"RMSNorm + quant (fp8) fusion might not work\")\n\n\n@config\n@dataclass\nclass CompilationConfig:\n    \"\"\"Configuration for compilation. It has three parts:\n\n    - Top-level Compilation control:\n        - {attr}`level`\n        - {attr}`debug_dump_path`\n        - {attr}`cache_dir`\n        - {attr}`backend`\n        - {attr}`custom_ops`\n        - {attr}`splitting_ops`\n    - CudaGraph capture:\n        - {attr}`use_cudagraph`\n        - {attr}`cudagraph_capture_sizes`\n        - {attr}`cudagraph_num_of_warmups`\n        - {attr}`cudagraph_copy_inputs`\n        - {attr}`full_cuda_graph`\n    - Inductor compilation:\n        - {attr}`use_inductor`\n        - {attr}`compile_sizes`\n        - {attr}`inductor_compile_config`\n        - {attr}`inductor_passes`\n        - custom inductor passes\n\n    Why we have different sizes for cudagraph and inductor:\n    - cudagraph: a cudagraph captured for a specific size can only be used\n        for the same size. We need to capture all the sizes we want to use.\n    - inductor: a graph compiled by inductor for a general shape can be used\n        for different sizes. Inductor can also compile for specific sizes,\n        where it can have more information to optimize the graph with fully\n        static shapes. However, we find the general shape compilation is\n        sufficient for most cases. It might be beneficial to compile for\n        certain small batchsizes, where inductor is good at optimizing.\n    \"\"\"\n    # Top-level Compilation control\n    level: int = 0\n    \"\"\"The level of compilation:\n\n    - 0: no compilation.\n    - 1: dynamo as is.\n    - 2: dynamo once.\n    - 3: piecewise compilation.\"\"\"\n    debug_dump_path: str = \"\"\n    \"\"\"The path to dump the debug information.\"\"\"\n    cache_dir: str = \"\"\n    \"\"\"The directory to store the compiled graph, to accelerate Inductor\n    compilation. By default, it will use model-related information to generate\n    a cache directory.\"\"\"\n    backend: str = \"\"\n    \"\"\"The backend for compilation. It needs to be a string:\n\n    - \"\" (empty string): use the default backend.\n    - \"eager\"/\"openxla\"/...: use the specified backend registered in PyTorch.\n    - \"full.module.name\": a qualified name which can be used to import the\n\n    backend function.\n    We use string to avoid serialization issues when using compilation in a\n    distributed setting. When the compilation level is 1 or 2, the backend is\n    used for the compilation directly (it sees the whole graph). When the\n    compilation level is 3, the backend is used for the piecewise compilation\n    (it sees a part of the graph).\"\"\"\n    custom_ops: list[str] = field(default_factory=list)\n    \"\"\"Fine-grained control over which custom ops to enable/disable. Use 'all'\n    to enable all, 'none' to disable all. Also specify a list of custom op\n    names to enable (prefixed with a '+'), or disable (prefixed with a '-').\n    Examples:\n\n    - 'all,-op1' to enable all except op1\n    - 'none,+op1,+op2' to enable only op1 and op2\n\n    By default, all custom ops are enabled when running without Inductor and\n    disabled when running with Inductor (compile_level >= Inductor).\"\"\"\n    splitting_ops: list[str] = field(default_factory=list)\n    \"\"\"A list of ops to split the full graph into subgraphs, used in piecewise\n    compilation.\"\"\"\n\n    # Inductor capture\n    use_inductor: bool = True\n    \"\"\"Whether to use inductor compilation:\n\n    - False: inductor compilation is not used. graph runs in eager.\n    - True: inductor compilation is used. one graph for symbolic shape\n        is compiled. In addition, compile for compile_sizes,\n        using configurations in inductor_compile_config.\"\"\"\n    compile_sizes: Optional[list[Union[int, str]]] = None\n    \"\"\"Sizes to compile for inductor. In addition\n    to integers, it also supports \"cudagraph_capture_sizes\" to\n    specify the sizes for cudagraph capture.\"\"\"\n    inductor_compile_config: dict = field(default_factory=dict)\n    \"\"\"Additional configurations for inductor.\n    - None: use default configurations.\"\"\"\n    inductor_passes: dict[str, str] = field(default_factory=dict)\n    \"\"\"Additional passes for inductor. It is a dictionary\n    from pass name to pass function qualified name. We use function\n    name because the config uses JSON format. If we pass the config\n    from Python, functions can also be passed directly via Python object\n    constructor, e.g. `CompilationConfig(inductor_passes={\"a\": func})`.\"\"\"\n\n    # CudaGraph compilation\n    use_cudagraph: bool = False\n    \"\"\"Whether to use cudagraph inside compilation.\n    - False: cudagraph inside compilation is not used.\n    - True: cudagraph inside compilation is used. It requires\n        that all input buffers have fixed addresses, and all\n        splitting ops write their outputs to input buffers.\n    Note that this is orthogonal to the cudagraph capture logic\n    outside of compilation.\n    TODO: move outside cudagraph logic into compilation.\n    torch.compile will handle cudagraph capture logic in the future.\"\"\"\n    cudagraph_num_of_warmups: int = 0\n    \"\"\"Number of warmup runs for cudagraph.\n    It means the first several runs will be treated as warmup runs.\n    Only after that, the execution will be recorded, and the recorded\n    cudagraph will be used for subsequent runs.\"\"\"\n    cudagraph_capture_sizes: Optional[list[int]] = None\n    \"\"\"Sizes to capture cudagraph.\n    - None (default): capture sizes are inferred from vllm config.\n    - list[int]: capture sizes are specified as given.\"\"\"\n    cudagraph_copy_inputs: bool = False\n    \"\"\"Whether to copy input tensors for\n    cudagraph. If the caller can guarantee that the same input buffers\n    are always used, it can set this to False. Otherwise, it should\n    set this to True, and the compiler will copy the input to an\n    internally managed buffer. Default is False.\"\"\"\n    full_cuda_graph: bool = False\n    \"\"\"whether to use a full cuda graph for the entire forward pass rather than\n    splitting certain operations such as attention into subgraphs. Thus this\n    flag cannot be used together with splitting_ops. This may provide\n    performance benefits for smaller models.\"\"\"\n\n    pass_config: PassConfig = field(default_factory=PassConfig)\n    \"\"\"Custom inductor passes, see PassConfig for more details\"\"\"\n\n    max_capture_size: int = field(default=None, init=False)  # type: ignore\n    \"\"\"not configurable, computed after init\"\"\"\n    local_cache_dir: str = field(default=None, init=False)  # type: ignore\n    \"\"\"local cache dir for each rank\"\"\"\n    bs_to_padded_graph_size: list[int] = field(\n        default=None,  # type: ignore\n        init=False)\n    \"\"\"optimization:\n    Intuitively, bs_to_padded_graph_size should be dict[int, int].\n    since we know all keys are in a range [0, max_capture_size],\n    we can optimize it to list[int] for better lookup performance.\"\"\"\n\n    # keep track of enabled and disabled custom ops\n    enabled_custom_ops: Counter[str] = field(default_factory=Counter,\n                                             init=False)\n    \"\"\"custom ops that are enabled\"\"\"\n    disabled_custom_ops: Counter[str] = field(default_factory=Counter,\n                                              init=False)\n    \"\"\"custom ops that are disabled\"\"\"\n    traced_files: set[str] = field(default_factory=set, init=False)\n    \"\"\"files that are traced for compilation\"\"\"\n    compilation_time: float = field(default=0.0, init=False)\n    \"\"\"time taken for compilation\"\"\"\n\n    static_forward_context: dict[str, Any] = field(default_factory=dict,\n                                                   init=False)\n    \"\"\"Per-model forward context\n    Map from layer name to layer objects that need to be accessed outside\n    model code, e.g., Attention, FusedMOE when dp_size>1.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n        factors.append(self.level)\n        factors.append(self.backend)\n        factors.append(self.custom_ops)\n        factors.append(self.splitting_ops)\n        factors.append(self.use_inductor)\n        factors.append(self.inductor_compile_config)\n        factors.append(self.inductor_passes)\n        factors.append(self.pass_config.uuid())\n        return hashlib.sha256(str(factors).encode()).hexdigest()\n\n    def __repr__(self) -> str:\n        exclude = {\n            \"static_forward_context\",\n            \"enabled_custom_ops\",\n            \"disabled_custom_ops\",\n            \"compilation_time\",\n            \"bs_to_padded_graph_size\",\n            \"pass_config\",\n            \"traced_files\",\n        }\n        include = dict()\n        for k, v in asdict(self).items():\n            if k in exclude:\n                continue\n            f = get_field(CompilationConfig, k)\n            if (d := f.default) is not MISSING and d == v:\n                continue\n            if (df := f.default_factory) is not MISSING and df() == v:\n                continue\n            include[k] = v\n        return json.dumps(include)\n\n    __str__ = __repr__\n\n    @classmethod\n    def from_cli(cls, cli_value: str) -> \"CompilationConfig\":\n        \"\"\"Parse the CLI value for the compilation config.\"\"\"\n        if cli_value in [\"0\", \"1\", \"2\", \"3\"]:\n            return cls(level=int(cli_value))\n        return cls(**json.loads(cli_value))\n\n    def __post_init__(self) -> None:\n        count_none = self.custom_ops.count(\"none\")\n        count_all = self.custom_ops.count(\"all\")\n        assert count_none + count_all <= 1, \"Can only specify 'none' or 'all'\"\n\n        # TODO(zou3519/luka): There are 2 issues with auto-functionalization V2:\n        # 1. A bug in PyTorch, fixed in 2.7:\n        #    https://github.com/pytorch/pytorch/issues/147924\n        # 2. Custom passes (fusion) rely on auto-functionalization V1 and don't\n        #    work with V2. Addressing this will take extra engineering effort\n        #    and it is not yet a priority. RFC here:\n        #    https://github.com/vllm-project/vllm/issues/14703\n\n        if is_torch_equal_or_newer(\"2.6\"):\n            KEY = 'enable_auto_functionalized_v2'\n            if KEY not in self.inductor_compile_config:\n                self.inductor_compile_config[KEY] = False\n\n        for k, v in self.inductor_passes.items():\n            if not isinstance(v, str):\n                assert callable(v), (\n                    f\"pass {k} should be callable or a qualified name\")\n                self.inductor_compile_config[k] = v if isinstance(\n                    v, InductorPass) else CallableInductorPass(v)\n                continue\n\n            # resolve function from qualified name\n            names = v.split(\".\")\n            module = \".\".join(names[:-1])\n            func_name = names[-1]\n            func = __import__(module).__dict__[func_name]\n            self.inductor_compile_config[k] = func if isinstance(\n                func, InductorPass) else CallableInductorPass(func)\n\n        if isinstance(self.pass_config, dict):\n            self.pass_config = PassConfig(**self.pass_config)\n\n    def init_backend(self, vllm_config: \"VllmConfig\") -> Union[str, Callable]:\n        if self.level == CompilationLevel.NO_COMPILATION:\n            raise ValueError(\"No compilation level is set.\")\n\n        from torch._dynamo.backends.registry import list_backends\n        torch_backends = list_backends(exclude_tags=tuple())\n        if self.level in [\n                CompilationLevel.DYNAMO_AS_IS, CompilationLevel.DYNAMO_ONCE\n        ]:\n            if self.backend == \"\":\n                return \"eager\"\n            if self.backend in torch_backends:\n                return self.backend\n            return resolve_obj_by_qualname(self.backend)\n\n        # TODO: pass user-specified backend to piecewise compilation\n        # merge with the config use_inductor\n        assert self.level == CompilationLevel.PIECEWISE\n\n        from vllm.compilation.backends import VllmBackend\n        return VllmBackend(vllm_config)\n\n    def init_with_cudagraph_sizes(self,\n                                  cudagraph_capture_sizes: list[int]) -> None:\n        \"\"\"To complete the initialization of config,\n        we need to know the cudagraph sizes.\"\"\"\n\n        if self.cudagraph_capture_sizes is None:\n            self.cudagraph_capture_sizes = cudagraph_capture_sizes\n        else:\n            # de-duplicate the sizes provided by the config\n            dedup_sizes = list(set(self.cudagraph_capture_sizes))\n            if len(dedup_sizes) < len(self.cudagraph_capture_sizes):\n                logger.info((\"cudagraph sizes specified by model runner\"\n                             \" %s is overridden by config %s\"),\n                            cudagraph_capture_sizes, dedup_sizes)\n            self.cudagraph_capture_sizes = dedup_sizes\n\n        computed_compile_sizes = []\n        if self.compile_sizes is not None:\n            # de-duplicate the sizes provided by the config\n            self.compile_sizes = list(set(self.compile_sizes))\n            for x in self.compile_sizes:\n                if isinstance(x, str):\n                    assert x == \"cudagraph_capture_sizes\", \\\n                    \"Unrecognized size type in compile_sizes, \" \\\n                    f\"expect 'cudagraph_capture_sizes', got {x}\"\n                    computed_compile_sizes.extend(self.cudagraph_capture_sizes)\n                else:\n                    assert isinstance(x, int)\n                    computed_compile_sizes.append(x)\n        self.compile_sizes = computed_compile_sizes  # type: ignore\n\n        # sort to make sure cudagraph capture sizes are in descending order\n        self.cudagraph_capture_sizes.sort(reverse=True)\n        self.max_capture_size = self.cudagraph_capture_sizes[\n            0] if self.cudagraph_capture_sizes else 0\n\n        # pre-compute the mapping from batch size to padded graph size\n        self.bs_to_padded_graph_size = [\n            0 for i in range(self.max_capture_size + 1)\n        ]\n        for end, start in zip(self.cudagraph_capture_sizes,\n                              self.cudagraph_capture_sizes[1:] + [0]):\n            for bs in range(start, end):\n                if bs == start:\n                    self.bs_to_padded_graph_size[bs] = start\n                else:\n                    self.bs_to_padded_graph_size[bs] = end\n        self.bs_to_padded_graph_size[\n            self.max_capture_size] = self.max_capture_size\n\n    def set_splitting_ops_for_v1(self):\n        # NOTE: this function needs to be called\n        if self.splitting_ops and self.full_cuda_graph:\n            raise ValueError(\"full_cuda_graph cannot be used together with \"\n                             \"splitting_ops, as Full CUDA graph will override \"\n                             f\"the splitting_ops: {self.splitting_ops}\")\n\n        if not self.splitting_ops:\n            self.splitting_ops = [] if self.full_cuda_graph else [\n                \"vllm.unified_attention\",\n                \"vllm.unified_attention_with_output\",\n            ]\n\n\n@config\n@dataclass\nclass VllmConfig:\n    \"\"\"Dataclass which contains all vllm-related configuration. This\n    simplifies passing around the distinct configurations in the codebase.\n    \"\"\"\n\n    # TODO: use default_factory once default constructing ModelConfig doesn't\n    # try to download a model\n    model_config: ModelConfig = None  # type: ignore\n    \"\"\"Model configuration.\"\"\"\n    cache_config: CacheConfig = field(default_factory=CacheConfig)\n    \"\"\"Cache configuration.\"\"\"\n    parallel_config: ParallelConfig = field(default_factory=ParallelConfig)\n    \"\"\"Parallel configuration.\"\"\"\n    scheduler_config: SchedulerConfig = field(default_factory=SchedulerConfig)\n    \"\"\"Scheduler configuration.\"\"\"\n    device_config: DeviceConfig = field(default_factory=DeviceConfig)\n    \"\"\"Device configuration.\"\"\"\n    load_config: LoadConfig = field(default_factory=LoadConfig)\n    \"\"\"Load configuration.\"\"\"\n    lora_config: Optional[LoRAConfig] = None\n    \"\"\"LoRA configuration.\"\"\"\n    speculative_config: Optional[SpeculativeConfig] = None\n    \"\"\"Speculative decoding configuration.\"\"\"\n    decoding_config: DecodingConfig = field(default_factory=DecodingConfig)\n    \"\"\"Decoding configuration.\"\"\"\n    observability_config: Optional[ObservabilityConfig] = None\n    \"\"\"Observability configuration.\"\"\"\n    prompt_adapter_config: Optional[PromptAdapterConfig] = None\n    \"\"\"Prompt adapter configuration.\"\"\"\n    quant_config: Optional[QuantizationConfig] = None\n    \"\"\"Quantization configuration.\"\"\"\n    compilation_config: CompilationConfig = field(\n        default_factory=CompilationConfig)\n    \"\"\"`torch.compile` configuration for the model.\n\n    When it is a number (0, 1, 2, 3), it will be interpreted as the\n    optimization level.\n\n    NOTE: level 0 is the default level without any optimization. level 1 and 2\n    are for internal testing only. level 3 is the recommended level for\n    production.\n\n    Following the convention of traditional compilers, using `-O` without space\n    is also supported. `-O3` is equivalent to `-O 3`.\n\n    You can specify the full compilation config like so:\n    `{\"level\": 3, \"cudagraph_capture_sizes\": [1, 2, 4, 8]}`\n    \"\"\"\n    kv_transfer_config: Optional[KVTransferConfig] = None\n    \"\"\"The configurations for distributed KV cache transfer.\"\"\"\n    kv_events_config: Optional[KVEventsConfig] = None\n    \"\"\"The configurations for event publishing.\"\"\"\n    # some opaque config, only used to provide additional information\n    # for the hash computation, mainly used for testing, debugging or out of\n    # tree config registration.\n    additional_config: Union[dict, SupportsHash] = field(default_factory=dict)\n    \"\"\"Additional config for specified platform. Different platforms may\n    support different configs. Make sure the configs are valid for the platform\n    you are using. Contents must be hashable.\"\"\"\n    instance_id: str = \"\"\n    \"\"\"The ID of the vLLM instance.\"\"\"\n\n    def compute_hash(self) -> str:\n        \"\"\"\n        WARNING: Whenever a new field is added to this config,\n        ensure that it is included in the factors list if\n        it affects the computation graph.\n\n        Provide a hash that uniquely identifies all the configs\n        that affect the structure of the computation\n        graph from input ids/embeddings to the final hidden states,\n        excluding anything before input ids/embeddings and after\n        the final hidden states.\n        \"\"\"\n        factors: list[Any] = []\n\n        # summarize vllm config\n        vllm_factors: list[Any] = []\n        from vllm import __version__\n        vllm_factors.append(__version__)\n        vllm_factors.append(envs.VLLM_USE_V1)\n        if self.model_config:\n            vllm_factors.append(self.model_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.cache_config:\n            vllm_factors.append(self.cache_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.parallel_config:\n            vllm_factors.append(self.parallel_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.scheduler_config:\n            vllm_factors.append(self.scheduler_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.device_config:\n            vllm_factors.append(self.device_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.load_config:\n            vllm_factors.append(self.load_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.lora_config:\n            vllm_factors.append(self.lora_config.compute_hash())\n            # LoRA creates static buffers based on max_num_batched_tokens.\n            # The tensor sizes and strides get captured in the torch.compile\n            # graph explicitly.\n            vllm_factors.append(\n                str(self.scheduler_config.max_num_batched_tokens))\n        else:\n            vllm_factors.append(\"None\")\n        if self.speculative_config:\n            vllm_factors.append(self.speculative_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.decoding_config:\n            vllm_factors.append(self.decoding_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.observability_config:\n            vllm_factors.append(self.observability_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.prompt_adapter_config:\n            vllm_factors.append(self.prompt_adapter_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.quant_config:\n            pass  # should be captured by model_config.quantization\n        if self.compilation_config:\n            vllm_factors.append(self.compilation_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.kv_transfer_config:\n            vllm_factors.append(self.kv_transfer_config.compute_hash())\n        else:\n            vllm_factors.append(\"None\")\n        if self.additional_config:\n            if isinstance(additional_config := self.additional_config, dict):\n                additional_config_hash = hashlib.md5(\n                    json.dumps(additional_config, sort_keys=True).encode(),\n                    usedforsecurity=False,\n                ).hexdigest()\n            else:\n                additional_config_hash = additional_config.compute_hash()\n            vllm_factors.append(additional_config_hash)\n        else:\n            vllm_factors.append(\"None\")\n        factors.append(vllm_factors)\n\n        hash_str = hashlib.md5(str(factors).encode(),\n                               usedforsecurity=False).hexdigest()[:10]\n        return hash_str\n\n    def pad_for_cudagraph(self, batch_size: int) -> int:\n        # if batch_size > self.compilation_config.max_capture_size,\n        # it should raise an IndexError.\n        # the caller should make sure the batch_size is within the range,\n        # i.e., batch_size <= self.compilation_config.max_capture_size\n        return self.compilation_config.bs_to_padded_graph_size[batch_size]\n\n    @staticmethod\n    def _get_quantization_config(\n            model_config: ModelConfig,\n            load_config: LoadConfig) -> Optional[QuantizationConfig]:\n        \"\"\"Get the quantization config.\"\"\"\n        from vllm.platforms import current_platform\n        if model_config.quantization is not None:\n            from vllm.model_executor.model_loader.weight_utils import (\n                get_quant_config)\n            quant_config = get_quant_config(model_config, load_config)\n            capability_tuple = current_platform.get_device_capability()\n\n            if capability_tuple is not None:\n                capability = capability_tuple.to_int()\n                if capability < quant_config.get_min_capability():\n                    raise ValueError(\n                        f\"The quantization method {model_config.quantization} \"\n                        \"is not supported for the current GPU. Minimum \"\n                        f\"capability: {quant_config.get_min_capability()}. \"\n                        f\"Current capability: {capability}.\")\n            supported_dtypes = quant_config.get_supported_act_dtypes()\n            if model_config.dtype not in supported_dtypes:\n                raise ValueError(\n                    f\"{model_config.dtype} is not supported for quantization \"\n                    f\"method {model_config.quantization}. Supported dtypes: \"\n                    f\"{supported_dtypes}\")\n            return quant_config\n        return None\n\n    @staticmethod\n    def get_quantization_config(\n            model_config: ModelConfig,\n            load_config: LoadConfig) -> Optional[QuantizationConfig]:\n        import copy\n\n        # For some reason, the _ version of this modifies the model_config\n        # object, so using deepcopy to avoid this problem.\n        return VllmConfig._get_quantization_config(copy.deepcopy(model_config),\n                                                   load_config)\n\n    def with_hf_config(\n        self,\n        hf_config: PretrainedConfig,\n        architectures: Optional[list[str]] = None,\n    ) -> \"VllmConfig\":\n        if architectures is not None:\n            hf_config = copy.deepcopy(hf_config)\n            hf_config.architectures = architectures\n\n        model_config = copy.deepcopy(self.model_config)\n        model_config.hf_config = hf_config\n\n        return replace(self, model_config=model_config)\n\n    def __post_init__(self):\n        \"\"\"Verify configs are valid & consistent with each other.\n        \"\"\"\n        if self.model_config is not None:\n            self.model_config.verify_async_output_proc(self.parallel_config,\n                                                       self.speculative_config,\n                                                       self.device_config)\n            self.model_config.verify_with_parallel_config(self.parallel_config)\n            self.model_config.verify_dual_chunk_attention_config(\n                self.load_config)\n\n        if self.cache_config is not None:\n            self.cache_config.verify_with_parallel_config(self.parallel_config)\n\n        if self.lora_config:\n            self.lora_config.verify_with_cache_config(self.cache_config)\n            self.lora_config.verify_with_model_config(self.model_config)\n            self.lora_config.verify_lora_support()\n        if self.prompt_adapter_config:\n            self.prompt_adapter_config.verify_with_model_config(\n                self.model_config)\n\n        if self.quant_config is None and \\\n            self.model_config is not None and self.load_config is not None:\n            self.quant_config = VllmConfig._get_quantization_config(\n                self.model_config, self.load_config)\n\n        from vllm.platforms import current_platform\n        if self.scheduler_config is not None and \\\n            self.model_config is not None and \\\n            self.scheduler_config.chunked_prefill_enabled and \\\n            self.model_config.dtype == torch.float32 and \\\n            current_platform.get_device_capability() == (7, 5):\n            logger.warning_once(\n                \"Turing devices tensor cores do not support float32 matmul. \"\n                \"To workaround this limitation, vLLM will set 'ieee' input \"\n                \"precision for chunked prefill triton kernels.\")\n\n        if self.compilation_config is None:\n            self.compilation_config = CompilationConfig()\n        if self.compilation_config.pass_config.enable_sequence_parallelism:\n            self.compilation_config.custom_ops.append(\"+rms_norm\")\n        if envs.VLLM_USE_V1 and self.model_config is not None and \\\n            not self.model_config.enforce_eager:\n            # NOTE(woosuk): Currently, we use inductor because the piecewise\n            # CUDA graphs do not work properly with the custom CUDA kernels.\n            # FIXME(woosuk): Disable inductor to reduce the compilation time\n            # and avoid any potential issues with the inductor.\n            # FIXME(rob): Add function to set all of these.\n            if not self.compilation_config.custom_ops:\n                self.compilation_config.custom_ops = [\"none\"]\n            self.compilation_config.use_cudagraph = True\n            self.compilation_config.use_inductor = True\n            self.compilation_config.cudagraph_num_of_warmups = 1\n            self.compilation_config.pass_config.enable_fusion = False\n            self.compilation_config.pass_config.enable_noop = False\n            self.compilation_config.level = CompilationLevel.PIECEWISE\n            self.compilation_config.set_splitting_ops_for_v1()\n\n        self._set_cudagraph_sizes()\n\n        if self.cache_config is not None and \\\n            self.cache_config.cpu_offload_gb > 0 and \\\n            self.compilation_config.level != CompilationLevel.NO_COMPILATION \\\n                and not envs.VLLM_USE_V1:\n            logger.warning(\n                \"CPU offload is not supported with `torch.compile` in v0 yet.\"\n                \" Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        if ((not envs.VLLM_USE_V1) and self.lora_config is not None\n                and self.compilation_config.level\n                != CompilationLevel.NO_COMPILATION):\n            logger.warning(\n                \"LoRA for V0 is not supported with `torch.compile` yet. \"\n                \"Disabling `torch.compile`.\")\n            self.compilation_config.level = CompilationLevel.NO_COMPILATION\n\n        if self.compilation_config.full_cuda_graph and \\\n            not self.model_config.disable_cascade_attn:\n            logger.warning_once(\n                \"full_cuda_graph is not supported with \"\n                \"cascade attention. Disabling cascade attention.\")\n            self.model_config.disable_cascade_attn = True\n\n        if self.model_config and self.model_config.use_mla and \\\n            not (current_platform.is_cuda() or current_platform.is_rocm()):\n            logger.info(\n                \"MLA is enabled on a non-GPU platform; forcing chunked \"\n                \"prefill and prefix caching to be disabled.\")\n            self.scheduler_config.enable_chunked_prefill = False\n            self.scheduler_config.chunked_prefill_enabled = False\n            self.scheduler_config.max_num_batched_tokens = max(\n                self.scheduler_config.max_model_len,\n                _DEFAULT_MAX_NUM_BATCHED_TOKENS)\n\n            if self.cache_config is not None:\n                self.cache_config.enable_prefix_caching = False\n\n        if (self.kv_events_config\n                and self.kv_events_config.enable_kv_cache_events\n                and not self.cache_config.enable_prefix_caching):\n            logger.warning(\n                \"KV cache events are on, but prefix caching is not enabled.\"\n                \"Use --enable-prefix-caching to enable.\")\n        if (self.kv_events_config and self.kv_events_config.publisher != \"null\"\n                and not self.kv_events_config.enable_kv_cache_events):\n            logger.warning(\"KV cache events are disabled,\"\n                           \"but the scheduler is configured to publish them.\"\n                           \"Modify KVEventsConfig.enable_kv_cache_events\"\n                           \"to True to enable.\")\n        current_platform.check_and_update_config(self)\n\n        if not self.instance_id:\n            self.instance_id = random_uuid()[:5]\n\n    def update_sizes_for_sequence_parallelism(self,\n                                              possible_sizes: list) -> list:\n        # remove the sizes that not multiple of tp_size when\n        # enable sequence parallelism\n        removed_sizes = [\n            size for size in possible_sizes\n            if size % self.parallel_config.tensor_parallel_size != 0\n        ]\n        if removed_sizes:\n            logger.warning(\n                \"Batch sizes %s are removed because they are not \"\n                \"multiple of tp_size %d when \"\n                \"sequence parallelism is enabled\", removed_sizes,\n                self.parallel_config.tensor_parallel_size)\n\n        return [\n            size for size in possible_sizes\n            if size % self.parallel_config.tensor_parallel_size == 0\n        ]\n\n    def _set_cudagraph_sizes(self):\n        \"\"\"\n        cudagraph batchsize padding logic:\n\n        `[1, 2, 4] + [8 * i for i in range(1, 1025)]` is a list of all possible\n        batch sizes that cudagraph will capture.\n\n        Depending on the engine's configuration of `max_num_seqs`, the\n        candidate batch sizes to capture cudagraph will shrink to the subset\n        which just cover the range of `[1, max_num_seqs]`. In the common case,\n        `max_num_seqs` is 256, and the cudagraph batch sizes will be\n        `[1, 2, 4, 8, 16, 24, 32, 40, ..., 256]`.\n\n        However, if users specify the cudagraph capture sizes through\n        compilation config, we will use the specified sizes instead.\n\n        In the end, `vllm_config.compilation_config.cudagraph_capture_sizes`\n        will be the final sizes to capture cudagraph (in descending order).\n\n        During runtime, if batchsize is larger than\n        `vllm_config.compilation_config.cudagraph_capture_sizes`,\n        no cudagraph will be used.\n        If the batch size is no larger than\n        `vllm_config.compilation_config.cudagraph_capture_sizes`,\n        we can quickly find the padded graph size for a given batch size by\n        looking up `vllm_config.compilation_config.bs_to_padded_graph_size`.\n        \"\"\"\n\n        # calculate the default `batch_size_capture_list`\n        if not envs.VLLM_USE_V1:\n            batch_size_capture_list = []\n            max_batchsize_to_capture = 0\n            if self.scheduler_config is not None and \\\n                self.model_config is not None and \\\n                    not self.model_config.enforce_eager:\n\n                possible_sizes = [1, 2, 4] + [8 * i for i in range(1, 1025)]\n                if self.parallel_config.tensor_parallel_size > 1 and \\\n                    self.compilation_config.pass_config.enable_sequence_parallelism:\n                    possible_sizes = self.update_sizes_for_sequence_parallelism(\n                        possible_sizes)\n\n                # find the minimum size that is larger than max_num_seqs,\n                # which then becomes the max_batchsize_to_capture\n                larger_sizes = [\n                    x for x in possible_sizes\n                    if x >= self.scheduler_config.max_num_seqs\n                ]\n                if larger_sizes:\n                    max_batchsize_to_capture = larger_sizes[0]\n                else:\n                    max_batchsize_to_capture = possible_sizes[-1]\n\n                # filter out the sizes that are\n                # larger than max_batchsize_to_capture\n                batch_size_capture_list = [\n                    size for size in possible_sizes\n                    if size <= max_batchsize_to_capture\n                ]\n        else:\n            batch_size_capture_list = []\n            if self.model_config is not None and \\\n                not self.model_config.enforce_eager:\n                cuda_graph_sizes = self.scheduler_config.cuda_graph_sizes\n                if len(cuda_graph_sizes) == 1:\n                    batch_size_capture_list = [1, 2, 4] + [\n                        i for i in range(8, cuda_graph_sizes[0] + 1, 8)\n                    ]\n                elif len(cuda_graph_sizes) > 1:\n                    batch_size_capture_list = sorted(cuda_graph_sizes)\n                else:\n                    raise TypeError(f\"Invalid value for {cuda_graph_sizes=}.\")\n                if self.parallel_config.tensor_parallel_size > 1 and \\\n                    self.compilation_config.pass_config.enable_sequence_parallelism:\n                    batch_size_capture_list = \\\n                        self.update_sizes_for_sequence_parallelism(batch_size_capture_list)\n                max_num_tokens = self.scheduler_config.max_num_batched_tokens\n                batch_size_capture_list = [\n                    size for size in batch_size_capture_list\n                    if size <= max_num_tokens\n                ]\n\n        self.compilation_config.init_with_cudagraph_sizes(\n            batch_size_capture_list)\n\n    def __str__(self):\n        return (\n            f\"model={self.model_config.model!r},\"\n            f\" speculative_config={self.speculative_config!r},\"\n            f\" tokenizer={self.model_config.tokenizer!r}, \"\n            f\"skip_tokenizer_init={self.model_config.skip_tokenizer_init},\"\n            f\" tokenizer_mode={self.model_config.tokenizer_mode}, \"\n            f\"revision={self.model_config.revision}, \"\n            f\"override_neuron_config={self.model_config.override_neuron_config},\"\n            f\" tokenizer_revision={self.model_config.tokenizer_revision}, \"\n            f\"trust_remote_code={self.model_config.trust_remote_code}, \"\n            f\"dtype={self.model_config.dtype}, \"\n            f\"max_seq_len={self.model_config.max_model_len},\"\n            f\" download_dir={self.load_config.download_dir!r}, \"\n            f\"load_format={self.load_config.load_format}, \"\n            f\"tensor_parallel_size={self.parallel_config.tensor_parallel_size},\"\n            f\" pipeline_parallel_size={self.parallel_config.pipeline_parallel_size}, \"  # noqa\n            f\"disable_custom_all_reduce={self.parallel_config.disable_custom_all_reduce}, \"  # noqa\n            f\"quantization={self.model_config.quantization}, \"\n            f\"enforce_eager={self.model_config.enforce_eager}, \"\n            f\"kv_cache_dtype={self.cache_config.cache_dtype}, \"\n            f\" device_config={self.device_config.device}, \"\n            f\"decoding_config={self.decoding_config!r}, \"\n            f\"observability_config={self.observability_config!r}, \"\n            f\"seed={self.model_config.seed}, \"\n            f\"served_model_name={self.model_config.served_model_name}, \"\n            f\"num_scheduler_steps={self.scheduler_config.num_scheduler_steps}, \"\n            f\"multi_step_stream_outputs={self.scheduler_config.multi_step_stream_outputs}, \"  # noqa\n            f\"enable_prefix_caching={self.cache_config.enable_prefix_caching}, \"\n            f\"chunked_prefill_enabled={self.scheduler_config.chunked_prefill_enabled}, \"  # noqa\n            f\"use_async_output_proc={self.model_config.use_async_output_proc}, \"\n            f\"pooler_config={self.model_config.pooler_config!r}, \"\n            f\"compilation_config={self.compilation_config!r}\")\n\n\n_current_vllm_config: Optional[VllmConfig] = None\n\n\n@contextmanager\ndef set_current_vllm_config(vllm_config: VllmConfig, check_compile=False):\n    \"\"\"\n    Temporarily set the current vLLM config.\n    Used during model initialization.\n    We save the current vLLM config in a global variable,\n    so that all modules can access it, e.g. custom ops\n    can access the vLLM config to determine how to dispatch.\n    \"\"\"\n    global _current_vllm_config\n    old_vllm_config = _current_vllm_config\n    from vllm.compilation.counter import compilation_counter\n    num_models_seen = compilation_counter.num_models_seen\n    try:\n        _current_vllm_config = vllm_config\n        yield\n    except Exception:\n        raise\n    else:\n        logger.debug(\"enabled custom ops: %s\",\n                     vllm_config.compilation_config.enabled_custom_ops)\n        logger.debug(\"disabled custom ops: %s\",\n                     vllm_config.compilation_config.disabled_custom_ops)\n        if check_compile and \\\n            vllm_config.compilation_config.level == CompilationLevel.PIECEWISE \\\n            and compilation_counter.num_models_seen == num_models_seen:\n            # If the model supports compilation,\n            # compilation_counter.num_models_seen should be increased\n            # by at least 1.\n            # If it is not increased, it means the model does not support\n            # compilation (does not have @support_torch_compile decorator).\n            logger.warning(\n                \"`torch.compile` is turned on, but the model %s\"\n                \" does not support it. Please open an issue on GitHub\"\n                \" if you want it to be supported.\",\n                vllm_config.model_config.model)\n    finally:\n        _current_vllm_config = old_vllm_config\n\n\ndef get_current_vllm_config() -> VllmConfig:\n    if _current_vllm_config is None:\n        # in ci, usually when we test custom ops/modules directly,\n        # we don't set the vllm config. In that case, we set a default\n        # config.\n        logger.warning(\"Current vLLM config is not set.\")\n        from vllm.config import VllmConfig\n        return VllmConfig()\n    return _current_vllm_config\n\n\ndef contains_object_print(text):\n    \"\"\"\n    Check if the text looks like a printed Python object, e.g.\n    contains any substring matching the pattern: \"at 0xFFFFFFF>\"\n    We match against 0x followed by 2-16 hex chars (there's\n    a max of 16 on a 64 bit system).\n\n    Args:\n        text (str): The text to check\n\n    Returns:\n        bool: True if a match is found, False otherwise\n    \"\"\"\n    pattern = r'at 0x[a-fA-F0-9]{2,16}>'\n    match = re.search(pattern, text)\n    return match is not None\n\n\ndef assert_hashable(text):\n    if not contains_object_print(text):\n        return True\n    raise AssertionError(\n        f\"vLLM tried to hash some configs that may have Python objects ids \"\n        f\"in them. This is a bug, please file an issue. \"\n        f\"Text being hashed: {text}\")\n\n\nT = TypeVar(\"T\")\n\n\ndef get_layers_from_vllm_config(vllm_config: VllmConfig,\n                                layer_type: type[T]) -> dict[str, T]:\n    return {\n        layer_name: layer\n        for layer_name, layer in\n        vllm_config.compilation_config.static_forward_context.items()\n        if isinstance(layer, layer_type)\n    }\n", 4578], "/home/jeromeku/vllm/vllm/utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom __future__ import annotations\n\nimport asyncio\nimport concurrent\nimport contextlib\nimport datetime\nimport enum\nimport gc\nimport getpass\nimport hashlib\nimport importlib\nimport importlib.metadata\nimport importlib.util\nimport inspect\nimport ipaddress\nimport json\nimport multiprocessing\nimport os\nimport pickle\nimport re\nimport signal\nimport socket\nimport subprocess\nimport sys\nimport tempfile\nimport textwrap\nimport threading\nimport time\nimport traceback\nimport types\nimport uuid\nimport warnings\nimport weakref\nfrom argparse import (Action, ArgumentDefaultsHelpFormatter, ArgumentParser,\n                      ArgumentTypeError, _ArgumentGroup)\nfrom asyncio import FIRST_COMPLETED, AbstractEventLoop, Task\nfrom collections import UserDict, defaultdict\nfrom collections.abc import (AsyncGenerator, Awaitable, Generator, Hashable,\n                             Iterable, Iterator, KeysView, Mapping)\nfrom concurrent.futures.process import ProcessPoolExecutor\nfrom dataclasses import dataclass, field\nfrom functools import cache, lru_cache, partial, wraps\nfrom types import MappingProxyType\nfrom typing import (TYPE_CHECKING, Any, Callable, Generic, Literal, NamedTuple,\n                    Optional, Sequence, Tuple, Type, TypeVar, Union, cast,\n                    overload)\nfrom urllib.parse import urlparse\nfrom uuid import uuid4\n\nimport cachetools\nimport cloudpickle\nimport numpy as np\nimport numpy.typing as npt\nimport psutil\nimport torch\nimport torch.types\nimport yaml\nimport zmq\nimport zmq.asyncio\nfrom packaging import version\nfrom packaging.version import Version\nfrom torch.library import Library\nfrom typing_extensions import Never, ParamSpec, TypeIs, assert_never\n\nimport vllm.envs as envs\n# NOTE: import triton_utils to make TritonPlaceholderModule work\n#       if triton is unavailable\nimport vllm.triton_utils  # noqa: F401\nfrom vllm.logger import enable_trace_function_call, init_logger\n\nif TYPE_CHECKING:\n    from argparse import Namespace\n\n    from vllm.config import ModelConfig, VllmConfig\n\nlogger = init_logger(__name__)\n\n# Exception strings for non-implemented encoder/decoder scenarios\n\n# Reminder: Please update docs/source/features/compatibility_matrix.md\n# If the feature combo become valid\n\nSTR_NOT_IMPL_ENC_DEC_SWA = \\\n    \"Sliding window attention for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_PREFIX_CACHE = \\\n    \"Prefix caching for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL = \\\n    \"Chunked prefill for encoder/decoder models \" + \\\n                    \"is not currently supported.\"\n\nSTR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP = (\n    \"Models with logits_soft_cap \"\n    \"require FlashInfer backend, which is \"\n    \"currently not supported for encoder/decoder \"\n    \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_LORA = (\"LoRA is currently not currently \"\n                             \"supported with encoder/decoder \"\n                             \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PP = (\"Pipeline parallelism is not \"\n                           \"currently supported with \"\n                           \"encoder/decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_MM = (\"Multimodal is not currently \"\n                           \"supported with encoder/decoder \"\n                           \"models.\")\n\nSTR_NOT_IMPL_ENC_DEC_SPEC_DEC = (\"Speculative decoding is not \"\n                                 \"currently supported with encoder/\"\n                                 \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_BACKEND = (\"XFormers and Flash-Attention are the only \"\n                                \"backends currently supported with encoder/\"\n                                \"decoder models.\")\n\nSTR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER = (\"Prompt adapters are not \"\n                                       \"currently supported with encoder/\"\n                                       \"decoder models.\")\n\n# Efficiently import all enc/dec error strings\n# rather than having to import all of the above\nSTR_NOT_IMPL_ENC_DEC_ERR_STRS = {\n    \"STR_NOT_IMPL_ENC_DEC_SWA\": STR_NOT_IMPL_ENC_DEC_SWA,\n    \"STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE\": STR_NOT_IMPL_ENC_DEC_PREFIX_CACHE,\n    \"STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL\":\n    STR_NOT_IMPL_ENC_DEC_CHUNKED_PREFILL,\n    \"STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP\": STR_NOT_IMPL_ENC_DEC_LOGIT_SOFTCAP,\n    \"STR_NOT_IMPL_ENC_DEC_LORA\": STR_NOT_IMPL_ENC_DEC_LORA,\n    \"STR_NOT_IMPL_ENC_DEC_PP\": STR_NOT_IMPL_ENC_DEC_PP,\n    \"STR_NOT_IMPL_ENC_DEC_MM\": STR_NOT_IMPL_ENC_DEC_MM,\n    \"STR_NOT_IMPL_ENC_DEC_SPEC_DEC\": STR_NOT_IMPL_ENC_DEC_SPEC_DEC,\n    \"STR_NOT_IMPL_ENC_DEC_BACKEND\": STR_NOT_IMPL_ENC_DEC_BACKEND,\n    \"STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER\": STR_NOT_IMPL_ENC_DEC_PROMPT_ADAPTER,\n}\n\n# Constants related to forcing the attention backend selection\n\n# String name of register which may be set in order to\n# force auto-selection of attention backend by Attention\n# wrapper\nSTR_BACKEND_ENV_VAR: str = \"VLLM_ATTENTION_BACKEND\"\n\n# Possible string values of STR_BACKEND_ENV_VAR\n# register, corresponding to possible backends\nSTR_FLASHINFER_ATTN_VAL: str = \"FLASHINFER\"\nSTR_TORCH_SDPA_ATTN_VAL: str = \"TORCH_SDPA\"\nSTR_ROCM_FLASH_ATTN_VAL: str = \"ROCM_FLASH\"\nSTR_XFORMERS_ATTN_VAL: str = \"XFORMERS\"\nSTR_FLASH_ATTN_VAL: str = \"FLASH_ATTN\"\nSTR_DUAL_CHUNK_FLASH_ATTN_VAL: str = \"DUAL_CHUNK_FLASH_ATTN\"\nSTR_INVALID_VAL: str = \"INVALID\"\n\nGB_bytes = 1_000_000_000\n\"\"\"The number of bytes in one gigabyte (GB).\"\"\"\n\nGiB_bytes = 1 << 30\n\"\"\"The number of bytes in one gibibyte (GiB).\"\"\"\n\nSTR_DTYPE_TO_TORCH_DTYPE = {\n    \"half\": torch.half,\n    \"bfloat16\": torch.bfloat16,\n    \"float\": torch.float,\n    \"fp8\": torch.uint8,\n    \"fp8_e4m3\": torch.uint8,\n    \"fp8_e5m2\": torch.uint8,\n    \"int8\": torch.int8,\n}\n\nTORCH_DTYPE_TO_NUMPY_DTYPE = {\n    torch.float16: np.float16,\n    torch.float32: np.float32,\n    torch.float64: np.float64,\n    torch.uint8: np.uint8,\n    torch.int32: np.int32,\n    torch.int64: np.int64,\n}\n\nP = ParamSpec('P')\nT = TypeVar(\"T\")\nU = TypeVar(\"U\")\n\n_K = TypeVar(\"_K\", bound=Hashable)\n_V = TypeVar(\"_V\")\n_T = TypeVar(\"_T\")\n\n\nclass _Sentinel:\n    ...\n\n\nALL_PINNED_SENTINEL = _Sentinel()\n\n\nclass Device(enum.Enum):\n    GPU = enum.auto()\n    CPU = enum.auto()\n\n\nclass LayerBlockType(enum.Enum):\n    attention = \"attention\"\n    mamba = \"mamba\"\n\n\nclass Counter:\n\n    def __init__(self, start: int = 0) -> None:\n        self.counter = start\n\n    def __next__(self) -> int:\n        i = self.counter\n        self.counter += 1\n        return i\n\n    def reset(self) -> None:\n        self.counter = 0\n\n\nclass _MappingOrderCacheView(UserDict[_K, _V]):\n\n    def __init__(self, data: Mapping[_K, _V], ordered_keys: Mapping[_K, None]):\n        super().__init__(data)\n        self.ordered_keys = ordered_keys\n\n    def __iter__(self) -> Iterator[_K]:\n        return iter(self.ordered_keys)\n\n    def keys(self) -> KeysView[_K]:\n        return KeysView(self.ordered_keys)\n\n\nclass CacheInfo(NamedTuple):\n    hits: int\n    total: int\n\n    @property\n    def hit_ratio(self) -> float:\n        if self.total == 0:\n            return 0\n\n        return self.hits / self.total\n\n    def __sub__(self, other: CacheInfo):\n        return CacheInfo(\n            hits=self.hits - other.hits,\n            total=self.total - other.total,\n        )\n\n\nclass LRUCache(cachetools.LRUCache[_K, _V], Generic[_K, _V]):\n\n    def __init__(self,\n                 capacity: float,\n                 getsizeof: Optional[Callable[[_V], float]] = None):\n        super().__init__(capacity, getsizeof)\n\n        self.pinned_items = set[_K]()\n\n        self._hits = 0\n        self._total = 0\n        self._last_info = CacheInfo(hits=0, total=0)\n\n    def __getitem__(self, key: _K, *, update_info: bool = True) -> _V:\n        value = super().__getitem__(key)\n\n        if update_info:\n            self._hits += 1\n            self._total += 1\n\n        return value\n\n    def __delitem__(self, key: _K) -> None:\n        run_on_remove = key in self\n        value = self.__getitem__(key,\n                                 update_info=False)  # type: ignore[call-arg]\n        super().__delitem__(key)\n        if key in self.pinned_items:\n            # Todo: add warning to inform that del pinned item\n            self._unpin(key)\n        if run_on_remove:\n            self._on_remove(key, value)\n\n    @property\n    def cache(self) -> Mapping[_K, _V]:\n        \"\"\"Return the internal cache dictionary in order (read-only).\"\"\"\n        return _MappingOrderCacheView(\n            self._Cache__data,  # type: ignore\n            self.order)\n\n    @property\n    def order(self) -> Mapping[_K, None]:\n        \"\"\"Return the internal order dictionary (read-only).\"\"\"\n        return MappingProxyType(self._LRUCache__order)  # type: ignore\n\n    @property\n    def capacity(self) -> float:\n        return self.maxsize\n\n    @property\n    def usage(self) -> float:\n        if self.maxsize == 0:\n            return 0\n\n        return self.currsize / self.maxsize\n\n    def stat(self, *, delta: bool = False) -> CacheInfo:\n        \"\"\"\n        Gets the cumulative number of hits and queries against this cache.\n\n        If `delta=True`, instead gets these statistics\n        since the last call that also passed `delta=True`.\n        \"\"\"\n        info = CacheInfo(hits=self._hits, total=self._total)\n\n        if delta:\n            info_delta = info - self._last_info\n            self._last_info = info\n            info = info_delta\n\n        return info\n\n    def touch(self, key: _K) -> None:\n        try:\n            self._LRUCache__order.move_to_end(key)  # type: ignore\n        except KeyError:\n            self._LRUCache__order[key] = None  # type: ignore\n\n    @overload\n    def get(self, key: _K, /) -> Optional[_V]:\n        ...\n\n    @overload\n    def get(self, key: _K, /, default: Union[_V, _T]) -> Union[_V, _T]:\n        ...\n\n    def get(self,\n            key: _K,\n            /,\n            default: Optional[Union[_V,\n                                    _T]] = None) -> Optional[Union[_V, _T]]:\n        value: Optional[Union[_V, _T]]\n        if key in self:\n            value = self.__getitem__(\n                key, update_info=False)  # type: ignore[call-arg]\n\n            self._hits += 1\n        else:\n            value = default\n\n        self._total += 1\n        return value\n\n    @overload\n    def pop(self, key: _K) -> _V:\n        ...\n\n    @overload\n    def pop(self, key: _K, default: Union[_V, _T]) -> Union[_V, _T]:\n        ...\n\n    def pop(self,\n            key: _K,\n            default: Optional[Union[_V,\n                                    _T]] = None) -> Optional[Union[_V, _T]]:\n        value: Optional[Union[_V, _T]]\n        if key not in self:\n            return default\n\n        value = self.__getitem__(key,\n                                 update_info=False)  # type: ignore[call-arg]\n        self.__delitem__(key)\n        return value\n\n    def put(self, key: _K, value: _V) -> None:\n        self.__setitem__(key, value)\n\n    def pin(self, key: _K) -> None:\n        \"\"\"\n        Pins a key in the cache preventing it from being\n        evicted in the LRU order.\n        \"\"\"\n        if key not in self:\n            raise ValueError(f\"Cannot pin key: {key} not in cache.\")\n        self.pinned_items.add(key)\n\n    def _unpin(self, key: _K) -> None:\n        \"\"\"\n        Unpins a key in the cache allowing it to be\n        evicted in the LRU order.\n        \"\"\"\n        self.pinned_items.remove(key)\n\n    def _on_remove(self, key: _K, value: Optional[_V]) -> None:\n        pass\n\n    def remove_oldest(self, *, remove_pinned: bool = False) -> None:\n        if len(self) == 0:\n            return\n\n        self.popitem(remove_pinned=remove_pinned)\n\n    def _remove_old_if_needed(self) -> None:\n        while self.currsize > self.capacity:\n            self.remove_oldest()\n\n    def popitem(self, remove_pinned: bool = False):\n        \"\"\"Remove and return the `(key, value)` pair least recently used.\"\"\"\n        if not remove_pinned:\n            # pop the oldest item in the cache that is not pinned\n            lru_key = next(\n                (key for key in self.order if key not in self.pinned_items),\n                ALL_PINNED_SENTINEL)\n            if lru_key is ALL_PINNED_SENTINEL:\n                raise RuntimeError(\"All items are pinned, \"\n                                   \"cannot remove oldest from the cache.\")\n        else:\n            lru_key = next(iter(self.order))\n        value = self.pop(cast(_K, lru_key))\n        return (lru_key, value)\n\n    def clear(self) -> None:\n        while len(self) > 0:\n            self.remove_oldest(remove_pinned=True)\n\n        self._hits = 0\n        self._total = 0\n        self._last_info = CacheInfo(hits=0, total=0)\n\n\nclass PyObjectCache:\n    \"\"\"Used to cache python objects to avoid object allocations\n    across scheduler iterations.\n    \"\"\"\n\n    def __init__(self, obj_builder):\n        self._obj_builder = obj_builder\n        self._index = 0\n\n        self._obj_cache = []\n        for _ in range(128):\n            self._obj_cache.append(self._obj_builder())\n\n    def _grow_cache(self):\n        # Double the size of the cache\n        num_objs = len(self._obj_cache)\n        for _ in range(num_objs):\n            self._obj_cache.append(self._obj_builder())\n\n    def get_object(self):\n        \"\"\"Returns a pre-allocated cached object. If there is not enough\n        objects, then the cache size will double.\n        \"\"\"\n        if self._index >= len(self._obj_cache):\n            self._grow_cache()\n            assert self._index < len(self._obj_cache)\n\n        obj = self._obj_cache[self._index]\n        self._index += 1\n\n        return obj\n\n    def reset(self):\n        \"\"\"Makes all cached-objects available for the next scheduler iteration.\n        \"\"\"\n        self._index = 0\n\n\n@cache\ndef get_max_shared_memory_bytes(gpu: int = 0) -> int:\n    \"\"\"Returns the maximum shared memory per thread block in bytes.\"\"\"\n    from vllm import _custom_ops as ops\n    max_shared_mem = (\n        ops.get_max_shared_memory_per_block_device_attribute(gpu))\n    # value 0 will cause MAX_SEQ_LEN become negative and test_attention.py\n    # will fail\n    assert max_shared_mem > 0, \"max_shared_mem can not be zero\"\n    return int(max_shared_mem)\n\n\ndef get_cpu_memory() -> int:\n    \"\"\"Returns the total CPU memory of the node in bytes.\"\"\"\n    return psutil.virtual_memory().total\n\n\ndef random_uuid() -> str:\n    return str(uuid.uuid4().hex)\n\n\ndef make_async(\n    func: Callable[P, T],\n    executor: Optional[concurrent.futures.Executor] = None\n) -> Callable[P, Awaitable[T]]:\n    \"\"\"Take a blocking function, and run it on in an executor thread.\n\n    This function prevents the blocking function from blocking the\n    asyncio event loop.\n    The code in this function needs to be thread safe.\n    \"\"\"\n\n    def _async_wrapper(*args: P.args, **kwargs: P.kwargs) -> asyncio.Future:\n        loop = asyncio.get_event_loop()\n        p_func = partial(func, *args, **kwargs)\n        return loop.run_in_executor(executor=executor, func=p_func)\n\n    return _async_wrapper\n\n\ndef _next_task(iterator: AsyncGenerator[T, None],\n               loop: AbstractEventLoop) -> Task:\n    # Can use anext() in python >= 3.10\n    return loop.create_task(iterator.__anext__())  # type: ignore[arg-type]\n\n\nasync def merge_async_iterators(\n    *iterators: AsyncGenerator[T,\n                               None], ) -> AsyncGenerator[tuple[int, T], None]:\n    \"\"\"Merge multiple asynchronous iterators into a single iterator.\n\n    This method handle the case where some iterators finish before others.\n    When it yields, it yields a tuple (i, item) where i is the index of the\n    iterator that yields the item.\n    \"\"\"\n    if len(iterators) == 1:\n        # Fast-path single iterator case.\n        async for item in iterators[0]:\n            yield 0, item\n        return\n\n    loop = asyncio.get_running_loop()\n\n    awaits = {_next_task(pair[1], loop): pair for pair in enumerate(iterators)}\n    try:\n        while awaits:\n            done, _ = await asyncio.wait(awaits.keys(),\n                                         return_when=FIRST_COMPLETED)\n            for d in done:\n                pair = awaits.pop(d)\n                try:\n                    item = await d\n                    i, it = pair\n                    awaits[_next_task(it, loop)] = pair\n                    yield i, item\n                except StopAsyncIteration:\n                    pass\n    finally:\n        # Cancel any remaining iterators\n        for f, (_, it) in awaits.items():\n            with contextlib.suppress(BaseException):\n                f.cancel()\n                await it.aclose()\n\n\nasync def collect_from_async_generator(\n        iterator: AsyncGenerator[T, None]) -> list[T]:\n    \"\"\"Collect all items from an async generator into a list.\"\"\"\n    items = []\n    async for item in iterator:\n        items.append(item)\n    return items\n\n\ndef get_ip() -> str:\n    host_ip = envs.VLLM_HOST_IP\n    if \"HOST_IP\" in os.environ and \"VLLM_HOST_IP\" not in os.environ:\n        logger.warning(\n            \"The environment variable HOST_IP is deprecated and ignored, as\"\n            \" it is often used by Docker and other software to\"\n            \" interact with the container's network stack. Please \"\n            \"use VLLM_HOST_IP instead to set the IP address for vLLM processes\"\n            \" to communicate with each other.\")\n    if host_ip:\n        return host_ip\n\n    # IP is not set, try to get it from the network interface\n\n    # try ipv4\n    s = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    try:\n        s.connect((\"8.8.8.8\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    # try ipv6\n    try:\n        s = socket.socket(socket.AF_INET6, socket.SOCK_DGRAM)\n        # Google's public DNS server, see\n        # https://developers.google.com/speed/public-dns/docs/using#addresses\n        s.connect((\"2001:4860:4860::8888\", 80))  # Doesn't need to be reachable\n        return s.getsockname()[0]\n    except Exception:\n        pass\n\n    warnings.warn(\n        \"Failed to get the IP address, using 0.0.0.0 by default.\"\n        \"The value can be set by the environment variable\"\n        \" VLLM_HOST_IP or HOST_IP.\",\n        stacklevel=2)\n    return \"0.0.0.0\"\n\n\ndef is_valid_ipv6_address(address: str) -> bool:\n    try:\n        ipaddress.IPv6Address(address)\n        return True\n    except ValueError:\n        return False\n\n\ndef get_distributed_init_method(ip: str, port: int) -> str:\n    return get_tcp_uri(ip, port)\n\n\ndef get_tcp_uri(ip: str, port: int) -> str:\n    # Brackets are not permitted in ipv4 addresses,\n    # see https://github.com/python/cpython/issues/103848\n    return f\"tcp://[{ip}]:{port}\" if \":\" in ip else f\"tcp://{ip}:{port}\"\n\n\ndef get_open_zmq_ipc_path() -> str:\n    base_rpc_path = envs.VLLM_RPC_BASE_PATH\n    return f\"ipc://{base_rpc_path}/{uuid4()}\"\n\n\ndef get_open_zmq_inproc_path() -> str:\n    return f\"inproc://{uuid4()}\"\n\n\ndef get_open_port() -> int:\n    \"\"\"\n    Get an open port for the vLLM process to listen on.\n    An edge case to handle, is when we run data parallel,\n    we need to avoid ports that are potentially used by\n    the data parallel master process.\n    Right now we reserve 10 ports for the data parallel master\n    process. Currently it uses 2 ports.\n    \"\"\"\n    if \"VLLM_DP_MASTER_PORT\" in os.environ:\n        dp_master_port = envs.VLLM_DP_MASTER_PORT\n        reserved_port_range = range(dp_master_port, dp_master_port + 10)\n        while True:\n            candidate_port = _get_open_port()\n            if candidate_port not in reserved_port_range:\n                return candidate_port\n    return _get_open_port()\n\n\ndef _get_open_port() -> int:\n    port = envs.VLLM_PORT\n    if port is not None:\n        while True:\n            try:\n                with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                    s.bind((\"\", port))\n                    return port\n            except OSError:\n                port += 1  # Increment port number if already in use\n                logger.info(\"Port %d is already in use, trying port %d\",\n                            port - 1, port)\n    # try ipv4\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n    except OSError:\n        # try ipv6\n        with socket.socket(socket.AF_INET6, socket.SOCK_STREAM) as s:\n            s.bind((\"\", 0))\n            return s.getsockname()[1]\n\n\ndef find_process_using_port(port: int) -> Optional[psutil.Process]:\n    # TODO: We can not check for running processes with network\n    # port on macOS. Therefore, we can not have a full graceful shutdown\n    # of vLLM. For now, let's not look for processes in this case.\n    # Ref: https://www.florianreinhard.de/accessdenied-in-psutil/\n    if sys.platform.startswith(\"darwin\"):\n        return None\n\n    for conn in psutil.net_connections():\n        if conn.laddr.port == port:\n            try:\n                return psutil.Process(conn.pid)\n            except psutil.NoSuchProcess:\n                return None\n    return None\n\n\ndef update_environment_variables(envs: dict[str, str]):\n    for k, v in envs.items():\n        if k in os.environ and os.environ[k] != v:\n            logger.warning(\n                \"Overwriting environment variable %s \"\n                \"from '%s' to '%s'\", k, os.environ[k], v)\n        os.environ[k] = v\n\n\ndef chunk_list(lst: list[T], chunk_size: int):\n    \"\"\"Yield successive chunk_size chunks from lst.\"\"\"\n    for i in range(0, len(lst), chunk_size):\n        yield lst[i:i + chunk_size]\n\n\ndef cdiv(a: int, b: int) -> int:\n    \"\"\"Ceiling division.\"\"\"\n    return -(a // -b)\n\n\ndef next_power_of_2(n) -> int:\n    \"\"\"The next power of 2 (inclusive)\"\"\"\n    if n < 1:\n        return 1\n    return 1 << (n - 1).bit_length()\n\n\ndef round_up(x: int, y: int) -> int:\n    return ((x + y - 1) // y) * y\n\n\ndef round_down(x: int, y: int) -> int:\n    return (x // y) * y\n\n\ndef _generate_random_fp8(\n    tensor: torch.Tensor,\n    low: float,\n    high: float,\n) -> None:\n    # NOTE(zhaoyang): Due to NaN and Inf representation for fp8 data type,\n    # it may occur Inf or NaN if we directly use torch.randint\n    # to generate random data for fp8 data.\n    # For example, s.11111.00 in fp8e5m2 format represents Inf.\n    #     | E4M3        | E5M2\n    #-----|-------------|-------------------\n    # Inf | N/A         | s.11111.00\n    # NaN | s.1111.111  | s.11111.{01,10,11}\n    from vllm import _custom_ops as ops\n    tensor_tmp = torch.empty_like(tensor, dtype=torch.float16)\n    tensor_tmp.uniform_(low, high)\n    ops.convert_fp8(tensor, tensor_tmp)\n    del tensor_tmp\n\n\ndef get_kv_cache_torch_dtype(\n        cache_dtype: Optional[Union[str, torch.dtype]],\n        model_dtype: Optional[Union[str, torch.dtype]] = None) -> torch.dtype:\n    if isinstance(cache_dtype, str):\n        if cache_dtype == \"auto\":\n            if isinstance(model_dtype, str):\n                torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[model_dtype]\n            elif isinstance(model_dtype, torch.dtype):\n                torch_dtype = model_dtype\n            else:\n                raise ValueError(f\"Invalid model dtype: {model_dtype}\")\n        elif cache_dtype in [\"half\", \"bfloat16\", \"float\"]:\n            torch_dtype = STR_DTYPE_TO_TORCH_DTYPE[cache_dtype]\n        elif cache_dtype == \"fp8\":\n            torch_dtype = torch.uint8\n        else:\n            raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    elif isinstance(cache_dtype, torch.dtype):\n        torch_dtype = cache_dtype\n    else:\n        raise ValueError(f\"Invalid kv cache dtype: {cache_dtype}\")\n    return torch_dtype\n\n\ndef create_kv_caches_with_random_flash(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: Optional[int] = None,\n    device: Optional[str] = \"cuda\",\n    cache_layout: Optional[str] = \"NHD\",\n) -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n    from vllm.platforms import current_platform\n    current_platform.seed_everything(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n    generic_kv_cache_shape = (num_blocks, 2, block_size, num_heads, head_size)\n    assert cache_layout in (\"NHD\", \"HND\")\n    stride_order = (0, 1, 2, 3, 4) if cache_layout == \"NHD\" else (0, 1, 3, 2,\n                                                                  4)\n\n    kv_cache_allocation_shape = tuple(generic_kv_cache_shape[i]\n                                      for i in stride_order)\n    scale = head_size**-0.5\n\n    key_caches: list[torch.Tensor] = []\n    value_caches: list[torch.Tensor] = []\n\n    for _ in range(num_layers):\n        key_value_cache = torch.empty(size=kv_cache_allocation_shape,\n                                      dtype=torch_dtype,\n                                      device=device).permute(*stride_order)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_value_cache[:, 0])\n        value_caches.append(key_value_cache[:, 1])\n    return key_caches, value_caches\n\n\ndef create_kv_caches_with_random(\n    num_blocks: int,\n    block_size: int,\n    num_layers: int,\n    num_heads: int,\n    head_size: int,\n    cache_dtype: Optional[Union[str, torch.dtype]],\n    model_dtype: Optional[Union[str, torch.dtype]] = None,\n    seed: Optional[int] = None,\n    device: Optional[str] = \"cuda\",\n) -> tuple[list[torch.Tensor], list[torch.Tensor]]:\n\n    if cache_dtype == \"fp8\" and head_size % 16:\n        raise ValueError(\n            f\"Does not support key cache of type fp8 with head_size {head_size}\"\n        )\n    from vllm.platforms import current_platform\n    current_platform.seed_everything(seed)\n\n    torch_dtype = get_kv_cache_torch_dtype(cache_dtype, model_dtype)\n\n    scale = head_size**-0.5\n    x = 16 // torch.tensor([], dtype=torch_dtype).element_size()\n    key_cache_shape = (num_blocks, num_heads, head_size // x, block_size, x)\n    key_caches: list[torch.Tensor] = []\n    for _ in range(num_layers):\n        key_cache = torch.empty(size=key_cache_shape,\n                                dtype=torch_dtype,\n                                device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            key_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(key_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support key cache of type {cache_dtype}\")\n        key_caches.append(key_cache)\n\n    value_cache_shape = (num_blocks, num_heads, head_size, block_size)\n    value_caches: list[torch.Tensor] = []\n    for _ in range(num_layers):\n        value_cache = torch.empty(size=value_cache_shape,\n                                  dtype=torch_dtype,\n                                  device=device)\n        if cache_dtype in [\"auto\", \"half\", \"bfloat16\", \"float\"]:\n            value_cache.uniform_(-scale, scale)\n        elif cache_dtype == 'fp8':\n            _generate_random_fp8(value_cache, -scale, scale)\n        else:\n            raise ValueError(\n                f\"Does not support value cache of type {cache_dtype}\")\n        value_caches.append(value_cache)\n    return key_caches, value_caches\n\n\n@cache\ndef is_pin_memory_available() -> bool:\n    from vllm.platforms import current_platform\n    return current_platform.is_pin_memory_available()\n\n\n@cache\ndef is_uva_available() -> bool:\n    \"\"\"Check if Unified Virtual Addressing (UVA) is available.\"\"\"\n    # UVA requires pinned memory.\n    # TODO: Add more requirements for UVA if needed.\n    return is_pin_memory_available()\n\n\nclass DeviceMemoryProfiler:\n\n    def __init__(self, device: Optional[torch.types.Device] = None):\n        self.device = device\n\n    def current_memory_usage(self) -> float:\n        # Return the memory usage in bytes.\n        from vllm.platforms import current_platform\n        return current_platform.get_current_memory_usage(self.device)\n\n    def __enter__(self):\n        self.initial_memory = self.current_memory_usage()\n        # This allows us to call methods of the context manager if needed\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.final_memory = self.current_memory_usage()\n        self.consumed_memory = self.final_memory - self.initial_memory\n\n        # Force garbage collection\n        gc.collect()\n\n\ndef make_ndarray_with_pad(\n    x: list[list[T]],\n    pad: T,\n    dtype: npt.DTypeLike,\n    *,\n    max_len: Optional[int] = None,\n) -> npt.NDArray:\n    \"\"\"\n    Make a padded array from 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    if max_len is None:\n        # Unlike for most functions, map is faster than a genexpr over `len`\n        max_len = max(map(len, x), default=0)\n\n    padded_x = np.full((len(x), max_len), pad, dtype=dtype)\n    for ind, blocktb in enumerate(x):\n        assert len(blocktb) <= max_len\n        padded_x[ind, :len(blocktb)] = blocktb\n\n    return padded_x\n\n\ndef make_tensor_with_pad(\n    x: list[list[T]],\n    pad: T,\n    dtype: torch.dtype,\n    *,\n    max_len: Optional[int] = None,\n    device: Optional[Union[str, torch.device]] = None,\n    pin_memory: bool = False,\n) -> torch.Tensor:\n    \"\"\"\n    Make a padded tensor from 2D inputs.\n\n    The padding is applied to the end of each inner list until it reaches\n    `max_len`.\n    \"\"\"\n    np_dtype = TORCH_DTYPE_TO_NUMPY_DTYPE[dtype]\n    padded_x = make_ndarray_with_pad(x, pad, np_dtype, max_len=max_len)\n\n    tensor = torch.from_numpy(padded_x).to(device)\n    if pin_memory:\n        tensor = tensor.pin_memory()\n\n    return tensor\n\n\ndef async_tensor_h2d(\n    data: list,\n    dtype: torch.dtype,\n    target_device: Union[str, torch.device],\n    pin_memory: bool,\n) -> torch.Tensor:\n    \"\"\"Asynchronously create a tensor and copy it from host to device.\"\"\"\n    t = torch.tensor(data, dtype=dtype, pin_memory=pin_memory, device=\"cpu\")\n    return t.to(device=target_device, non_blocking=True)\n\n\ndef get_dtype_size(dtype: torch.dtype) -> int:\n    \"\"\"Get the size of the data type in bytes.\"\"\"\n    return torch.tensor([], dtype=dtype).element_size()\n\n\n# `collections` helpers\ndef is_list_of(\n    value: object,\n    typ: Union[type[T], tuple[type[T], ...]],\n    *,\n    check: Literal[\"first\", \"all\"] = \"first\",\n) -> TypeIs[list[T]]:\n    if not isinstance(value, list):\n        return False\n\n    if check == \"first\":\n        return len(value) == 0 or isinstance(value[0], typ)\n    elif check == \"all\":\n        return all(isinstance(v, typ) for v in value)\n\n    assert_never(check)\n\n\ndef flatten_2d_lists(lists: Iterable[Iterable[T]]) -> list[T]:\n    \"\"\"Flatten a list of lists to a single list.\"\"\"\n    return [item for sublist in lists for item in sublist]\n\n\ndef full_groupby(values: Iterable[_V], *, key: Callable[[_V], _K]):\n    \"\"\"\n    Unlike {class}`itertools.groupby`, groups are not broken by\n    non-contiguous data.\n    \"\"\"\n    groups = defaultdict[_K, list[_V]](list)\n\n    for value in values:\n        groups[key(value)].append(value)\n\n    return groups.items()\n\n\n# TODO: This function can be removed if transformer_modules classes are\n# serialized by value when communicating between processes\ndef init_cached_hf_modules() -> None:\n    \"\"\"\n    Lazy initialization of the Hugging Face modules.\n    \"\"\"\n    from transformers.dynamic_module_utils import init_hf_modules\n    init_hf_modules()\n\n\n@cache\ndef find_library(lib_name: str) -> str:\n    \"\"\"\n    Find the library file in the system.\n    `lib_name` is full filename, with both prefix and suffix.\n    This function resolves `lib_name` to the full path of the library.\n    \"\"\"\n    # Adapted from https://github.com/openai/triton/blob/main/third_party/nvidia/backend/driver.py#L19 # noqa\n    # According to https://en.wikipedia.org/wiki/Filesystem_Hierarchy_Standard\n    # `/sbin/ldconfig` should exist in all Linux systems.\n    # `/sbin/ldconfig` searches the library in the system\n    libs = subprocess.check_output([\"/sbin/ldconfig\", \"-p\"]).decode()\n    # each line looks like the following:\n    # libcuda.so.1 (libc6,x86-64) => /lib/x86_64-linux-gnu/libcuda.so.1\n    locs = [line.split()[-1] for line in libs.splitlines() if lib_name in line]\n    # `LD_LIBRARY_PATH` searches the library in the user-defined paths\n    env_ld_library_path = envs.LD_LIBRARY_PATH\n    if not locs and env_ld_library_path:\n        locs = [\n            os.path.join(dir, lib_name)\n            for dir in env_ld_library_path.split(\":\")\n            if os.path.exists(os.path.join(dir, lib_name))\n        ]\n    if not locs:\n        raise ValueError(f\"Cannot find {lib_name} in the system.\")\n    return locs[0]\n\n\ndef find_nccl_library() -> str:\n    \"\"\"\n    We either use the library file specified by the `VLLM_NCCL_SO_PATH`\n    environment variable, or we find the library file brought by PyTorch.\n    After importing `torch`, `libnccl.so.2` or `librccl.so.1` can be\n    found by `ctypes` automatically.\n    \"\"\"\n    so_file = envs.VLLM_NCCL_SO_PATH\n\n    # manually load the nccl library\n    if so_file:\n        logger.info(\n            \"Found nccl from environment variable VLLM_NCCL_SO_PATH=%s\",\n            so_file)\n    else:\n        if torch.version.cuda is not None:\n            so_file = \"libnccl.so.2\"\n        elif torch.version.hip is not None:\n            so_file = \"librccl.so.1\"\n        else:\n            raise ValueError(\"NCCL only supports CUDA and ROCm backends.\")\n        logger.info(\"Found nccl from library %s\", so_file)\n    return so_file\n\n\nprev_set_stream = torch.cuda.set_stream\n\n_current_stream = None\n\n\ndef _patched_set_stream(stream: torch.cuda.Stream) -> None:\n    global _current_stream\n    _current_stream = stream\n    prev_set_stream(stream)\n\n\ntorch.cuda.set_stream = _patched_set_stream\n\n\ndef current_stream() -> torch.cuda.Stream:\n    \"\"\"\n    replace `torch.cuda.current_stream()` with `vllm.utils.current_stream()`.\n    it turns out that `torch.cuda.current_stream()` is quite expensive,\n    as it will construct a new stream object at each call.\n    here we patch `torch.cuda.set_stream` to keep track of the current stream\n    directly, so that we can avoid calling `torch.cuda.current_stream()`.\n\n    the underlying hypothesis is that we do not call `torch._C._cuda_setStream`\n    from C/C++ code.\n    \"\"\"\n    from vllm.platforms import current_platform\n    global _current_stream\n    if _current_stream is None:\n        # when this function is called before any stream is set,\n        # we return the default stream.\n        # On ROCm using the default 0 stream in combination with RCCL\n        # is hurting performance. Therefore creating a dedicated stream\n        # per process\n        _current_stream = torch.cuda.Stream() if current_platform.is_rocm(\n        ) else torch.cuda.current_stream()\n    return _current_stream\n\n\ndef enable_trace_function_call_for_thread(vllm_config: VllmConfig) -> None:\n    \"\"\"Set up function tracing for the current thread,\n    if enabled via the VLLM_TRACE_FUNCTION environment variable\n    \"\"\"\n\n    if envs.VLLM_TRACE_FUNCTION:\n        tmp_dir = tempfile.gettempdir()\n        # add username to tmp_dir to avoid permission issues\n        tmp_dir = os.path.join(tmp_dir, getpass.getuser())\n        filename = (f\"VLLM_TRACE_FUNCTION_for_process_{os.getpid()}\"\n                    f\"_thread_{threading.get_ident()}_\"\n                    f\"at_{datetime.datetime.now()}.log\").replace(\" \", \"_\")\n        log_path = os.path.join(tmp_dir, \"vllm\",\n                                f\"vllm-instance-{vllm_config.instance_id}\",\n                                filename)\n        os.makedirs(os.path.dirname(log_path), exist_ok=True)\n        enable_trace_function_call(log_path)\n\n\n# `functools` helpers\ndef identity(value: T, **kwargs) -> T:\n    \"\"\"Returns the first provided value.\"\"\"\n    return value\n\n\nF = TypeVar('F', bound=Callable[..., Any])\n\n\ndef deprecate_args(\n    start_index: int,\n    is_deprecated: Union[bool, Callable[[], bool]] = True,\n    additional_message: Optional[str] = None,\n) -> Callable[[F], F]:\n\n    if not callable(is_deprecated):\n        is_deprecated = partial(identity, is_deprecated)\n\n    def wrapper(fn: F) -> F:\n\n        params = inspect.signature(fn).parameters\n        pos_types = (\n            inspect.Parameter.POSITIONAL_ONLY,\n            inspect.Parameter.POSITIONAL_OR_KEYWORD,\n        )\n        pos_kws = [\n            kw for kw, param in params.items() if param.kind in pos_types\n        ]\n\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            if is_deprecated():\n                deprecated_args = pos_kws[start_index:len(args)]\n                if deprecated_args:\n                    msg = (\n                        f\"The positional arguments {deprecated_args} are \"\n                        \"deprecated and will be removed in a future update.\")\n                    if additional_message is not None:\n                        msg += f\" {additional_message}\"\n\n                    warnings.warn(\n                        DeprecationWarning(msg),\n                        stacklevel=3,  # The inner function takes up one level\n                    )\n\n            return fn(*args, **kwargs)\n\n        return inner  # type: ignore\n\n    return wrapper\n\n\ndef deprecate_kwargs(\n    *kws: str,\n    is_deprecated: Union[bool, Callable[[], bool]] = True,\n    additional_message: Optional[str] = None,\n) -> Callable[[F], F]:\n    deprecated_kws = set(kws)\n\n    if not callable(is_deprecated):\n        is_deprecated = partial(identity, is_deprecated)\n\n    def wrapper(fn: F) -> F:\n\n        @wraps(fn)\n        def inner(*args, **kwargs):\n            if is_deprecated():\n                deprecated_kwargs = kwargs.keys() & deprecated_kws\n                if deprecated_kwargs:\n                    msg = (\n                        f\"The keyword arguments {deprecated_kwargs} are \"\n                        \"deprecated and will be removed in a future update.\")\n                    if additional_message is not None:\n                        msg += f\" {additional_message}\"\n\n                    warnings.warn(\n                        DeprecationWarning(msg),\n                        stacklevel=3,  # The inner function takes up one level\n                    )\n\n            return fn(*args, **kwargs)\n\n        return inner  # type: ignore\n\n    return wrapper\n\n\n@lru_cache(maxsize=8)\ndef _cuda_device_count_stateless(\n        cuda_visible_devices: Optional[str] = None) -> int:\n    # Note: cuda_visible_devices is not used, but we keep it as an argument for\n    # LRU Cache purposes.\n\n    # Code below is based on\n    # https://github.com/pytorch/pytorch/blob/\n    # c1cd946818442aca8c7f812b16d187ce1586c3bc/\n    # torch/cuda/__init__.py#L831C1-L831C17\n    import torch.cuda\n    import torch.version\n\n    from vllm.platforms import current_platform\n    if not torch.cuda._is_compiled():\n        return 0\n    if current_platform.is_rocm():\n        # ROCm uses amdsmi instead of nvml for stateless device count\n        # This requires a sufficiently modern version of Torch 2.4.0\n        raw_count = torch.cuda._device_count_amdsmi() if (hasattr(\n            torch.cuda, \"_device_count_amdsmi\")) else -1\n    else:\n        raw_count = torch.cuda._device_count_nvml()\n    r = torch._C._cuda_getDeviceCount() if raw_count < 0 else raw_count\n    return r\n\n\ndef cuda_device_count_stateless() -> int:\n    \"\"\"Get number of CUDA devices, caching based on the value of\n    CUDA_VISIBLE_DEVICES at the time of call.\n\n    This should be used instead of torch.cuda.device_count()\n    unless CUDA_VISIBLE_DEVICES has already been set to the desired\n    value.\"\"\"\n\n    # This can be removed and simply replaced with torch.cuda.get_device_count\n    # after https://github.com/pytorch/pytorch/pull/122815 is released.\n    return _cuda_device_count_stateless(envs.CUDA_VISIBLE_DEVICES)\n\n\ndef cuda_is_initialized() -> bool:\n    \"\"\"Check if CUDA is initialized.\"\"\"\n    if not torch.cuda._is_compiled():\n        return False\n    return torch.cuda.is_initialized()\n\n\ndef cuda_get_device_properties(device,\n                               names: Sequence[str],\n                               init_cuda=False) -> tuple[Any, ...]:\n    \"\"\"Get specified CUDA device property values without initializing CUDA in\n    the current process.\"\"\"\n    if init_cuda or cuda_is_initialized():\n        props = torch.cuda.get_device_properties(device)\n        return tuple(getattr(props, name) for name in names)\n\n    # Run in subprocess to avoid initializing CUDA as a side effect.\n    mp_ctx = multiprocessing.get_context(\"fork\")\n    with ProcessPoolExecutor(max_workers=1, mp_context=mp_ctx) as executor:\n        return executor.submit(cuda_get_device_properties, device, names,\n                               True).result()\n\n\ndef weak_bind(bound_method: Callable[..., Any], ) -> Callable[..., None]:\n    \"\"\"Make an instance method that weakly references\n    its associated instance and no-ops once that\n    instance is collected.\"\"\"\n    ref = weakref.ref(bound_method.__self__)  # type: ignore[attr-defined]\n    unbound = bound_method.__func__  # type: ignore[attr-defined]\n\n    def weak_bound(*args, **kwargs) -> None:\n        if inst := ref():\n            unbound(inst, *args, **kwargs)\n\n    return weak_bound\n\n\n#From: https://stackoverflow.com/a/4104188/2749989\ndef run_once(f: Callable[P, None]) -> Callable[P, None]:\n\n    def wrapper(*args: P.args, **kwargs: P.kwargs) -> None:\n        if not wrapper.has_run:  # type: ignore[attr-defined]\n            wrapper.has_run = True  # type: ignore[attr-defined]\n            return f(*args, **kwargs)\n\n    wrapper.has_run = False  # type: ignore[attr-defined]\n    return wrapper\n\n\nclass StoreBoolean(Action):\n\n    def __call__(self, parser, namespace, values, option_string=None):\n        if values.lower() == \"true\":\n            setattr(namespace, self.dest, True)\n        elif values.lower() == \"false\":\n            setattr(namespace, self.dest, False)\n        else:\n            raise ValueError(f\"Invalid boolean value: {values}. \"\n                             \"Expected 'true' or 'false'.\")\n\n\nclass SortedHelpFormatter(ArgumentDefaultsHelpFormatter):\n    \"\"\"SortedHelpFormatter that sorts arguments by their option strings.\"\"\"\n\n    def _split_lines(self, text, width):\n        \"\"\"\n        1. Sentences split across lines have their single newlines removed.\n        2. Paragraphs and explicit newlines are split into separate lines.\n        3. Each line is wrapped to the specified width (width of terminal).\n        \"\"\"\n        # The patterns also include whitespace after the newline\n        single_newline = re.compile(r\"(?<!\\n)\\n(?!\\n)\\s*\")\n        multiple_newlines = re.compile(r\"\\n{2,}\\s*\")\n        text = single_newline.sub(' ', text)\n        lines = re.split(multiple_newlines, text)\n        return sum([textwrap.wrap(line, width) for line in lines], [])\n\n    def add_arguments(self, actions):\n        actions = sorted(actions, key=lambda x: x.option_strings)\n        super().add_arguments(actions)\n\n\nclass FlexibleArgumentParser(ArgumentParser):\n    \"\"\"ArgumentParser that allows both underscore and dash in names.\"\"\"\n\n    _deprecated: set[Action] = set()\n\n    def __init__(self, *args, **kwargs):\n        # Set the default 'formatter_class' to SortedHelpFormatter\n        if 'formatter_class' not in kwargs:\n            kwargs['formatter_class'] = SortedHelpFormatter\n        super().__init__(*args, **kwargs)\n\n    if sys.version_info < (3, 13):\n        # Enable the deprecated kwarg for Python 3.12 and below\n\n        def parse_known_args(self, args=None, namespace=None):\n            namespace, args = super().parse_known_args(args, namespace)\n            for action in FlexibleArgumentParser._deprecated:\n                if (hasattr(namespace, dest := action.dest)\n                        and getattr(namespace, dest) != action.default):\n                    logger.warning_once(\"argument '%s' is deprecated\", dest)\n            return namespace, args\n\n        def add_argument(self, *args, **kwargs):\n            deprecated = kwargs.pop(\"deprecated\", False)\n            action = super().add_argument(*args, **kwargs)\n            if deprecated:\n                FlexibleArgumentParser._deprecated.add(action)\n            return action\n\n        class _FlexibleArgumentGroup(_ArgumentGroup):\n\n            def add_argument(self, *args, **kwargs):\n                deprecated = kwargs.pop(\"deprecated\", False)\n                action = super().add_argument(*args, **kwargs)\n                if deprecated:\n                    FlexibleArgumentParser._deprecated.add(action)\n                return action\n\n        def add_argument_group(self, *args, **kwargs):\n            group = self._FlexibleArgumentGroup(self, *args, **kwargs)\n            self._action_groups.append(group)\n            return group\n\n    def parse_args(  # type: ignore[override]\n        self,\n        args: list[str] | None = None,\n        namespace: Namespace | None = None,\n    ):\n        if args is None:\n            args = sys.argv[1:]\n\n        # Check for --model in command line arguments first\n        if args and args[0] == \"serve\":\n            model_in_cli_args = any(arg == '--model' for arg in args)\n\n            if model_in_cli_args:\n                raise ValueError(\n                    \"With `vllm serve`, you should provide the model as a \"\n                    \"positional argument or in a config file instead of via \"\n                    \"the `--model` option.\")\n\n        if '--config' in args:\n            args = self._pull_args_from_config(args)\n\n        # Convert underscores to dashes and vice versa in argument names\n        processed_args = []\n        for arg in args:\n            if arg.startswith('--'):\n                if '=' in arg:\n                    key, value = arg.split('=', 1)\n                    key = '--' + key[len('--'):].replace('_', '-')\n                    processed_args.append(f'{key}={value}')\n                else:\n                    processed_args.append('--' +\n                                          arg[len('--'):].replace('_', '-'))\n            elif arg.startswith('-O') and arg != '-O' and len(arg) == 2:\n                # allow -O flag to be used without space, e.g. -O3\n                processed_args.append('-O')\n                processed_args.append(arg[2:])\n            else:\n                processed_args.append(arg)\n\n        def create_nested_dict(keys: list[str], value: str):\n            \"\"\"Creates a nested dictionary from a list of keys and a value.\n\n            For example, `keys = [\"a\", \"b\", \"c\"]` and `value = 1` will create:\n            `{\"a\": {\"b\": {\"c\": 1}}}`\n            \"\"\"\n            nested_dict: Any = value\n            for key in reversed(keys):\n                nested_dict = {key: nested_dict}\n            return nested_dict\n\n        def recursive_dict_update(original: dict, update: dict):\n            \"\"\"Recursively updates a dictionary with another dictionary.\"\"\"\n            for k, v in update.items():\n                if isinstance(v, dict) and isinstance(original.get(k), dict):\n                    recursive_dict_update(original[k], v)\n                else:\n                    original[k] = v\n\n        delete = set()\n        dict_args: dict[str, dict] = defaultdict(dict)\n        for i, processed_arg in enumerate(processed_args):\n            if processed_arg.startswith(\"--\") and \".\" in processed_arg:\n                if \"=\" in processed_arg:\n                    processed_arg, value = processed_arg.split(\"=\", 1)\n                    if \".\" not in processed_arg:\n                        # False positive, . was only in the value\n                        continue\n                else:\n                    value = processed_args[i + 1]\n                    delete.add(i + 1)\n                key, *keys = processed_arg.split(\".\")\n                # Merge all values with the same key into a single dict\n                arg_dict = create_nested_dict(keys, value)\n                recursive_dict_update(dict_args[key], arg_dict)\n                delete.add(i)\n        # Filter out the dict args we set to None\n        processed_args = [\n            a for i, a in enumerate(processed_args) if i not in delete\n        ]\n        # Add the dict args back as if they were originally passed as JSON\n        for dict_arg, dict_value in dict_args.items():\n            processed_args.append(dict_arg)\n            processed_args.append(json.dumps(dict_value))\n\n        return super().parse_args(processed_args, namespace)\n\n    def check_port(self, value):\n        try:\n            value = int(value)\n        except ValueError:\n            msg = \"Port must be an integer\"\n            raise ArgumentTypeError(msg) from None\n\n        if not (1024 <= value <= 65535):\n            raise ArgumentTypeError(\"Port must be between 1024 and 65535\")\n\n        return value\n\n    def _pull_args_from_config(self, args: list[str]) -> list[str]:\n        \"\"\"Method to pull arguments specified in the config file\n        into the command-line args variable.\n\n        The arguments in config file will be inserted between\n        the argument list.\n\n        example:\n        ```yaml\n            port: 12323\n            tensor-parallel-size: 4\n        ```\n        ```python\n        $: vllm {serve,chat,complete} \"facebook/opt-12B\" \\\n            --config config.yaml -tp 2\n        $: args = [\n            \"serve,chat,complete\",\n            \"facebook/opt-12B\",\n            '--config', 'config.yaml',\n            '-tp', '2'\n        ]\n        $: args = [\n            \"serve,chat,complete\",\n            \"facebook/opt-12B\",\n            '--port', '12323',\n            '--tensor-parallel-size', '4',\n            '-tp', '2'\n            ]\n        ```\n\n        Please note how the config args are inserted after the sub command.\n        this way the order of priorities is maintained when these are args\n        parsed by super().\n        \"\"\"\n        assert args.count(\n            '--config') <= 1, \"More than one config file specified!\"\n\n        index = args.index('--config')\n        if index == len(args) - 1:\n            raise ValueError(\"No config file specified! \\\n                             Please check your command-line arguments.\")\n\n        file_path = args[index + 1]\n\n        config_args = self._load_config_file(file_path)\n\n        # 0th index is for {serve,chat,complete}\n        # optionally followed by model_tag (only for serve)\n        # followed by config args\n        # followed by rest of cli args.\n        # maintaining this order will enforce the precedence\n        # of cli > config > defaults\n        if args[0] == \"serve\":\n            model_in_cli = len(args) > 1 and not args[1].startswith('-')\n            model_in_config = any(arg == '--model' for arg in config_args)\n\n            if not model_in_cli and not model_in_config:\n                raise ValueError(\n                    \"No model specified! Please specify model either \"\n                    \"as a positional argument or in a config file.\")\n\n            if model_in_cli:\n                # Model specified as positional arg, keep CLI version\n                args = [args[0]] + [\n                    args[1]\n                ] + config_args + args[2:index] + args[index + 2:]\n            else:\n                # No model in CLI, use config if available\n                args = [args[0]\n                        ] + config_args + args[1:index] + args[index + 2:]\n        else:\n            args = [args[0]] + config_args + args[1:index] + args[index + 2:]\n\n        return args\n\n    def _load_config_file(self, file_path: str) -> list[str]:\n        \"\"\"Loads a yaml file and returns the key value pairs as a\n        flattened list with argparse like pattern\n        ```yaml\n            port: 12323\n            tensor-parallel-size: 4\n        ```\n        returns:\n            processed_args: list[str] = [\n                '--port': '12323',\n                '--tensor-parallel-size': '4'\n            ]\n        \"\"\"\n        extension: str = file_path.split('.')[-1]\n        if extension not in ('yaml', 'yml'):\n            raise ValueError(\n                \"Config file must be of a yaml/yml type.\\\n                              %s supplied\", extension)\n\n        # only expecting a flat dictionary of atomic types\n        processed_args: list[str] = []\n\n        config: dict[str, Union[int, str]] = {}\n        try:\n            with open(file_path) as config_file:\n                config = yaml.safe_load(config_file)\n        except Exception as ex:\n            logger.error(\n                \"Unable to read the config file at %s. \\\n                Make sure path is correct\", file_path)\n            raise ex\n\n        store_boolean_arguments = [\n            action.dest for action in self._actions\n            if isinstance(action, StoreBoolean)\n        ]\n\n        for key, value in config.items():\n            if isinstance(value, bool) and key not in store_boolean_arguments:\n                if value:\n                    processed_args.append('--' + key)\n            else:\n                processed_args.append('--' + key)\n                processed_args.append(str(value))\n\n        return processed_args\n\n\nasync def _run_task_with_lock(task: Callable, lock: asyncio.Lock, *args,\n                              **kwargs):\n    \"\"\"Utility function to run async task in a lock\"\"\"\n    async with lock:\n        return await task(*args, **kwargs)\n\n\ndef supports_kw(\n    callable: Callable[..., object],\n    kw_name: str,\n    *,\n    requires_kw_only: bool = False,\n    allow_var_kwargs: bool = True,\n) -> bool:\n    \"\"\"Check if a keyword is a valid kwarg for a callable; if requires_kw_only\n    disallows kwargs names that can also be positional arguments.\n    \"\"\"\n    params = inspect.signature(callable).parameters\n    if not params:\n        return False\n\n    param_val = params.get(kw_name)\n\n    # Types where the it may be valid, i.e., explicitly defined & nonvariadic\n    passable_kw_types = set((inspect.Parameter.POSITIONAL_ONLY,\n                             inspect.Parameter.POSITIONAL_OR_KEYWORD,\n                             inspect.Parameter.KEYWORD_ONLY))\n\n    if param_val:\n        is_sig_param = param_val.kind in passable_kw_types\n        # We want kwargs only, but this is passable as a positional arg\n        if (requires_kw_only and is_sig_param\n                and param_val.kind != inspect.Parameter.KEYWORD_ONLY):\n            return False\n        if ((requires_kw_only\n             and param_val.kind == inspect.Parameter.KEYWORD_ONLY)\n                or (not requires_kw_only and is_sig_param)):\n            return True\n\n    # If we're okay with var-kwargs, it's supported as long as\n    # the kw_name isn't something like *args, **kwargs\n    if allow_var_kwargs:\n        # Get the last param; type is ignored here because params is a proxy\n        # mapping, but it wraps an ordered dict, and they appear in order.\n        # Ref: https://docs.python.org/3/library/inspect.html#inspect.Signature.parameters\n        last_param = params[next(reversed(params))]  # type: ignore\n        return (last_param.kind == inspect.Parameter.VAR_KEYWORD\n                and last_param.name != kw_name)\n    return False\n\n\ndef resolve_mm_processor_kwargs(\n    init_kwargs: Optional[Mapping[str, object]],\n    inference_kwargs: Optional[Mapping[str, object]],\n    callable: Callable[..., object],\n    *,\n    requires_kw_only: bool = True,\n    allow_var_kwargs: bool = False,\n) -> dict[str, Any]:\n    \"\"\"Applies filtering to eliminate invalid mm_processor_kwargs, i.e.,\n    those who are not explicit keywords to the given callable (of one is\n    given; otherwise no filtering is done), then merges the kwarg dicts,\n    giving priority to inference_kwargs if there are any collisions.\n\n    In the case that no kwarg overrides are provided, returns an empty\n    dict so that it can still be kwarg expanded into the callable later on.\n\n    If allow_var_kwargs=True, allows for things that can be expanded into\n    kwargs as long as they aren't naming collision for var_kwargs or potential\n    positional arguments.\n    \"\"\"\n    # Filter inference time multimodal processor kwargs provided\n    runtime_mm_kwargs = get_allowed_kwarg_only_overrides(\n        callable,\n        overrides=inference_kwargs,\n        requires_kw_only=requires_kw_only,\n        allow_var_kwargs=allow_var_kwargs,\n    )\n\n    # Filter init time multimodal processor kwargs provided\n    init_mm_kwargs = get_allowed_kwarg_only_overrides(\n        callable,\n        overrides=init_kwargs,\n        requires_kw_only=requires_kw_only,\n        allow_var_kwargs=allow_var_kwargs,\n    )\n\n    # Merge the final processor kwargs, prioritizing inference\n    # time values over the initialization time values.\n    mm_processor_kwargs = {**init_mm_kwargs, **runtime_mm_kwargs}\n    return mm_processor_kwargs\n\n\ndef get_allowed_kwarg_only_overrides(\n    callable: Callable[..., object],\n    overrides: Optional[Mapping[str, object]],\n    *,\n    requires_kw_only: bool = True,\n    allow_var_kwargs: bool = False,\n) -> dict[str, Any]:\n    \"\"\"\n    Given a callable which has one or more keyword only params and a dict\n    mapping param names to values, drop values that can be not be kwarg\n    expanded to overwrite one or more keyword-only args. This is used in a\n    few places to handle custom processor overrides for multimodal models,\n    e.g., for profiling when processor options provided by the user\n    may affect the number of mm tokens per instance.\n\n    Args:\n        callable: Callable which takes 0 or more keyword only arguments.\n                  If None is provided, all overrides names are allowed.\n        overrides: Potential overrides to be used when invoking the callable.\n        allow_var_kwargs: Allows overrides that are expandable for var kwargs.\n\n    Returns:\n        Dictionary containing the kwargs to be leveraged which may be used\n        to overwrite one or more keyword only arguments when invoking the\n        callable.\n    \"\"\"\n    if not overrides:\n        return {}\n\n    # Drop any mm_processor_kwargs provided by the user that\n    # are not kwargs, unless it can fit it var_kwargs param\n    filtered_overrides = {\n        kwarg_name: val\n        for kwarg_name, val in overrides.items()\n        if supports_kw(callable,\n                       kwarg_name,\n                       requires_kw_only=requires_kw_only,\n                       allow_var_kwargs=allow_var_kwargs)\n    }\n\n    # If anything is dropped, log a warning\n    dropped_keys = overrides.keys() - filtered_overrides.keys()\n    if dropped_keys:\n        if requires_kw_only:\n            logger.warning(\n                \"The following intended overrides are not keyword-only args \"\n                \"and will be dropped: %s\", dropped_keys)\n        else:\n            logger.warning(\n                \"The following intended overrides are not keyword args \"\n                \"and will be dropped: %s\", dropped_keys)\n\n    return filtered_overrides\n\n\n# Using dynamo with vLLM doesn't really work well with PyTorch versions < 2.4.0.\n# In particular, the FakeScalarType is not supported for earlier versions of\n# PyTorch which breaks dynamo for any ops registered using ScalarType.\ndef supports_dynamo() -> bool:\n    base_torch_version = Version(Version(torch.__version__).base_version)\n    return base_torch_version >= Version(\"2.4.0\")\n\n\n# Some backends use pytorch version < 2.4.0 which doesn't\n# support `torch.library.custom_op`.\ndef supports_custom_op() -> bool:\n    return hasattr(torch.library, \"custom_op\")\n\n\nclass AtomicCounter:\n    \"\"\"An atomic, thread-safe counter\"\"\"\n\n    def __init__(self, initial=0):\n        \"\"\"Initialize a new atomic counter to given initial value\"\"\"\n        self._value = initial\n        self._lock = threading.Lock()\n\n    def inc(self, num=1):\n        \"\"\"Atomically increment the counter by num and return the new value\"\"\"\n        with self._lock:\n            self._value += num\n            return self._value\n\n    def dec(self, num=1):\n        \"\"\"Atomically decrement the counter by num and return the new value\"\"\"\n        with self._lock:\n            self._value -= num\n            return self._value\n\n    @property\n    def value(self):\n        return self._value\n\n\n# Adapted from: https://stackoverflow.com/a/47212782/5082708\nclass LazyDict(Mapping[str, T], Generic[T]):\n\n    def __init__(self, factory: dict[str, Callable[[], T]]):\n        self._factory = factory\n        self._dict: dict[str, T] = {}\n\n    def __getitem__(self, key: str) -> T:\n        if key not in self._dict:\n            if key not in self._factory:\n                raise KeyError(key)\n            self._dict[key] = self._factory[key]()\n        return self._dict[key]\n\n    def __setitem__(self, key: str, value: Callable[[], T]):\n        self._factory[key] = value\n\n    def __iter__(self):\n        return iter(self._factory)\n\n    def __len__(self):\n        return len(self._factory)\n\n\nclass ClassRegistry(UserDict[Type[T], _V]):\n\n    def __getitem__(self, key: Type[T]) -> _V:\n        for cls in key.mro():\n            if cls in self.data:\n                return self.data[cls]\n\n        raise KeyError(key)\n\n    def __contains__(self, key: object) -> bool:\n        return self.contains(key)\n\n    def contains(self, key: object, *, strict: bool = False) -> bool:\n        if not isinstance(key, type):\n            return False\n\n        if strict:\n            return key in self.data\n\n        return any(cls in self.data for cls in key.mro())\n\n\ndef weak_ref_tensor(tensor: Any) -> Any:\n    \"\"\"\n    Create a weak reference to a tensor.\n    The new tensor will share the same data as the original tensor,\n    but will not keep the original tensor alive.\n    \"\"\"\n    if isinstance(tensor, torch.Tensor):\n        return torch.ops._C.weak_ref_tensor(tensor)\n    else:\n        return tensor\n\n\ndef weak_ref_tensors(\n    tensors: Union[torch.Tensor, list[torch.Tensor], tuple[torch.Tensor]]\n) -> Union[torch.Tensor, list[Any], tuple[Any], Any]:\n    \"\"\"\n    Convenience function to create weak references to tensors,\n    for single tensor, list of tensors or tuple of tensors.\n    \"\"\"\n    if isinstance(tensors, torch.Tensor):\n        return weak_ref_tensor(tensors)\n    if isinstance(tensors, list):\n        return [weak_ref_tensor(t) for t in tensors]\n    if isinstance(tensors, tuple):\n        return tuple(weak_ref_tensor(t) for t in tensors)\n    raise ValueError(\"Invalid type for tensors\")\n\n\ndef get_cuda_view_from_cpu_tensor(cpu_tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"\n    Get a CUDA view of a CPU tensor using Unified Virtual Addressing (UVA).\n    \"\"\"\n    assert cpu_tensor.is_pinned(), \"CPU tensor must be pinned\"\n    return torch.ops._C.get_cuda_view_from_cpu_tensor(cpu_tensor)\n\n\ndef is_in_doc_build() -> bool:\n    try:\n        from sphinx.ext.autodoc.mock import _MockModule\n        return isinstance(zmq, _MockModule)\n    except ModuleNotFoundError:\n        return False\n\n\ndef import_from_path(module_name: str, file_path: Union[str, os.PathLike]):\n    \"\"\"\n    Import a Python file according to its file path.\n\n    Based on the official recipe:\n    https://docs.python.org/3/library/importlib.html#importing-a-source-file-directly\n    \"\"\"\n    spec = importlib.util.spec_from_file_location(module_name, file_path)\n    if spec is None:\n        raise ModuleNotFoundError(f\"No module named '{module_name}'\")\n\n    assert spec.loader is not None\n\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n    return module\n\n\n@cache\ndef get_vllm_optional_dependencies():\n    metadata = importlib.metadata.metadata(\"vllm\")\n    requirements = metadata.get_all(\"Requires-Dist\", [])\n    extras = metadata.get_all(\"Provides-Extra\", [])\n\n    return {\n        extra: [\n            re.split(r\";|>=|<=|==\", req)[0] for req in requirements\n            if req.endswith(f'extra == \"{extra}\"')\n        ]\n        for extra in extras\n    }\n\n\nclass _PlaceholderBase:\n    \"\"\"\n    Disallows downstream usage of placeholder modules.\n\n    We need to explicitly override each dunder method because\n    {meth}`__getattr__` is not called when they are accessed.\n\n    :::{seealso}\n    [Special method lookup](https://docs.python.org/3/reference/datamodel.html#special-lookup)\n    :::\n    \"\"\"\n\n    def __getattr__(self, key: str) -> Never:\n        \"\"\"\n        The main class should implement this to throw an error\n        for attribute accesses representing downstream usage.\n        \"\"\"\n        raise NotImplementedError\n\n    # [Basic customization]\n\n    def __lt__(self, other: object):\n        return self.__getattr__(\"__lt__\")\n\n    def __le__(self, other: object):\n        return self.__getattr__(\"__le__\")\n\n    def __eq__(self, other: object):\n        return self.__getattr__(\"__eq__\")\n\n    def __ne__(self, other: object):\n        return self.__getattr__(\"__ne__\")\n\n    def __gt__(self, other: object):\n        return self.__getattr__(\"__gt__\")\n\n    def __ge__(self, other: object):\n        return self.__getattr__(\"__ge__\")\n\n    def __hash__(self):\n        return self.__getattr__(\"__hash__\")\n\n    def __bool__(self):\n        return self.__getattr__(\"__bool__\")\n\n    # [Callable objects]\n\n    def __call__(self, *args: object, **kwargs: object):\n        return self.__getattr__(\"__call__\")\n\n    # [Container types]\n\n    def __len__(self):\n        return self.__getattr__(\"__len__\")\n\n    def __getitem__(self, key: object):\n        return self.__getattr__(\"__getitem__\")\n\n    def __setitem__(self, key: object, value: object):\n        return self.__getattr__(\"__setitem__\")\n\n    def __delitem__(self, key: object):\n        return self.__getattr__(\"__delitem__\")\n\n    # __missing__ is optional according to __getitem__ specification,\n    # so it is skipped\n\n    # __iter__ and __reversed__ have a default implementation\n    # based on __len__ and __getitem__, so they are skipped.\n\n    # [Numeric Types]\n\n    def __add__(self, other: object):\n        return self.__getattr__(\"__add__\")\n\n    def __sub__(self, other: object):\n        return self.__getattr__(\"__sub__\")\n\n    def __mul__(self, other: object):\n        return self.__getattr__(\"__mul__\")\n\n    def __matmul__(self, other: object):\n        return self.__getattr__(\"__matmul__\")\n\n    def __truediv__(self, other: object):\n        return self.__getattr__(\"__truediv__\")\n\n    def __floordiv__(self, other: object):\n        return self.__getattr__(\"__floordiv__\")\n\n    def __mod__(self, other: object):\n        return self.__getattr__(\"__mod__\")\n\n    def __divmod__(self, other: object):\n        return self.__getattr__(\"__divmod__\")\n\n    def __pow__(self, other: object, modulo: object = ...):\n        return self.__getattr__(\"__pow__\")\n\n    def __lshift__(self, other: object):\n        return self.__getattr__(\"__lshift__\")\n\n    def __rshift__(self, other: object):\n        return self.__getattr__(\"__rshift__\")\n\n    def __and__(self, other: object):\n        return self.__getattr__(\"__and__\")\n\n    def __xor__(self, other: object):\n        return self.__getattr__(\"__xor__\")\n\n    def __or__(self, other: object):\n        return self.__getattr__(\"__or__\")\n\n    # r* and i* methods have lower priority than\n    # the methods for left operand so they are skipped\n\n    def __neg__(self):\n        return self.__getattr__(\"__neg__\")\n\n    def __pos__(self):\n        return self.__getattr__(\"__pos__\")\n\n    def __abs__(self):\n        return self.__getattr__(\"__abs__\")\n\n    def __invert__(self):\n        return self.__getattr__(\"__invert__\")\n\n    # __complex__, __int__ and __float__ have a default implementation\n    # based on __index__, so they are skipped.\n\n    def __index__(self):\n        return self.__getattr__(\"__index__\")\n\n    def __round__(self, ndigits: object = ...):\n        return self.__getattr__(\"__round__\")\n\n    def __trunc__(self):\n        return self.__getattr__(\"__trunc__\")\n\n    def __floor__(self):\n        return self.__getattr__(\"__floor__\")\n\n    def __ceil__(self):\n        return self.__getattr__(\"__ceil__\")\n\n    # [Context managers]\n\n    def __enter__(self):\n        return self.__getattr__(\"__enter__\")\n\n    def __exit__(self, *args: object, **kwargs: object):\n        return self.__getattr__(\"__exit__\")\n\n\nclass PlaceholderModule(_PlaceholderBase):\n    \"\"\"\n    A placeholder object to use when a module does not exist.\n\n    This enables more informative errors when trying to access attributes\n    of a module that does not exists.\n    \"\"\"\n\n    def __init__(self, name: str) -> None:\n        super().__init__()\n\n        # Apply name mangling to avoid conflicting with module attributes\n        self.__name = name\n\n    def placeholder_attr(self, attr_path: str):\n        return _PlaceholderModuleAttr(self, attr_path)\n\n    def __getattr__(self, key: str):\n        name = self.__name\n\n        try:\n            importlib.import_module(name)\n        except ImportError as exc:\n            for extra, names in get_vllm_optional_dependencies().items():\n                if name in names:\n                    msg = f\"Please install vllm[{extra}] for {extra} support\"\n                    raise ImportError(msg) from exc\n\n            raise exc\n\n        raise AssertionError(\"PlaceholderModule should not be used \"\n                             \"when the original module can be imported\")\n\n\nclass _PlaceholderModuleAttr(_PlaceholderBase):\n\n    def __init__(self, module: PlaceholderModule, attr_path: str) -> None:\n        super().__init__()\n\n        # Apply name mangling to avoid conflicting with module attributes\n        self.__module = module\n        self.__attr_path = attr_path\n\n    def placeholder_attr(self, attr_path: str):\n        return _PlaceholderModuleAttr(self.__module,\n                                      f\"{self.__attr_path}.{attr_path}\")\n\n    def __getattr__(self, key: str):\n        getattr(self.__module, f\"{self.__attr_path}.{key}\")\n\n        raise AssertionError(\"PlaceholderModule should not be used \"\n                             \"when the original module can be imported\")\n\n\n# create a library to hold the custom op\nvllm_lib = Library(\"vllm\", \"FRAGMENT\")  # noqa\n\n\ndef direct_register_custom_op(\n        op_name: str,\n        op_func: Callable,\n        mutates_args: list[str],\n        fake_impl: Optional[Callable] = None,\n        target_lib: Optional[Library] = None,\n        dispatch_key: str = \"CUDA\",\n        tags: Tuple[torch.Tag, ...] = (),\n):\n    \"\"\"\n    `torch.library.custom_op` can have significant overhead because it\n    needs to consider complicated dispatching logic. This function\n    directly registers a custom op and dispatches it to the CUDA backend.\n    See https://gist.github.com/youkaichao/ecbea9ec9fc79a45d2adce1784d7a9a5\n    for more details.\n\n    By default, the custom op is registered to the vLLM library. If you\n    want to register it to a different library, you can pass the library\n    object to the `target_lib` argument.\n\n    IMPORTANT: the lifetime of the operator is tied to the lifetime of the\n    library object. If you want to bind the operator to a different library,\n    make sure the library object is alive when the operator is used.\n    \"\"\"\n    if not supports_custom_op():\n        from vllm.platforms import current_platform\n        assert not current_platform.is_cuda_alike(), (\n            \"cuda platform needs torch>=2.4 to support custom op, \"\n            \"chances are you are using an old version of pytorch \"\n            \"or a custom build of pytorch. It is recommended to \"\n            \"use vLLM in a fresh new environment and let it install \"\n            \"the required dependencies.\")\n        return\n\n    import torch.library\n    if hasattr(torch.library, \"infer_schema\"):\n        schema_str = torch.library.infer_schema(op_func,\n                                                mutates_args=mutates_args)\n    else:\n        # for pytorch 2.4\n        import torch._custom_op.impl\n        schema_str = torch._custom_op.impl.infer_schema(op_func, mutates_args)\n    my_lib = target_lib or vllm_lib\n    my_lib.define(op_name + schema_str, tags=tags)\n    my_lib.impl(op_name, op_func, dispatch_key=dispatch_key)\n    if fake_impl is not None:\n        my_lib._register_fake(op_name, fake_impl)\n\n\ndef resolve_obj_by_qualname(qualname: str) -> Any:\n    \"\"\"\n    Resolve an object by its fully qualified name.\n    \"\"\"\n    module_name, obj_name = qualname.rsplit(\".\", 1)\n    module = importlib.import_module(module_name)\n    return getattr(module, obj_name)\n\n\ndef kill_process_tree(pid: int):\n    \"\"\"\n    Kills all descendant processes of the given pid by sending SIGKILL.\n\n    Args:\n        pid (int): Process ID of the parent process\n    \"\"\"\n    try:\n        parent = psutil.Process(pid)\n    except psutil.NoSuchProcess:\n        return\n\n    # Get all children recursively\n    children = parent.children(recursive=True)\n\n    # Send SIGKILL to all children first\n    for child in children:\n        with contextlib.suppress(ProcessLookupError):\n            os.kill(child.pid, signal.SIGKILL)\n\n    # Finally kill the parent\n    with contextlib.suppress(ProcessLookupError):\n        os.kill(pid, signal.SIGKILL)\n\n\n@dataclass\nclass MemorySnapshot:\n    \"\"\"Memory snapshot.\"\"\"\n    torch_peak: int = 0\n    cuda_memory: int = 0\n    torch_memory: int = 0\n    non_torch_memory: int = 0\n    timestamp: float = 0.0\n    auto_measure: bool = True\n\n    def __post_init__(self):\n        if self.auto_measure:\n            self.measure()\n\n    def measure(self):\n        # we measure the torch peak memory usage via allocated_bytes,\n        # rather than `torch.cuda.memory_reserved()` .\n        # After `torch.cuda.reset_peak_memory_stats()`,\n        # `torch.cuda.memory_reserved()` will keep growing, and only shrink\n        # when we call `torch.cuda.empty_cache()` or OOM happens.\n        self.torch_peak = torch.cuda.memory_stats().get(\n            \"allocated_bytes.all.peak\", 0)\n\n        self.cuda_memory = torch.cuda.mem_get_info(\n        )[1] - torch.cuda.mem_get_info()[0]\n\n        # torch.cuda.memory_reserved() is how many bytes\n        # PyTorch gets from cuda (by calling cudaMalloc, etc.)\n        # this is used to measure the non-torch memory usage\n        self.torch_memory = torch.cuda.memory_reserved()\n\n        self.non_torch_memory = self.cuda_memory - self.torch_memory\n        self.timestamp = time.time()\n\n    def __sub__(self, other: MemorySnapshot) -> MemorySnapshot:\n        return MemorySnapshot(\n            torch_peak=self.torch_peak - other.torch_peak,\n            cuda_memory=self.cuda_memory - other.cuda_memory,\n            torch_memory=self.torch_memory - other.torch_memory,\n            non_torch_memory=self.non_torch_memory - other.non_torch_memory,\n            timestamp=self.timestamp - other.timestamp,\n            auto_measure=False,\n        )\n\n\n@dataclass\nclass MemoryProfilingResult:\n    \"\"\"Memory profiling result. All numbers are in bytes.\n    \"\"\"\n    non_kv_cache_memory: int = 0\n    torch_peak_increase: int = 0\n    non_torch_increase: int = 0\n    weights_memory: float = 0\n    before_create: MemorySnapshot = field(default_factory=MemorySnapshot)\n    before_profile: MemorySnapshot = field(default_factory=MemorySnapshot)\n    after_profile: MemorySnapshot = field(default_factory=MemorySnapshot)\n    profile_time: float = 0.0\n\n\n@contextlib.contextmanager\ndef memory_profiling(\n        baseline_snapshot: MemorySnapshot,\n        weights_memory: int) -> Generator[MemoryProfilingResult, None, None]:\n    \"\"\"Memory profiling context manager.\n    baseline_snapshot: the memory snapshot before the current vLLM instance.\n    weights_memory: memory used by PyTorch when loading the model weights.\n        Note that, before loading the model weights, we also initialize the device\n        and distributed environment, which may consume some memory. This part is not\n        included in the weights_memory because PyTorch does not control it.\n\n    The memory in one GPU can be classified into 3 categories:\n    1. memory used by anything other than the current vLLM instance.\n    2. memory used by torch in the current vLLM instance.\n    3. memory used in the current vLLM instance, but not by torch.\n\n    A quantitive example:\n\n    Before creating the current vLLM instance:\n        category 1: 1 GiB\n        category 2: 0 GiB\n        category 3: 0 GiB\n\n    After creating the current vLLM instance and loading the model,\n    (i.e. before profiling):\n        category 1: 1 GiB\n        category 2: 2 GiB (model weights take 2 GiB)\n        category 3: 0.5 GiB (memory used by NCCL)\n\n    During profiling (peak):\n        category 1: 1 GiB\n        category 2: 4 GiB (peak activation tensors take 2 GiB)\n        category 3: 1 GiB (memory used by NCCL + buffers for some attention backends)\n\n    After profiling:\n        category 1: 1 GiB\n        category 2: 3 GiB (after garbage-collecting activation tensors)\n        category 3: 1 GiB (memory used by NCCL + buffers for some attention backends)\n\n    In this case, non-kv cache takes 5 GiB in total, including:\n    a. 2 GiB used by the model weights (category 2)\n    b. 2 GiB reserved for the peak activation tensors (category 2)\n    c. 1 GiB used by non-torch components (category 3)\n\n    The memory used for loading weights (a.) is directly given from the argument `weights_memory`.\n\n    The increase of `torch.cuda.memory_stats()[\"allocated_bytes.all.peak\"]` during profiling gives (b.).\n\n    The increase of `non_torch_memory` from creating the current vLLM instance until after profiling to get (c.).\n    \"\"\" # noqa\n    gc.collect()\n    torch.cuda.empty_cache()\n    torch.cuda.reset_peak_memory_stats()\n\n    result = MemoryProfilingResult()\n\n    result.before_create = baseline_snapshot\n    # the part of memory used for holding the model weights\n    result.weights_memory = weights_memory\n\n    result.before_profile.measure()\n\n    yield result\n\n    gc.collect()\n    torch.cuda.empty_cache()\n\n    result.after_profile.measure()\n\n    diff_profile = result.after_profile - result.before_profile\n    diff_from_create = result.after_profile - result.before_create\n    result.torch_peak_increase = diff_profile.torch_peak\n    result.non_torch_increase = diff_from_create.non_torch_memory\n    result.profile_time = diff_profile.timestamp\n    result.non_kv_cache_memory = result.non_torch_increase + result.torch_peak_increase + result.weights_memory  # noqa\n\n\n# Adapted from: https://github.com/sgl-project/sglang/blob/v0.4.1/python/sglang/srt/utils.py#L630 # noqa: E501\ndef set_ulimit(target_soft_limit=65535):\n    if sys.platform.startswith('win'):\n        logger.info(\"Windows detected, skipping ulimit adjustment.\")\n        return\n\n    import resource\n    resource_type = resource.RLIMIT_NOFILE\n    current_soft, current_hard = resource.getrlimit(resource_type)\n\n    if current_soft < target_soft_limit:\n        try:\n            resource.setrlimit(resource_type,\n                               (target_soft_limit, current_hard))\n        except ValueError as e:\n            logger.warning(\n                \"Found ulimit of %s and failed to automatically increase \"\n                \"with error %s. This can cause fd limit errors like \"\n                \"`OSError: [Errno 24] Too many open files`. Consider \"\n                \"increasing with ulimit -n\", current_soft, e)\n\n\n# Adapted from: https://github.com/sgl-project/sglang/blob/v0.4.1/python/sglang/utils.py#L28 # noqa: E501\ndef get_exception_traceback():\n    etype, value, tb = sys.exc_info()\n    err_str = \"\".join(traceback.format_exception(etype, value, tb))\n    return err_str\n\n\ndef split_zmq_path(path: str) -> Tuple[str, str, str]:\n    \"\"\"Split a zmq path into its parts.\"\"\"\n    parsed = urlparse(path)\n    if not parsed.scheme:\n        raise ValueError(f\"Invalid zmq path: {path}\")\n\n    scheme = parsed.scheme\n    host = parsed.hostname or \"\"\n    port = str(parsed.port or \"\")\n\n    if scheme == \"tcp\" and not all((host, port)):\n        # The host and port fields are required for tcp\n        raise ValueError(f\"Invalid zmq path: {path}\")\n\n    if scheme != \"tcp\" and port:\n        # port only makes sense with tcp\n        raise ValueError(f\"Invalid zmq path: {path}\")\n\n    return scheme, host, port\n\n\ndef make_zmq_path(scheme: str, host: str, port: Optional[int] = None) -> str:\n    \"\"\"Make a ZMQ path from its parts.\n\n    Args:\n        scheme: The ZMQ transport scheme (e.g. tcp, ipc, inproc).\n        host: The host - can be an IPv4 address, IPv6 address, or hostname.\n        port: Optional port number, only used for TCP sockets.\n\n    Returns:\n        A properly formatted ZMQ path string.\n    \"\"\"\n    if not port:\n        return f\"{scheme}://{host}\"\n    if is_valid_ipv6_address(host):\n        return f\"{scheme}://[{host}]:{port}\"\n    return f\"{scheme}://{host}:{port}\"\n\n\n# Adapted from: https://github.com/sgl-project/sglang/blob/v0.4.1/python/sglang/srt/utils.py#L783 # noqa: E501\ndef make_zmq_socket(\n    ctx: Union[zmq.asyncio.Context, zmq.Context],  # type: ignore[name-defined]\n    path: str,\n    socket_type: Any,\n    bind: Optional[bool] = None,\n    identity: Optional[bytes] = None,\n) -> Union[zmq.Socket, zmq.asyncio.Socket]:  # type: ignore[name-defined]\n    \"\"\"Make a ZMQ socket with the proper bind/connect semantics.\"\"\"\n\n    mem = psutil.virtual_memory()\n    socket = ctx.socket(socket_type)\n\n    # Calculate buffer size based on system memory\n    total_mem = mem.total / 1024**3\n    available_mem = mem.available / 1024**3\n    # For systems with substantial memory (>32GB total, >16GB available):\n    # - Set a large 0.5GB buffer to improve throughput\n    # For systems with less memory:\n    # - Use system default (-1) to avoid excessive memory consumption\n    if total_mem > 32 and available_mem > 16:\n        buf_size = int(0.5 * 1024**3)  # 0.5GB in bytes\n    else:\n        buf_size = -1  # Use system default buffer size\n\n    if bind is None:\n        bind = socket_type != zmq.PUSH\n\n    if socket_type in (zmq.PULL, zmq.DEALER, zmq.ROUTER):\n        socket.setsockopt(zmq.RCVHWM, 0)\n        socket.setsockopt(zmq.RCVBUF, buf_size)\n\n    if socket_type in (zmq.PUSH, zmq.DEALER, zmq.ROUTER):\n        socket.setsockopt(zmq.SNDHWM, 0)\n        socket.setsockopt(zmq.SNDBUF, buf_size)\n\n    if identity is not None:\n        socket.setsockopt(zmq.IDENTITY, identity)\n\n    # Determine if the path is a TCP socket with an IPv6 address.\n    # Enable IPv6 on the zmq socket if so.\n    scheme, host, _ = split_zmq_path(path)\n    if scheme == \"tcp\" and is_valid_ipv6_address(host):\n        socket.setsockopt(zmq.IPV6, 1)\n\n    if bind:\n        socket.bind(path)\n    else:\n        socket.connect(path)\n\n    return socket\n\n\n@contextlib.contextmanager\ndef zmq_socket_ctx(\n    path: str,\n    socket_type: Any,\n    bind: Optional[bool] = None,\n    linger: int = 0,\n    identity: Optional[bytes] = None,\n) -> Iterator[zmq.Socket]:\n    \"\"\"Context manager for a ZMQ socket\"\"\"\n\n    ctx = zmq.Context()  # type: ignore[attr-defined]\n    try:\n        yield make_zmq_socket(ctx,\n                              path,\n                              socket_type,\n                              bind=bind,\n                              identity=identity)\n    except KeyboardInterrupt:\n        logger.debug(\"Got Keyboard Interrupt.\")\n\n    finally:\n        ctx.destroy(linger=linger)\n\n\ndef is_in_ray_actor():\n    \"\"\"Check if we are in a Ray actor.\"\"\"\n\n    try:\n        import ray\n        return (ray.is_initialized()\n                and ray.get_runtime_context().get_actor_id() is not None)\n    except ImportError:\n        return False\n\n\ndef _maybe_force_spawn():\n    \"\"\"Check if we need to force the use of the `spawn` multiprocessing start\n    method.\n    \"\"\"\n    if os.environ.get(\"VLLM_WORKER_MULTIPROC_METHOD\") == \"spawn\":\n        return\n\n    reason = None\n    if cuda_is_initialized():\n        reason = \"CUDA is initialized\"\n    elif is_in_ray_actor():\n        # even if we choose to spawn, we need to pass the ray address\n        # to the subprocess so that it knows how to connect to the ray cluster.\n        # env vars are inherited by subprocesses, even if we use spawn.\n        import ray\n        os.environ[\"RAY_ADDRESS\"] = ray.get_runtime_context().gcs_address\n        reason = \"In a Ray actor and can only be spawned\"\n\n    if reason is not None:\n        logger.warning(\n            \"We must use the `spawn` multiprocessing start method. \"\n            \"Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. \"\n            \"See https://docs.vllm.ai/en/latest/getting_started/\"\n            \"troubleshooting.html#python-multiprocessing \"\n            \"for more information. Reason: %s\", reason)\n        os.environ[\"VLLM_WORKER_MULTIPROC_METHOD\"] = \"spawn\"\n\n\ndef get_mp_context():\n    \"\"\"Get a multiprocessing context with a particular method (spawn or fork).\n    By default we follow the value of the VLLM_WORKER_MULTIPROC_METHOD to\n    determine the multiprocessing method (default is fork). However, under\n    certain conditions, we may enforce spawn and override the value of\n    VLLM_WORKER_MULTIPROC_METHOD.\n    \"\"\"\n    _maybe_force_spawn()\n    mp_method = envs.VLLM_WORKER_MULTIPROC_METHOD\n    return multiprocessing.get_context(mp_method)\n\n\ndef bind_kv_cache(\n        ctx: dict[str, Any],\n        kv_cache: list[list[torch.Tensor]],  # [virtual_engine][layer_index]\n) -> None:\n    # Bind the kv_cache tensor to Attention modules, similar to\n    # ctx[layer_name].kv_cache[ve]=kv_cache[ve][extract_layer_index(layer_name)]\n    # Special things handled here:\n    # 1. Some models have non-attention layers, e.g., Jamba\n    # 2. Pipeline parallelism, each rank only has a subset of layers\n    # 3. Encoder attention has no kv cache\n    # 4. Encoder-decoder models, encoder-decoder attention and decoder-only\n    #    attention of the same layer (e.g., bart's decoder.layers.1.self_attn\n    #    and decoder.layers.1.encoder_attn) is mapped to the same kv cache\n    #    tensor\n    from vllm.attention import AttentionType\n    from vllm.model_executor.models.utils import extract_layer_index\n    layer_need_kv_cache = [\n        layer_name for layer_name in ctx\n        if (hasattr(ctx[layer_name], 'attn_type') and ctx[layer_name].attn_type\n            in (AttentionType.DECODER, AttentionType.ENCODER_DECODER))\n    ]\n    layer_index_sorted = sorted(\n        set(\n            extract_layer_index(layer_name)\n            for layer_name in layer_need_kv_cache))\n    for layer_name in layer_need_kv_cache:\n        kv_cache_idx = layer_index_sorted.index(\n            extract_layer_index(layer_name))\n        forward_ctx = ctx[layer_name]\n        assert len(forward_ctx.kv_cache) == len(kv_cache)\n        for ve, ve_kv_cache in enumerate(kv_cache):\n            forward_ctx.kv_cache[ve] = ve_kv_cache[kv_cache_idx]\n\n\ndef run_method(obj: Any, method: Union[str, bytes, Callable], args: tuple[Any],\n               kwargs: dict[str, Any]) -> Any:\n    \"\"\"\n    Run a method of an object with the given arguments and keyword arguments.\n    If the method is string, it will be converted to a method using getattr.\n    If the method is serialized bytes and will be deserialized using\n    cloudpickle.\n    If the method is a callable, it will be called directly.\n    \"\"\"\n    if isinstance(method, bytes):\n        func = partial(cloudpickle.loads(method), obj)\n    elif isinstance(method, str):\n        try:\n            func = getattr(obj, method)\n        except AttributeError:\n            raise NotImplementedError(f\"Method {method!r} is not\"\n                                      \" implemented.\") from None\n    else:\n        func = partial(method, obj)  # type: ignore\n    return func(*args, **kwargs)\n\n\ndef import_pynvml():\n    \"\"\"\n    Historical comments:\n\n    libnvml.so is the library behind nvidia-smi, and\n    pynvml is a Python wrapper around it. We use it to get GPU\n    status without initializing CUDA context in the current process.\n    Historically, there are two packages that provide pynvml:\n    - `nvidia-ml-py` (https://pypi.org/project/nvidia-ml-py/): The official\n        wrapper. It is a dependency of vLLM, and is installed when users\n        install vLLM. It provides a Python module named `pynvml`.\n    - `pynvml` (https://pypi.org/project/pynvml/): An unofficial wrapper.\n        Prior to version 12.0, it also provides a Python module `pynvml`,\n        and therefore conflicts with the official one. What's worse,\n        the module is a Python package, and has higher priority than\n        the official one which is a standalone Python file.\n        This causes errors when both of them are installed.\n        Starting from version 12.0, it migrates to a new module\n        named `pynvml_utils` to avoid the conflict.\n    It is so confusing that many packages in the community use the\n    unofficial one by mistake, and we have to handle this case.\n    For example, `nvcr.io/nvidia/pytorch:24.12-py3` uses the unofficial\n    one, and it will cause errors, see the issue\n    https://github.com/vllm-project/vllm/issues/12847 for example.\n    After all the troubles, we decide to copy the official `pynvml`\n    module to our codebase, and use it directly.\n    \"\"\"\n    import vllm.third_party.pynvml as pynvml\n    return pynvml\n\n\ndef warn_for_unimplemented_methods(cls: type[T]) -> type[T]:\n    \"\"\"\n    A replacement for `abc.ABC`.\n    When we use `abc.ABC`, subclasses will fail to instantiate\n    if they do not implement all abstract methods.\n    Here, we only require `raise NotImplementedError` in the\n    base class, and log a warning if the method is not implemented\n    in the subclass.\n    \"\"\"\n\n    original_init = cls.__init__\n\n    def find_unimplemented_methods(self: object):\n        unimplemented_methods = []\n        for attr_name in dir(self):\n            # bypass inner method\n            if attr_name.startswith('_'):\n                continue\n\n            try:\n                attr = getattr(self, attr_name)\n                # get the func of callable method\n                if callable(attr):\n                    attr_func = attr.__func__\n            except AttributeError:\n                continue\n            src = inspect.getsource(attr_func)\n            if \"NotImplementedError\" in src:\n                unimplemented_methods.append(attr_name)\n        if unimplemented_methods:\n            method_names = ','.join(unimplemented_methods)\n            msg = (f\"Methods {method_names} not implemented in {self}\")\n            logger.warning(msg)\n\n    @wraps(original_init)\n    def wrapped_init(self, *args, **kwargs) -> None:\n        original_init(self, *args, **kwargs)\n        find_unimplemented_methods(self)\n\n    type.__setattr__(cls, '__init__', wrapped_init)\n    return cls\n\n\nclass LazyLoader(types.ModuleType):\n    \"\"\"\n    LazyLoader module borrowed from Tensorflow\n    https://github.com/tensorflow/tensorflow/blob/main/tensorflow/python/util/lazy_loader.py\n    with a addition of \"module caching\".\n\n    Lazily import a module, mainly to avoid pulling in large dependencies.\n    Modules such as `xgrammar` might do additional side effects, so we\n    only want to use this when it is needed, delaying all eager effects\n    \"\"\"\n\n    def __init__(\n        self,\n        local_name: str,\n        parent_module_globals: dict[str, Any],\n        name: str,\n    ):\n        self._local_name = local_name\n        self._parent_module_globals = parent_module_globals\n        self._module: types.ModuleType | None = None\n\n        super().__init__(str(name))\n\n    def _load(self) -> types.ModuleType:\n        # Import the target module and insert it into the parent's namespace\n        try:\n            module = importlib.import_module(self.__name__)\n            self._parent_module_globals[self._local_name] = module\n            # The additional add to sys.modules\n            # ensures library is actually loaded.\n            sys.modules[self._local_name] = module\n        except ModuleNotFoundError as err:\n            raise err from None\n\n        # Update this object's dict so that if someone keeps a\n        # reference to the LazyLoader, lookups are efficient\n        # (__getattr__ is only called on lookups that fail).\n        self.__dict__.update(module.__dict__)\n        return module\n\n    def __getattr__(self, item: Any) -> Any:\n        if self._module is None:\n            self._module = self._load()\n        return getattr(self._module, item)\n\n    def __dir__(self) -> list[str]:\n        if self._module is None:\n            self._module = self._load()\n        return dir(self._module)\n\n\ndef swap_dict_values(obj: dict[_K, _V], key1: _K, key2: _K) -> None:\n    \"\"\"\n    Helper function to swap values for two keys\n    \"\"\"\n    v1 = obj.get(key1)\n    v2 = obj.get(key2)\n    if v1 is not None:\n        obj[key2] = v1\n    else:\n        obj.pop(key2, None)\n    if v2 is not None:\n        obj[key1] = v2\n    else:\n        obj.pop(key1, None)\n\n\n@contextlib.contextmanager\ndef cprofile_context(save_file: Optional[str] = None):\n    \"\"\"Run a cprofile\n\n    Args:\n        save_file: path to save the profile result. \"1\" or\n          None will result in printing to stdout.\n    \"\"\"\n    import cProfile\n\n    prof = cProfile.Profile()\n    prof.enable()\n\n    try:\n        yield\n    finally:\n        prof.disable()\n        if save_file and save_file != \"1\":\n            prof.dump_stats(save_file)\n        else:\n            prof.print_stats(sort=\"cumtime\")\n\n\ndef cprofile(save_file: Optional[str] = None, enabled: bool = True):\n    \"\"\"Decorator to profile a Python method using cProfile.\n\n    Args:\n        save_file: Path to save the profile result.\n            If \"1\", None, or \"\", results will be printed to stdout.\n        enabled: Set to false to turn this into a no-op\n    \"\"\"\n\n    def decorator(func: Callable):\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            if not enabled:\n                # If profiling is disabled, just call the function directly.\n                return func(*args, **kwargs)\n\n            with cprofile_context(save_file):\n                return func(*args, **kwargs)\n\n        return wrapper\n\n    return decorator\n\n\n# Only relevant for models using ALiBi (e.g, MPT)\ndef check_use_alibi(model_config: ModelConfig) -> bool:\n    return (getattr(model_config.hf_text_config, \"alibi\", False)  # Falcon\n            or (\"BloomForCausalLM\" in getattr(model_config.hf_config,\n                                              \"architectures\", []))  # Bloom\n            or getattr(model_config.hf_text_config, \"position_encoding_type\",\n                       \"\") == \"alibi\"  # codellm_1b_alibi\n            or\n            (hasattr(model_config.hf_text_config, \"attn_config\")  # MPT\n             and model_config.hf_text_config.attn_config.get(\"alibi\", False)))\n\n\ndef sha256(input) -> int:\n    \"\"\"Hash any picklable Python object using SHA-256.\n\n    The input is serialized using pickle before hashing, which allows\n    arbitrary Python objects to be used. Note that this function does\n    not use a hash seed\u2014if you need one, prepend it explicitly to the input.\n\n    Args:\n        input: Any picklable Python object.\n\n    Returns:\n        An integer representing the SHA-256 hash of the serialized input.\n    \"\"\"\n    input_bytes = pickle.dumps(input, protocol=pickle.HIGHEST_PROTOCOL)\n    return int.from_bytes(hashlib.sha256(input_bytes).digest(),\n                          byteorder=\"big\")\n\n\ndef is_torch_equal_or_newer(target: str) -> bool:\n    \"\"\"Check if the installed torch version is >= the target version.\n\n    Args:\n        target: a version string, like \"2.6.0\".\n\n    Returns:\n        Whether the condition meets.\n    \"\"\"\n    try:\n        torch_version = version.parse(str(torch.__version__))\n        return torch_version >= version.parse(target)\n    except Exception:\n        # Fallback to PKG-INFO to load the package info, needed by the doc gen.\n        return Version(importlib.metadata.version('torch')) >= Version(target)\n", 2834], "/home/jeromeku/vllm/vllm/v1/engine/processor.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport time\nfrom collections.abc import Mapping, Sequence\nfrom typing import Any, Literal, Optional, Union\n\nfrom vllm.config import VllmConfig\nfrom vllm.inputs import ProcessorInputs, PromptType, SingletonInputs\nfrom vllm.inputs.parse import split_enc_dec_inputs\nfrom vllm.inputs.preprocess import InputPreprocessor\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import (MULTIMODAL_REGISTRY, MultiModalKwargs,\n                             MultiModalRegistry)\nfrom vllm.multimodal.inputs import PlaceholderRange\nfrom vllm.multimodal.processing import EncDecMultiModalProcessor\nfrom vllm.multimodal.utils import merge_and_sort_multimodal_metadata\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\nfrom vllm.v1.engine import EngineCoreRequest\nfrom vllm.v1.engine.mm_input_cache import MirroredProcessingCache\nfrom vllm.v1.structured_output.backend_guidance import (\n    validate_guidance_grammar)\nfrom vllm.v1.structured_output.backend_xgrammar import (\n    validate_xgrammar_grammar)\n\n\nclass Processor:\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        tokenizer: TokenizerGroup,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ):\n\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n        self.lora_config = vllm_config.lora_config\n        self.decoding_config = vllm_config.decoding_config\n        self.tokenizer = tokenizer\n\n        self.generation_config_fields = (\n            self.model_config.try_get_generation_config())\n        self.input_preprocessor = InputPreprocessor(self.model_config,\n                                                    self.tokenizer,\n                                                    mm_registry)\n\n        self.mm_input_cache_client = MirroredProcessingCache(self.model_config)\n\n        # Multi-modal hasher (for images)\n        self.use_hash = self.mm_input_cache_client.use_cache or \\\n            self.cache_config.enable_prefix_caching\n\n    @property\n    def mm_registry(self):\n        return self.input_preprocessor.mm_registry\n\n    def _validate_logprobs(\n        self,\n        params: SamplingParams,\n    ) -> None:\n        max_logprobs = self.model_config.max_logprobs\n        # Validate sample logprobs.\n        if params.logprobs and params.logprobs > max_logprobs:\n            raise ValueError(\n                f\"Requested sample logprobs of {params.logprobs}, \"\n                f\"which is greater than max allowed: {max_logprobs}\")\n\n        # Validate prompt logprobs.\n        if params.prompt_logprobs and params.prompt_logprobs > max_logprobs:\n            raise ValueError(\n                f\"Requested prompt logprobs of {params.prompt_logprobs}, \"\n                f\"which is greater than max allowed: {max_logprobs}\")\n\n    def _validate_sampling_params(\n        self,\n        params: SamplingParams,\n        lora_request: Optional[LoRARequest],\n    ) -> None:\n        self._validate_structured_output(params)\n        self._validate_logit_bias(params)\n\n        if params.allowed_token_ids is None:\n            return\n        if not params.allowed_token_ids:\n            raise ValueError(\"allowed_token_ids is not None and empty!\")\n        tokenizer = self.tokenizer.get_lora_tokenizer(lora_request)\n        vocab_size = len(tokenizer)\n        if not all(0 <= tid < vocab_size for tid in params.allowed_token_ids):\n            raise ValueError(\n                \"allowed_token_ids contains out-of-vocab token id!\")\n\n    def _validate_logit_bias(\n        self,\n        params: SamplingParams,\n    ) -> None:\n        \"\"\"Validate logit_bias token IDs are within vocabulary range.\"\"\"\n        if not params.logit_bias:\n            return\n\n        vocab_size = self.model_config.get_vocab_size()\n        invalid_token_ids = []\n\n        for token_id in params.logit_bias:\n            if token_id < 0 or token_id >= vocab_size:\n                invalid_token_ids.append(token_id)\n\n        if invalid_token_ids:\n            raise ValueError(\n                f\"token_id(s) {invalid_token_ids} in logit_bias contain \"\n                f\"out-of-vocab token ids. Vocabulary size: {vocab_size}\")\n\n    def _validate_supported_sampling_params(\n        self,\n        params: SamplingParams,\n    ) -> None:\n        # Best of not yet supported.\n        if params.best_of is not None and params.best_of > 1:\n            raise ValueError(\"vLLM V1 does not yet support best_of.\")\n        # Logits processors not supported.\n        if params.logits_processors:\n            raise ValueError(\"vLLM V1 does not support per request \"\n                             \"user provided logits processors.\")\n\n    def _validate_params(\n        self,\n        params: Union[SamplingParams, PoolingParams],\n        lora_request: Optional[LoRARequest],\n    ):\n        \"\"\"\n        Validate supported SamplingParam.\n        Should raise ValueError if unsupported for API Server.\n        \"\"\"\n\n        if not isinstance(params, SamplingParams):\n            raise ValueError(\"V1 does not yet support Pooling models.\")\n\n        self._validate_logprobs(params)\n        self._validate_sampling_params(params, lora_request)\n        self._validate_supported_sampling_params(params)\n\n    def _validate_lora(self, lora_request: Optional[LoRARequest]) -> None:\n        if lora_request is not None and not self.lora_config:\n            raise ValueError(f\"Got lora_request {lora_request} but LoRA is \"\n                             \"not enabled!\")\n\n    def _validate_structured_output(self, params: SamplingParams) -> None:\n        if not params.guided_decoding or not self.decoding_config:\n            return\n\n        engine_level_backend = self.decoding_config.backend\n        if params.guided_decoding.backend:\n            # Request-level backend selection is not supported in V1.\n            # The values may differ if `params` is reused and was set\n            # to a specific backend based on `auto` behavior in a previous\n            # request. We remember that it was set as a result of `auto`\n            # using the `_auto` option set on the backend in the params.\n            if (params.guided_decoding.backend != engine_level_backend\n                    and not (engine_level_backend == \"auto\"\n                             and params.guided_decoding.backend_was_auto)):\n                raise ValueError(\n                    \"Request-level structured output backend selection is no \"\n                    \"longer supported. The request specified \"\n                    f\"'{params.guided_decoding.backend}', but vLLM was \"\n                    f\"initialised with '{engine_level_backend}'. This error \"\n                    \"can be resolved by removing backend selection from the \"\n                    \"request.\")\n        else:\n            params.guided_decoding.backend = engine_level_backend\n\n        # Request content validation\n        if engine_level_backend.startswith(\"xgrammar\"):\n            # xgrammar with no fallback\n            validate_xgrammar_grammar(params)\n        elif engine_level_backend.startswith(\"guidance\"):\n            # TODO: ideally we would have the LLTokenizer here as Lark syntax\n            # allows <|special_token|> and similar, see\n            # https://github.com/guidance-ai/llguidance/blob/main/docs/syntax.md#special-tokens\n            # Without tokenizer these are disallowed in grammars.\n            validate_guidance_grammar(params, tokenizer=None)\n        else:\n            # NOTE: engine_level_backend must be \"auto\" here, because we have\n            # checked supported_backends above.\n            # \"auto\" is an opt-in to opinionated behavior where we try to\n            # choose a backend based on request contents. This is not the\n            # default as it is less predictable and subject to change\n            # between releases as feature support changes.\n            try:\n                validate_xgrammar_grammar(params)\n                params.guided_decoding.backend = \"xgrammar\"\n            except ValueError:\n                # The request either failed validation\n                # or includes some jsonschema feature(s) that\n                # are not supported in xgrammar. Fall back to guidance.\n                validate_guidance_grammar(params, tokenizer=None)\n                params.guided_decoding.backend = \"guidance\"\n            # Remember that this backend was set automatically\n            params.guided_decoding.backend_was_auto = True\n\n    def process_inputs(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> tuple[Optional[str], EngineCoreRequest]:\n\n        # TODO(woosuk): Support pooling models.\n        # TODO(woosuk): Support encoder-decoder models.\n        self._validate_lora(lora_request)\n        self._validate_params(params, lora_request)\n        if priority != 0:\n            raise ValueError(\"V1 does not support priority yet.\")\n        if trace_headers is not None:\n            raise ValueError(\"V1 does not support tracing yet.\")\n        if prompt_adapter_request is not None:\n            raise ValueError(\"V1 does not support prompt_adapter_request.\")\n\n        if arrival_time is None:\n            arrival_time = time.time()\n\n        # Process inputs, which includes:\n        # 1. Tokenize text prompt, with LoRA request if one exists.\n        # 2. For multimodal models with a merged preprocessor, preprocess\n        #   multimodal data and expand prompt token ids accordingly.\n        # 3. Apply prompt adapter to prompt token ids if one exists.\n        processed_inputs: ProcessorInputs = self.input_preprocessor.preprocess(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            return_mm_hashes=self.use_hash,\n        )\n        from vllm.platforms import current_platform\n        current_platform.validate_request(\n            prompt=prompt,\n            params=params,\n            processed_inputs=processed_inputs,\n        )\n        eos_token_id = self.input_preprocessor.get_eos_token_id(lora_request)\n\n        self._validate_model_inputs(processed_inputs, lora_request)\n\n        encoder_inputs, decoder_inputs = split_enc_dec_inputs(processed_inputs)\n\n        # TODO: Impl encoder-decoder\n        if encoder_inputs is not None:\n            raise NotImplementedError\n\n        assert isinstance(params, SamplingParams)\n        # TODO: can we avoid cloning here in multiproc case?\n        sampling_params = params.clone()\n        # If unset max tokens, then generate up to the max_model_len.\n        if sampling_params.max_tokens is None:\n            sampling_params.max_tokens = (\n                self.model_config.max_model_len -\n                len(decoder_inputs[\"prompt_token_ids\"]))\n        sampling_params.update_from_generation_config(\n            self.generation_config_fields, eos_token_id)\n        sampling_params.update_from_tokenizer(\n            self.tokenizer.get_lora_tokenizer(lora_request))\n\n        # Multimodal related.\n        sorted_mm_inputs: Optional[Sequence[Optional[MultiModalKwargs]]] = None\n        sorted_mm_positions: Optional[list[PlaceholderRange]] = None\n        sorted_mm_hashes: Optional[list[str]] = None\n        if decoder_inputs[\"type\"] == \"multimodal\":\n            decoder_mm_inputs = decoder_inputs[\"mm_kwargs\"]\n\n            # Merge and flatten multimodal placeholders, hashes and inputs\n            # from dictionaries to lists, and sort them by each item's position\n            # in the input sequence.\n            (\n                sorted_item_modalities,\n                sorted_mm_positions,\n                sorted_mm_hashes,\n            ) = merge_and_sort_multimodal_metadata(\n                decoder_inputs[\"mm_placeholders\"],\n                decoder_inputs[\"mm_hashes\"] if self.use_hash else None,\n            )\n\n            # The output of merged multi-modal processor (`decoder_mm_inputs`)\n            # is a single MultiModalKwargs for all items from all modalities.\n            # This code flattens kwargs for individual items in a list and\n            # sorts them by each item's position in the input sequence if there\n            # are multiple modalities.\n            unique_modalities = set(sorted_item_modalities)\n            if len(unique_modalities) > 1:\n                orig_sorted_mm_inputs = []\n                used_indices = {modality: 0 for modality in unique_modalities}\n\n                for modality in sorted_item_modalities:\n                    items = decoder_mm_inputs.get_items(modality)\n                    item = items[used_indices[modality]]\n\n                    orig_sorted_mm_inputs.append(\n                        MultiModalKwargs.from_items([item]))\n                    used_indices[modality] += 1\n            else:\n                orig_sorted_mm_inputs = [\n                    MultiModalKwargs.from_items([item]) for item in\n                    decoder_mm_inputs.get_items(sorted_item_modalities[0])\n                ]\n\n            if sorted_mm_hashes is not None:\n                sorted_mm_inputs = self.mm_input_cache_client.get_and_update_p0(\n                    orig_sorted_mm_inputs, sorted_mm_hashes)\n            else:\n                sorted_mm_inputs = orig_sorted_mm_inputs\n\n        return decoder_inputs.get(\"prompt\"), EngineCoreRequest(\n            request_id=request_id,\n            prompt_token_ids=decoder_inputs[\"prompt_token_ids\"],\n            mm_inputs=sorted_mm_inputs,\n            mm_hashes=sorted_mm_hashes,\n            mm_placeholders=sorted_mm_positions,\n            sampling_params=sampling_params,\n            eos_token_id=eos_token_id,\n            arrival_time=arrival_time,\n            lora_request=lora_request,\n            cache_salt=decoder_inputs.get(\"cache_salt\"),\n        )\n\n    def _validate_model_inputs(self,\n                               inputs: ProcessorInputs,\n                               lora_request: Optional[LoRARequest] = None):\n        encoder_inputs, decoder_inputs = split_enc_dec_inputs(inputs)\n\n        if encoder_inputs is not None:\n            self._validate_model_input(encoder_inputs,\n                                       lora_request,\n                                       prompt_type=\"encoder\")\n\n        self._validate_model_input(decoder_inputs,\n                                   lora_request,\n                                   prompt_type=\"decoder\")\n\n    def _validate_model_input(\n        self,\n        prompt_inputs: SingletonInputs,\n        lora_request: Optional[LoRARequest],\n        *,\n        prompt_type: Literal[\"encoder\", \"decoder\"],\n    ):\n        model_config = self.model_config\n        tokenizer = self.tokenizer.get_lora_tokenizer(lora_request)\n\n        prompt_ids = prompt_inputs[\"prompt_token_ids\"]\n        if not prompt_ids:\n            if prompt_type == \"encoder\" and model_config.is_multimodal_model:\n                pass  # Mllama may have empty encoder inputs for text-only data\n            else:\n                raise ValueError(f\"The {prompt_type} prompt cannot be empty\")\n\n        max_input_id = max(prompt_ids, default=0)\n        if max_input_id > tokenizer.max_token_id:\n            raise ValueError(f\"Token id {max_input_id} is out of vocabulary\")\n\n        max_prompt_len = self.model_config.max_model_len\n        if len(prompt_ids) > max_prompt_len:\n            if prompt_type == \"encoder\" and model_config.is_multimodal_model:\n                mm_registry = self.input_preprocessor.mm_registry\n                mm_processor = mm_registry.create_processor(\n                    model_config,\n                    tokenizer=tokenizer,\n                )\n                assert isinstance(mm_processor, EncDecMultiModalProcessor)\n\n                if mm_processor.pad_dummy_encoder_prompt:\n                    return  # Skip encoder length check for Whisper\n\n            if model_config.is_multimodal_model:\n                suggestion = (\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens plus multimodal tokens. For image \"\n                    \"inputs, the number of image tokens depends on the number \"\n                    \"of images, and possibly their aspect ratios as well.\")\n            else:\n                suggestion = (\n                    \"Make sure that `max_model_len` is no smaller than the \"\n                    \"number of text tokens.\")\n\n            raise ValueError(\n                f\"The {prompt_type} prompt (length {len(prompt_ids)}) is \"\n                f\"longer than the maximum model length of {max_prompt_len}. \"\n                f\"{suggestion}\")\n\n            # TODO: Find out how many placeholder tokens are there so we can\n            # check that chunked prefill does not truncate them\n            # max_batch_len = self.scheduler_config.max_num_batched_tokens\n", 398], "/home/jeromeku/vllm/vllm/transformers_utils/config.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport enum\nimport json\nimport os\nimport time\nfrom functools import cache\nfrom pathlib import Path\nfrom typing import Any, Callable, Literal, Optional, Union\n\nimport huggingface_hub\nfrom huggingface_hub import hf_hub_download\nfrom huggingface_hub import list_repo_files as hf_list_repo_files\nfrom huggingface_hub import try_to_load_from_cache\nfrom huggingface_hub.utils import (EntryNotFoundError, HfHubHTTPError,\n                                   HFValidationError, LocalEntryNotFoundError,\n                                   RepositoryNotFoundError,\n                                   RevisionNotFoundError)\nfrom torch import nn\nfrom transformers import GenerationConfig, PretrainedConfig\nfrom transformers.models.auto.image_processing_auto import (\n    get_image_processor_config)\nfrom transformers.models.auto.modeling_auto import (\n    MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\nfrom transformers.utils import CONFIG_NAME as HF_CONFIG_NAME\n\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\n# yapf conflicts with isort for this block\n# yapf: disable\nfrom vllm.transformers_utils.configs import (ChatGLMConfig, Cohere2Config,\n                                             DbrxConfig, DeepseekVLV2Config,\n                                             EAGLEConfig, ExaoneConfig,\n                                             H2OVLChatConfig,\n                                             InternVLChatConfig, JAISConfig,\n                                             KimiVLConfig, MedusaConfig,\n                                             MiniMaxText01Config,\n                                             MiniMaxVL01Config, MllamaConfig,\n                                             MLPSpeculatorConfig, MPTConfig,\n                                             NemotronConfig, NVLM_D_Config,\n                                             OvisConfig, RWConfig,\n                                             SkyworkR1VChatConfig, SolarConfig,\n                                             Telechat2Config, UltravoxConfig)\n# yapf: enable\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.utils import resolve_obj_by_qualname\n\nif VLLM_USE_MODELSCOPE:\n    from modelscope import AutoConfig\nelse:\n    from transformers import AutoConfig\n\nMISTRAL_CONFIG_NAME = \"params.json\"\nHF_TOKEN = os.getenv('HF_TOKEN', None)\n\nlogger = init_logger(__name__)\n\n_CONFIG_REGISTRY_OVERRIDE_HF: dict[str, type[PretrainedConfig]] = {\n    \"mllama\": MllamaConfig\n}\n\n_CONFIG_REGISTRY: dict[str, type[PretrainedConfig]] = {\n    \"chatglm\": ChatGLMConfig,\n    \"cohere2\": Cohere2Config,\n    \"dbrx\": DbrxConfig,\n    \"deepseek_vl_v2\": DeepseekVLV2Config,\n    \"kimi_vl\": KimiVLConfig,\n    \"mpt\": MPTConfig,\n    \"RefinedWeb\": RWConfig,  # For tiiuae/falcon-40b(-instruct)\n    \"RefinedWebModel\": RWConfig,  # For tiiuae/falcon-7b(-instruct)\n    \"jais\": JAISConfig,\n    \"mlp_speculator\": MLPSpeculatorConfig,\n    \"medusa\": MedusaConfig,\n    \"eagle\": EAGLEConfig,\n    \"exaone\": ExaoneConfig,\n    \"h2ovl_chat\": H2OVLChatConfig,\n    \"internvl_chat\": InternVLChatConfig,\n    \"minimax_text_01\": MiniMaxText01Config,\n    \"minimax_vl_01\": MiniMaxVL01Config,\n    \"nemotron\": NemotronConfig,\n    \"NVLM_D\": NVLM_D_Config,\n    \"ovis\": OvisConfig,\n    \"solar\": SolarConfig,\n    \"skywork_chat\": SkyworkR1VChatConfig,\n    \"telechat\": Telechat2Config,\n    \"ultravox\": UltravoxConfig,\n    **_CONFIG_REGISTRY_OVERRIDE_HF\n}\n\n\nclass ConfigFormat(str, enum.Enum):\n    AUTO = \"auto\"\n    HF = \"hf\"\n    MISTRAL = \"mistral\"\n\n\ndef with_retry(func: Callable[[], Any],\n               log_msg: str,\n               max_retries: int = 2,\n               retry_delay: int = 2):\n    for attempt in range(max_retries):\n        try:\n            return func()\n        except Exception as e:\n            if attempt == max_retries - 1:\n                logger.error(\"%s: %s\", log_msg, e)\n                raise\n            logger.error(\"%s: %s, retrying %d of %d\", log_msg, e, attempt + 1,\n                         max_retries)\n            time.sleep(retry_delay)\n            retry_delay *= 2\n\n\n# @cache doesn't cache exceptions\n@cache\ndef list_repo_files(\n    repo_id: str,\n    *,\n    revision: Optional[str] = None,\n    repo_type: Optional[str] = None,\n    token: Union[str, bool, None] = None,\n) -> list[str]:\n\n    def lookup_files() -> list[str]:\n        # directly list files if model is local\n        if (local_path := Path(repo_id)).exists():\n            return [\n                str(file.relative_to(local_path))\n                for file in local_path.rglob('*') if file.is_file()\n            ]\n        # if model is remote, use hf_hub api to list files\n        try:\n            if VLLM_USE_MODELSCOPE:\n                from vllm.transformers_utils.utils import (\n                    modelscope_list_repo_files)\n                return modelscope_list_repo_files(repo_id,\n                                                  revision=revision,\n                                                  token=token)\n            return hf_list_repo_files(repo_id,\n                                      revision=revision,\n                                      repo_type=repo_type,\n                                      token=token)\n        except huggingface_hub.errors.OfflineModeIsEnabled:\n            # Don't raise in offline mode,\n            # all we know is that we don't have this\n            # file cached.\n            return []\n\n    return with_retry(lookup_files, \"Error retrieving file list\")\n\n\ndef file_exists(\n    repo_id: str,\n    file_name: str,\n    *,\n    repo_type: Optional[str] = None,\n    revision: Optional[str] = None,\n    token: Union[str, bool, None] = None,\n) -> bool:\n    file_list = list_repo_files(repo_id,\n                                repo_type=repo_type,\n                                revision=revision,\n                                token=token)\n    return file_name in file_list\n\n\n# In offline mode the result can be a false negative\ndef file_or_path_exists(model: Union[str, Path], config_name: str,\n                        revision: Optional[str]) -> bool:\n    if (local_path := Path(model)).exists():\n        return (local_path / config_name).is_file()\n\n    # Offline mode support: Check if config file is cached already\n    cached_filepath = try_to_load_from_cache(repo_id=model,\n                                             filename=config_name,\n                                             revision=revision)\n    if isinstance(cached_filepath, str):\n        # The config file exists in cache- we can continue trying to load\n        return True\n\n    # NB: file_exists will only check for the existence of the config file on\n    # hf_hub. This will fail in offline mode.\n\n    # Call HF to check if the file exists\n    return file_exists(str(model),\n                       config_name,\n                       revision=revision,\n                       token=HF_TOKEN)\n\n\ndef patch_rope_scaling(config: PretrainedConfig) -> None:\n    \"\"\"Provide backwards compatibility for RoPE.\"\"\"\n    text_config = getattr(config, \"text_config\", None)\n    if text_config is not None:\n        patch_rope_scaling(text_config)\n\n    rope_scaling = getattr(config, \"rope_scaling\", None)\n    if rope_scaling is not None:\n        patch_rope_scaling_dict(rope_scaling)\n\n\ndef patch_rope_scaling_dict(rope_scaling: dict[str, Any]) -> None:\n    if \"rope_type\" in rope_scaling and \"type\" in rope_scaling:\n        rope_type = rope_scaling[\"rope_type\"]\n        rope_type_legacy = rope_scaling[\"type\"]\n        if rope_type != rope_type_legacy:\n            raise ValueError(\n                f\"Found conflicts between 'rope_type={rope_type}' (modern \"\n                f\"field) and 'type={rope_type_legacy}' (legacy field). \"\n                \"You should only specify one of them.\")\n\n    if \"rope_type\" not in rope_scaling and \"type\" in rope_scaling:\n        rope_scaling[\"rope_type\"] = rope_scaling[\"type\"]\n        logger.info(\"Replacing legacy 'type' key with 'rope_type'\")\n\n    if \"rope_type\" not in rope_scaling:\n        raise ValueError(\"rope_scaling should have a 'rope_type' key\")\n\n    if rope_scaling[\"rope_type\"] == \"su\":\n        rope_scaling[\"rope_type\"] = \"longrope\"\n        logger.warning(\"Replacing legacy rope_type 'su' with 'longrope'\")\n    elif rope_scaling[\"rope_type\"] == \"mrope\":\n        assert \"mrope_section\" in rope_scaling\n        rope_scaling[\"rope_type\"] = \"default\"\n        logger.warning(\"Replacing legacy rope_type 'mrope' with 'default'\")\n\n\ndef _uses_mrope(config: PretrainedConfig) -> bool:\n    rope_scaling = getattr(config, \"rope_scaling\", None)\n    if rope_scaling is None:\n        return False\n\n    return \"mrope_section\" in rope_scaling\n\n\ndef uses_mrope(config: PretrainedConfig) -> bool:\n    \"\"\"Detect if the model with this config uses M-ROPE.\"\"\"\n    return _uses_mrope(config) or thinker_uses_mrope(config)\n\n\ndef thinker_uses_mrope(config: PretrainedConfig) -> bool:\n    \"\"\"Detect if the model contains a thinker config and it uses M-ROPE.\"\"\"\n    thinker_config = getattr(config, \"thinker_config\", None)\n    if thinker_config is None:\n        return False\n\n    thinker_text_config = getattr(thinker_config, \"text_config\", None)\n    if thinker_text_config is None:\n        return False\n\n    return uses_mrope(thinker_text_config)\n\n\ndef is_encoder_decoder(config: PretrainedConfig) -> bool:\n    \"\"\"Detect if the model with this config is used as an encoder/decoder.\"\"\"\n    text_config = getattr(config, \"text_config\", None)\n    if text_config is not None:\n        return is_encoder_decoder(text_config)\n\n    return getattr(config, \"is_encoder_decoder\", False)\n\n\ndef get_config(\n    model: Union[str, Path],\n    trust_remote_code: bool,\n    revision: Optional[str] = None,\n    code_revision: Optional[str] = None,\n    config_format: ConfigFormat = ConfigFormat.AUTO,\n    **kwargs,\n) -> PretrainedConfig:\n    # Separate model folder from file path for GGUF models\n\n    is_gguf = check_gguf_file(model)\n    if is_gguf:\n        kwargs[\"gguf_file\"] = Path(model).name\n        model = Path(model).parent\n\n    if config_format == ConfigFormat.AUTO:\n        try:\n            if is_gguf or file_or_path_exists(\n                    model, HF_CONFIG_NAME, revision=revision):\n                config_format = ConfigFormat.HF\n            elif file_or_path_exists(model,\n                                     MISTRAL_CONFIG_NAME,\n                                     revision=revision):\n                config_format = ConfigFormat.MISTRAL\n            else:\n                raise ValueError(\n                    \"Could not detect config format for no config file found. \"\n                    \"Ensure your model has either config.json (HF format) \"\n                    \"or params.json (Mistral format).\")\n\n        except Exception as e:\n            error_message = (\n                \"Invalid repository ID or local directory specified:\"\n                \" '{model}'.\\nPlease verify the following requirements:\\n\"\n                \"1. Provide a valid Hugging Face repository ID.\\n\"\n                \"2. Specify a local directory that contains a recognized \"\n                \"configuration file.\\n\"\n                \"   - For Hugging Face models: ensure the presence of a \"\n                \"'config.json'.\\n\"\n                \"   - For Mistral models: ensure the presence of a \"\n                \"'params.json'.\\n\").format(model=model)\n\n            raise ValueError(error_message) from e\n\n    if config_format == ConfigFormat.HF:\n        config_dict, _ = PretrainedConfig.get_config_dict(\n            model,\n            revision=revision,\n            code_revision=code_revision,\n            token=HF_TOKEN,\n            **kwargs,\n        )\n\n        # Use custom model class if it's in our registry\n        model_type = config_dict.get(\"model_type\")\n        if model_type in _CONFIG_REGISTRY:\n            config_class = _CONFIG_REGISTRY[model_type]\n            config = config_class.from_pretrained(\n                model,\n                revision=revision,\n                code_revision=code_revision,\n                token=HF_TOKEN,\n                **kwargs,\n            )\n        else:\n            try:\n                config = AutoConfig.from_pretrained(\n                    model,\n                    trust_remote_code=trust_remote_code,\n                    revision=revision,\n                    code_revision=code_revision,\n                    token=HF_TOKEN,\n                    **kwargs,\n                )\n            except ValueError as e:\n                if (not trust_remote_code\n                        and \"requires you to execute the configuration file\"\n                        in str(e)):\n                    err_msg = (\n                        \"Failed to load the model config. If the model \"\n                        \"is a custom model not yet available in the \"\n                        \"HuggingFace transformers library, consider setting \"\n                        \"`trust_remote_code=True` in LLM or using the \"\n                        \"`--trust-remote-code` flag in the CLI.\")\n                    raise RuntimeError(err_msg) from e\n                else:\n                    raise e\n\n    elif config_format == ConfigFormat.MISTRAL:\n        config = load_params_config(model, revision, token=HF_TOKEN, **kwargs)\n    else:\n        supported_formats = [\n            fmt.value for fmt in ConfigFormat if fmt != ConfigFormat.AUTO\n        ]\n        raise ValueError(\n            f\"Unsupported config format: {config_format}. \"\n            f\"Supported formats are: {', '.join(supported_formats)}. \"\n            f\"Ensure your model uses one of these configuration formats \"\n            f\"or specify the correct format explicitly.\")\n\n    # Special architecture mapping check for GGUF models\n    if is_gguf:\n        if config.model_type not in MODEL_FOR_CAUSAL_LM_MAPPING_NAMES:\n            raise RuntimeError(\n                f\"Can't get gguf config for {config.model_type}.\")\n        model_type = MODEL_FOR_CAUSAL_LM_MAPPING_NAMES[config.model_type]\n        config.update({\"architectures\": [model_type]})\n\n    patch_rope_scaling(config)\n\n    if trust_remote_code:\n        maybe_register_config_serialize_by_value()\n\n    return config\n\n\ndef try_get_local_file(model: Union[str, Path],\n                       file_name: str,\n                       revision: Optional[str] = 'main') -> Optional[Path]:\n    file_path = Path(model) / file_name\n    if file_path.is_file():\n        return file_path\n    else:\n        try:\n            cached_filepath = try_to_load_from_cache(repo_id=model,\n                                                     filename=file_name,\n                                                     revision=revision)\n            if isinstance(cached_filepath, str):\n                return Path(cached_filepath)\n        except HFValidationError:\n            ...\n    return None\n\n\ndef get_hf_file_to_dict(file_name: str,\n                        model: Union[str, Path],\n                        revision: Optional[str] = 'main'):\n    \"\"\"\n    Downloads a file from the Hugging Face Hub and returns\n    its contents as a dictionary.\n\n    Parameters:\n    - file_name (str): The name of the file to download.\n    - model (str): The name of the model on the Hugging Face Hub.\n    - revision (str): The specific version of the model.\n\n    Returns:\n    - config_dict (dict): A dictionary containing\n    the contents of the downloaded file.\n    \"\"\"\n\n    file_path = try_get_local_file(model=model,\n                                   file_name=file_name,\n                                   revision=revision)\n\n    if file_path is None:\n        try:\n            hf_hub_file = hf_hub_download(model, file_name, revision=revision)\n        except huggingface_hub.errors.OfflineModeIsEnabled:\n            return None\n        except (RepositoryNotFoundError, RevisionNotFoundError,\n                EntryNotFoundError, LocalEntryNotFoundError) as e:\n            logger.debug(\"File or repository not found in hf_hub_download\", e)\n            return None\n        except HfHubHTTPError as e:\n            logger.warning(\n                \"Cannot connect to Hugging Face Hub. Skipping file \"\n                \"download for '%s':\",\n                file_name,\n                exc_info=e)\n            return None\n        file_path = Path(hf_hub_file)\n\n    if file_path is not None and file_path.is_file():\n        with open(file_path) as file:\n            return json.load(file)\n\n    return None\n\n\n@cache\ndef get_pooling_config(model: str, revision: Optional[str] = 'main'):\n    \"\"\"\n    This function gets the pooling and normalize\n    config from the model - only applies to\n    sentence-transformers models.\n\n    Args:\n        model (str): The name of the Hugging Face model.\n        revision (str, optional): The specific version\n        of the model to use. Defaults to 'main'.\n\n    Returns:\n        dict: A dictionary containing the pooling\n        type and whether normalization is used.\n    \"\"\"\n\n    modules_file_name = \"modules.json\"\n\n    modules_dict = None\n    if file_or_path_exists(model=model,\n                           config_name=modules_file_name,\n                           revision=revision):\n        modules_dict = get_hf_file_to_dict(modules_file_name, model, revision)\n\n    if modules_dict is None:\n        return None\n\n    logger.info(\"Found sentence-transformers modules configuration.\")\n\n    pooling = next((item for item in modules_dict\n                    if item[\"type\"] == \"sentence_transformers.models.Pooling\"),\n                   None)\n    normalize = bool(\n        next((item for item in modules_dict\n              if item[\"type\"] == \"sentence_transformers.models.Normalize\"),\n             False))\n\n    if pooling:\n\n        pooling_file_name = \"{}/config.json\".format(pooling[\"path\"])\n        pooling_dict = get_hf_file_to_dict(pooling_file_name, model, revision)\n        pooling_type_name = next(\n            (item for item, val in pooling_dict.items() if val is True), None)\n\n        if pooling_type_name is not None:\n            pooling_type_name = get_pooling_config_name(pooling_type_name)\n\n        logger.info(\"Found pooling configuration.\")\n        return {\"pooling_type\": pooling_type_name, \"normalize\": normalize}\n\n    return None\n\n\ndef get_pooling_config_name(pooling_name: str) -> Union[str, None]:\n    if \"pooling_mode_\" in pooling_name:\n        pooling_name = pooling_name.replace(\"pooling_mode_\", \"\")\n\n    if \"_\" in pooling_name:\n        pooling_name = pooling_name.split(\"_\")[0]\n\n    if \"lasttoken\" in pooling_name:\n        pooling_name = \"last\"\n\n    supported_pooling_types = ['LAST', 'ALL', 'CLS', 'STEP', 'MEAN']\n    pooling_type_name = pooling_name.upper()\n\n    try:\n        if pooling_type_name in supported_pooling_types:\n            return pooling_type_name\n    except NotImplementedError as e:\n        logger.debug(\"Pooling type not supported\", e)\n        return None\n    return None\n\n\n@cache\ndef get_sentence_transformer_tokenizer_config(model: str,\n                                              revision: Optional[str] = 'main'\n                                              ):\n    \"\"\"\n    Returns the tokenization configuration dictionary for a\n    given Sentence Transformer BERT model.\n\n    Parameters:\n    - model (str): The name of the Sentence Transformer\n    BERT model.\n    - revision (str, optional): The revision of the m\n    odel to use. Defaults to 'main'.\n\n    Returns:\n    - dict: A dictionary containing the configuration parameters\n    for the Sentence Transformer BERT model.\n    \"\"\"\n    sentence_transformer_config_files = [\n        \"sentence_bert_config.json\",\n        \"sentence_roberta_config.json\",\n        \"sentence_distilbert_config.json\",\n        \"sentence_camembert_config.json\",\n        \"sentence_albert_config.json\",\n        \"sentence_xlm-roberta_config.json\",\n        \"sentence_xlnet_config.json\",\n    ]\n    encoder_dict = None\n\n    for config_file in sentence_transformer_config_files:\n        if try_get_local_file(model=model,\n                              file_name=config_file,\n                              revision=revision) is not None:\n            encoder_dict = get_hf_file_to_dict(config_file, model, revision)\n            if encoder_dict:\n                break\n\n    if not encoder_dict and not model.startswith(\"/\"):\n        try:\n            # If model is on HuggingfaceHub, get the repo files\n            repo_files = list_repo_files(model,\n                                         revision=revision,\n                                         token=HF_TOKEN)\n        except Exception:\n            repo_files = []\n\n        for config_name in sentence_transformer_config_files:\n            if config_name in repo_files:\n                encoder_dict = get_hf_file_to_dict(config_name, model,\n                                                   revision)\n                if encoder_dict:\n                    break\n\n    if not encoder_dict:\n        return None\n\n    logger.info(\"Found sentence-transformers tokenize configuration.\")\n\n    if all(k in encoder_dict for k in (\"max_seq_length\", \"do_lower_case\")):\n        return encoder_dict\n    return None\n\n\ndef maybe_register_config_serialize_by_value() -> None:\n    \"\"\"Try to register HF model configuration class to serialize by value\n\n        If trust_remote_code is set, and the model's config file specifies an\n        `AutoConfig` class, then the config class is typically an instance of\n        a custom class imported from the HF modules cache.\n\n        Examples:\n\n        >>> from transformers import AutoConfig\n        >>> klass = AutoConfig.from_pretrained('meta-llama/Meta-Llama-3-8B', trust_remote_code=True)\n        >>> klass.__class__ # transformers.models.llama.configuration_llama.LlamaConfig\n        >>> import transformers_modules # error, not initialized\n        >>> klass = AutoConfig.from_pretrained('deepseek-ai/DeepSeek-V2.5', trust_remote_code=True)\n        >>> import transformers_modules # success, initialized\n        >>> klass.__class__ # transformers_modules.deepseek-ai.DeepSeek-V2.5.98b11844770b2c3ffc18b175c758a803640f4e77.configuration_deepseek.DeepseekV2Config\n\n        In the DeepSeek example, the config class is an instance of a custom\n        class that is not serializable by default. This class will not be\n        importable in spawned workers, and won't exist at all on\n        other nodes, which breaks serialization of the config.\n\n        In this function we tell the cloudpickle serialization library to pass\n        instances of these generated classes by value instead of by reference,\n        i.e. the class definition is serialized along with its data so that the\n        class module does not need to be importable on the receiving end.\n\n        See: https://github.com/cloudpipe/cloudpickle?tab=readme-ov-file#overriding-pickles-serialization-mechanism-for-importable-constructs\n    \"\"\" # noqa\n    try:\n        import transformers_modules\n    except ImportError:\n        # the config does not need trust_remote_code\n        return\n\n    try:\n        import cloudpickle\n        cloudpickle.register_pickle_by_value(transformers_modules)\n\n        # ray vendors its own version of cloudpickle\n        from vllm.executor.ray_utils import ray\n        if ray:\n            ray.cloudpickle.register_pickle_by_value(transformers_modules)\n\n        # multiprocessing uses pickle to serialize arguments when using spawn\n        # Here we get pickle to use cloudpickle to serialize config objects\n        # that contain instances of the custom config class to avoid\n        # serialization problems if the generated module (and model) has a `.`\n        # in its name\n        import multiprocessing\n        import pickle\n\n        from vllm.config import VllmConfig\n\n        def _reduce_config(config: VllmConfig):\n            return (pickle.loads, (cloudpickle.dumps(config), ))\n\n        multiprocessing.reducer.register(VllmConfig, _reduce_config)\n\n    except Exception as e:\n        logger.warning(\n            \"Unable to register remote classes used by\"\n            \" trust_remote_code with by-value serialization. This may\"\n            \" lead to a later error. If remote code is not needed\"\n            \" remove `--trust-remote-code`\",\n            exc_info=e)\n\n\ndef load_params_config(model: Union[str, Path], revision: Optional[str],\n                       **kwargs) -> PretrainedConfig:\n    # This function loads a params.json config which\n    # should be used when loading models in mistral format\n\n    config_file_name = \"params.json\"\n\n    config_dict = get_hf_file_to_dict(config_file_name, model, revision)\n    if config_dict is None:\n        raise ValueError(\n            f\"Failed to load mistral '{config_file_name}' config for model \"\n            f\"{model}. Please check if the model is a mistral-format model \"\n            f\"and if the config file exists.\")\n    assert isinstance(config_dict, dict)\n\n    config_mapping = {\n        \"dim\": \"hidden_size\",\n        \"norm_eps\": \"rms_norm_eps\",\n        \"n_kv_heads\": \"num_key_value_heads\",\n        \"n_layers\": \"num_hidden_layers\",\n        \"n_heads\": \"num_attention_heads\",\n        \"hidden_dim\": \"intermediate_size\",\n    }\n\n    def recurse_elems(elem: Any):\n        if isinstance(elem, dict):\n            config_dict = {}\n            for key, value in elem.items():\n                key = config_mapping.get(key, key)\n                config_dict[key] = recurse_elems(value)\n\n            return config_dict\n        else:\n            return elem\n\n    config_dict[\"model_type\"] = config_dict.get(\"model_type\", \"transformer\")\n    config_dict[\"hidden_act\"] = config_dict.get(\"activation\", \"silu\")\n    config_dict[\"tie_word_embeddings\"] = config_dict.get(\n        \"tie_embeddings\", False)\n\n    if config_dict.get(\"max_position_embeddings\") is None:\n        max_position_embeddings = 128_000\n        try:\n            trust_remote_code_val = kwargs.get(\"trust_remote_code\", False)\n            hf_config = get_config(model=model,\n                                   trust_remote_code=trust_remote_code_val,\n                                   revision=revision,\n                                   config_format=ConfigFormat.HF)\n            if hf_value := hf_config.get_text_config().max_position_embeddings:\n                max_position_embeddings = hf_value\n        except Exception as e:\n            logger.warning(\n                \"The params.json file is missing 'max_position_embeddings'\"\n                \" and could not get a value from the HF config.\"\n                \" Defaulting to 128000\",\n                exc_info=e)\n        config_dict[\"max_position_embeddings\"] = max_position_embeddings\n\n    if config_dict.get(\"quantization\") is not None:\n        quantization = config_dict.get(\"quantization\", {})\n        if quantization.get(\"qformat_weight\") == \"fp8_e4m3\":\n            # This maps to the FP8 static per-tensor quantization scheme\n            quantization_config = {\n                \"quant_method\": \"fp8\",\n                \"activation_scheme\": \"static\"\n            }\n        elif quantization.get(\"quant_method\") == \"compressed-tensors\":\n            # Pass through the quantization config to compressed-tensors\n            quantization_config = quantization\n        else:\n            raise ValueError(\n                f\"Found unknown quantization='{quantization}' in config\")\n\n        config_dict[\"quantization_config\"] = quantization_config\n\n    config_type: Literal[\"text\",\n                         \"multimodal\"] = \"multimodal\" if config_dict.get(\n                             \"vision_encoder\") is not None else \"text\"\n\n    if config_dict.get(\"moe\") is not None:\n        config_dict[\"architectures\"] = [\"MixtralForCausalLM\"]\n    else:\n        config_dict[\"architectures\"] = [\"MistralForCausalLM\"]\n\n    if config_type == \"multimodal\":\n        multimodal_config = config_dict.pop(\"vision_encoder\")\n        quantization_config = config_dict.get(\"quantization_config\", {})\n\n        config_dict = {\n            \"text_config\": config_dict,\n            \"vision_config\": multimodal_config\n        }\n        config_dict[\"architectures\"] = [\"PixtralForConditionalGeneration\"]\n        config_dict[\"model_type\"] = \"pixtral\"\n        if quantization_config:\n            config_dict[\"quantization_config\"] = quantization_config\n\n    config_dict.update(kwargs)\n\n    config_dict = recurse_elems(config_dict)\n\n    # transform to HF config format\n    if config_type == \"multimodal\":\n        config_dict[\"text_config\"] = PretrainedConfig(\n            **config_dict[\"text_config\"])\n        config_dict[\"vision_config\"] = PretrainedConfig(\n            **config_dict[\"vision_config\"])\n\n    return PretrainedConfig(**config_dict)\n\n\ndef get_hf_image_processor_config(\n    model: Union[str, Path],\n    hf_token: Optional[Union[bool, str]] = None,\n    revision: Optional[str] = None,\n    **kwargs,\n) -> dict[str, Any]:\n    # ModelScope does not provide an interface for image_processor\n    if VLLM_USE_MODELSCOPE:\n        return dict()\n    # Separate model folder from file path for GGUF models\n    if check_gguf_file(model):\n        model = Path(model).parent\n    return get_image_processor_config(model,\n                                      token=hf_token,\n                                      revision=revision,\n                                      **kwargs)\n\n\ndef get_hf_text_config(config: PretrainedConfig):\n    \"\"\"Get the \"sub\" config relevant to llm for multi modal models.\n    No op for pure text models.\n    \"\"\"\n    # This block should be unnecessary after https://github.com/huggingface/transformers/pull/37517\n    if hasattr(config, \"thinker_config\"):\n        # TODO(suyang.fy): Refactor code.\n        #  For Qwen2.5-Omni, change hf_text_config to\n        #  thinker_config.text_config.\n        return config.thinker_config.text_config\n\n    text_config = config.get_text_config()\n\n    if text_config is not config:\n        # The code operates under the assumption that text_config should have\n        # `num_attention_heads` (among others). Assert here to fail early\n        # if transformers config doesn't align with this assumption.\n        assert hasattr(text_config, \"num_attention_heads\")\n\n    return text_config\n\n\ndef try_get_generation_config(\n    model: str,\n    trust_remote_code: bool,\n    revision: Optional[str] = None,\n) -> Optional[GenerationConfig]:\n    try:\n        return GenerationConfig.from_pretrained(\n            model,\n            revision=revision,\n        )\n    except OSError:  # Not found\n        try:\n            config = get_config(\n                model,\n                trust_remote_code=trust_remote_code,\n                revision=revision,\n            )\n            return GenerationConfig.from_model_config(config)\n        except OSError:  # Not found\n            return None\n\n\ndef get_cross_encoder_activation_function(config: PretrainedConfig):\n    if (hasattr(config, \"sbert_ce_default_activation_function\")\n            and config.sbert_ce_default_activation_function is not None):\n\n        function_name = config.sbert_ce_default_activation_function\n        assert function_name.startswith(\"torch.nn.modules.\"), \\\n            \"Loading of activation functions is restricted to \" \\\n            \"torch.nn.modules for security reasons\"\n        return resolve_obj_by_qualname(function_name)()\n    else:\n        return nn.Sigmoid() if config.num_labels == 1 else nn.Identity()\n", 833], "/home/jeromeku/vllm/vllm/inputs/parse.py": ["# SPDX-License-Identifier: Apache-2.0\nfrom collections.abc import Sequence\nfrom typing import Literal, Optional, TypedDict, Union, cast, overload\n\nfrom typing_extensions import TypeIs\n\nfrom vllm.utils import is_list_of\n\nfrom .data import (EmbedsPrompt, ExplicitEncoderDecoderPrompt, ProcessorInputs,\n                   PromptType, SingletonInputs, SingletonPrompt, TextPrompt,\n                   TokensPrompt)\n\n\nclass ParsedText(TypedDict):\n    content: str\n    is_tokens: Literal[False]\n\n\nclass ParsedTokens(TypedDict):\n    content: list[int]\n    is_tokens: Literal[True]\n\n\n@overload\ndef parse_and_batch_prompt(\n        prompt: Union[str, list[str]]) -> Sequence[ParsedText]:\n    ...\n\n\n@overload\ndef parse_and_batch_prompt(\n        prompt: Union[list[int], list[list[int]]]) -> Sequence[ParsedTokens]:\n    ...\n\n\ndef parse_and_batch_prompt(\n    prompt: Union[str, list[str], list[int], list[list[int]]],\n) -> Union[Sequence[ParsedText], Sequence[ParsedTokens]]:\n    if isinstance(prompt, str):\n        # case 1: a string\n        return [ParsedText(content=prompt, is_tokens=False)]\n\n    if isinstance(prompt, list):\n        if len(prompt) == 0:\n            raise ValueError(\"please provide at least one prompt\")\n\n        if is_list_of(prompt, str):\n            # case 2: array of strings\n            prompt = cast(list[str], prompt)\n            return [\n                ParsedText(content=elem, is_tokens=False) for elem in prompt\n            ]\n        if is_list_of(prompt, int):\n            # case 3: array of tokens\n            prompt = cast(list[int], prompt)\n            return [ParsedTokens(content=prompt, is_tokens=True)]\n        if is_list_of(prompt, list):\n            prompt = cast(list[list[int]], prompt)\n            if len(prompt[0]) == 0:\n                raise ValueError(\"please provide at least one prompt\")\n\n            if is_list_of(prompt[0], int):\n                # case 4: array of token arrays\n                return [\n                    ParsedTokens(content=elem, is_tokens=True)\n                    for elem in prompt\n                ]\n\n    raise TypeError(\"prompt must be a string, array of strings, \"\n                    \"array of tokens, or array of token arrays\")\n\n\nclass ParsedStrPrompt(TypedDict):\n    type: Literal[\"str\"]\n    content: str\n\n\nclass ParsedTextPrompt(TypedDict):\n    type: Literal[\"text\"]\n    content: TextPrompt\n\n\nclass ParsedTokensPrompt(TypedDict):\n    type: Literal[\"tokens\"]\n    content: TokensPrompt\n\n\nclass ParsedEmbedsPrompt(TypedDict):\n    type: Literal['embeds']\n    content: EmbedsPrompt\n\n\nParsedSingletonPrompt = Union[ParsedStrPrompt, ParsedTextPrompt,\n                              ParsedTokensPrompt, ParsedEmbedsPrompt]\n\n\n@overload\ndef parse_singleton_prompt(prompt: str) -> ParsedStrPrompt:\n    ...\n\n\n@overload\ndef parse_singleton_prompt(prompt: TextPrompt) -> ParsedTextPrompt:\n    ...\n\n\n@overload\ndef parse_singleton_prompt(prompt: TokensPrompt) -> ParsedTokensPrompt:\n    ...\n\n\n@overload\ndef parse_singleton_prompt(prompt: EmbedsPrompt) -> ParsedEmbedsPrompt:\n    ...\n\n\ndef parse_singleton_prompt(prompt: SingletonPrompt) -> ParsedSingletonPrompt:\n    if isinstance(prompt, str):\n        return ParsedStrPrompt(type=\"str\", content=prompt)\n    elif isinstance(prompt, dict):\n        # Type ignores are because mypy does not correctly infer the TypedDicts\n        # Pyright does succeed.\n        if \"prompt_embeds\" in prompt:\n            return ParsedEmbedsPrompt(\n                type=\"embeds\", content=prompt)  # type: ignore[typeddict-item]\n        elif \"prompt_token_ids\" in prompt:\n            return ParsedTokensPrompt(\n                type=\"tokens\", content=prompt)  # type: ignore[typeddict-item]\n        elif \"prompt\" in prompt:\n            return ParsedTextPrompt(type=\"text\", content=prompt)\n    raise TypeError(\n        \"inputs must be a string, TextPrompt, TokensPrompt, or EmbedsPrompt\")\n\n\ndef is_explicit_encoder_decoder_prompt(\n        prompt: PromptType) -> TypeIs[ExplicitEncoderDecoderPrompt]:\n    return isinstance(prompt, dict) and \"encoder_prompt\" in prompt\n\n\ndef split_enc_dec_inputs(\n    inputs: ProcessorInputs,\n) -> tuple[Optional[SingletonInputs], SingletonInputs]:\n    if \"encoder\" in inputs and \"decoder\" in inputs:\n        # NOTE: This passes pyright but not mypy\n        return (\n            inputs[\"encoder\"],  # type: ignore[typeddict-item]\n            inputs[\"decoder\"],  # type: ignore[typeddict-item]\n        )\n\n    return None, inputs\n", 150], "/home/jeromeku/vllm/vllm/inputs/preprocess.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport asyncio\nfrom collections.abc import Mapping\nfrom typing import Any, Optional, Union, cast\n\nfrom typing_extensions import assert_never\n\nfrom vllm.config import ModelConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.multimodal.inputs import (MultiModalDataDict, MultiModalEncDecInputs,\n                                    MultiModalInputs)\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\n\nfrom .data import (DecoderOnlyInputs, EmbedsInputs, EmbedsPrompt,\n                   EncoderDecoderInputs, ProcessorInputs, PromptType,\n                   SingletonInputs, SingletonPrompt, TextPrompt, TokenInputs,\n                   TokensPrompt, embeds_inputs, token_inputs)\nfrom .parse import is_explicit_encoder_decoder_prompt, parse_singleton_prompt\n\nlogger = init_logger(__name__)\n\n\nclass InputPreprocessor:\n\n    def __init__(\n        self,\n        model_config: ModelConfig,\n        tokenizer: Optional[TokenizerGroup],\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n    ) -> None:\n        super().__init__()\n\n        self.model_config = model_config\n        self.tokenizer = tokenizer\n        self.mm_registry = mm_registry\n\n    def get_tokenizer_group(self) -> TokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(\"You cannot pass text prompts when \"\n                             \"`skip_tokenizer_init` is True\")\n\n        return self.tokenizer\n\n    def get_bos_token_id(self,\n                         lora_request: Optional[LoRARequest] = None\n                         ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for BOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).bos_token_id\n\n    def get_eos_token_id(self,\n                         lora_request: Optional[LoRARequest] = None\n                         ) -> Optional[int]:\n        if self.tokenizer is None:\n            logger.warning(\"Using None for EOS token id because tokenizer \"\n                           \"is not initialized\")\n            return None\n\n        return self.tokenizer.get_lora_tokenizer(lora_request).eos_token_id\n\n    def get_decoder_start_token_id(self) -> Optional[int]:\n        '''\n        Obtain the decoder start token id employed by an encoder/decoder\n        model. Returns None for non-encoder/decoder models or if the\n        model config is unavailable.\n        '''\n\n        if not self.model_config.is_encoder_decoder:\n            logger.warning_once(\n                \"Using None for decoder start token id because \"\n                \"this is not an encoder/decoder model.\")\n            return None\n\n        if (self.model_config is None or self.model_config.hf_config is None):\n            logger.warning_once(\n                \"Using None for decoder start token id because \"\n                \"model config is not available.\")\n            return None\n\n        dec_start_token_id = getattr(self.model_config.hf_config,\n                                     'decoder_start_token_id', None)\n        if dec_start_token_id is None:\n            logger.warning_once(\n                \"Falling back on <BOS> for decoder start token \"\n                \"id because decoder start token id is not \"\n                \"available.\")\n            dec_start_token_id = self.get_bos_token_id()\n\n        return dec_start_token_id\n\n    def _get_default_enc_dec_decoder_prompt(self) -> list[int]:\n        '''\n        Specifically for encoder/decoder models:\n        generate a default decoder prompt for when\n        the user specifies only the encoder prompt.\n\n        Encoder/decoder models utilize the decoder\n        prompt in different ways; as new models are\n        added, it is intended that this function\n        will be extended to produce differing\n        default decoder prompts, depending on the\n        model variety.\n\n        Absent a special case, the default behavior\n        of this method is to mirror the behavior of\n        the HuggingFace (HF) GenerationMixin for a None\n        decoder prompt, which is to employ a logit processor\n        setting to force the first decoded token to be <BOS>.\n        Here, this behavior is approximated by having the\n        \"default\" decoder prompt be <BOS>.\n\n        However, it is possible that in the future\n        other models may have different or more\n        complex logic for the default decoder prompt.\n        This motivates having a special helper method\n        for default decoder prompts.\n\n        Returns:\n\n        * prompt_token_ids\n        '''\n\n        bos_token_id = self.get_bos_token_id()\n        assert bos_token_id is not None\n        return [bos_token_id]\n\n    def _prepare_decoder_input_ids_for_generation(\n        self,\n        decoder_input_ids: Optional[list[int]],\n    ) -> list[int]:\n        \"\"\"\n        Prepares `decoder_input_ids` for generation with encoder-decoder models.\n\n        Based on:\n        https://github.com/huggingface/transformers/blob/4037a2b5b1278736e566aec12e169100275545ea/src/transformers/generation/utils.py\n        specifically,\n        `GenerationMixin._prepare_decoder_input_ids_for_generation()`.\n\n        Arguments:\n\n        * decoder_input_ids: input token ids to preprocess\n\n        Returns:\n\n        * Processed token list\n        \"\"\"\n\n        decoder_start_token_id = self.get_decoder_start_token_id()\n        assert decoder_start_token_id is not None\n\n        if decoder_input_ids is None:\n            # no decoder prompt input ->\n            # use decoder_start_token_id as decoder_input_ids\n            decoder_input_ids = self._get_default_enc_dec_decoder_prompt()\n\n        if (len(decoder_input_ids) == 0\n                or decoder_input_ids[0] != decoder_start_token_id):\n            decoder_input_ids = [decoder_start_token_id] + decoder_input_ids\n\n        return decoder_input_ids\n\n    def _apply_prompt_adapter(\n        self,\n        prompt_token_ids: list[int],\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n    ) -> list[int]:\n        if prompt_adapter_request:\n            prompt_token_ids = (\n                [0] * prompt_adapter_request.prompt_adapter_num_virtual_tokens\n                + prompt_token_ids)\n\n        return prompt_token_ids\n\n    def _get_tokenization_kw(\n        self,\n        overrides: Optional[dict[str, Any]] = None,\n    ) -> dict[str, Any]:\n        kwargs = dict[str, Any]()\n\n        if self.model_config.hf_config.model_type == \"whisper\":\n            # For Whisper, special tokens should be provided by the user based\n            # on the task and language of their request. Also needed to avoid\n            # appending an EOS token to the prompt which disrupts generation.\n            kwargs[\"add_special_tokens\"] = False\n\n        if overrides:\n            kwargs.update(overrides)\n\n        return kwargs\n\n    def _tokenize_prompt(\n        self,\n        prompt: str,\n        lora_request: Optional[LoRARequest],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> list[int]:\n        \"\"\"\n        Apply the model's tokenizer to a text prompt, returning the\n        corresponding token IDs.\n        \"\"\"\n        tokenizer = self.get_tokenizer_group()\n        tokenization_kwargs = self._get_tokenization_kw(tokenization_kwargs)\n\n        encoder_config = self.model_config.encoder_config\n\n        if encoder_config and encoder_config.get(\"do_lower_case\", False):\n            prompt = prompt.lower()\n\n        return tokenizer.encode(prompt=prompt,\n                                lora_request=lora_request,\n                                **tokenization_kwargs)\n\n    async def _tokenize_prompt_async(\n        self,\n        prompt: str,\n        lora_request: Optional[LoRARequest],\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> list[int]:\n        \"\"\"Async version of {meth}`_tokenize_prompt`.\"\"\"\n        tokenizer = self.get_tokenizer_group()\n        tokenization_kwargs = self._get_tokenization_kw(tokenization_kwargs)\n\n        return await tokenizer.encode_async(prompt=prompt,\n                                            lora_request=lora_request,\n                                            **tokenization_kwargs)\n\n    def _get_mm_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest],\n    ) -> AnyTokenizer:\n        # PrithviGeoSpatialMAE needs to be initialized without a tokenizer\n        # while using also multi-modal input\n        if not self.tokenizer:\n            return cast(AnyTokenizer, object())  # Dummy\n\n        tokenizer_group = self.get_tokenizer_group()\n        return tokenizer_group.get_lora_tokenizer(lora_request)\n\n    async def _get_mm_tokenizer_async(\n        self,\n        lora_request: Optional[LoRARequest],\n    ) -> AnyTokenizer:\n        # PrithviGeoSpatialMAE needs to be initialized without a tokenizer\n        # while using also multi-modal input\n        if not self.tokenizer:\n            return cast(AnyTokenizer, object())  # Dummy\n\n        tokenizer_group = self.get_tokenizer_group()\n        return await tokenizer_group.get_lora_tokenizer_async(lora_request)\n\n    def _process_multimodal(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        mm_processor_kwargs: Optional[Mapping[str, object]],\n        lora_request: Optional[LoRARequest],\n        return_mm_hashes: bool = False,\n    ) -> MultiModalInputs:\n        \"\"\"\n        Apply the model's multi-modal processor to a multi-modal prompt,\n        returning the corresponding token IDs and metadata.\n        \"\"\"\n        tokenizer = self._get_mm_tokenizer(lora_request)\n\n        mm_processor = self.mm_registry.create_processor(self.model_config,\n                                                         tokenizer=tokenizer)\n\n        if mm_processor_kwargs is None:\n            mm_processor_kwargs = {}\n\n        return mm_processor.apply(prompt, mm_data, mm_processor_kwargs,\n                                  return_mm_hashes)\n\n    async def _process_multimodal_async(\n        self,\n        prompt: Union[str, list[int]],\n        mm_data: MultiModalDataDict,\n        mm_processor_kwargs: Optional[Mapping[str, object]],\n        lora_request: Optional[LoRARequest],\n        return_mm_hashes: bool = False,\n    ) -> MultiModalInputs:\n        \"\"\"Async version of {meth}`_process_multimodal`.\"\"\"\n        tokenizer = await self._get_mm_tokenizer_async(lora_request)\n\n        mm_processor = self.mm_registry.create_processor(self.model_config,\n                                                         tokenizer=tokenizer)\n        if mm_processor_kwargs is None:\n            mm_processor_kwargs = {}\n\n        return mm_processor.apply(prompt, mm_data, mm_processor_kwargs,\n                                  return_mm_hashes)\n\n    def _process_embeds(\n        self,\n        parsed_content: EmbedsPrompt,\n    ) -> EmbedsInputs:\n        if not self.model_config.enable_prompt_embeds:\n            raise ValueError(\"You must set `--enable-prompt-embeds` to input \"\n                             \"`prompt_embeds`.\")\n\n        prompt_embeds = parsed_content[\"prompt_embeds\"]\n\n        # prompt_embeds must be (seq_len, hidden_size), but if the user\n        # passes in a batch of size 1, i.e. (1, seq_len, hidden_size),\n        # we can unambiguously process the intent by squeezing the batch\n        # dimension.\n        if prompt_embeds.ndim == 3:\n            prompt_embeds = prompt_embeds.squeeze(dim=0)\n\n        if prompt_embeds.ndim != 2:\n            raise ValueError(\n                \"prompt_embeds must be of shape (seq_len, hidden_size).\")\n\n        return embeds_inputs(prompt_embeds=prompt_embeds,\n                             cache_salt=parsed_content.get(\"cache_salt\"))\n\n    async def _process_embeds_async(\n        self,\n        parsed_content: EmbedsPrompt,\n    ) -> EmbedsInputs:\n        return self._process_embeds(parsed_content)\n\n    def _process_tokens(\n        self,\n        parsed_content: TokensPrompt,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_token_ids = parsed_content[\"prompt_token_ids\"]\n        token_type_ids = parsed_content.get(\"token_type_ids\")\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = self._process_multimodal(\n                prompt_token_ids,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            inputs = token_inputs(\n                prompt_token_ids=prompt_token_ids,\n                token_type_ids=token_type_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    async def _process_tokens_async(\n        self,\n        parsed_content: TokensPrompt,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_token_ids = parsed_content[\"prompt_token_ids\"]\n        token_type_ids = parsed_content.get(\"token_type_ids\")\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = await self._process_multimodal_async(\n                prompt_token_ids,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            inputs = token_inputs(\n                prompt_token_ids=prompt_token_ids,\n                token_type_ids=token_type_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    def _process_text(\n        self,\n        parsed_content: TextPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_text = parsed_content[\"prompt\"]\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = self._process_multimodal(\n                prompt_text,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            prompt_token_ids = self._tokenize_prompt(\n                prompt_text,\n                lora_request=lora_request,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            inputs = token_inputs(\n                prompt=prompt_text,\n                prompt_token_ids=prompt_token_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    async def _process_text_async(\n        self,\n        parsed_content: TextPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> Union[TokenInputs, MultiModalInputs]:\n        prompt_text = parsed_content[\"prompt\"]\n\n        inputs: Union[TokenInputs, MultiModalInputs]\n        if multi_modal_data := parsed_content.get(\"multi_modal_data\"):\n            inputs = await self._process_multimodal_async(\n                prompt_text,\n                multi_modal_data,\n                parsed_content.get(\"mm_processor_kwargs\"),\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        else:\n            prompt_token_ids = await self._tokenize_prompt_async(\n                prompt_text,\n                lora_request=lora_request,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            inputs = token_inputs(\n                prompt=prompt_text,\n                prompt_token_ids=prompt_token_ids,\n            )\n\n        if cache_salt := parsed_content.get(\"cache_salt\"):\n            inputs[\"cache_salt\"] = cache_salt\n\n        return inputs\n\n    def _prompt_to_llm_inputs(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> SingletonInputs:\n        \"\"\"\n        Extract the singleton inputs from a prompt.\n\n        Arguments:\n\n        * prompt: single encoder or decoder input prompt\n        * lora_request: this is only valid for decoder prompts\n        * return_mm_hashes: whether to return multimodal hashes\n\n        Returns:\n\n        * {class}`SingletonInputs` instance\n        \"\"\"\n        parsed = parse_singleton_prompt(prompt)\n\n        if parsed[\"type\"] == \"embeds\":\n            return self._process_embeds(parsed[\"content\"])\n        if parsed[\"type\"] == \"tokens\":\n            return self._process_tokens(\n                parsed[\"content\"],\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"text\":\n            return self._process_text(\n                parsed[\"content\"],\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"str\":\n            return self._process_text(\n                TextPrompt(prompt=parsed[\"content\"]),\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n\n        assert_never(parsed)\n\n    async def _prompt_to_llm_inputs_async(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> SingletonInputs:\n        \"\"\"Async version of {meth}`_prompt_to_llm_inputs`.\"\"\"\n        parsed = parse_singleton_prompt(prompt)\n\n        if parsed[\"type\"] == \"embeds\":\n            return await self._process_embeds_async(parsed[\"content\"])\n        if parsed[\"type\"] == \"tokens\":\n            return await self._process_tokens_async(\n                parsed[\"content\"],\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"text\":\n            return await self._process_text_async(\n                parsed[\"content\"],\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n        if parsed[\"type\"] == \"str\":\n            return await self._process_text_async(\n                TextPrompt(prompt=parsed[\"content\"]),\n                tokenization_kwargs=tokenization_kwargs,\n                lora_request=lora_request,\n                return_mm_hashes=return_mm_hashes,\n            )\n\n        assert_never(parsed)\n\n    def _build_enc_dec_llm_inputs(\n        self,\n        encoder_inputs: SingletonInputs,\n        decoder_inputs: Optional[SingletonInputs],\n    ) -> EncoderDecoderInputs:\n        if (encoder_inputs[\"type\"] == \"embeds\"\n                or decoder_inputs and decoder_inputs[\"type\"] == \"embeds\"):\n            raise ValueError(\"Embedding inputs are not supported for encoder-\"\n                             \"decoder models\")\n\n        # Needed for mypy\n        encoder_inputs = cast(Union[TokenInputs, MultiModalInputs],\n                              encoder_inputs)\n        decoder_inputs = cast(Optional[Union[TokenInputs, MultiModalInputs]],\n                              decoder_inputs)\n\n        if decoder_inputs is None:\n            if self.model_config.hf_config.model_type == \"whisper\":\n                # For Whisper models, the text prompt should go to the decoder.\n                # If no explicit encoder/decoder inputs, then copy the prompt\n                # from the encoder to the decoder. The encoder tokens are later\n                # overridden by the audio features.\n                dec_token_ids = encoder_inputs[\"prompt_token_ids\"].copy()\n            else:\n                dec_token_ids = self._prepare_decoder_input_ids_for_generation(\n                    None)\n            decoder_inputs = token_inputs(dec_token_ids)\n        else:\n            if \"multi_modal_data\" in decoder_inputs:\n                raise ValueError(\"Multi-modal decoder inputs of encoder-\"\n                                 \"decoder models are not supported yet\")\n\n            dec_token_ids = self._prepare_decoder_input_ids_for_generation(\n                decoder_inputs[\"prompt_token_ids\"])\n            decoder_inputs[\"prompt_token_ids\"] = dec_token_ids\n\n        return EncoderDecoderInputs(\n            encoder=encoder_inputs,\n            decoder=decoder_inputs,\n        )\n\n    def _split_enc_dec_mm_inputs(\n        self,\n        inputs: Union[SingletonInputs, MultiModalEncDecInputs],\n        decoder_inputs_to_override: Optional[SingletonInputs] = None,\n    ) -> tuple[SingletonInputs, SingletonInputs]:\n        \"\"\"\n        For encoder/decoder models only:\n        Separate Encoder/Decoder inputs from a MultiModalEncDecInputs\n        \"\"\"\n        if (inputs[\"type\"] == \"embeds\" or decoder_inputs_to_override\n                and decoder_inputs_to_override[\"type\"] == \"embeds\"):\n            raise ValueError(\"Embedding inputs are not supported for encoder-\"\n                             \"decoder models\")\n\n        # Needed for mypy\n        inputs = cast(\n            Union[TokenInputs, MultiModalInputs, MultiModalEncDecInputs],\n            inputs,\n        )\n        decoder_inputs_to_override = cast(\n            Optional[Union[TokenInputs, MultiModalInputs]],\n            decoder_inputs_to_override,\n        )\n\n        encoder_inputs: SingletonInputs\n        decoder_inputs: SingletonInputs\n\n        if inputs[\"type\"] == \"multimodal\":  # Multimodal data inputs\n            if not (\"encoder_prompt\" in inputs\n                    and \"encoder_prompt_token_ids\" in inputs):\n                raise RuntimeError(\"You should register an encoder-decoder \"\n                                   \"multi-modal processor for encoder-decoder \"\n                                   \"models.\")\n            inputs = cast(MultiModalEncDecInputs, inputs)\n\n            encoder_inputs = token_inputs(\n                prompt=inputs[\"encoder_prompt\"],\n                prompt_token_ids=inputs[\"encoder_prompt_token_ids\"],\n            )\n\n            decoder_prompt_inputs = decoder_inputs_to_override or inputs\n            decoder_inputs = MultiModalInputs(\n                type=\"multimodal\",\n                prompt=decoder_prompt_inputs.get(\"prompt\", \"\"),\n                prompt_token_ids=decoder_prompt_inputs[\"prompt_token_ids\"],\n                mm_kwargs=inputs[\"mm_kwargs\"],\n                mm_hashes=inputs[\"mm_hashes\"],\n                mm_placeholders=inputs[\"mm_placeholders\"],\n            )\n            if cache_salt := inputs.get(\"cache_salt\"):\n                decoder_inputs[\"cache_salt\"] = cache_salt\n\n        elif inputs[\"type\"] == \"token\":  # Text-only inputs\n            encoder_inputs = token_inputs(prompt=\"\", prompt_token_ids=[])\n            decoder_inputs = decoder_inputs_to_override or inputs\n        else:\n            assert_never(inputs)  # type: ignore[arg-type]\n\n        return encoder_inputs, decoder_inputs\n\n    def _process_encoder_decoder_prompt(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> EncoderDecoderInputs:\n        \"\"\"\n        For encoder/decoder models only:\n        Process an input prompt into an {class}`EncoderDecoderInputs` instance.\n\n        There are two types of input prompts:\n        singleton prompts which carry only the\n        encoder prompt, and explicit encoder/decoder\n        prompts which carry both the encoder and the\n        decoder prompts as member variables.\n\n        This function handles the following scenarios:\n        * Singleton encoder prompt: extract encoder prompt\n          token ids & infer default decoder prompt token ids\n        * Explicit encoder/decoder prompt: extract encoder\n          and decoder prompt token ids\n\n        Note that for Explicit encoder/decoder prompts,\n        each sub-prompt (encoder or decoder prompt) can\n        have any possible singleton type; thus this\n        method relies on helper functions to obtain\n        token ids for the sub-prompts.\n\n        Arguments:\n\n        * prompt: an input prompt\n\n        Returns:\n\n        * {class}`EncoderDecoderInputs` instance\n        \"\"\"\n        encoder_inputs: SingletonInputs\n        decoder_inputs: Optional[SingletonInputs]\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            encoder_inputs = self._prompt_to_llm_inputs(\n                prompt[\"encoder_prompt\"],\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            if (decoder_input := prompt[\"decoder_prompt\"]) is None:\n                decoder_inputs = None\n            else:\n                decoder_inputs = self._prompt_to_llm_inputs(decoder_input)\n            # For multimodal model, override decoder prompt from processor\n            # with explicit decoder prompt.\n            if self.model_config.is_multimodal_model:\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(encoder_inputs,\n                                                  decoder_inputs))\n        else:\n            inputs = self._prompt_to_llm_inputs(\n                prompt,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            if self.model_config.is_multimodal_model:\n                # Encoder-Decoder Multimodal model\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(inputs))\n            else:\n                encoder_inputs = inputs\n                decoder_inputs = None\n\n        return self._build_enc_dec_llm_inputs(encoder_inputs, decoder_inputs)\n\n    async def _process_encoder_decoder_prompt_async(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n    ) -> EncoderDecoderInputs:\n        \"\"\"Async version of {meth}`_process_encoder_decoder_prompt`.\"\"\"\n        encoder_inputs: SingletonInputs\n        decoder_inputs: Optional[SingletonInputs]\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            encoder_task = self._prompt_to_llm_inputs_async(\n                prompt[\"encoder_prompt\"],\n                tokenization_kwargs=tokenization_kwargs,\n            )\n\n            if (decoder_input := prompt[\"decoder_prompt\"]) is None:\n                encoder_inputs = await encoder_task\n                decoder_inputs = None\n            else:\n                decoder_task = self._prompt_to_llm_inputs_async(\n                    decoder_input,\n                    tokenization_kwargs=tokenization_kwargs,\n                )\n\n                encoder_inputs, decoder_inputs = await asyncio.gather(\n                    encoder_task, decoder_task)\n\n            # For multimodal model, override decoder prompt from processor\n            # with explicit decoder prompt.\n            if self.model_config.is_multimodal_model:\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(encoder_inputs,\n                                                  decoder_inputs))\n        else:\n            inputs = await self._prompt_to_llm_inputs_async(\n                prompt,\n                tokenization_kwargs=tokenization_kwargs,\n            )\n            if self.model_config.is_multimodal_model:\n                # Encoder-Decoder Multimodal model\n                encoder_inputs, decoder_inputs = (\n                    self._split_enc_dec_mm_inputs(inputs))\n            else:\n                encoder_inputs = inputs\n                decoder_inputs = None\n\n        return self._build_enc_dec_llm_inputs(encoder_inputs, decoder_inputs)\n\n    def _build_decoder_only_llm_inputs(\n        self,\n        prompt_inputs: DecoderOnlyInputs,\n        prompt_adapter_request: Optional[PromptAdapterRequest],\n    ) -> DecoderOnlyInputs:\n        if \"prompt_token_ids\" in prompt_inputs:\n            prompt_inputs = cast(Union[TokenInputs, MultiModalInputs],\n                                 prompt_inputs)  # Needed for mypy\n            prompt_inputs[\"prompt_token_ids\"] = self._apply_prompt_adapter(\n                prompt_inputs[\"prompt_token_ids\"],\n                prompt_adapter_request=prompt_adapter_request,\n            )\n\n        return prompt_inputs\n\n    def _process_decoder_only_prompt(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> DecoderOnlyInputs:\n        \"\"\"\n        For decoder-only models:\n        Process an input prompt into an {class}`DecoderOnlyInputs` instance.\n\n        Arguments:\n\n        * prompt: input prompt\n        * lora_request\n        * prompt_adapter_request\n        * return_mm_hashes\n\n        Returns:\n\n        * {class}`DecoderOnlyInputs` instance\n        \"\"\"\n\n        prompt_comps = self._prompt_to_llm_inputs(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    async def _process_decoder_only_prompt_async(\n        self,\n        prompt: SingletonPrompt,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> DecoderOnlyInputs:\n        \"\"\"Async version of {meth}`_process_decoder_only_prompt`.\"\"\"\n        prompt_comps = await self._prompt_to_llm_inputs_async(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n        return self._build_decoder_only_llm_inputs(\n            prompt_comps,\n            prompt_adapter_request=prompt_adapter_request,\n        )\n\n    def preprocess(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> ProcessorInputs:\n        \"\"\"Preprocess the input prompt.\"\"\"\n        if self.model_config.is_encoder_decoder:\n            assert not return_mm_hashes, (\n                \"Multimodal hashes for encoder-decoder models should not be \",\n                \"returned until they are supported on vLLM V1.\")\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            return self._process_encoder_decoder_prompt(prompt)\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                             \"to decoder-only models\")\n\n        # Decoder-only operation\n        return self._process_decoder_only_prompt(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n\n    async def preprocess_async(\n        self,\n        prompt: PromptType,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        lora_request: Optional[LoRARequest] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        return_mm_hashes: bool = False,\n    ) -> ProcessorInputs:\n        \"\"\"Async version of {meth}`preprocess`.\"\"\"\n        if self.model_config.is_encoder_decoder:\n            assert not return_mm_hashes, (\n                \"Multimodal hashes for encoder-decoder models should not be \",\n                \"returned until they are supported on vLLM V1.\")\n            # Encoder-decoder model requires special mapping of\n            # input prompts to encoder & decoder\n            return await self._process_encoder_decoder_prompt_async(prompt)\n\n        if is_explicit_encoder_decoder_prompt(prompt):\n            raise ValueError(\"Cannot pass encoder-decoder prompt \"\n                             \"to decoder-only models\")\n\n        # Decoder-only operation\n        return await self._process_decoder_only_prompt_async(\n            prompt,\n            tokenization_kwargs=tokenization_kwargs,\n            lora_request=lora_request,\n            prompt_adapter_request=prompt_adapter_request,\n            return_mm_hashes=return_mm_hashes,\n        )\n", 886], "/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom typing import Optional\n\nfrom vllm.config import LoRAConfig, ModelConfig, SchedulerConfig\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizer import (AnyTokenizer, encode_tokens,\n                                               get_lora_tokenizer,\n                                               get_lora_tokenizer_async,\n                                               get_tokenizer)\nfrom vllm.utils import LRUCache\n\n\nclass TokenizerGroup:\n    \"\"\"A group of tokenizers that can be used for LoRA adapters.\"\"\"\n\n    def __init__(self, tokenizer_id: str, enable_lora: bool, max_num_seqs: int,\n                 max_input_length: Optional[int], **tokenizer_config):\n        self.tokenizer_id = tokenizer_id\n        self.tokenizer_config = tokenizer_config\n        self.enable_lora = enable_lora\n        self.max_input_length = max_input_length\n        self.tokenizer = get_tokenizer(self.tokenizer_id, **tokenizer_config)\n        max_loras = tokenizer_config.get(\"max_loras\", 0)\n        self.lora_tokenizers = LRUCache[int, AnyTokenizer](\n            capacity=max(max_loras, max_num_seqs) if enable_lora else 0)\n\n    def get_max_input_len(self,\n                          lora_request: Optional[LoRARequest] = None\n                          ) -> Optional[int]:\n        \"\"\"Get the maximum input length for the LoRA request.\"\"\"\n        return self.max_input_length\n\n    def _raise_if_input_too_long(self,\n                                 encoded_tokens: list[int],\n                                 lora_request: Optional[LoRARequest] = None):\n        input_length = len(encoded_tokens)\n        if lora_request:\n            max_input_length = (lora_request.long_lora_max_len\n                                or self.max_input_length)\n        else:\n            max_input_length = self.max_input_length\n        if max_input_length is not None and input_length > max_input_length:\n            raise ValueError(\"Input too long.\", input_length, max_input_length)\n\n    def encode(self,\n               prompt: str,\n               max_length: Optional[int] = None,\n               truncation: Optional[bool] = None,\n               lora_request: Optional[LoRARequest] = None,\n               add_special_tokens: Optional[bool] = None) -> list[int]:\n\n        tokenizer = self.get_lora_tokenizer(lora_request)\n        ret = encode_tokens(tokenizer,\n                            prompt,\n                            max_length=max_length,\n                            truncation=truncation,\n                            add_special_tokens=add_special_tokens)\n        self._raise_if_input_too_long(ret, lora_request)\n        return ret\n\n    async def encode_async(\n            self,\n            prompt: str,\n            max_length: Optional[int] = None,\n            truncation: Optional[bool] = None,\n            lora_request: Optional[LoRARequest] = None,\n            add_special_tokens: Optional[bool] = None) -> list[int]:\n        tokenizer = await self.get_lora_tokenizer_async(lora_request)\n        ret = encode_tokens(tokenizer,\n                            prompt,\n                            max_length=max_length,\n                            truncation=truncation,\n                            add_special_tokens=add_special_tokens)\n        self._raise_if_input_too_long(ret, lora_request)\n        return ret\n\n    def get_lora_tokenizer(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        if not lora_request or not self.enable_lora:\n            return self.tokenizer\n        if lora_request.lora_int_id not in self.lora_tokenizers:\n            tokenizer = (get_lora_tokenizer(\n                lora_request, **self.tokenizer_config) or self.tokenizer)\n            self.lora_tokenizers.put(lora_request.lora_int_id, tokenizer)\n            return tokenizer\n        else:\n            return self.lora_tokenizers[lora_request.lora_int_id]\n\n    async def get_lora_tokenizer_async(\n        self,\n        lora_request: Optional[LoRARequest] = None,\n    ) -> AnyTokenizer:\n        if not lora_request or not self.enable_lora:\n            return self.tokenizer\n        if lora_request.lora_int_id not in self.lora_tokenizers:\n            tokenizer = (await get_lora_tokenizer_async(\n                lora_request, **self.tokenizer_config) or self.tokenizer)\n            self.lora_tokenizers.put(lora_request.lora_int_id, tokenizer)\n            return tokenizer\n        else:\n            return self.lora_tokenizers[lora_request.lora_int_id]\n\n\ndef init_tokenizer_from_configs(model_config: ModelConfig,\n                                scheduler_config: SchedulerConfig,\n                                lora_config: Optional[LoRAConfig]):\n    return TokenizerGroup(\n        tokenizer_id=model_config.tokenizer,\n        enable_lora=bool(lora_config),\n        max_num_seqs=scheduler_config.max_num_seqs,\n        max_loras=lora_config.max_loras if lora_config else 0,\n        max_input_length=None,\n        tokenizer_mode=model_config.tokenizer_mode,\n        trust_remote_code=model_config.trust_remote_code,\n        revision=model_config.tokenizer_revision,\n        truncation_side=model_config.truncation_side)\n", 119], "/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport contextlib\nimport copy\nimport os\nimport warnings\nfrom functools import lru_cache\nfrom pathlib import Path\nfrom types import MethodType\nfrom typing import TYPE_CHECKING, Any, Optional, Union\n\nimport huggingface_hub\nfrom transformers import (AutoTokenizer, PreTrainedTokenizer,\n                          PreTrainedTokenizerFast)\n\nfrom vllm.envs import VLLM_USE_MODELSCOPE\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.transformers_utils.tokenizer_base import (TokenizerBase,\n                                                    TokenizerRegistry)\nfrom vllm.transformers_utils.tokenizers import MistralTokenizer\nfrom vllm.transformers_utils.utils import check_gguf_file\nfrom vllm.utils import make_async\n\nif TYPE_CHECKING:\n    from vllm.config import ModelConfig\n\nlogger = init_logger(__name__)\n\nAnyTokenizer = Union[PreTrainedTokenizer, PreTrainedTokenizerFast,\n                     TokenizerBase]\n\n\ndef decode_tokens(\n    tokenizer: AnyTokenizer,\n    token_ids: list[int],\n    *,\n    skip_special_tokens: Optional[bool] = None,\n) -> str:\n    \"\"\"\n    Backend-agnostic equivalent of HF's\n    `tokenizer.decode(token_ids, ...)`.\n\n    `skip_special_tokens=None` means to use the backend's default\n    settings.\n    \"\"\"\n    if skip_special_tokens is not None:\n        return tokenizer.decode(token_ids,\n                                skip_special_tokens=skip_special_tokens)\n\n    return tokenizer.decode(token_ids)\n\n\ndef encode_tokens(\n    tokenizer: AnyTokenizer,\n    text: str,\n    *,\n    truncation: Optional[bool] = None,\n    max_length: Optional[int] = None,\n    add_special_tokens: Optional[bool] = None,\n) -> list[int]:\n    \"\"\"\n    Backend-agnostic equivalent of HF's\n    `tokenizer.encode(text, ...)`.\n\n    `add_special_tokens=None` means to use the backend's default\n    settings.\n    \"\"\"\n\n    kw_args: dict[str, Any] = {}\n    if max_length is not None:\n        kw_args[\"max_length\"] = max_length\n\n    if truncation is not None:\n        kw_args[\"truncation\"] = truncation\n\n    if add_special_tokens is not None:\n        kw_args[\"add_special_tokens\"] = add_special_tokens\n\n    return tokenizer.encode(text, **kw_args)\n\n\ndef get_cached_tokenizer(tokenizer: AnyTokenizer) -> AnyTokenizer:\n    \"\"\"\n    By default, transformers will recompute multiple tokenizer properties\n    each time they are called, leading to a significant slowdown.\n    This proxy caches these properties for faster access.\n    \"\"\"\n    cached_tokenizer = copy.copy(tokenizer)\n\n    tokenizer_all_special_ids = tokenizer.all_special_ids\n    tokenizer_all_special_tokens = tokenizer.all_special_tokens\n    tokenizer_all_special_tokens_extended = (\n        tokenizer.all_special_tokens_extended)\n    tokenizer_vocab = tokenizer.get_vocab()\n    tokenizer_len = len(tokenizer)\n\n    max_token_id = max(tokenizer_vocab.values())\n    # Some tokenizers (e.g., QwenTokenizer) have special tokens that\n    # are added and included in the implementation of the vocab_size\n    # property, but not in get_vocab(); if there is an implementation\n    # of vocab size, we should take the greater value.\n    if hasattr(tokenizer, \"vocab_size\"):\n        with contextlib.suppress(NotImplementedError):\n            max_token_id = max(max_token_id, tokenizer.vocab_size)\n\n    class CachedTokenizer(tokenizer.__class__):  # type: ignore\n\n        @property\n        def all_special_ids(self) -> list[int]:\n            return tokenizer_all_special_ids\n\n        @property\n        def all_special_tokens(self) -> list[str]:\n            return tokenizer_all_special_tokens\n\n        @property\n        def all_special_tokens_extended(self) -> list[str]:\n            return tokenizer_all_special_tokens_extended\n\n        @property\n        def max_token_id(self) -> int:\n            return max_token_id\n\n        def get_vocab(self) -> dict[str, int]:\n            return tokenizer_vocab\n\n        def __len__(self) -> int:\n            return tokenizer_len\n\n        def __reduce__(self):\n            return get_cached_tokenizer, (tokenizer, )\n\n    CachedTokenizer.__name__ = f\"Cached{tokenizer.__class__.__name__}\"\n\n    cached_tokenizer.__class__ = CachedTokenizer\n    return cached_tokenizer\n\n\ndef patch_padding_side(tokenizer: PreTrainedTokenizer) -> None:\n    \"\"\"Patch _pad method to accept `padding_side` for older tokenizers.\"\"\"\n    orig_pad = tokenizer._pad\n\n    def _pad(\n        self: PreTrainedTokenizer,\n        *args,\n        padding_side: Optional[str] = None,\n        **kwargs,\n    ):\n        if padding_side is not None and padding_side != self.padding_side:\n            msg = (\"`padding_side` argument is not supported by \"\n                   f\"{type(tokenizer).__name__} and will be ignored.\")\n            warnings.warn(msg, stacklevel=2)\n\n        return orig_pad(*args, **kwargs)\n\n    tokenizer._pad = MethodType(_pad, tokenizer)\n\n\ndef get_tokenizer(\n    tokenizer_name: Union[str, Path],\n    *args,\n    tokenizer_mode: str = \"auto\",\n    trust_remote_code: bool = False,\n    revision: Optional[str] = None,\n    download_dir: Optional[str] = None,\n    **kwargs,\n) -> AnyTokenizer:\n    \"\"\"Gets a tokenizer for the given model name via HuggingFace or ModelScope.\n    \"\"\"\n    if VLLM_USE_MODELSCOPE:\n        # download model from ModelScope hub,\n        # lazy import so that modelscope is not required for normal use.\n        # pylint: disable=C.\n        from modelscope.hub.snapshot_download import snapshot_download\n\n        # avoid circuit import\n        from vllm.model_executor.model_loader.weight_utils import get_lock\n\n        # Only set the tokenizer here, model will be downloaded on the workers.\n        if not os.path.exists(tokenizer_name):\n            # Use file lock to prevent multiple processes from\n            # downloading the same file at the same time.\n            with get_lock(tokenizer_name, download_dir):\n                tokenizer_path = snapshot_download(\n                    model_id=tokenizer_name,\n                    cache_dir=download_dir,\n                    revision=revision,\n                    local_files_only=huggingface_hub.constants.HF_HUB_OFFLINE,\n                    # Ignore weights - we only need the tokenizer.\n                    ignore_file_pattern=[\".*.pt\", \".*.safetensors\", \".*.bin\"])\n                tokenizer_name = tokenizer_path\n\n    if tokenizer_mode == \"slow\":\n        if kwargs.get(\"use_fast\", False):\n            raise ValueError(\n                \"Cannot use the fast tokenizer in slow tokenizer mode.\")\n        kwargs[\"use_fast\"] = False\n\n    if \"truncation_side\" not in kwargs:\n        kwargs[\"truncation_side\"] = \"left\"\n\n    # Separate model folder from file path for GGUF models\n    is_gguf = check_gguf_file(tokenizer_name)\n    if is_gguf:\n        kwargs[\"gguf_file\"] = Path(tokenizer_name).name\n        tokenizer_name = Path(tokenizer_name).parent\n\n    # if tokenizer is from official mistral org\n    is_from_mistral_org = str(tokenizer_name).split(\"/\")[0] == \"mistralai\"\n    if is_from_mistral_org and tokenizer_mode != \"mistral\":\n        warnings.warn(\n            'It is strongly recommended to run mistral models with '\n            '`--tokenizer-mode \"mistral\"` to ensure correct '\n            'encoding and decoding.',\n            FutureWarning,\n            stacklevel=2)\n\n    tokenizer: AnyTokenizer\n    if tokenizer_mode == \"mistral\":\n        tokenizer = MistralTokenizer.from_pretrained(str(tokenizer_name),\n                                                     revision=revision)\n    elif tokenizer_mode == \"custom\":\n        tokenizer = TokenizerRegistry.get_tokenizer(str(tokenizer_name),\n                                                    *args,\n                                                    revision=revision,\n                                                    download_dir=download_dir,\n                                                    **kwargs)\n    else:\n        try:\n            tokenizer = AutoTokenizer.from_pretrained(\n                tokenizer_name,\n                *args,\n                trust_remote_code=trust_remote_code,\n                revision=revision,\n                **kwargs,\n            )\n        except ValueError as e:\n            # If the error pertains to the tokenizer class not existing or not\n            # currently being imported,\n            # suggest using the --trust-remote-code flag.\n            if not trust_remote_code and (\n                    \"does not exist or is not currently imported.\" in str(e)\n                    or \"requires you to execute the tokenizer file\" in str(e)):\n                err_msg = (\"Failed to load the tokenizer. If the tokenizer \"\n                           \"is a custom tokenizer not yet available in the \"\n                           \"HuggingFace transformers library, consider \"\n                           \"setting `trust_remote_code=True` in LLM or using \"\n                           \"the `--trust-remote-code` flag in the CLI.\")\n                raise RuntimeError(err_msg) from e\n            else:\n                raise e\n\n        # NOTE: We can remove this after https://github.com/THUDM/ChatGLM3/issues/1324\n        if type(tokenizer).__name__ in (\"ChatGLMTokenizer\",\n                                        \"ChatGLM4Tokenizer\"):\n            assert isinstance(tokenizer, PreTrainedTokenizer)\n            patch_padding_side(tokenizer)\n\n        if not isinstance(tokenizer, PreTrainedTokenizerFast):\n            logger.warning(\n                \"Using a slow tokenizer. This might cause a significant \"\n                \"slowdown. Consider using a fast tokenizer instead.\")\n        tokenizer = get_cached_tokenizer(tokenizer)\n\n    return tokenizer\n\n\ncached_get_tokenizer = lru_cache(get_tokenizer)\n\n\ndef cached_tokenizer_from_config(\n    model_config: \"ModelConfig\",\n    **kwargs: Any,\n):\n    return cached_get_tokenizer(\n        model_config.tokenizer,\n        tokenizer_mode=model_config.tokenizer_mode,\n        tokenizer_revision=model_config.tokenizer_revision,\n        trust_remote_code=model_config.trust_remote_code,\n        **kwargs,\n    )\n\n\ndef get_lora_tokenizer(lora_request: LoRARequest, *args,\n                       **kwargs) -> Optional[AnyTokenizer]:\n    if lora_request is None:\n        return None\n    try:\n        tokenizer = get_tokenizer(lora_request.lora_path, *args, **kwargs)\n    except Exception as e:\n        # No tokenizer was found in the LoRA folder,\n        # use base model tokenizer\n        logger.warning(\n            \"No tokenizer found in %s, using base model tokenizer instead. \"\n            \"(Exception: %s)\", lora_request.lora_path, e)\n        tokenizer = None\n    return tokenizer\n\n\nget_lora_tokenizer_async = make_async(get_lora_tokenizer)\n", 301], "/home/jeromeku/vllm/vllm/inputs/data.py": ["# SPDX-License-Identifier: Apache-2.0\nfrom collections.abc import Iterable\nfrom typing import TYPE_CHECKING, Any, Generic, Literal, Optional, Union, cast\n\nimport torch\nfrom typing_extensions import NotRequired, TypedDict, TypeIs, TypeVar\n\nif TYPE_CHECKING:\n    from vllm.multimodal.inputs import MultiModalDataDict, MultiModalInputs\n\n\nclass TextPrompt(TypedDict):\n    \"\"\"Schema for a text prompt.\"\"\"\n\n    prompt: str\n    \"\"\"The input text to be tokenized before passing to the model.\"\"\"\n\n    multi_modal_data: NotRequired[\"MultiModalDataDict\"]\n    \"\"\"\n    Optional multi-modal data to pass to the model,\n    if the model supports it.\n    \"\"\"\n\n    mm_processor_kwargs: NotRequired[dict[str, Any]]\n    \"\"\"\n    Optional multi-modal processor kwargs to be forwarded to the\n    multimodal input mapper & processor. Note that if multiple modalities\n    have registered mappers etc for the model being considered, we attempt\n    to pass the mm_processor_kwargs to each of them.\n    \"\"\"\n\n    cache_salt: NotRequired[str]\n    \"\"\"\n    Optional cache salt to be used for prefix caching.\n    \"\"\"\n\n\nclass TokensPrompt(TypedDict):\n    \"\"\"Schema for a tokenized prompt.\"\"\"\n\n    prompt_token_ids: list[int]\n    \"\"\"A list of token IDs to pass to the model.\"\"\"\n\n    token_type_ids: NotRequired[list[int]]\n    \"\"\"A list of token type IDs to pass to the cross encoder model.\"\"\"\n\n    multi_modal_data: NotRequired[\"MultiModalDataDict\"]\n    \"\"\"\n    Optional multi-modal data to pass to the model,\n    if the model supports it.\n    \"\"\"\n\n    mm_processor_kwargs: NotRequired[dict[str, Any]]\n    \"\"\"\n    Optional multi-modal processor kwargs to be forwarded to the\n    multimodal input mapper & processor. Note that if multiple modalities\n    have registered mappers etc for the model being considered, we attempt\n    to pass the mm_processor_kwargs to each of them.\n    \"\"\"\n\n    cache_salt: NotRequired[str]\n    \"\"\"\n    Optional cache salt to be used for prefix caching.\n    \"\"\"\n\n\nclass EmbedsPrompt(TypedDict):\n    \"\"\"Schema for a prompt provided via token embeddings.\"\"\"\n\n    prompt_embeds: torch.Tensor\n    \"\"\"The embeddings of the prompt.\"\"\"\n\n    cache_salt: NotRequired[str]\n    \"\"\"\n    Optional cache salt to be used for prefix caching.\n    \"\"\"\n\n\nSingletonPrompt = Union[str, TextPrompt, TokensPrompt, EmbedsPrompt]\n\"\"\"\nSet of possible schemas for a single prompt:\n\n- A text prompt ({class}`str` or {class}`TextPrompt`)\n- A tokenized prompt ({class}`TokensPrompt`)\n- An embeddings prompt ({class}`EmbedsPrompt`)\n\nNote that \"singleton\" is as opposed to a data structure\nwhich encapsulates multiple prompts, i.e. of the sort\nwhich may be utilized for encoder/decoder models when\nthe user desires to express both the encoder & decoder\nprompts explicitly, i.e. {class}`ExplicitEncoderDecoderPrompt`\n\nA prompt of type {class}`SingletonPrompt` may be employed\nas (1) input to a decoder-only model, (2) input to\nthe encoder of an encoder/decoder model, in the scenario\nwhere the decoder-prompt is not specified explicitly, or\n(3) as a member of a larger data structure encapsulating\nmore than one prompt, i.e. {class}`ExplicitEncoderDecoderPrompt`\n\"\"\"\n\n\ndef is_tokens_prompt(prompt: SingletonPrompt) -> TypeIs[TokensPrompt]:\n    return (isinstance(prompt, dict) and \"prompt_token_ids\" in prompt\n            and \"prompt_embeds\" not in prompt)\n\n\ndef is_embeds_prompt(prompt: SingletonPrompt) -> TypeIs[EmbedsPrompt]:\n    return (isinstance(prompt, dict) and \"prompt_token_ids\" not in prompt\n            and \"prompt_embeds\" in prompt)\n\n\n_T1_co = TypeVar(\"_T1_co\",\n                 bound=SingletonPrompt,\n                 default=SingletonPrompt,\n                 covariant=True)\n_T2_co = TypeVar(\"_T2_co\",\n                 bound=SingletonPrompt,\n                 default=SingletonPrompt,\n                 covariant=True)\n\n\n# TODO: Make fields ReadOnly once mypy supports it\nclass ExplicitEncoderDecoderPrompt(TypedDict, Generic[_T1_co, _T2_co]):\n    \"\"\"\n    Represents an encoder/decoder model input prompt,\n    comprising an explicit encoder prompt and a decoder prompt.\n\n    The encoder and decoder prompts, respectively, may be formatted\n    according to any of the {class}`SingletonPrompt` schemas,\n    and are not required to have the same schema.\n\n    Only the encoder prompt may have multi-modal data. mm_processor_kwargs\n    should be at the top-level, and should not be set in the encoder/decoder\n    prompts, since they are agnostic to the encoder/decoder.\n\n    Note that an {class}`ExplicitEncoderDecoderPrompt` may not\n    be used as an input to a decoder-only model,\n    and that the `encoder_prompt` and `decoder_prompt`\n    fields of this data structure themselves must be\n    {class}`SingletonPrompt` instances.\n    \"\"\"\n\n    encoder_prompt: _T1_co\n\n    decoder_prompt: Optional[_T2_co]\n\n    mm_processor_kwargs: NotRequired[dict[str, Any]]\n\n\nPromptType = Union[SingletonPrompt, ExplicitEncoderDecoderPrompt]\n\"\"\"\nSet of possible schemas for an LLM input, including\nboth decoder-only and encoder/decoder input types:\n\n- A text prompt ({class}`str` or {class}`TextPrompt`)\n- A tokenized prompt ({class}`TokensPrompt`)\n- An embeddings prompt ({class}`EmbedsPrompt`)\n- A single data structure containing both an encoder and a decoder prompt\n  ({class}`ExplicitEncoderDecoderPrompt`)\n\"\"\"\n\n\nclass TokenInputs(TypedDict):\n    \"\"\"Represents token-based inputs.\"\"\"\n\n    type: Literal[\"token\"]\n    \"\"\"The type of inputs.\"\"\"\n\n    prompt_token_ids: list[int]\n    \"\"\"The token IDs of the prompt.\"\"\"\n\n    token_type_ids: NotRequired[list[int]]\n    \"\"\"The token type IDs of the prompt.\"\"\"\n\n    prompt: NotRequired[str]\n    \"\"\"\n    The original prompt text corresponding to the token IDs, if available.\n    \"\"\"\n\n    cache_salt: NotRequired[str]\n    \"\"\"\n    Optional cache salt to be used for prefix caching.\n    \"\"\"\n\n\ndef token_inputs(\n    prompt_token_ids: list[int],\n    token_type_ids: Optional[list[int]] = None,\n    prompt: Optional[str] = None,\n    cache_salt: Optional[str] = None,\n) -> TokenInputs:\n    \"\"\"Construct {class}`TokenInputs` from optional values.\"\"\"\n    inputs = TokenInputs(type=\"token\", prompt_token_ids=prompt_token_ids)\n\n    if prompt is not None:\n        inputs[\"prompt\"] = prompt\n    if token_type_ids is not None:\n        inputs[\"token_type_ids\"] = token_type_ids\n    if cache_salt is not None:\n        inputs[\"cache_salt\"] = cache_salt\n\n    return inputs\n\n\nclass EmbedsInputs(TypedDict):\n    \"\"\"Represents embeddings-based inputs.\"\"\"\n\n    type: Literal[\"embeds\"]\n    \"\"\"The type of inputs.\"\"\"\n\n    prompt_embeds: torch.Tensor\n    \"\"\"The embeddings of the prompt.\"\"\"\n\n    cache_salt: NotRequired[str]\n    \"\"\"\n    Optional cache salt to be used for prefix caching.\n    \"\"\"\n\n\ndef embeds_inputs(\n    prompt_embeds: torch.Tensor,\n    cache_salt: Optional[str] = None,\n) -> EmbedsInputs:\n    \"\"\"Construct :class:`EmbedsInputs` from optional values.\"\"\"\n    inputs = EmbedsInputs(type=\"embeds\", prompt_embeds=prompt_embeds)\n\n    if cache_salt is not None:\n        inputs[\"cache_salt\"] = cache_salt\n\n    return inputs\n\n\nDecoderOnlyInputs = Union[TokenInputs, EmbedsInputs, \"MultiModalInputs\"]\n\"\"\"\nThe inputs in {class}`~vllm.LLMEngine` before they are\npassed to the model executor.\nThis specifies the data required for decoder-only models.\n\"\"\"\n\n\nclass EncoderDecoderInputs(TypedDict):\n    \"\"\"\n    The inputs in {class}`~vllm.LLMEngine` before they are\n    passed to the model executor.\n\n    This specifies the required data for encoder-decoder models.\n    \"\"\"\n    encoder: Union[TokenInputs, \"MultiModalInputs\"]\n    \"\"\"The inputs for the encoder portion.\"\"\"\n\n    decoder: Union[TokenInputs, \"MultiModalInputs\"]\n    \"\"\"The inputs for the decoder portion.\"\"\"\n\n\nSingletonInputs = Union[TokenInputs, EmbedsInputs, \"MultiModalInputs\"]\n\"\"\"\nA processed {class}`SingletonPrompt` which can be passed to\n{class}`vllm.sequence.Sequence`.\n\"\"\"\n\nProcessorInputs = Union[DecoderOnlyInputs, EncoderDecoderInputs]\n\"\"\"\nThe inputs to {data}`vllm.inputs.InputProcessor`.\n\"\"\"\n\n_T1 = TypeVar(\"_T1\", bound=SingletonPrompt, default=SingletonPrompt)\n_T2 = TypeVar(\"_T2\", bound=SingletonPrompt, default=SingletonPrompt)\n\n\ndef build_explicit_enc_dec_prompt(\n    encoder_prompt: _T1,\n    decoder_prompt: Optional[_T2],\n    mm_processor_kwargs: Optional[dict[str, Any]] = None,\n) -> ExplicitEncoderDecoderPrompt[_T1, _T2]:\n    if mm_processor_kwargs is None:\n        mm_processor_kwargs = {}\n    return ExplicitEncoderDecoderPrompt(\n        encoder_prompt=encoder_prompt,\n        decoder_prompt=decoder_prompt,\n        mm_processor_kwargs=mm_processor_kwargs)\n\n\ndef zip_enc_dec_prompts(\n    enc_prompts: Iterable[_T1],\n    dec_prompts: Iterable[Optional[_T2]],\n    mm_processor_kwargs: Optional[Union[Iterable[dict[str, Any]],\n                                        dict[str, Any]]] = None,\n) -> list[ExplicitEncoderDecoderPrompt[_T1, _T2]]:\n    \"\"\"\n    Zip encoder and decoder prompts together into a list of\n    {class}`ExplicitEncoderDecoderPrompt` instances.\n\n    ``mm_processor_kwargs`` may also be provided; if a dict is passed, the same\n    dictionary will be used for every encoder/decoder prompt. If an iterable is\n    provided, it will be zipped with the encoder/decoder prompts.\n    \"\"\"\n    if mm_processor_kwargs is None:\n        mm_processor_kwargs = cast(dict[str, Any], {})\n    if isinstance(mm_processor_kwargs, dict):\n        return [\n            build_explicit_enc_dec_prompt(\n                encoder_prompt, decoder_prompt,\n                cast(dict[str, Any], mm_processor_kwargs))\n            for (encoder_prompt,\n                 decoder_prompt) in zip(enc_prompts, dec_prompts)\n        ]\n    return [\n        build_explicit_enc_dec_prompt(encoder_prompt, decoder_prompt,\n                                      mm_proc_kwargs)\n        for (encoder_prompt, decoder_prompt, mm_proc_kwargs\n             ) in zip(enc_prompts, dec_prompts, mm_processor_kwargs)\n    ]\n\n\ndef to_enc_dec_tuple_list(\n    enc_dec_prompts: Iterable[ExplicitEncoderDecoderPrompt[_T1, _T2]],\n) -> list[tuple[_T1, Optional[_T2]]]:\n    return [(enc_dec_prompt[\"encoder_prompt\"],\n             enc_dec_prompt[\"decoder_prompt\"])\n            for enc_dec_prompt in enc_dec_prompts]\n", 320], "/home/jeromeku/vllm/vllm/platforms/__init__.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport logging\nimport traceback\nfrom itertools import chain\nfrom typing import TYPE_CHECKING, Optional\n\nfrom vllm.plugins import load_plugins_by_group\nfrom vllm.utils import resolve_obj_by_qualname\n\nfrom .interface import _Backend  # noqa: F401\nfrom .interface import CpuArchEnum, Platform, PlatformEnum\n\nlogger = logging.getLogger(__name__)\n\n\ndef vllm_version_matches_substr(substr: str) -> bool:\n    \"\"\"\n    Check to see if the vLLM version matches a substring.\n    \"\"\"\n    from importlib.metadata import PackageNotFoundError, version\n    try:\n        vllm_version = version(\"vllm\")\n    except PackageNotFoundError as e:\n        logger.warning(\n            \"The vLLM package was not found, so its version could not be \"\n            \"inspected. This may cause platform detection to fail.\")\n        raise e\n    return substr in vllm_version\n\n\ndef tpu_platform_plugin() -> Optional[str]:\n    is_tpu = False\n    logger.debug(\"Checking if TPU platform is available.\")\n    try:\n        # While it's technically possible to install libtpu on a\n        # non-TPU machine, this is a very uncommon scenario. Therefore,\n        # we assume that libtpu is installed if and only if the machine\n        # has TPUs.\n        import libtpu  # noqa: F401\n        is_tpu = True\n        logger.debug(\"Confirmed TPU platform is available.\")\n    except Exception as e:\n        logger.debug(\"TPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.tpu.TpuPlatform\" if is_tpu else None\n\n\ndef cuda_platform_plugin() -> Optional[str]:\n    is_cuda = False\n    logger.debug(\"Checking if CUDA platform is available.\")\n    try:\n        from vllm.utils import import_pynvml\n        pynvml = import_pynvml()\n        pynvml.nvmlInit()\n        try:\n            # NOTE: Edge case: vllm cpu build on a GPU machine.\n            # Third-party pynvml can be imported in cpu build,\n            # we need to check if vllm is built with cpu too.\n            # Otherwise, vllm will always activate cuda plugin\n            # on a GPU machine, even if in a cpu build.\n            is_cuda = (pynvml.nvmlDeviceGetCount() > 0\n                       and not vllm_version_matches_substr(\"cpu\"))\n            if pynvml.nvmlDeviceGetCount() <= 0:\n                logger.debug(\n                    \"CUDA platform is not available because no GPU is found.\")\n            if vllm_version_matches_substr(\"cpu\"):\n                logger.debug(\"CUDA platform is not available because\"\n                             \" vLLM is built with CPU.\")\n            if is_cuda:\n                logger.debug(\"Confirmed CUDA platform is available.\")\n        finally:\n            pynvml.nvmlShutdown()\n    except Exception as e:\n        logger.debug(\"Exception happens when checking CUDA platform: %s\",\n                     str(e))\n        if \"nvml\" not in e.__class__.__name__.lower():\n            # If the error is not related to NVML, re-raise it.\n            raise e\n\n        # CUDA is supported on Jetson, but NVML may not be.\n        import os\n\n        def cuda_is_jetson() -> bool:\n            return os.path.isfile(\"/etc/nv_tegra_release\") \\\n                or os.path.exists(\"/sys/class/tegra-firmware\")\n\n        if cuda_is_jetson():\n            logger.debug(\"Confirmed CUDA platform is available on Jetson.\")\n            is_cuda = True\n        else:\n            logger.debug(\"CUDA platform is not available because: %s\", str(e))\n\n    return \"vllm.platforms.cuda.CudaPlatform\" if is_cuda else None\n\n\ndef rocm_platform_plugin() -> Optional[str]:\n    is_rocm = False\n    logger.debug(\"Checking if ROCm platform is available.\")\n    try:\n        import amdsmi\n        amdsmi.amdsmi_init()\n        try:\n            if len(amdsmi.amdsmi_get_processor_handles()) > 0:\n                is_rocm = True\n                logger.debug(\"Confirmed ROCm platform is available.\")\n            else:\n                logger.debug(\"ROCm platform is not available because\"\n                             \" no GPU is found.\")\n        finally:\n            amdsmi.amdsmi_shut_down()\n    except Exception as e:\n        logger.debug(\"ROCm platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.rocm.RocmPlatform\" if is_rocm else None\n\n\ndef hpu_platform_plugin() -> Optional[str]:\n    is_hpu = False\n    logger.debug(\"Checking if HPU platform is available.\")\n    try:\n        from importlib import util\n        is_hpu = util.find_spec('habana_frameworks') is not None\n        if is_hpu:\n            logger.debug(\"Confirmed HPU platform is available.\")\n        else:\n            logger.debug(\"HPU platform is not available because \"\n                         \"habana_frameworks is not found.\")\n    except Exception as e:\n        logger.debug(\"HPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.hpu.HpuPlatform\" if is_hpu else None\n\n\ndef xpu_platform_plugin() -> Optional[str]:\n    is_xpu = False\n    logger.debug(\"Checking if XPU platform is available.\")\n    try:\n        # installed IPEX if the machine has XPUs.\n        import intel_extension_for_pytorch  # noqa: F401\n        import oneccl_bindings_for_pytorch  # noqa: F401\n        import torch\n        if hasattr(torch, 'xpu') and torch.xpu.is_available():\n            is_xpu = True\n            logger.debug(\"Confirmed XPU platform is available.\")\n    except Exception as e:\n        logger.debug(\"XPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.xpu.XPUPlatform\" if is_xpu else None\n\n\ndef cpu_platform_plugin() -> Optional[str]:\n    is_cpu = False\n    logger.debug(\"Checking if CPU platform is available.\")\n    try:\n        is_cpu = vllm_version_matches_substr(\"cpu\")\n        if is_cpu:\n            logger.debug(\"Confirmed CPU platform is available because\"\n                         \" vLLM is built with CPU.\")\n        if not is_cpu:\n            import sys\n            is_cpu = sys.platform.startswith(\"darwin\")\n            if is_cpu:\n                logger.debug(\"Confirmed CPU platform is available\"\n                             \" because the machine is MacOS.\")\n\n    except Exception as e:\n        logger.debug(\"CPU platform is not available because: %s\", str(e))\n        pass\n\n    return \"vllm.platforms.cpu.CpuPlatform\" if is_cpu else None\n\n\ndef neuron_platform_plugin() -> Optional[str]:\n    tnx_installed = False\n    nxd_installed = False\n    logger.debug(\"Checking if Neuron platform is available.\")\n    try:\n        import transformers_neuronx  # noqa: F401\n        tnx_installed = True\n        logger.debug(\"Confirmed Neuron platform is available because\"\n                     \" transformers_neuronx is found.\")\n    except ImportError:\n        pass\n\n    try:\n        import neuronx_distributed_inference  # noqa: F401\n        nxd_installed = True\n        logger.debug(\"Confirmed Neuron platform is available because\"\n                     \" neuronx_distributed_inference is found.\")\n    except ImportError:\n        pass\n\n    is_neuron = tnx_installed or nxd_installed\n    return \"vllm.platforms.neuron.NeuronPlatform\" if is_neuron else None\n\n\nbuiltin_platform_plugins = {\n    'tpu': tpu_platform_plugin,\n    'cuda': cuda_platform_plugin,\n    'rocm': rocm_platform_plugin,\n    'hpu': hpu_platform_plugin,\n    'xpu': xpu_platform_plugin,\n    'cpu': cpu_platform_plugin,\n    'neuron': neuron_platform_plugin,\n}\n\n\ndef resolve_current_platform_cls_qualname() -> str:\n    platform_plugins = load_plugins_by_group('vllm.platform_plugins')\n\n    activated_plugins = []\n\n    for name, func in chain(builtin_platform_plugins.items(),\n                            platform_plugins.items()):\n        try:\n            assert callable(func)\n            platform_cls_qualname = func()\n            if platform_cls_qualname is not None:\n                activated_plugins.append(name)\n        except Exception:\n            pass\n\n    activated_builtin_plugins = list(\n        set(activated_plugins) & set(builtin_platform_plugins.keys()))\n    activated_oot_plugins = list(\n        set(activated_plugins) & set(platform_plugins.keys()))\n\n    if len(activated_oot_plugins) >= 2:\n        raise RuntimeError(\n            \"Only one platform plugin can be activated, but got: \"\n            f\"{activated_oot_plugins}\")\n    elif len(activated_oot_plugins) == 1:\n        platform_cls_qualname = platform_plugins[activated_oot_plugins[0]]()\n        logger.info(\"Platform plugin %s is activated\",\n                    activated_oot_plugins[0])\n    elif len(activated_builtin_plugins) >= 2:\n        raise RuntimeError(\n            \"Only one platform plugin can be activated, but got: \"\n            f\"{activated_builtin_plugins}\")\n    elif len(activated_builtin_plugins) == 1:\n        platform_cls_qualname = builtin_platform_plugins[\n            activated_builtin_plugins[0]]()\n        logger.info(\"Automatically detected platform %s.\",\n                    activated_builtin_plugins[0])\n    else:\n        platform_cls_qualname = \"vllm.platforms.interface.UnspecifiedPlatform\"\n        logger.info(\n            \"No platform detected, vLLM is running on UnspecifiedPlatform\")\n    return platform_cls_qualname\n\n\n_current_platform = None\n_init_trace: str = ''\n\nif TYPE_CHECKING:\n    current_platform: Platform\n\n\ndef __getattr__(name: str):\n    if name == 'current_platform':\n        # lazy init current_platform.\n        # 1. out-of-tree platform plugins need `from vllm.platforms import\n        #    Platform` so that they can inherit `Platform` class. Therefore,\n        #    we cannot resolve `current_platform` during the import of\n        #    `vllm.platforms`.\n        # 2. when users use out-of-tree platform plugins, they might run\n        #    `import vllm`, some vllm internal code might access\n        #    `current_platform` during the import, and we need to make sure\n        #    `current_platform` is only resolved after the plugins are loaded\n        #    (we have tests for this, if any developer violate this, they will\n        #    see the test failures).\n        global _current_platform\n        if _current_platform is None:\n            platform_cls_qualname = resolve_current_platform_cls_qualname()\n            _current_platform = resolve_obj_by_qualname(\n                platform_cls_qualname)()\n            global _init_trace\n            _init_trace = \"\".join(traceback.format_stack())\n        return _current_platform\n    elif name in globals():\n        return globals()[name]\n    else:\n        raise AttributeError(\n            f\"No attribute named '{name}' exists in {__name__}.\")\n\n\n__all__ = [\n    'Platform', 'PlatformEnum', 'current_platform', 'CpuArchEnum',\n    \"_init_trace\"\n]\n", 295], "/home/jeromeku/vllm/vllm/platforms/interface.py": ["# SPDX-License-Identifier: Apache-2.0\nimport enum\nimport os\nimport platform\nimport random\nfrom platform import uname\nfrom typing import TYPE_CHECKING, NamedTuple, Optional, Union\n\nimport numpy as np\nimport torch\n\nfrom vllm.inputs import ProcessorInputs, PromptType\nfrom vllm.logger import init_logger\n\nif TYPE_CHECKING:\n    from vllm.config import ModelConfig, VllmConfig\n    from vllm.lora.request import LoRARequest\n    from vllm.pooling_params import PoolingParams\n    from vllm.sampling_params import SamplingParams\n    from vllm.utils import FlexibleArgumentParser\nelse:\n    ModelConfig = None\n    VllmConfig = None\n    LoRARequest = None\n    PoolingParams = None\n    SamplingParams = None\n    FlexibleArgumentParser = None\n\nlogger = init_logger(__name__)\n\n\ndef in_wsl() -> bool:\n    # Reference: https://github.com/microsoft/WSL/issues/4071\n    return \"microsoft\" in \" \".join(uname()).lower()\n\n\nclass _Backend(enum.Enum):\n    FLASH_ATTN = enum.auto()\n    FLASH_ATTN_VLLM_V1 = enum.auto()\n    TRITON_ATTN_VLLM_V1 = enum.auto()\n    XFORMERS = enum.auto()\n    ROCM_FLASH = enum.auto()\n    ROCM_AITER_MLA = enum.auto()  # Supported by V1\n    ROCM_AITER_MLA_VLLM_V1 = enum.auto()\n    TORCH_SDPA = enum.auto()\n    FLASHINFER = enum.auto()\n    TRITON_MLA = enum.auto()  # Supported by V1\n    FLASHMLA = enum.auto()  # Supported by V1\n    HPU_ATTN = enum.auto()\n    PALLAS = enum.auto()\n    PALLAS_VLLM_V1 = enum.auto()\n    IPEX = enum.auto()\n    BLOCK_SPARSE_FLASH_ATTN = enum.auto()\n    DUAL_CHUNK_FLASH_ATTN = enum.auto()\n    NO_ATTENTION = enum.auto()\n\n\nclass PlatformEnum(enum.Enum):\n    CUDA = enum.auto()\n    ROCM = enum.auto()\n    TPU = enum.auto()\n    HPU = enum.auto()\n    XPU = enum.auto()\n    CPU = enum.auto()\n    NEURON = enum.auto()\n    OOT = enum.auto()\n    UNSPECIFIED = enum.auto()\n\n\nclass CpuArchEnum(enum.Enum):\n    X86 = enum.auto()\n    ARM = enum.auto()\n    POWERPC = enum.auto()\n    OTHER = enum.auto()\n    UNKNOWN = enum.auto()\n\n\nclass DeviceCapability(NamedTuple):\n    major: int\n    minor: int\n\n    def as_version_str(self) -> str:\n        return f\"{self.major}.{self.minor}\"\n\n    def to_int(self) -> int:\n        \"\"\"\n        Express device capability as an integer ``<major><minor>``.\n\n        It is assumed that the minor version is always a single digit.\n        \"\"\"\n        assert 0 <= self.minor < 10\n        return self.major * 10 + self.minor\n\n\nclass Platform:\n    _enum: PlatformEnum\n    device_name: str\n    device_type: str\n\n    # available dispatch keys:\n    # check https://github.com/pytorch/pytorch/blob/313dac6c1ca0fa0cde32477509cce32089f8532a/torchgen/model.py#L134 # noqa\n    # use \"CPU\" as a fallback for platforms not registered in PyTorch\n    dispatch_key: str = \"CPU\"\n\n    # available ray device keys:\n    # https://github.com/ray-project/ray/blob/10ba5adadcc49c60af2c358a33bb943fb491a171/python/ray/_private/ray_constants.py#L438 # noqa\n    # empty string means the device does not support ray\n    ray_device_key: str = \"\"\n\n    # platform-agnostic way to specify the device control environment variable,\n    # .e.g. CUDA_VISIBLE_DEVICES for CUDA.\n    # hint: search for \"get_visible_accelerator_ids_env_var\" in\n    # https://github.com/ray-project/ray/tree/master/python/ray/_private/accelerators # noqa\n    device_control_env_var: str = \"VLLM_DEVICE_CONTROL_ENV_VAR_PLACEHOLDER\"\n\n    # The torch.compile backend for compiling simple and\n    # standalone functions. The default value is \"inductor\" to keep\n    # the same behavior as PyTorch.\n    # NOTE: for the forward part of the model, vLLM has another separate\n    # compilation strategy.\n    simple_compile_backend: str = \"inductor\"\n\n    supported_quantization: list[str] = []\n\n    additional_env_vars: list[str] = []\n\n    @property\n    def supported_dtypes(self) -> list[torch.dtype]:\n        \"\"\"Returns the supported dtypes for the current platform.\"\"\"\n        # Be careful with the order of the dtypes. The first dtype will\n        # be used as the default dtype fallback for the current platform,\n        # when encountering unsupported dtypes in \"auto\" dtype.\n        return [torch.bfloat16, torch.float16, torch.float32]\n\n    def is_cuda(self) -> bool:\n        return self._enum == PlatformEnum.CUDA\n\n    def is_rocm(self) -> bool:\n        return self._enum == PlatformEnum.ROCM\n\n    def is_tpu(self) -> bool:\n        return self._enum == PlatformEnum.TPU\n\n    def is_hpu(self) -> bool:\n        return self._enum == PlatformEnum.HPU\n\n    def is_xpu(self) -> bool:\n        return self._enum == PlatformEnum.XPU\n\n    def is_cpu(self) -> bool:\n        return self._enum == PlatformEnum.CPU\n\n    def is_neuron(self) -> bool:\n        return self._enum == PlatformEnum.NEURON\n\n    def is_out_of_tree(self) -> bool:\n        return self._enum == PlatformEnum.OOT\n\n    def is_cuda_alike(self) -> bool:\n        \"\"\"Stateless version of {func}`torch.cuda.is_available`.\"\"\"\n        return self._enum in (PlatformEnum.CUDA, PlatformEnum.ROCM)\n\n    def is_sleep_mode_available(self) -> bool:\n        return self._enum == PlatformEnum.CUDA\n\n    @classmethod\n    def device_id_to_physical_device_id(cls, device_id: int):\n        if cls.device_control_env_var in os.environ:\n            device_ids = os.environ[cls.device_control_env_var].split(\",\")\n            if device_ids == [\"\"]:\n                msg = (f\"{cls.device_control_env_var} is set to empty string, \"\n                       \"which means current platform support is disabled. If \"\n                       \"you are using ray, please unset the environment \"\n                       f\"variable `{cls.device_control_env_var}` inside the \"\n                       \"worker/actor. Check \"\n                       \"https://github.com/vllm-project/vllm/issues/8402 for \"\n                       \"more information.\")\n                raise RuntimeError(msg)\n            physical_device_id = device_ids[device_id]\n            return int(physical_device_id)\n        else:\n            return device_id\n\n    @classmethod\n    def get_attn_backend_cls(cls, selected_backend: _Backend, head_size: int,\n                             dtype: torch.dtype, kv_cache_dtype: Optional[str],\n                             block_size: int, use_v1: bool,\n                             use_mla: bool) -> str:\n        \"\"\"Get the attention backend class of a device.\"\"\"\n        return \"\"\n\n    @classmethod\n    def get_device_capability(\n        cls,\n        device_id: int = 0,\n    ) -> Optional[DeviceCapability]:\n        \"\"\"Stateless version of {func}`torch.cuda.get_device_capability`.\"\"\"\n        return None\n\n    @classmethod\n    def has_device_capability(\n        cls,\n        capability: Union[tuple[int, int], int],\n        device_id: int = 0,\n    ) -> bool:\n        \"\"\"\n        Test whether this platform is compatible with a device capability.\n\n        The ``capability`` argument can either be:\n\n        - A tuple ``(major, minor)``.\n        - An integer ``<major><minor>``. (See {meth}`DeviceCapability.to_int`)\n        \"\"\"\n        current_capability = cls.get_device_capability(device_id=device_id)\n        if current_capability is None:\n            return False\n\n        if isinstance(capability, tuple):\n            return current_capability >= capability\n\n        return current_capability.to_int() >= capability\n\n    @classmethod\n    def get_device_name(cls, device_id: int = 0) -> str:\n        \"\"\"Get the name of a device.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_device_uuid(cls, device_id: int = 0) -> str:\n        \"\"\"Get the uuid of a device, e.g. the PCI bus ID.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_device_total_memory(cls, device_id: int = 0) -> int:\n        \"\"\"Get the total memory of a device in bytes.\"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def is_async_output_supported(cls, enforce_eager: Optional[bool]) -> bool:\n        \"\"\"\n        Check if the current platform supports async output.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def inference_mode(cls):\n        \"\"\"A device-specific wrapper of `torch.inference_mode`.\n\n        This wrapper is recommended because some hardware backends such as TPU\n        do not support `torch.inference_mode`. In such a case, they will fall\n        back to `torch.no_grad` by overriding this method.\n        \"\"\"\n        return torch.inference_mode(mode=True)\n\n    @classmethod\n    def seed_everything(cls, seed: Optional[int] = None) -> None:\n        \"\"\"\n        Set the seed of each random module.\n        `torch.manual_seed` will set seed on all devices.\n\n        Loosely based on: https://github.com/Lightning-AI/pytorch-lightning/blob/2.4.0/src/lightning/fabric/utilities/seed.py#L20\n        \"\"\"\n        if seed is not None:\n            random.seed(seed)\n            np.random.seed(seed)\n            torch.manual_seed(seed)\n\n    @classmethod\n    def pre_register_and_update(cls,\n                                parser: Optional[FlexibleArgumentParser] = None\n                                ) -> None:\n        \"\"\"\n        Do some pre-registration or update action for the current platform.\n\n        This function is called before global VllmConfig is initialized or cli\n        arguments are parsed. It's used for out-of-tree platforms to register or\n        update the configuration.\n\n        For example, the out-of-tree quantization config can be imported and\n        registered here dynamically.\n        \"\"\"\n        pass\n\n    @classmethod\n    def check_and_update_config(cls, vllm_config: VllmConfig) -> None:\n        \"\"\"\n        Check and update the configuration for the current platform.\n\n        It can raise an exception if the configuration is not compatible with\n        the current platform, or it can update the configuration to make it\n        compatible with the current platform.\n\n        The config is passed by reference, so it can be modified in place.\n        \"\"\"\n        pass\n\n    @classmethod\n    def verify_model_arch(cls, model_arch: str) -> None:\n        \"\"\"\n        Verify whether the current platform supports the specified model\n        architecture.\n\n        - This will raise an Error or Warning based on the model support on\n        the current platform.\n        - By default all models are considered supported.\n        \"\"\"\n        pass\n\n    @classmethod\n    def verify_quantization(cls, quant: str) -> None:\n        \"\"\"\n        Verify whether the quantization is supported by the current platform.\n        \"\"\"\n        if cls.supported_quantization and \\\n            quant not in cls.supported_quantization:\n            raise ValueError(\n                f\"{quant} quantization is currently not supported in \"\n                f\"{cls.device_name}.\")\n\n    @classmethod\n    def get_cpu_architecture(cls) -> CpuArchEnum:\n        \"\"\"\n        Determine the CPU architecture of the current system.\n        Returns CpuArchEnum indicating the architecture type.\n        \"\"\"\n        machine = platform.machine().lower()\n\n        if machine in (\"x86_64\", \"amd64\", \"i386\", \"i686\"):\n            return CpuArchEnum.X86\n        elif machine.startswith(\"arm\") or machine.startswith(\"aarch\"):\n            return CpuArchEnum.ARM\n        elif machine.startswith(\"ppc\"):\n            return CpuArchEnum.POWERPC\n\n        return CpuArchEnum.OTHER if machine else CpuArchEnum.UNKNOWN\n\n    @classmethod\n    def is_pin_memory_available(cls) -> bool:\n        \"\"\"Checks whether pin memory is available on the current platform.\"\"\"\n        if in_wsl():\n            # Pinning memory in WSL is not supported.\n            # https://docs.nvidia.com/cuda/wsl-user-guide/index.html#known-limitations-for-linux-cuda-applications\n            logger.warning(\"Using 'pin_memory=False' as WSL is detected. \"\n                           \"This may slow down the performance.\")\n            return False\n        return True\n\n    @classmethod\n    def get_current_memory_usage(cls,\n                                 device: Optional[torch.types.Device] = None\n                                 ) -> float:\n        \"\"\"\n        Return the memory usage in bytes.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_punica_wrapper(cls) -> str:\n        \"\"\"\n        Return the punica wrapper for current platform.\n        \"\"\"\n        raise NotImplementedError\n\n    @classmethod\n    def get_infinity_values(cls, dtype: torch.dtype) -> tuple[float, float]:\n        \"\"\"\n        Return the platform specific values for (-inf, inf)\n        \"\"\"\n        return float(\"-inf\"), float(\"inf\")\n\n    @classmethod\n    def can_update_inplace(cls) -> bool:\n        \"\"\"\n        Checks if the platform allows inplace memory updates\n        \"\"\"\n        return True\n\n    @classmethod\n    def get_lora_vocab_padding_size(cls) -> int:\n        \"\"\"\n        Returns how much padding the LoRA logits need for kernels\n        \"\"\"\n        return 256\n\n    @classmethod\n    def get_device_communicator_cls(cls) -> str:\n        \"\"\"\n        Get device specific communicator class for distributed communication.\n        \"\"\"\n        return \"vllm.distributed.device_communicators.base_device_communicator.DeviceCommunicatorBase\"  # noqa\n\n    @classmethod\n    def supports_mx(cls) -> bool:\n        \"\"\"\n        Returns whether the current platform supports MX types.\n        \"\"\"\n        return False\n\n    @classmethod\n    def supports_fp8(cls) -> bool:\n        \"\"\"\n        Returns whether the current platform supports FP8 types.\n        \"\"\"\n        return False\n\n    @classmethod\n    def is_fp8_fnuz(cls) -> bool:\n        \"\"\"\n        Returns whether the preferred FP8 type is FNUZ on the current platform.\n\n        There are two representations of FP8, OCP FP8 and FNUZ FP8.\n        The OCP specification can be found at https://tinyurl.com/b7jvwpft.\n        The FNUZ specification can be found at https://tinyurl.com/5n6hwwu5.\n\n        AMD's MI300 and MI325 have native hardware support for FNUZ. All other\n        hardware has converged on the OCP FP8 standard.\n        \"\"\"\n        return False\n\n    @classmethod\n    def fp8_dtype(cls) -> torch.dtype:\n        \"\"\"\n        Returns the preferred FP8 type on the current platform.\n\n        See the documentation for is_fp8_fnuz for details.\n        \"\"\"\n        return torch.float8_e4m3fn\n\n    @classmethod\n    def use_all_gather(cls) -> bool:\n        \"\"\"\n        Whether to use allgather in LogitsProcessor to gather the logits.\n        \"\"\"\n        import vllm.envs as envs\n        from vllm.config import get_current_vllm_config\n\n        parallel_config = get_current_vllm_config().parallel_config\n        return (envs.VLLM_USE_V1\n                or parallel_config.distributed_executor_backend\n                == \"external_launcher\")\n\n    @classmethod\n    def supports_v1(cls, model_config: ModelConfig) -> bool:\n        \"\"\"Returns whether the current platform can support v1 for the supplied\n        model configuration.\n        \"\"\"\n        return False\n\n    @classmethod\n    def use_custom_allreduce(cls) -> bool:\n        \"\"\"\n        Returns if custom allreduce is supported on the current platform\n        \"\"\"\n        return False\n\n    @classmethod\n    def validate_request(\n        cls,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        processed_inputs: ProcessorInputs,\n    ) -> None:\n        \"\"\"Raises if this request is unsupported on this platform\"\"\"\n\n    def __getattr__(self, key: str):\n        device = getattr(torch, self.device_type, None)\n        if device is not None and hasattr(device, key):\n            return getattr(device, key)\n        else:\n            logger.warning(\"Current platform %s does not have '%s'\" \\\n            \" attribute.\", self.device_type, key)\n            return None\n\n    @classmethod\n    def get_cu_count(cls, device_id: int = 0) -> int:\n        \"\"\"\n        Returns the total number of compute units (CU) on single GPU.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass UnspecifiedPlatform(Platform):\n    _enum = PlatformEnum.UNSPECIFIED\n    device_type = \"\"\n", 484], "/home/jeromeku/vllm/vllm/sampling_params.py": ["# SPDX-License-Identifier: Apache-2.0\n\"\"\"Sampling parameters for text generation.\"\"\"\nimport copy\nfrom dataclasses import dataclass\nfrom enum import Enum, IntEnum\nfrom functools import cached_property\nfrom typing import Annotated, Any, Optional, Union\n\nimport msgspec\nfrom pydantic import BaseModel\nfrom typing_extensions import deprecated\n\nfrom vllm.logger import init_logger\nfrom vllm.logits_process import LogitsProcessor\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\n\nlogger = init_logger(__name__)\n\n_SAMPLING_EPS = 1e-5\n_MAX_TEMP = 1e-2\n\n\nclass SamplingType(IntEnum):\n    GREEDY = 0\n    RANDOM = 1\n    RANDOM_SEED = 2\n\n\n# maybe make msgspec?\n@dataclass\nclass GuidedDecodingParams:\n    \"\"\"One of these fields will be used to build a logit processor.\"\"\"\n    json: Optional[Union[str, dict]] = None\n    regex: Optional[str] = None\n    choice: Optional[list[str]] = None\n    grammar: Optional[str] = None\n    json_object: Optional[bool] = None\n    \"\"\"These are other options that can be set\"\"\"\n    backend: Optional[str] = None\n    backend_was_auto: bool = False\n    disable_fallback: bool = False\n    disable_any_whitespace: bool = False\n    disable_additional_properties: bool = False\n    whitespace_pattern: Optional[str] = None\n    structural_tag: Optional[str] = None\n\n    @staticmethod\n    def from_optional(\n        json: Optional[Union[dict, BaseModel, str]] = None,\n        regex: Optional[str] = None,\n        choice: Optional[list[str]] = None,\n        grammar: Optional[str] = None,\n        json_object: Optional[bool] = None,\n        backend: Optional[str] = None,\n        whitespace_pattern: Optional[str] = None,\n        structural_tag: Optional[str] = None,\n    ) -> Optional[\"GuidedDecodingParams\"]:\n        if all(arg is None for arg in (json, regex, choice, grammar,\n                                       json_object, structural_tag)):\n            return None\n        # Extract json schemas from pydantic models\n        if isinstance(json, (BaseModel, type(BaseModel))):\n            json = json.model_json_schema()\n        return GuidedDecodingParams(\n            json=json,\n            regex=regex,\n            choice=choice,\n            grammar=grammar,\n            json_object=json_object,\n            backend=backend,\n            whitespace_pattern=whitespace_pattern,\n            structural_tag=structural_tag,\n        )\n\n    def __post_init__(self):\n        \"\"\"Validate that some fields are mutually exclusive.\"\"\"\n        guide_count = sum([\n            self.json is not None, self.regex is not None, self.choice\n            is not None, self.grammar is not None, self.json_object is not None\n        ])\n        if guide_count > 1:\n            raise ValueError(\n                \"You can only use one kind of guided decoding but multiple are \"\n                f\"specified: {self.__dict__}\")\n\n        if self.backend is not None and \":\" in self.backend:\n            self._extract_backend_options()\n\n    @deprecated(\n        \"Passing guided decoding backend options inside backend in the format \"\n        \"'backend:...' is deprecated. This will be removed in v0.10.0. Please \"\n        \"use the dedicated arguments '--disable-fallback', \"\n        \"'--disable-any-whitespace' and '--disable-additional-properties' \"\n        \"instead.\")\n    def _extract_backend_options(self):\n        \"\"\"Extract backend options from the backend string.\"\"\"\n        assert isinstance(self.backend, str)\n        self.backend, options = self.backend.split(\":\")\n        options_set = set(options.strip().split(\",\"))\n        if \"no-fallback\" in options_set:\n            self.disable_fallback = True\n        if \"disable-any-whitespace\" in options_set:\n            self.disable_any_whitespace = True\n        if \"no-additional-properties\" in options_set:\n            self.disable_additional_properties = True\n\n\nclass RequestOutputKind(Enum):\n    # Return entire output so far in every RequestOutput\n    CUMULATIVE = 0\n    # Return only deltas in each RequestOutput\n    DELTA = 1\n    # Do not return intermediate RequestOutput\n    FINAL_ONLY = 2\n\n\nclass SamplingParams(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        # required for @cached_property.\n        dict=True):  # type: ignore[call-arg]\n    \"\"\"Sampling parameters for text generation.\n\n    Overall, we follow the sampling parameters from the OpenAI text completion\n    API (https://platform.openai.com/docs/api-reference/completions/create).\n    In addition, we support beam search, which is not supported by OpenAI.\n\n    Args:\n        n: Number of output sequences to return for the given prompt.\n        best_of: Number of output sequences that are generated from the prompt.\n            From these `best_of` sequences, the top `n` sequences are returned.\n            `best_of` must be greater than or equal to `n`. By default,\n            `best_of` is set to `n`. Warning, this is only supported in V0.\n        presence_penalty: Float that penalizes new tokens based on whether they\n            appear in the generated text so far. Values > 0 encourage the model\n            to use new tokens, while values < 0 encourage the model to repeat\n            tokens.\n        frequency_penalty: Float that penalizes new tokens based on their\n            frequency in the generated text so far. Values > 0 encourage the\n            model to use new tokens, while values < 0 encourage the model to\n            repeat tokens.\n        repetition_penalty: Float that penalizes new tokens based on whether\n            they appear in the prompt and the generated text so far. Values > 1\n            encourage the model to use new tokens, while values < 1 encourage\n            the model to repeat tokens.\n        temperature: Float that controls the randomness of the sampling. Lower\n            values make the model more deterministic, while higher values make\n            the model more random. Zero means greedy sampling.\n        top_p: Float that controls the cumulative probability of the top tokens\n            to consider. Must be in (0, 1]. Set to 1 to consider all tokens.\n        top_k: Integer that controls the number of top tokens to consider. Set\n            to 0 (or -1) to consider all tokens.\n        min_p: Float that represents the minimum probability for a token to be\n            considered, relative to the probability of the most likely token.\n            Must be in [0, 1]. Set to 0 to disable this.\n        seed: Random seed to use for the generation.\n        stop: list of strings that stop the generation when they are generated.\n            The returned output will not contain the stop strings.\n        stop_token_ids: list of tokens that stop the generation when they are\n            generated. The returned output will contain the stop tokens unless\n            the stop tokens are special tokens.\n        bad_words: list of words that are not allowed to be generated.\n            More precisely, only the last token of a corresponding\n            token sequence is not allowed when the next generated token\n            can complete the sequence.\n        include_stop_str_in_output: Whether to include the stop strings in\n            output text. Defaults to False.\n        ignore_eos: Whether to ignore the EOS token and continue generating\n            tokens after the EOS token is generated.\n        max_tokens: Maximum number of tokens to generate per output sequence.\n        min_tokens: Minimum number of tokens to generate per output sequence\n            before EOS or stop_token_ids can be generated\n        logprobs: Number of log probabilities to return per output token.\n            When set to None, no probability is returned. If set to a non-None\n            value, the result includes the log probabilities of the specified\n            number of most likely tokens, as well as the chosen tokens.\n            Note that the implementation follows the OpenAI API: The API will\n            always return the log probability of the sampled token, so there\n            may be up to `logprobs+1` elements in the response.\n        prompt_logprobs: Number of log probabilities to return per prompt token.\n        detokenize: Whether to detokenize the output. Defaults to True.\n        skip_special_tokens: Whether to skip special tokens in the output.\n        spaces_between_special_tokens: Whether to add spaces between special\n            tokens in the output.  Defaults to True.\n        logits_processors: list of functions that modify logits based on\n            previously generated tokens, and optionally prompt tokens as\n            a first argument.\n        truncate_prompt_tokens: If set to -1, will use the truncation size\n            supported by the model. If set to an integer k, will use only\n            the last k tokens from the prompt (i.e., left truncation).\n            Defaults to None (i.e., no truncation).\n        guided_decoding: If provided, the engine will construct a guided\n            decoding logits processor from these parameters. Defaults to None.\n        logit_bias: If provided, the engine will construct a logits processor\n            that applies these logit biases. Defaults to None.\n        allowed_token_ids: If provided, the engine will construct a logits\n            processor which only retains scores for the given token ids.\n            Defaults to None.\n        extra_args: Arbitrary additional args, that can be used by custom\n            sampling implementations. Not used by any in-tree sampling\n            implementations.\n    \"\"\"\n\n    n: int = 1\n    best_of: Optional[int] = None\n    _real_n: Optional[int] = None\n    presence_penalty: float = 0.0\n    frequency_penalty: float = 0.0\n    repetition_penalty: float = 1.0\n    temperature: float = 1.0\n    top_p: float = 1.0\n    top_k: int = 0\n    min_p: float = 0.0\n    seed: Optional[int] = None\n    stop: Optional[Union[str, list[str]]] = None\n    stop_token_ids: Optional[list[int]] = None\n    ignore_eos: bool = False\n    max_tokens: Optional[int] = 16\n    min_tokens: int = 0\n    logprobs: Optional[int] = None\n    prompt_logprobs: Optional[int] = None\n    # NOTE: This parameter is only exposed at the engine level for now.\n    # It is not exposed in the OpenAI API server, as the OpenAI API does\n    # not support returning only a list of token IDs.\n    detokenize: bool = True\n    skip_special_tokens: bool = True\n    spaces_between_special_tokens: bool = True\n    # Optional[list[LogitsProcessor]] type. We use Any here because\n    # Optional[list[LogitsProcessor]] type is not supported by msgspec.\n    logits_processors: Optional[Any] = None\n    include_stop_str_in_output: bool = False\n    truncate_prompt_tokens: Optional[Annotated[int, msgspec.Meta(ge=1)]] = None\n    output_kind: RequestOutputKind = RequestOutputKind.CUMULATIVE\n\n    # The below fields are not supposed to be used as an input.\n    # They are set in post_init.\n    output_text_buffer_length: int = 0\n    _all_stop_token_ids: set[int] = msgspec.field(default_factory=set)\n\n    # Fields used to construct logits processors\n    guided_decoding: Optional[GuidedDecodingParams] = None\n    logit_bias: Optional[dict[int, float]] = None\n    allowed_token_ids: Optional[list[int]] = None\n    extra_args: Optional[dict[str, Any]] = None\n\n    # Fields used for bad words\n    bad_words: Optional[list[str]] = None\n    _bad_words_token_ids: Optional[list[list[int]]] = None\n\n    @staticmethod\n    def from_optional(\n        n: Optional[int] = 1,\n        best_of: Optional[int] = None,\n        presence_penalty: Optional[float] = 0.0,\n        frequency_penalty: Optional[float] = 0.0,\n        repetition_penalty: Optional[float] = 1.0,\n        temperature: Optional[float] = 1.0,\n        top_p: Optional[float] = 1.0,\n        top_k: int = 0,\n        min_p: float = 0.0,\n        seed: Optional[int] = None,\n        stop: Optional[Union[str, list[str]]] = None,\n        stop_token_ids: Optional[list[int]] = None,\n        bad_words: Optional[list[str]] = None,\n        include_stop_str_in_output: bool = False,\n        ignore_eos: bool = False,\n        max_tokens: Optional[int] = 16,\n        min_tokens: int = 0,\n        logprobs: Optional[int] = None,\n        prompt_logprobs: Optional[int] = None,\n        detokenize: bool = True,\n        skip_special_tokens: bool = True,\n        spaces_between_special_tokens: bool = True,\n        logits_processors: Optional[list[LogitsProcessor]] = None,\n        truncate_prompt_tokens: Optional[Annotated[int,\n                                                   msgspec.Meta(ge=1)]] = None,\n        output_kind: RequestOutputKind = RequestOutputKind.CUMULATIVE,\n        guided_decoding: Optional[GuidedDecodingParams] = None,\n        logit_bias: Optional[Union[dict[int, float], dict[str, float]]] = None,\n        allowed_token_ids: Optional[list[int]] = None,\n        extra_args: Optional[dict[str, Any]] = None,\n    ) -> \"SamplingParams\":\n        if logit_bias is not None:\n            # Convert token_id to integer\n            # Clamp the bias between -100 and 100 per OpenAI API spec\n            logit_bias = {\n                int(token): min(100.0, max(-100.0, bias))\n                for token, bias in logit_bias.items()\n            }\n\n        return SamplingParams(\n            n=1 if n is None else n,\n            best_of=best_of,\n            presence_penalty=0.0\n            if presence_penalty is None else presence_penalty,\n            frequency_penalty=0.0\n            if frequency_penalty is None else frequency_penalty,\n            repetition_penalty=1.0\n            if repetition_penalty is None else repetition_penalty,\n            temperature=1.0 if temperature is None else temperature,\n            top_p=1.0 if top_p is None else top_p,\n            top_k=top_k,\n            min_p=min_p,\n            seed=seed,\n            stop=stop,\n            stop_token_ids=stop_token_ids,\n            bad_words=bad_words,\n            include_stop_str_in_output=include_stop_str_in_output,\n            ignore_eos=ignore_eos,\n            max_tokens=max_tokens,\n            min_tokens=min_tokens,\n            logprobs=logprobs,\n            prompt_logprobs=prompt_logprobs,\n            detokenize=detokenize,\n            skip_special_tokens=skip_special_tokens,\n            spaces_between_special_tokens=spaces_between_special_tokens,\n            logits_processors=logits_processors,\n            truncate_prompt_tokens=truncate_prompt_tokens,\n            output_kind=output_kind,\n            guided_decoding=guided_decoding,\n            logit_bias=logit_bias,\n            allowed_token_ids=allowed_token_ids,\n            extra_args=extra_args,\n        )\n\n    def __post_init__(self) -> None:\n        # how we deal with `best_of``:\n        # if `best_of`` is not set, we default to `n`;\n        # if `best_of`` is set, we set `n`` to `best_of`,\n        # and set `_real_n`` to the original `n`.\n        # when we return the result, we will check\n        # if we need to return `n` or `_real_n` results\n        if self.best_of:\n            if self.best_of < self.n:\n                raise ValueError(\n                    f\"best_of must be greater than or equal to n, \"\n                    f\"got n={self.n} and best_of={self.best_of}.\")\n            if not self._real_n:\n                self._real_n = self.n\n                self.n = self.best_of\n\n        if 0 < self.temperature < _MAX_TEMP:\n            logger.warning(\n                \"temperature %s is less than %s, which may cause numerical \"\n                \"errors nan or inf in tensors. We have maxed it out to %s.\",\n                self.temperature, _MAX_TEMP, _MAX_TEMP)\n            self.temperature = max(self.temperature, _MAX_TEMP)\n\n        if self.seed == -1:\n            self.seed = None\n\n        if self.stop is None:\n            self.stop = []\n        elif isinstance(self.stop, str):\n            self.stop = [self.stop]\n\n        if self.stop_token_ids is None:\n            self.stop_token_ids = []\n\n        if self.bad_words is None:\n            self.bad_words = []\n\n        if self.logprobs is True:\n            self.logprobs = 1\n\n        if self.prompt_logprobs is True:\n            self.prompt_logprobs = 1\n\n        # Number of characters to hold back for stop string evaluation\n        # until sequence is finished.\n        if self.stop and not self.include_stop_str_in_output:\n            self.output_text_buffer_length = max(len(s) for s in self.stop) - 1\n\n        self._verify_args()\n\n        if self.temperature < _SAMPLING_EPS:\n            # Zero temperature means greedy sampling.\n            self.top_p = 1.0\n            self.top_k = 0\n            self.min_p = 0.0\n            self._verify_greedy_sampling()\n\n        # eos_token_id is added to this by the engine\n        self._all_stop_token_ids.update(self.stop_token_ids)\n\n    def _verify_args(self) -> None:\n        if not isinstance(self.n, int):\n            raise ValueError(f\"n must be an int, but is of \"\n                             f\"type {type(self.n)}\")\n        if self.n < 1:\n            raise ValueError(f\"n must be at least 1, got {self.n}.\")\n        if not -2.0 <= self.presence_penalty <= 2.0:\n            raise ValueError(\"presence_penalty must be in [-2, 2], got \"\n                             f\"{self.presence_penalty}.\")\n        if not -2.0 <= self.frequency_penalty <= 2.0:\n            raise ValueError(\"frequency_penalty must be in [-2, 2], got \"\n                             f\"{self.frequency_penalty}.\")\n        if self.repetition_penalty <= 0.0:\n            raise ValueError(\n                \"repetition_penalty must be greater than zero, got \"\n                f\"{self.repetition_penalty}.\")\n        if self.temperature < 0.0:\n            raise ValueError(\n                f\"temperature must be non-negative, got {self.temperature}.\")\n        if not 0.0 < self.top_p <= 1.0:\n            raise ValueError(f\"top_p must be in (0, 1], got {self.top_p}.\")\n        # quietly accept -1 as disabled, but prefer 0\n        if self.top_k < -1:\n            raise ValueError(f\"top_k must be 0 (disable), or at least 1, \"\n                             f\"got {self.top_k}.\")\n        if not isinstance(self.top_k, int):\n            raise TypeError(\n                f\"top_k must be an integer, got {type(self.top_k).__name__}\")\n        if not 0.0 <= self.min_p <= 1.0:\n            raise ValueError(\"min_p must be in [0, 1], got \"\n                             f\"{self.min_p}.\")\n        if self.max_tokens is not None and self.max_tokens < 1:\n            raise ValueError(\n                f\"max_tokens must be at least 1, got {self.max_tokens}.\")\n        if self.min_tokens < 0:\n            raise ValueError(f\"min_tokens must be greater than or equal to 0, \"\n                             f\"got {self.min_tokens}.\")\n        if self.max_tokens is not None and self.min_tokens > self.max_tokens:\n            raise ValueError(\n                f\"min_tokens must be less than or equal to \"\n                f\"max_tokens={self.max_tokens}, got {self.min_tokens}.\")\n        if self.logprobs is not None and self.logprobs < 0:\n            raise ValueError(\n                f\"logprobs must be non-negative, got {self.logprobs}.\")\n        if self.prompt_logprobs is not None and self.prompt_logprobs < 0:\n            raise ValueError(f\"prompt_logprobs must be non-negative, got \"\n                             f\"{self.prompt_logprobs}.\")\n        if (self.truncate_prompt_tokens is not None\n                and self.truncate_prompt_tokens < 1):\n            raise ValueError(f\"truncate_prompt_tokens must be >= 1, \"\n                             f\"got {self.truncate_prompt_tokens}\")\n        assert isinstance(self.stop_token_ids, list)\n        if not all(isinstance(st_id, int) for st_id in self.stop_token_ids):\n            raise ValueError(f\"stop_token_ids must contain only integers, \"\n                             f\"got {self.stop_token_ids}.\")\n        assert isinstance(self.stop, list)\n        if any(not stop_str for stop_str in self.stop):\n            raise ValueError(\"stop cannot contain an empty string.\")\n        if self.stop and not self.detokenize:\n            raise ValueError(\n                \"stop strings are only supported when detokenize is True. \"\n                \"Set detokenize=True to use stop.\")\n        if self.best_of != self._real_n and self.output_kind == (\n                RequestOutputKind.DELTA):\n            raise ValueError(\"best_of must equal n to use output_kind=DELTA\")\n\n    def _verify_greedy_sampling(self) -> None:\n        if self.n > 1:\n            raise ValueError(\"n must be 1 when using greedy sampling, \"\n                             f\"got {self.n}.\")\n\n    def update_from_generation_config(\n            self,\n            generation_config: dict[str, Any],\n            model_eos_token_id: Optional[int] = None) -> None:\n        \"\"\"Update if there are non-default values from generation_config\"\"\"\n\n        if model_eos_token_id is not None:\n            # Add the eos token id into the sampling_params to support\n            # min_tokens processing.\n            self._all_stop_token_ids.add(model_eos_token_id)\n\n        # Update eos_token_id for generation\n        if (eos_ids := generation_config.get(\"eos_token_id\")) is not None:\n            # it can be either int or list of int\n            eos_ids = {eos_ids} if isinstance(eos_ids, int) else set(eos_ids)\n            if model_eos_token_id is not None:\n                # We don't need to include the primary eos_token_id in\n                # stop_token_ids since it's handled separately for stopping\n                # purposes.\n                eos_ids.discard(model_eos_token_id)\n            if eos_ids:\n                self._all_stop_token_ids.update(eos_ids)\n                if not self.ignore_eos:\n                    eos_ids.update(self.stop_token_ids)\n                    self.stop_token_ids = list(eos_ids)\n\n    def update_from_tokenizer(self, tokenizer: AnyTokenizer) -> None:\n        if not self.bad_words:\n            return\n        self._bad_words_token_ids = []\n        for bad_word in self.bad_words:\n            # To prohibit words both at the beginning\n            # and in the middle of text\n            # (related to add_prefix_space tokenizer parameter)\n            for add_prefix_space in [False, True]:\n                prefix = \" \" if add_prefix_space else \"\"\n                prompt = prefix + bad_word.lstrip()\n                prompt_token_ids = tokenizer.encode(text=prompt,\n                                                    add_special_tokens=False)\n\n                # If no space at the beginning\n                # or if prefix space produces a new word token\n                if (not add_prefix_space) or (\n                        add_prefix_space and prompt_token_ids[0]\n                        != self._bad_words_token_ids[-1][0]\n                        and len(prompt_token_ids) == len(\n                            self._bad_words_token_ids[-1])):\n                    self._bad_words_token_ids.append(prompt_token_ids)\n\n        invalid_token_ids = [\n            token_id for bad_words_token_ids in self._bad_words_token_ids\n            for token_id in bad_words_token_ids\n            if token_id < 0 or token_id > tokenizer.max_token_id\n        ]\n        if len(invalid_token_ids) > 0:\n            raise ValueError(\n                f\"The model vocabulary size is {tokenizer.max_token_id+1},\"\n                f\" but the following tokens\"\n                f\" were specified as bad: {invalid_token_ids}.\"\n                f\" All token id values should be integers satisfying:\"\n                f\" 0 <= token_id <= {tokenizer.max_token_id}.\")\n\n    @cached_property\n    def sampling_type(self) -> SamplingType:\n        if self.temperature < _SAMPLING_EPS:\n            return SamplingType.GREEDY\n        if self.seed is not None:\n            return SamplingType.RANDOM_SEED\n        return SamplingType.RANDOM\n\n    @property\n    def all_stop_token_ids(self) -> set[int]:\n        return self._all_stop_token_ids\n\n    @property\n    def bad_words_token_ids(self) -> Optional[list[list[int]]]:\n        # For internal use only. Backward compatibility not guaranteed\n        return self._bad_words_token_ids\n\n    def clone(self) -> \"SamplingParams\":\n        \"\"\"Deep copy, but maybe not the LogitsProcessor objects.\n\n        LogitsProcessor objects may contain an arbitrary, nontrivial amount of\n        data that is expensive to copy. However, if not copied, the processor\n        needs to support parallel decoding for multiple sequences\n        See https://github.com/vllm-project/vllm/issues/3087\n        \"\"\"\n\n        logit_processor_refs = None if self.logits_processors is None else {\n            id(lp): lp.clone() if hasattr(lp, 'clone') else lp\n            for lp in self.logits_processors\n        }\n        return copy.deepcopy(self, memo=logit_processor_refs)\n\n    def __repr__(self) -> str:\n        return (\n            f\"SamplingParams(n={self.n}, \"\n            f\"presence_penalty={self.presence_penalty}, \"\n            f\"frequency_penalty={self.frequency_penalty}, \"\n            f\"repetition_penalty={self.repetition_penalty}, \"\n            f\"temperature={self.temperature}, \"\n            f\"top_p={self.top_p}, \"\n            f\"top_k={self.top_k}, \"\n            f\"min_p={self.min_p}, \"\n            f\"seed={self.seed}, \"\n            f\"stop={self.stop}, \"\n            f\"stop_token_ids={self.stop_token_ids}, \"\n            f\"bad_words={self.bad_words}, \"\n            f\"include_stop_str_in_output={self.include_stop_str_in_output}, \"\n            f\"ignore_eos={self.ignore_eos}, \"\n            f\"max_tokens={self.max_tokens}, \"\n            f\"min_tokens={self.min_tokens}, \"\n            f\"logprobs={self.logprobs}, \"\n            f\"prompt_logprobs={self.prompt_logprobs}, \"\n            f\"skip_special_tokens={self.skip_special_tokens}, \"\n            \"spaces_between_special_tokens=\"\n            f\"{self.spaces_between_special_tokens}, \"\n            f\"truncate_prompt_tokens={self.truncate_prompt_tokens}, \"\n            f\"guided_decoding={self.guided_decoding}, \"\n            f\"extra_args={self.extra_args})\")\n\n\nclass BeamSearchParams(\n        msgspec.Struct,\n        omit_defaults=True,  # type: ignore[call-arg]\n        # required for @cached_property.\n        dict=True):  # type: ignore[call-arg]\n    \"\"\"Beam search parameters for text generation.\"\"\"\n    beam_width: int\n    max_tokens: int\n    ignore_eos: bool = False\n    temperature: float = 0.0\n    length_penalty: float = 1.0\n    include_stop_str_in_output: bool = False\n", 590], "/home/jeromeku/vllm/vllm/v1/engine/logprobs.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport itertools\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import Optional\n\nfrom vllm.logger import init_logger\nfrom vllm.sequence import Logprob, PromptLogprobs, SampleLogprobs\nfrom vllm.transformers_utils.detokenizer_utils import (\n    AnyTokenizer, convert_ids_list_to_tokens)\nfrom vllm.v1.engine import EngineCoreOutput, EngineCoreRequest\nfrom vllm.v1.outputs import LogprobsLists, LogprobsTensors\n\nlogger = init_logger(__name__)\n\nNONES = itertools.repeat(None)\n\n\n@dataclass\nclass LogprobsProcessor:\n\n    # Tokenizer for this request,\n    # None if detokenization is disabled.\n    tokenizer: Optional[AnyTokenizer]\n\n    # Logprobs for this request\n    logprobs: Optional[SampleLogprobs]\n    prompt_logprobs: Optional[PromptLogprobs]\n    cumulative_logprob: Optional[float]\n    num_logprobs: Optional[int]\n    num_prompt_logprobs: Optional[int]\n\n    @classmethod\n    def from_new_request(\n        cls,\n        tokenizer: Optional[AnyTokenizer],\n        request: EngineCoreRequest,\n    ) -> \"LogprobsProcessor\":\n        num_logprobs = request.sampling_params.logprobs\n        num_prompt_logprobs = request.sampling_params.prompt_logprobs\n        return cls(\n            tokenizer=tokenizer,\n            cumulative_logprob=(None if num_logprobs is None else 0.),\n            logprobs=(None if num_logprobs is None else []),\n            # NOTE: logprob of first prompt token is None.\n            prompt_logprobs=(None if num_prompt_logprobs is None else [None]),\n            num_prompt_logprobs=num_prompt_logprobs,\n            num_logprobs=num_logprobs,\n        )\n\n    def _update_sample_logprobs(self, logprobs_lists: LogprobsLists) -> None:\n        \"\"\"Update with sample logprobs from EngineCore.\n\n        Outer lists are only of len > 1 if EngineCore made\n        >1 tokens in prior step (e.g. in spec decoding).\n\n        Args:\n          logprobs_lists: the lists of logprob tokens, logprobs, and ranks.\n\n        \"\"\"\n\n        assert self.num_logprobs is not None\n        assert self.logprobs is not None\n        assert self.cumulative_logprob is not None\n\n        token_ids_lst, logprobs_lst, ranks_lst = logprobs_lists\n\n        for rank, logprobs, token_ids in zip(ranks_lst, logprobs_lst,\n                                             token_ids_lst):\n\n            # Detokenize (non-incrementally).\n            decoded_tokens = NONES if self.tokenizer is None else (\n                convert_ids_list_to_tokens(self.tokenizer, token_ids))\n\n            # Sampler puts the sampled logprob in first.\n            sampled_token_logprob = logprobs[0]\n            self.cumulative_logprob += sampled_token_logprob\n\n            # Update with the Logprob dictionary for this pos.\n            self.logprobs.append(\n                self._make_logprob_dict(\n                    logprobs,\n                    token_ids,\n                    decoded_tokens,\n                    rank,\n                    self.num_logprobs,\n                ))\n\n    def _update_prompt_logprobs(\n        self,\n        prompt_logprobs_tensors: LogprobsTensors,\n    ) -> None:\n        \"\"\"Update with prompt logprobs from EngineCore.\n\n        Args:\n          prompt_logprobs_tensors: tuple containing the prompt logprobs\n                                   tensors.\n\n        \"\"\"\n\n        # Prompt logprobs are enabled.\n        assert self.num_prompt_logprobs is not None\n        assert self.prompt_logprobs is not None\n\n        token_ids, logprobs, ranks = prompt_logprobs_tensors\n\n        # Detokenize non-incrementally.\n        # Output is flat: [num_tok, num_lps] -> [num_tok * num_lps]\n        decoded_tokens = None if self.tokenizer is None else (\n            convert_ids_list_to_tokens(self.tokenizer,\n                                       token_ids.flatten().tolist()))\n\n        # Recover shapes.\n        num_prompt_tokens, num_logprobs = logprobs.shape\n\n        # Pythonize the torch tensors.\n        prompt_token_ranks = ranks.tolist()\n        prompt_logprobs = logprobs.tolist()\n        token_ids = token_ids.tolist()\n\n        # Make Logprob for each position.\n        for pos in range(num_prompt_tokens):\n            # Handle flattening.\n            offset = pos * num_logprobs\n            offset_end = offset + num_logprobs\n            decoded_tokens_for_pos = NONES \\\n            if decoded_tokens is None else decoded_tokens[offset:offset_end]\n\n            # Update with the Logprob dictionary for this pos.\n            self.prompt_logprobs.append(\n                self._make_logprob_dict(prompt_logprobs[pos], token_ids[pos],\n                                        decoded_tokens_for_pos,\n                                        prompt_token_ranks[pos],\n                                        self.num_prompt_logprobs))\n\n    def pop_prompt_logprobs(self) -> Optional[PromptLogprobs]:\n        \"\"\"Pop and return all request prompt logprobs\n        \n        The logprobs processor aggregates prompt chunk logprobs\n        over one or more prefill chunks. This method returns\n        all prompt logprobs at once and then forgets them.\n        Ensures correct RequestOutputKind.DELTA semantics\n        wherein all prompt logprobs are returned at once at\n        the end of prefill.\n\n        Returns:\n          None if prompt logprobs are disabled for this request.\n          List of all prompt logprobs, otherwise.\n        \"\"\"\n        plp = self.prompt_logprobs\n        if plp:\n            self.prompt_logprobs = []\n        return plp\n\n    @staticmethod\n    def _make_logprob_dict(\n        logprobs: list[float],\n        logprob_token_ids: list[int],\n        decoded_tokens: Iterable[Optional[str]],\n        rank: int,\n        num_logprobs: int,\n    ) -> dict[int, Logprob]:\n        \"\"\"Make a Logprob dictionary for a position.\n\n        Args:\n          logprobs: list of log probabilities\n          logprob_token_ids: list of top token ids\n          decoded_tokens: list of decoded top tokens\n          rank: rank of the sampled token\n          num_logprobs: number of logprobs requested\n            by the user (in addition to sampled logprob)\n\n        Returns:\n          dict[token id, Logprob]\n        \"\"\"\n\n        # We do not need a special case for the sampled token\n        # being in the topk, since inserting duplicated data\n        # into a dictionary twice is the same as doing it once.\n        topk_ranks = range(1, num_logprobs + 1)\n        ranks = itertools.chain((rank, ), topk_ranks)\n\n        return {\n            token_id: Logprob(\n                logprob=logprob,\n                rank=rank,\n                decoded_token=token,\n            )\n            for token_id, logprob, rank, token in zip(\n                logprob_token_ids, logprobs, ranks, decoded_tokens)\n        }\n\n    def update_from_output(self, output: EngineCoreOutput) -> None:\n        if output.new_logprobs is not None:\n            self._update_sample_logprobs(output.new_logprobs)\n        if output.new_prompt_logprobs_tensors is not None:\n            self._update_prompt_logprobs(output.new_prompt_logprobs_tensors)\n", 198], "/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py": ["# SPDX-License-Identifier: Apache-2.0\nfrom abc import ABC, abstractmethod\nfrom typing import Optional\n\nimport tokenizers\nfrom packaging import version\nfrom tokenizers import Tokenizer\nfrom tokenizers.decoders import DecodeStream\nfrom transformers import PreTrainedTokenizerFast\n\nfrom vllm.engine.output_processor.stop_checker import StopChecker\nfrom vllm.logger import init_logger\nfrom vllm.transformers_utils.detokenizer_utils import (\n    AnyTokenizer, convert_prompt_ids_to_tokens, detokenize_incrementally)\nfrom vllm.v1.engine import EngineCoreRequest\n\nlogger = init_logger(__name__)\n\n\nclass IncrementalDetokenizer:\n\n    def __init__(self):\n        self.token_ids: list[int] = []\n\n    @property\n    def output_token_ids(self) -> list[int]:\n        return self.token_ids\n\n    def update(self, new_token_ids: list[int],\n               stop_terminated: bool) -> Optional[str]:\n        self.token_ids.extend(new_token_ids)\n        return None\n\n    def get_next_output_text(self, finished: bool, delta: bool) -> str:\n        return \"\"\n\n    @classmethod\n    def from_new_request(\n        cls,\n        tokenizer: Optional[AnyTokenizer],\n        request: EngineCoreRequest,\n    ) -> \"IncrementalDetokenizer\":\n\n        if tokenizer is None:\n            # No tokenizer => skipping detokenization.\n            return IncrementalDetokenizer()\n\n        if (isinstance(tokenizer, PreTrainedTokenizerFast) and version.parse(\n                tokenizers.__version__) >= version.parse(\"0.21.1\")):\n            # Fast tokenizer => use tokenizers library DecodeStream.\n            # And only tokenizers >= 0.21.1 supports Fast Detokenizer.\n            return FastIncrementalDetokenizer(tokenizer, request)\n\n        # Fall back to slow python-based incremental detokenization.\n        return SlowIncrementalDetokenizer(tokenizer, request)\n\n\nclass BaseIncrementalDetokenizer(IncrementalDetokenizer, ABC):\n\n    def __init__(self, request: EngineCoreRequest):\n        super().__init__()\n\n        # Stop strings\n        params = request.sampling_params\n        self.stop = stop = params.stop\n        self.include_stop_str_in_output = params.include_stop_str_in_output\n\n        # Number of chars to hold back when stop strings are to be excluded\n        # from streamed output.\n        if stop and not self.include_stop_str_in_output:\n            self.stop_buffer_length = max(len(s) for s in stop) - 1\n        else:\n            self.stop_buffer_length = 0\n        self._last_output_text_offset: int = 0\n\n        # Generation data\n        self.output_text = \"\"\n\n    def update(self, new_token_ids: list[int],\n               stop_terminated: bool) -> Optional[str]:\n        \"\"\"\n        Update RequestState for the request_id by:\n            1) Detokenize the new token ids incrementally.\n            2) Evaluate stop criteria.\n\n        Return matched stop string or None.\n        \"\"\"\n        if not new_token_ids:\n            # Skip detokenization if no new token ids.\n            return None\n\n        if stop_terminated and not self.include_stop_str_in_output:\n            # If stop-terminated, exclude last token from detokenization\n            # based on include_stop_str_in_output parameter.\n            skipped_stop_token_id = new_token_ids[-1]\n            new_token_ids = new_token_ids[:-1]\n        else:\n            skipped_stop_token_id = None\n\n        # 1) Detokenize the new token ids incrementally.\n        # TODO(woosuk): This method becomes very inefficient when the number of\n        # new_token_ids is more than 1. We need to optimize this.\n        offset_before = len(self.output_text)\n        for new_token_id in new_token_ids:\n            self.token_ids.append(new_token_id)\n            self.output_text += self.decode_next(new_token_id)\n\n        if stop_terminated:\n            if skipped_stop_token_id is not None:\n                # Cleanup after skipping detokenization.\n                self.token_ids.append(skipped_stop_token_id)\n            # Stop token triggered; skip stop string check.\n            return None\n\n        # 2) Evaluate stop strings.\n        stop_string = None\n        if self.stop:\n            stop = StopChecker.check_stop_strings(\n                output_text=self.output_text,\n                new_char_count=len(self.output_text) - offset_before,\n                stop=self.stop,\n                include_in_output=self.include_stop_str_in_output,\n            )\n            if stop is not None:\n                stop_string, truncate_to = stop\n                if truncate_to != -1:\n                    self.output_text = self.output_text[:truncate_to]\n\n        return stop_string\n\n    @abstractmethod\n    def decode_next(self, next_token_id: int) -> str:\n        raise NotImplementedError\n\n    def get_next_output_text(self, finished: bool, delta: bool) -> str:\n        \"\"\"If delta is True, only new text since the last call to\n        this method is returned\"\"\"\n\n        # We return the full output text if the sequence is finished.\n        buffer_length = 0 if finished else self.stop_buffer_length\n        if not delta:\n            return self.output_text[:-buffer_length] if buffer_length else (\n                self.output_text)\n        length = len(self.output_text) - buffer_length\n        last_offset = self._last_output_text_offset\n        if last_offset < length:\n            self._last_output_text_offset = length\n            return self.output_text[last_offset:length]\n        return \"\"\n\n\nclass FastIncrementalDetokenizer(BaseIncrementalDetokenizer):\n\n    def __init__(self, tokenizer: PreTrainedTokenizerFast,\n                 request: EngineCoreRequest):\n        super().__init__(request)\n\n        sampling_params = request.sampling_params\n        self.stream = DecodeStream(\n            skip_special_tokens=sampling_params.skip_special_tokens)\n\n        self.tokenizer: Tokenizer = tokenizer._tokenizer\n\n        # Find a safe place to start.\n        prompt_suffix = request.prompt_token_ids\n        prompt_len = len(prompt_suffix)\n        if prompt_len > 4:\n            for i in range(4, min(prompt_len + 1, 24)):\n                suffix = request.prompt_token_ids[-i:]\n                if '\ufffd' not in self.tokenizer.decode(suffix):\n                    prompt_suffix = suffix\n                    break\n\n        # Prime the stream.\n        for tid in prompt_suffix:\n            self.stream.step(self.tokenizer, tid)\n\n        self.spaces_between_special_tokens = (\n            sampling_params.skip_special_tokens\n            or sampling_params.spaces_between_special_tokens)\n\n        if not self.spaces_between_special_tokens:\n            # Store dict of added token ids so that we can suppress\n            # the spaces between them.\n            if (added_token_ids := getattr(self.tokenizer, \"added_token_ids\",\n                                           None)) is None:\n                self.tokenizer.added_token_ids = added_token_ids = {\n                    tid: tok.content\n                    for tid, tok in\n                    self.tokenizer.get_added_tokens_decoder().items()\n                }\n\n            if added_token_ids:\n                self.last_special = False\n                self.added_token_ids = added_token_ids\n            else:\n                # No added tokens.\n                self.spaces_between_special_tokens = True\n\n    def decode_next(self, next_token_id: int) -> str:\n        token = self.stream.step(self.tokenizer, next_token_id)\n\n        if not self.spaces_between_special_tokens:\n            special_token = self.added_token_ids.get(next_token_id)\n            is_special = special_token is not None\n            if is_special and self.last_special:\n                # Return raw token string without any prefixed spaces.\n                token = special_token\n            self.last_special = is_special\n\n        return token or \"\"\n\n\nclass SlowIncrementalDetokenizer(BaseIncrementalDetokenizer):\n\n    def __init__(self, tokenizer: AnyTokenizer, request: EngineCoreRequest):\n        super().__init__(request)\n\n        self.tokenizer = tokenizer\n\n        # Metadata for incremental detokenization.\n        self.tokens, self.prefix_offset, self.read_offset = (\n            convert_prompt_ids_to_tokens(\n                tokenizer=tokenizer,\n                prompt_ids=request.prompt_token_ids,\n                skip_special_tokens=request.sampling_params.\n                skip_special_tokens,\n            ))\n\n        self.token_ids.extend(request.prompt_token_ids)\n        self.prompt_len = len(request.prompt_token_ids)\n\n        params = request.sampling_params\n        self.skip_special_tokens = params.skip_special_tokens\n        self.spaces_between_special_tokens = (\n            params.spaces_between_special_tokens)\n\n    @property\n    def output_token_ids(self) -> list[int]:\n        return self.token_ids if not self.prompt_len else (\n            self.token_ids[self.prompt_len:])\n\n    def decode_next(self, next_token_id: int) -> str:\n        new_tokens, decoded_text, prefix_offset, read_offset = (\n            detokenize_incrementally(\n                tokenizer=self.tokenizer,\n                all_input_ids=self.token_ids,\n                prev_tokens=self.tokens,\n                prefix_offset=self.prefix_offset,\n                read_offset=self.read_offset,\n                skip_special_tokens=self.skip_special_tokens,\n                spaces_between_special_tokens=self.\n                spaces_between_special_tokens,\n            ))\n\n        self.tokens.extend(new_tokens)\n        self.prefix_offset = prefix_offset\n        self.read_offset = read_offset\n\n        return decoded_text\n", 260], "/home/jeromeku/vllm/vllm/v1/engine/output_processor.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport asyncio\nfrom collections.abc import Iterable\nfrom dataclasses import dataclass\nfrom typing import Any, Optional, Union\n\nfrom vllm.outputs import CompletionOutput, RequestOutput\nfrom vllm.sampling_params import RequestOutputKind\nfrom vllm.transformers_utils.tokenizer import AnyTokenizer\nfrom vllm.transformers_utils.tokenizer_group import TokenizerGroup\nfrom vllm.v1.engine import EngineCoreOutput, EngineCoreRequest, FinishReason\nfrom vllm.v1.engine.detokenizer import IncrementalDetokenizer\nfrom vllm.v1.engine.logprobs import LogprobsProcessor\nfrom vllm.v1.engine.parallel_sampling import ParentRequest\nfrom vllm.v1.metrics.stats import (IterationStats, LoRARequestStates,\n                                   RequestStateStats)\n\n\nclass RequestOutputCollector:\n    \"\"\"\n    Collects streamed RequestOutputs per individual request,\n    for hand-off to the consuming asyncio generate task.\n\n    When streaming deltas, RequestOutputs are merged if the\n    producer gets ahead of the consumer.\n    \"\"\"\n\n    def __init__(self, output_kind: RequestOutputKind):\n        self.aggregate = output_kind == RequestOutputKind.DELTA\n        self.output: Optional[Union[RequestOutput, Exception]] = None\n        self.ready = asyncio.Event()\n\n    def put(self, output: Union[RequestOutput, Exception]) -> None:\n        \"\"\"Non-blocking put operation.\"\"\"\n        if self.output is None or isinstance(output, Exception):\n            self.output = output\n            self.ready.set()\n        elif isinstance(self.output, RequestOutput):\n            # This ensures that request outputs with different request indexes\n            # (if n > 1) do not override each other.\n            self.output.add(output, aggregate=self.aggregate)\n\n    async def get(self) -> RequestOutput:\n        \"\"\"Get operation blocks on put event.\"\"\"\n        while (output := self.output) is None:\n            await self.ready.wait()\n        self.output = None\n        self.ready.clear()\n        if isinstance(output, Exception):\n            raise output\n        return output\n\n    def get_nowait(self) -> Optional[RequestOutput]:\n        \"\"\"Non-blocking get operation.\"\"\"\n        output = self.output\n        if output is not None:\n            self.output = None\n            self.ready.clear()\n        if isinstance(output, Exception):\n            raise output\n        return output\n\n\n@dataclass\nclass OutputProcessorOutput:\n\n    request_outputs: list[RequestOutput]\n    reqs_to_abort: list[str]\n\n\nclass RequestState:\n\n    def __init__(\n        self,\n        request_id: str,\n        parent_req: Optional[ParentRequest],\n        request_index: int,\n        lora_name: Optional[str],\n        output_kind: RequestOutputKind,\n        prompt: Optional[str],\n        prompt_token_ids: list[int],\n        logprobs_processor: LogprobsProcessor,\n        detokenizer: IncrementalDetokenizer,\n        max_tokens_param: Optional[int],\n        arrival_time: float,\n        queue: Optional[RequestOutputCollector],\n        log_stats: bool,\n    ):\n        self.request_id = request_id\n        self.parent_req = parent_req\n        self.request_index = request_index\n        self.lora_name = lora_name\n        self.output_kind = output_kind\n        self.prompt = prompt\n        self.prompt_token_ids = prompt_token_ids\n        self.prompt_len = len(prompt_token_ids)\n        self.logprobs_processor = logprobs_processor\n        self.detokenizer = detokenizer\n        self.max_tokens_param = max_tokens_param\n        self.is_prefilling = True\n        self.queue = queue\n\n        self.stats = RequestStateStats(\n            arrival_time=arrival_time) if log_stats else None\n\n    @classmethod\n    def from_new_request(\n        cls,\n        tokenizer: AnyTokenizer,\n        request: EngineCoreRequest,\n        prompt: Optional[str],\n        parent_req: Optional[ParentRequest],\n        request_index: int,\n        queue: Optional[RequestOutputCollector],\n        log_stats: bool,\n    ) -> \"RequestState\":\n        if not request.sampling_params.detokenize:\n            tokenizer = None\n        return cls(\n            request_id=request.request_id,\n            parent_req=parent_req,\n            request_index=request_index,\n            lora_name=(request.lora_request.name\n                       if request.lora_request is not None else None),\n            output_kind=request.sampling_params.output_kind,\n            prompt=prompt,\n            prompt_token_ids=request.prompt_token_ids,\n            logprobs_processor=LogprobsProcessor.from_new_request(\n                tokenizer=tokenizer,\n                request=request,\n            ),\n            detokenizer=IncrementalDetokenizer.from_new_request(\n                tokenizer=tokenizer,\n                request=request,\n            ),\n            max_tokens_param=(request.sampling_params.max_tokens if\n                              request.sampling_params is not None else None),\n            arrival_time=request.arrival_time,\n            queue=queue,\n            log_stats=log_stats,\n        )\n\n    def make_request_output(\n        self,\n        new_token_ids: list[int],\n        finish_reason: Optional[FinishReason],\n        stop_reason: Union[int, str, None],\n        kv_transfer_params: Optional[dict[str, Any]] = None,\n    ) -> Optional[RequestOutput]:\n\n        finished = finish_reason is not None\n        final_only = self.output_kind == RequestOutputKind.FINAL_ONLY\n\n        if not finished and final_only:\n            # Only the final output is required in FINAL_ONLY mode.\n            return None\n\n        completion_output = self._new_completion_output(\n            new_token_ids, finish_reason, stop_reason)\n\n        request_id = self.request_id\n        if self.parent_req is None:\n            outputs = [completion_output]\n        else:\n            request_id, outputs, finished = self.parent_req.get_outputs(\n                request_id, completion_output)\n            if not outputs:\n                return None\n\n        return self._new_request_output(request_id, outputs, finished,\n                                        kv_transfer_params)\n\n    def _new_request_output(\n        self,\n        request_id: str,\n        outputs: list[CompletionOutput],\n        finished: bool,\n        kv_transfer_params: Optional[dict[str, Any]] = None,\n    ) -> RequestOutput:\n\n        if self.output_kind == RequestOutputKind.DELTA:\n            # Side effect: logprobs processor forgets prompt logprobs\n            prompt_logprobs = self.logprobs_processor.pop_prompt_logprobs()\n        else:\n            prompt_logprobs = self.logprobs_processor.prompt_logprobs\n\n        return RequestOutput(\n            request_id=request_id,\n            prompt=self.prompt,\n            prompt_token_ids=self.prompt_token_ids,\n            prompt_logprobs=prompt_logprobs,\n            outputs=outputs,\n            finished=finished,\n            kv_transfer_params=kv_transfer_params,\n        )\n\n    def _new_completion_output(\n        self,\n        token_ids: list[int],\n        finish_reason: Optional[FinishReason],\n        stop_reason: Union[int, str, None],\n    ) -> CompletionOutput:\n\n        finished = finish_reason is not None\n        delta = self.output_kind == RequestOutputKind.DELTA\n\n        # Prepare text and token_ids, based on delta mode\n        text = self.detokenizer.get_next_output_text(finished, delta)\n        if not delta:\n            token_ids = self.detokenizer.output_token_ids\n\n        # Prepare logprobs, based on delta mode\n        logprobs = self.logprobs_processor.logprobs\n        if delta and logprobs:\n            logprobs = logprobs[-len(token_ids):]\n\n        return CompletionOutput(\n            index=self.request_index,\n            text=text,\n            token_ids=token_ids,\n            logprobs=logprobs,\n            cumulative_logprob=self.logprobs_processor.cumulative_logprob,\n            finish_reason=str(finish_reason) if finished else None,\n            stop_reason=stop_reason if finished else None)\n\n\nclass OutputProcessor:\n    \"\"\"Process EngineCoreOutputs into RequestOutputs.\"\"\"\n\n    def __init__(\n        self,\n        tokenizer: TokenizerGroup,\n        log_stats: bool,\n    ):\n        self.log_stats = log_stats\n        self.tokenizer = tokenizer\n        self.request_states: dict[str, RequestState] = {}\n        self.parent_requests: dict[str, ParentRequest] = {}\n        self.lora_states = LoRARequestStates()\n\n    def get_num_unfinished_requests(self):\n        return len(self.request_states)\n\n    def has_unfinished_requests(self) -> bool:\n        return len(self.request_states) > 0\n\n    def propagate_error(self, e: Exception):\n        \"\"\"Propagate error to all generate() tasks.\"\"\"\n\n        for _, state in self.request_states.items():\n            assert state.queue is not None\n            state.queue.put(e)\n\n    def abort_requests(\n        self,\n        request_ids: Iterable[str],\n    ) -> list[str]:\n        request_ids_to_abort = []\n        for request_id in request_ids:\n            req_state = self.request_states.pop(request_id, None)\n            if req_state is not None:\n                self.lora_states.abort_request(req_state)\n                request_ids_to_abort.append(request_id)\n            else:\n                parent = self.parent_requests.pop(request_id, None)\n                if parent and parent.child_requests:\n                    self.abort_requests(parent.child_requests)\n                    request_ids_to_abort.extend(parent.child_requests)\n        return request_ids_to_abort\n\n    def add_request(\n        self,\n        request: EngineCoreRequest,\n        prompt: Optional[str],\n        parent_req: Optional[ParentRequest] = None,\n        request_index: int = 0,\n        queue: Optional[RequestOutputCollector] = None,\n    ) -> None:\n        request_id = request.request_id\n        if request_id in self.request_states:\n            raise ValueError(f\"Request id {request_id} already running.\")\n\n        req_state = RequestState.from_new_request(\n            tokenizer=self.tokenizer.get_lora_tokenizer(request.lora_request),\n            request=request,\n            prompt=prompt,\n            parent_req=parent_req,\n            request_index=request_index,\n            queue=queue,\n            log_stats=self.log_stats)\n        self.request_states[request_id] = req_state\n        self.lora_states.add_request(req_state)\n        if parent_req:\n            self.parent_requests[parent_req.request_id] = parent_req\n\n    def process_outputs(\n        self,\n        engine_core_outputs: list[EngineCoreOutput],\n        engine_core_timestamp: Optional[float] = None,\n        iteration_stats: Optional[IterationStats] = None,\n    ) -> OutputProcessorOutput:\n        \"\"\"\n        Process the EngineCoreOutputs:\n        1) Compute stats for logging\n        2) Detokenize\n        3) Create and handle RequestOutput objects:\n            * If there is a queue (for usage with AsyncLLM), \n              put the RequestOutput objects into the queue for\n              handling by the per-request generate() tasks.\n\n            * If there is no queue (for usage with LLMEngine), \n              return a list of RequestOutput objects.\n\n        NOTE FOR DEVELOPERS\n\n        vLLM V1 minimizes the number of python loops over the full\n        batch to ensure system overheads are minimized. This is the \n        only function that should loop over EngineCoreOutputs.\n\n        If you need to touch every element of the batch, do it from\n        within the loop below.\n        \"\"\"\n\n        request_outputs: list[RequestOutput] = []\n        reqs_to_abort: list[str] = []\n        for engine_core_output in engine_core_outputs:\n            req_id = engine_core_output.request_id\n            req_state = self.request_states.get(req_id)\n            if req_state is None:\n                # Ignore output for already-aborted request.\n                continue\n\n            # 1) Compute stats for this iteration.\n            self._update_stats_from_output(req_state, engine_core_output,\n                                           engine_core_timestamp,\n                                           iteration_stats)\n\n            new_token_ids = engine_core_output.new_token_ids\n            finish_reason = engine_core_output.finish_reason\n            stop_reason = engine_core_output.stop_reason\n            kv_transfer_params = engine_core_output.kv_transfer_params\n\n            req_state.is_prefilling = False\n\n            # 2) Detokenize the token ids into text and perform stop checks.\n            stop_string = req_state.detokenizer.update(\n                new_token_ids, finish_reason == FinishReason.STOP)\n            if stop_string:\n                finish_reason = FinishReason.STOP\n                stop_reason = stop_string\n\n            # 3) Compute sample and prompt logprobs for request, if required.\n            req_state.logprobs_processor.update_from_output(engine_core_output)\n\n            # 4) Create and handle RequestOutput objects.\n            if request_output := req_state.make_request_output(\n                    new_token_ids, finish_reason, stop_reason,\n                    kv_transfer_params):\n                if req_state.queue is not None:\n                    # AsyncLLM: put into queue for handling by generate().\n                    req_state.queue.put(request_output)\n                else:\n                    # LLMEngine: return list of RequestOutputs.\n                    request_outputs.append(request_output)\n\n            # Free completed requests.\n            if finish_reason is not None:\n                self.request_states.pop(req_id)\n                # Remove parent request if applicable.\n                parent_req = req_state.parent_req\n                if parent_req and not parent_req.child_requests:\n                    self.parent_requests.pop(parent_req.request_id, None)\n                if not engine_core_output.finished:\n                    # If req not finished in EngineCore, but Detokenizer\n                    # detected stop string, abort needed in EngineCore.\n                    reqs_to_abort.append(req_id)\n\n                # Track per-request stats\n                self._update_stats_from_finished(req_state, finish_reason,\n                                                 iteration_stats)\n\n        self.lora_states.update_iteration_stats(iteration_stats)\n\n        return OutputProcessorOutput(\n            request_outputs=request_outputs,\n            reqs_to_abort=reqs_to_abort,\n        )\n\n    def _update_stats_from_output(self, req_state: RequestState,\n                                  engine_core_output: EngineCoreOutput,\n                                  engine_core_timestamp: Optional[float],\n                                  iteration_stats: Optional[IterationStats]):\n        if iteration_stats is None:\n            return\n\n        lora_stats = self.lora_states.get_stats(req_state)\n\n        assert engine_core_timestamp is not None\n        assert req_state.stats is not None\n        iteration_stats.update_from_output(engine_core_output,\n                                           engine_core_timestamp,\n                                           req_state.is_prefilling,\n                                           req_state.prompt_len,\n                                           req_state.stats, lora_stats)\n\n    def _update_stats_from_finished(self, req_state: RequestState,\n                                    finish_reason: Optional[FinishReason],\n                                    iteration_stats: Optional[IterationStats]):\n        if iteration_stats is None:\n            return\n\n        assert finish_reason is not None\n        assert req_state.stats is not None\n        iteration_stats.update_from_finished_request(\n            finish_reason=finish_reason,\n            num_prompt_tokens=len(req_state.prompt_token_ids),\n            max_tokens_param=req_state.max_tokens_param,\n            req_stats=req_state.stats)\n        self.lora_states.finish_request(req_state)\n\n        ParentRequest.observe_finished_request(\n            req_state.parent_req, iteration_stats,\n            req_state.stats.num_generation_tokens)\n", 424], "/home/jeromeku/vllm/vllm/v1/metrics/stats.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport time\nfrom dataclasses import dataclass, field\nfrom typing import TYPE_CHECKING, Optional\n\nfrom vllm.v1.spec_decode.metrics import SpecDecodingStats\n\nif TYPE_CHECKING:\n    from vllm.v1.engine import EngineCoreEvent, EngineCoreOutput, FinishReason\n    from vllm.v1.engine.output_processor import RequestState\n\n\n@dataclass\nclass PrefixCacheStats:\n    \"\"\"Stores prefix cache hit statistics.\"\"\"\n    # Whether reset_prefix_cache was invoked.\n    reset: bool = False\n    # The number of requests in this update.\n    requests: int = 0\n    # The number of queries in these requests. Note that \"queries\" here\n    # means the number of tokens that were queried from the cache.\n    queries: int = 0\n    # The number of hits in these requests.\n    hits: int = 0\n\n\n@dataclass\nclass SchedulerStats:\n    \"\"\"Stats associated with the scheduler.\"\"\"\n\n    num_running_reqs: int = 0\n    num_waiting_reqs: int = 0\n\n    gpu_cache_usage: float = 0.0\n\n    prefix_cache_stats: PrefixCacheStats = field(\n        default_factory=PrefixCacheStats)\n\n    spec_decoding_stats: Optional[SpecDecodingStats] = None\n\n\n@dataclass\nclass LoRAStats:\n    waiting_requests: set[str] = field(default_factory=set)\n    running_requests: set[str] = field(default_factory=set)\n\n\n@dataclass\nclass RequestStateStats:\n    \"\"\"Stats that need to be tracked across delta updates.\"\"\"\n\n    num_generation_tokens: int = 0\n\n    # This is a engine frontend timestamp (wall-clock)\n    arrival_time: float = 0.0\n\n    # These are engine core timestamps (monotonic)\n    queued_ts: float = 0.0\n    scheduled_ts: float = 0.0\n    first_token_ts: float = 0.0\n    last_token_ts: float = 0.0\n\n\n@dataclass\nclass FinishedRequestStats:\n    \"\"\"Stats associated with a finished request.\"\"\"\n\n    finish_reason: \"FinishReason\"\n    e2e_latency: float = 0.0\n    num_prompt_tokens: int = 0\n    num_generation_tokens: int = 0\n    max_tokens_param: Optional[int] = None\n    queued_time: float = 0.0\n    prefill_time: float = 0.0\n    inference_time: float = 0.0\n    decode_time: float = 0.0\n\n\nclass IterationStats:\n    \"\"\"Stats associated with a single set of EngineCoreOutputs.\"\"\"\n\n    def __init__(self):\n        self.iteration_timestamp = time.time()\n        self.num_generation_tokens = 0\n        self.num_prompt_tokens = 0\n        self.num_preempted_reqs = 0\n        self.finished_requests: list[FinishedRequestStats] = []\n        self.max_num_generation_tokens_iter: list[int] = []\n        self.n_params_iter: list[int] = []\n        self.time_to_first_tokens_iter: list[float] = []\n        self.time_per_output_tokens_iter: list[float] = []\n        self.waiting_lora_adapters: dict[str, int] = {}\n        self.running_lora_adapters: dict[str, int] = {}\n\n    def _time_since(self, start: float) -> float:\n        \"\"\"Calculate an interval relative to this iteration's timestamp.\"\"\"\n        return self.iteration_timestamp - start\n\n    def update_from_output(self, output: \"EngineCoreOutput\",\n                           engine_core_timestamp: float, is_prefilling: bool,\n                           prompt_len: int, req_stats: RequestStateStats,\n                           lora_stats: Optional[LoRAStats]):\n        num_new_generation_tokens = len(output.new_token_ids)\n\n        self.num_generation_tokens += num_new_generation_tokens\n        if is_prefilling:\n            assert num_new_generation_tokens > 0\n            self.num_prompt_tokens += prompt_len\n\n            first_token_latency = self._time_since(req_stats.arrival_time)\n            self.time_to_first_tokens_iter.append(first_token_latency)\n\n        req_stats.num_generation_tokens += num_new_generation_tokens\n\n        # Process request-level engine core events\n        if output.events is not None:\n            self.update_from_events(output.request_id, output.events,\n                                    is_prefilling, req_stats, lora_stats)\n\n        # Process the batch-level \"new tokens\" engine core event\n        if is_prefilling:\n            req_stats.first_token_ts = engine_core_timestamp\n        else:\n            tpot = engine_core_timestamp - req_stats.last_token_ts\n            self.time_per_output_tokens_iter.append(tpot)\n\n        req_stats.last_token_ts = engine_core_timestamp\n\n    def update_from_events(self, req_id: str, events: list[\"EngineCoreEvent\"],\n                           is_prefilling: bool, req_stats: RequestStateStats,\n                           lora_stats: Optional[LoRAStats]):\n        # Avoid circular dependency\n        from vllm.v1.engine import EngineCoreEventType\n        for event in events:\n            if event.type == EngineCoreEventType.QUEUED:\n                req_stats.queued_ts = event.timestamp\n                if lora_stats is not None:\n                    lora_stats.waiting_requests.add(req_id)\n            elif event.type == EngineCoreEventType.SCHEDULED:\n                if req_stats.scheduled_ts == 0.0:  # ignore preemptions\n                    req_stats.scheduled_ts = event.timestamp\n                LoRARequestStates.scheduled_request(lora_stats, req_id)\n            elif event.type == EngineCoreEventType.PREEMPTED:\n                self.num_preempted_reqs += 1\n                LoRARequestStates.preempted_request(lora_stats, req_id)\n\n    def update_from_finished_request(self, finish_reason: \"FinishReason\",\n                                     num_prompt_tokens: int,\n                                     max_tokens_param: Optional[int],\n                                     req_stats: RequestStateStats):\n        e2e_latency = self._time_since(req_stats.arrival_time)\n\n        # Queued interval is from first QUEUED event to first SCHEDULED\n        queued_time = req_stats.scheduled_ts - req_stats.queued_ts\n\n        # Prefill interval is from first SCHEDULED to first NEW_TOKEN\n        # Any preemptions during prefill is included in the interval\n        prefill_time = req_stats.first_token_ts - req_stats.scheduled_ts\n\n        # Decode interval is from first NEW_TOKEN to last NEW_TOKEN\n        # Any preemptions during decode are included\n        decode_time = req_stats.last_token_ts - req_stats.first_token_ts\n\n        # Inference interval is from first SCHEDULED to last NEW_TOKEN\n        # Any preemptions during prefill or decode are included\n        inference_time = req_stats.last_token_ts - req_stats.scheduled_ts\n\n        finished_req = \\\n            FinishedRequestStats(finish_reason=finish_reason,\n                                 e2e_latency=e2e_latency,\n                                 num_prompt_tokens=num_prompt_tokens,\n                                 num_generation_tokens=req_stats.num_generation_tokens,\n                                 max_tokens_param=max_tokens_param,\n                                 queued_time=queued_time,\n                                 prefill_time=prefill_time,\n                                 inference_time=inference_time,\n                                 decode_time=decode_time)\n        self.finished_requests.append(finished_req)\n\n\nclass LoRARequestStates:\n    \"\"\"Per-LoRA request state stats.\"\"\"\n\n    def __init__(self):\n        self.lora_name_to_stats: dict[str, LoRAStats] = {}\n\n    def get_stats(self, req_state: 'RequestState') -> Optional[LoRAStats]:\n        if req_state.lora_name is None:\n            return None\n        if req_state.lora_name not in self.lora_name_to_stats:\n            self.lora_name_to_stats[req_state.lora_name] = LoRAStats()\n        return self.lora_name_to_stats[req_state.lora_name]\n\n    def add_request(self, req_state: 'RequestState'):\n        if (lora_stats := self.get_stats(req_state)) is not None:\n            lora_stats.waiting_requests.add(req_state.request_id)\n\n    def finish_request(self, req_state: 'RequestState'):\n        if req_state.lora_name is None:\n            return\n        lora_stats = self.lora_name_to_stats[req_state.lora_name]\n        lora_stats.running_requests.remove(req_state.request_id)\n\n    def abort_request(self, req_state: 'RequestState'):\n        if req_state.lora_name is None:\n            return\n        lora_stats = self.lora_name_to_stats[req_state.lora_name]\n        lora_stats.waiting_requests.discard(req_state.request_id)\n        lora_stats.running_requests.discard(req_state.request_id)\n\n    # Break the pattern for this lifecycle methods so we can\n    # call this from IterationStats.update_from_events()\n    @staticmethod\n    def scheduled_request(lora_stats: Optional[LoRAStats], request_id: str):\n        if lora_stats is None:\n            return\n        lora_stats.waiting_requests.remove(request_id)\n        lora_stats.running_requests.add(request_id)\n\n    @staticmethod\n    def preempted_request(lora_stats: Optional[LoRAStats], request_id: str):\n        if lora_stats is None:\n            return\n        lora_stats.running_requests.remove(request_id)\n        lora_stats.waiting_requests.add(request_id)\n\n    def update_iteration_stats(self,\n                               iteration_stats: Optional[IterationStats]):\n        if iteration_stats is None:\n            return\n        for lora_name, stats in self.lora_name_to_stats.items():\n            if stats.waiting_requests:\n                iteration_stats.waiting_lora_adapters[lora_name] = \\\n                    len(stats.waiting_requests)\n            if stats.running_requests:\n                iteration_stats.running_lora_adapters[lora_name] = \\\n                    len(stats.running_requests)\n", 238], "/home/jeromeku/vllm/vllm/v1/engine/core_client.py": ["# SPDX-License-Identifier: Apache-2.0\nimport asyncio\nimport contextlib\nimport queue\nimport uuid\nimport weakref\nfrom abc import ABC, abstractmethod\nfrom collections import deque\nfrom collections.abc import Awaitable, Sequence\nfrom concurrent.futures import Future\nfrom dataclasses import dataclass\nfrom enum import Enum, auto\nfrom threading import Thread\nfrom typing import Any, Callable, Optional, TypeVar, Union\n\nimport msgspec\nimport zmq\nimport zmq.asyncio\n\nfrom vllm.config import ParallelConfig, VllmConfig\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.utils import (\n    get_open_port,\n    get_open_zmq_inproc_path,\n    get_open_zmq_ipc_path,\n    get_tcp_uri,\n    make_zmq_socket,\n)\nfrom vllm.v1.engine import (\n    EngineCoreOutputs,\n    EngineCoreRequest,\n    EngineCoreRequestType,\n    UtilityOutput,\n)\nfrom vllm.v1.engine.core import EngineCore, EngineCoreProc\nfrom vllm.v1.engine.exceptions import EngineDeadError\nfrom vllm.v1.executor.abstract import Executor\nfrom vllm.v1.serial_utils import MsgpackDecoder, MsgpackEncoder, bytestr\nfrom vllm.v1.utils import CoreEngineProcManager\n\nlogger = init_logger(__name__)\n\nAnyFuture = Union[asyncio.Future[Any], Future[Any]]\n\n_R = TypeVar('_R')  # Return type for collective_rpc\n\nSTARTUP_POLL_PERIOD_MS = 10000\n\n\nclass EngineCoreClient(ABC):\n    \"\"\"\n    EngineCoreClient: subclasses handle different methods for pushing \n        and pulling from the EngineCore for asyncio / multiprocessing.\n\n    Subclasses:\n    * InprocClient: In process EngineCore (for V0-style LLMEngine use)\n    * SyncMPClient: ZMQ + background proc EngineCore (for LLM)\n    * AsyncMPClient: ZMQ + background proc EngineCore w/ asyncio (for AsyncLLM)\n    \"\"\"\n\n    @staticmethod\n    def make_client(\n        multiprocess_mode: bool,\n        asyncio_mode: bool,\n        vllm_config: VllmConfig,\n        executor_class: type[Executor],\n        log_stats: bool,\n    ) -> \"EngineCoreClient\":\n\n        # TODO: support this for debugging purposes.\n        if asyncio_mode and not multiprocess_mode:\n            raise NotImplementedError(\n                \"Running EngineCore in asyncio without multiprocessing \"\n                \"is not currently supported.\")\n\n        if multiprocess_mode and asyncio_mode:\n            if vllm_config.parallel_config.data_parallel_size > 1:\n                return DPAsyncMPClient(vllm_config, executor_class, log_stats)\n\n            return AsyncMPClient(vllm_config, executor_class, log_stats)\n\n        if multiprocess_mode and not asyncio_mode:\n            return SyncMPClient(vllm_config, executor_class, log_stats)\n\n        return InprocClient(vllm_config, executor_class, log_stats)\n\n    @abstractmethod\n    def shutdown(self):\n        ...\n\n    def get_output(self) -> EngineCoreOutputs:\n        raise NotImplementedError\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        raise NotImplementedError\n\n    def profile(self, is_start: bool = True) -> None:\n        raise NotImplementedError\n\n    def reset_mm_cache(self) -> None:\n        raise NotImplementedError\n\n    def reset_prefix_cache(self) -> None:\n        raise NotImplementedError\n\n    def sleep(self, level: int = 1) -> None:\n        raise NotImplementedError\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        raise NotImplementedError\n\n    def is_sleeping(self) -> bool:\n        raise NotImplementedError\n\n    def execute_dummy_batch(self) -> None:\n        raise NotImplementedError\n\n    async def execute_dummy_batch_async(self) -> None:\n        raise NotImplementedError\n\n    def abort_requests(self, request_ids: list[str]) -> None:\n        raise NotImplementedError\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        raise NotImplementedError\n\n    def remove_lora(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    def list_loras(self) -> set[int]:\n        raise NotImplementedError\n\n    def pin_lora(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    def save_sharded_state(self,\n                           path: str,\n                           pattern: Optional[str] = None,\n                           max_size: Optional[int] = None) -> None:\n        raise NotImplementedError\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        raise NotImplementedError\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        raise NotImplementedError\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        raise NotImplementedError\n\n    async def profile_async(self, is_start: bool = True) -> None:\n        raise NotImplementedError\n\n    async def reset_mm_cache_async(self) -> None:\n        raise NotImplementedError\n\n    async def reset_prefix_cache_async(self) -> None:\n        raise NotImplementedError\n\n    async def sleep_async(self, level: int = 1) -> None:\n        raise NotImplementedError\n\n    async def wake_up_async(self, tags: Optional[list[str]] = None) -> None:\n        raise NotImplementedError\n\n    async def is_sleeping_async(self) -> bool:\n        raise NotImplementedError\n\n    async def abort_requests_async(self, request_ids: list[str]) -> None:\n        raise NotImplementedError\n\n    async def add_lora_async(self, lora_request: LoRARequest) -> bool:\n        raise NotImplementedError\n\n    async def remove_lora_async(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    async def list_loras_async(self) -> set[int]:\n        raise NotImplementedError\n\n    async def pin_lora_async(self, lora_id: int) -> bool:\n        raise NotImplementedError\n\n    async def save_sharded_state_async(self,\n                                       path: str,\n                                       pattern: Optional[str] = None,\n                                       max_size: Optional[int] = None) -> None:\n        raise NotImplementedError\n\n    async def collective_rpc_async(\n            self,\n            method: Union[str, Callable[..., _R]],\n            timeout: Optional[float] = None,\n            args: tuple = (),\n            kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        raise NotImplementedError\n\n\nclass InprocClient(EngineCoreClient):\n    \"\"\"\n    InprocClient: client for in-process EngineCore. Intended \n    for use in LLMEngine for V0-style add_request() and step()\n        EngineCore setup in this process (no busy loop).\n\n        * pushes EngineCoreRequest directly into the EngineCore\n        * pulls EngineCoreOutputs by stepping the EngineCore\n    \"\"\"\n\n    def __init__(self, *args, **kwargs):\n        self.engine_core = EngineCore(*args, **kwargs)\n\n    def get_output(self) -> EngineCoreOutputs:\n        return self.engine_core.step()\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        self.engine_core.add_request(request)\n\n    def abort_requests(self, request_ids: list[str]) -> None:\n        if len(request_ids) > 0:\n            self.engine_core.abort_requests(request_ids)\n\n    def shutdown(self) -> None:\n        self.engine_core.shutdown()\n\n    def profile(self, is_start: bool = True) -> None:\n        self.engine_core.profile(is_start)\n\n    def reset_mm_cache(self) -> None:\n        self.engine_core.reset_mm_cache()\n\n    def reset_prefix_cache(self) -> None:\n        self.engine_core.reset_prefix_cache()\n\n    def sleep(self, level: int = 1) -> None:\n        self.engine_core.sleep(level)\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        self.engine_core.wake_up(tags)\n\n    def is_sleeping(self) -> bool:\n        return self.engine_core.is_sleeping()\n\n    def execute_dummy_batch(self) -> None:\n        self.engine_core.execute_dummy_batch()\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.engine_core.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.engine_core.remove_lora(lora_id)\n\n    def list_loras(self) -> set[int]:\n        return self.engine_core.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.engine_core.pin_lora(lora_id)\n\n    def save_sharded_state(self,\n                           path: str,\n                           pattern: Optional[str] = None,\n                           max_size: Optional[int] = None) -> None:\n        self.engine_core.save_sharded_state(path, pattern, max_size)\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.engine_core.collective_rpc(method, timeout, args, kwargs)\n\n\nclass CoreEngineState(Enum):\n    NEW = auto()\n    CONNECTED = auto()\n    READY = auto()\n\n\nclass CoreEngine:\n    \"\"\"One per data parallel rank.\"\"\"\n\n    def __init__(self, index: int = 0, local: bool = True):\n        self.local = local\n        self.index = index\n        self.identity = index.to_bytes(length=2, byteorder=\"little\")\n\n        self.state = CoreEngineState.NEW\n        self.num_reqs_in_flight = 0\n\n\n@dataclass\nclass BackgroundResources:\n    \"\"\"Used as a finalizer for clean shutdown, avoiding\n    circular reference back to the client object.\"\"\"\n\n    ctx: Union[zmq.Context]\n    local_engine_manager: Optional[CoreEngineProcManager] = None\n    output_socket: Optional[Union[zmq.Socket, zmq.asyncio.Socket]] = None\n    input_socket: Optional[Union[zmq.Socket, zmq.asyncio.Socket]] = None\n    output_queue_task: Optional[asyncio.Task] = None\n    shutdown_path: Optional[str] = None\n\n    # Set if any of the engines are dead. Here so that the output\n    # processing threads can access it without holding a ref to the client.\n    engine_dead: bool = False\n\n    def __call__(self):\n        \"\"\"Clean up background resources.\"\"\"\n\n        self.engine_dead = True\n        if self.local_engine_manager is not None:\n            self.local_engine_manager.close()\n\n        if self.output_queue_task is not None:\n            self.output_queue_task.cancel()\n\n        # ZMQ context termination can hang if the sockets\n        # aren't explicitly closed first.\n        if self.output_socket is not None:\n            self.output_socket.close(linger=0)\n        if self.input_socket is not None:\n            self.input_socket.close(linger=0)\n        if self.shutdown_path is not None:\n            # We must ensure that the sync output socket is\n            # closed cleanly in its own thread.\n            with self.ctx.socket(zmq.PAIR) as shutdown_sender:\n                shutdown_sender.connect(self.shutdown_path)\n                # Send shutdown signal.\n                shutdown_sender.send(b'')\n\n    def validate_alive(self, frames: Sequence[zmq.Frame]):\n        if len(frames) == 1 and (frames[0].buffer\n                                 == EngineCoreProc.ENGINE_CORE_DEAD):\n            self.engine_dead = True\n            raise EngineDeadError()\n\n\nclass MPClient(EngineCoreClient):\n    \"\"\"\n    MPClient: base client for multi-proc EngineCore.\n        EngineCore runs in a background process busy loop, getting\n        new EngineCoreRequests and returning EngineCoreOutputs\n\n        * pushes EngineCoreRequests via input_socket\n        * pulls EngineCoreOutputs via output_socket\n    \n        * AsyncMPClient subclass for AsyncLLM usage\n        * SyncMPClient subclass for LLM usage\n    \"\"\"\n\n    def __init__(\n        self,\n        asyncio_mode: bool,\n        vllm_config: VllmConfig,\n        executor_class: type[Executor],\n        log_stats: bool,\n    ):\n        self.vllm_config = vllm_config\n        # Serialization setup.\n        self.encoder = MsgpackEncoder()\n        self.decoder = MsgpackDecoder(EngineCoreOutputs)\n\n        # ZMQ setup.\n        sync_ctx = zmq.Context(io_threads=2)\n        self.ctx = zmq.asyncio.Context(sync_ctx) if asyncio_mode else sync_ctx\n\n        # This will ensure resources created so far are closed\n        # when the client is garbage collected, even if an\n        # exception is raised mid-construction.\n        self.resources = BackgroundResources(ctx=sync_ctx)\n        self._finalizer = weakref.finalize(self, self.resources)\n        success = False\n        try:\n            parallel_config = vllm_config.parallel_config\n            local_engine_count = parallel_config.data_parallel_size_local\n            start_index = parallel_config.data_parallel_rank\n            local_start_index = parallel_config.data_parallel_rank_local\n\n            # SPMD mode is where there is an LLM instance per DP rank and\n            # one core engine per LLM, see\n            # examples/offline_inference/data_parallel.py.\n            spmd_mode = local_start_index is not None\n            if spmd_mode:\n                assert local_engine_count == 1\n                self.core_engines = [\n                    CoreEngine(index=local_start_index, local=True)\n                ]\n            else:\n                assert start_index == 0\n                local_start_index = 0\n                self.core_engines = [\n                    CoreEngine(index=i, local=(i < local_engine_count))\n                    for i in range(parallel_config.data_parallel_size)\n                ]\n\n            input_address, output_address = self._get_zmq_addresses(\n                parallel_config, spmd_mode)\n\n            # Create input and output sockets.\n            self.input_socket = self.resources.input_socket = make_zmq_socket(\n                self.ctx, input_address, zmq.ROUTER, bind=True)\n\n            self.resources.output_socket = make_zmq_socket(\n                self.ctx, output_address, zmq.constants.PULL)\n            # Start local engines.\n            if local_engine_count:\n                # In server mode, start_index and local_start_index will\n                # both be 0.\n                self.resources.local_engine_manager = CoreEngineProcManager(\n                    EngineCoreProc.run_engine_core,\n                    vllm_config=vllm_config,\n                    executor_class=executor_class,\n                    log_stats=log_stats,\n                    input_address=input_address,\n                    on_head_node=True,\n                    local_engine_count=local_engine_count,\n                    start_index=start_index,\n                    local_start_index=local_start_index)\n\n            self.core_engine = self.core_engines[0]\n\n            # Wait for engine core process(es) to start.\n            self._wait_for_engine_startup(output_address, parallel_config)\n\n            self.utility_results: dict[int, AnyFuture] = {}\n\n            # Request objects which may contain pytorch-allocated tensors\n            # that we need to keep references to until zmq is done with the\n            # underlying data.\n            self.pending_messages = deque[tuple[zmq.MessageTracker, Any]]()\n\n            success = True\n        finally:\n            if not success:\n                self._finalizer()\n\n    @staticmethod\n    def _get_zmq_addresses(parallel_config: ParallelConfig,\n                           spmd_mode: bool) -> tuple[str, str]:\n        \"\"\"Returns (input_address, output_address).\"\"\"\n        dp_size = parallel_config.data_parallel_size\n        local_engine_count = parallel_config.data_parallel_size_local\n\n        if local_engine_count == dp_size or spmd_mode:\n            input_address = get_open_zmq_ipc_path()\n            output_address = get_open_zmq_ipc_path()\n        else:\n            host = parallel_config.data_parallel_master_ip\n            input_port = parallel_config.data_parallel_rpc_port\n            output_port = get_open_port()\n            input_address = get_tcp_uri(host, input_port)\n            output_address = get_tcp_uri(host, output_port)\n\n        return input_address, output_address\n\n    def _wait_for_engine_startup(self, output_address: str,\n                                 parallel_config: ParallelConfig):\n        # Get a sync handle to the socket which can be sync or async.\n        sync_input_socket = zmq.Socket.shadow(self.input_socket)\n\n        # Wait for engine core process(es) to send ready messages.\n        local_count = parallel_config.data_parallel_size_local\n        remote_count = len(self.core_engines) - local_count\n        # [local, remote] counts\n        conn_pending, start_pending = [local_count, remote_count], [0, 0]\n\n        poller = zmq.Poller()\n        poller.register(sync_input_socket, zmq.POLLIN)\n        proc_manager = self.resources.local_engine_manager\n        if proc_manager is not None:\n            for sentinel in proc_manager.sentinels():\n                poller.register(sentinel, zmq.POLLIN)\n        while any(conn_pending) or any(start_pending):\n            events = poller.poll(STARTUP_POLL_PERIOD_MS)\n            if not events:\n                if any(conn_pending):\n                    logger.debug(\n                        \"Waiting for %d local, %d remote core engine proc(s) \"\n                        \"to connect.\", *conn_pending)\n                if any(start_pending):\n                    logger.debug(\n                        \"Waiting for %d local, %d remote core engine proc(s) \"\n                        \"to start.\", *start_pending)\n                continue\n            if len(events) > 1 or events[0][0] != sync_input_socket:\n                # One of the local core processes exited.\n                finished = proc_manager.finished_procs(\n                ) if proc_manager else {}\n                raise RuntimeError(\"Engine core initialization failed. \"\n                                   \"See root cause above. \"\n                                   f\"Failed core proc(s): {finished}\")\n\n            # Receive HELLO and READY messages from the input socket.\n            eng_identity, ready_msg_bytes = sync_input_socket.recv_multipart()\n            eng_index = int.from_bytes(eng_identity, byteorder=\"little\")\n            engine = next(\n                (e for e in self.core_engines if e.identity == eng_identity),\n                None)\n            if engine is None:\n                raise RuntimeError(f\"Message from engine with unexpected data \"\n                                   f\"parallel rank: {eng_index}\")\n            msg = msgspec.msgpack.decode(ready_msg_bytes)\n            status, local = msg[\"status\"], msg[\"local\"]\n            if local != engine.local:\n                raise RuntimeError(f\"{status} message from \"\n                                   f\"{'local' if local else 'remote'} \"\n                                   f\"engine {eng_index}, expected it to be \"\n                                   f\"{'local' if engine.local else 'remote'}\")\n\n            if status == \"HELLO\" and engine.state == CoreEngineState.NEW:\n\n                # Send init message with DP config info.\n                init_message = self.encoder.encode({\n                    \"output_socket_address\": output_address,\n                    \"parallel_config\": {\n                        \"data_parallel_master_ip\":\n                        parallel_config.data_parallel_master_ip,\n                        \"data_parallel_master_port\":\n                        parallel_config.data_parallel_master_port,\n                        \"data_parallel_size\":\n                        parallel_config.data_parallel_size,\n                    },\n                })\n                sync_input_socket.send_multipart((eng_identity, *init_message),\n                                                 copy=False)\n                conn_pending[0 if local else 1] -= 1\n                start_pending[0 if local else 1] += 1\n                engine.state = CoreEngineState.CONNECTED\n            elif status == \"READY\" and (engine.state\n                                        == CoreEngineState.CONNECTED):\n                # Setup KV cache config with initialization state from\n                # engine core process. Sum values from all engines in DP case.\n                cache_config = self.vllm_config.cache_config\n                num_gpu_blocks = cache_config.num_gpu_blocks or 0\n                num_gpu_blocks += msg['num_gpu_blocks']\n                cache_config.num_gpu_blocks = num_gpu_blocks\n\n                start_pending[0 if local else 1] -= 1\n                engine.state = CoreEngineState.READY\n            else:\n                raise RuntimeError(f\"Unexpected {status} message for \"\n                                   f\"{'local' if local else 'remote'} engine \"\n                                   f\"{eng_index} in {engine.state} state.\")\n\n            logger.debug(\"%s from %s core engine process %s.\", status,\n                         \"local\" if local else \"remote\", eng_index)\n\n    def shutdown(self):\n        # Terminate background resources.\n        self._finalizer()\n\n    def _format_exception(self, e: Exception) -> Exception:\n        \"\"\"If errored, use EngineDeadError so root cause is clear.\"\"\"\n        return EngineDeadError(\n            suppress_context=True) if self.resources.engine_dead else e\n\n    def ensure_alive(self):\n        if self.resources.engine_dead:\n            raise EngineDeadError()\n\n    def add_pending_message(self, tracker: zmq.MessageTracker, msg: Any):\n        if not tracker.done:\n            self.pending_messages.appendleft((tracker, msg))\n\n    def free_pending_messages(self):\n        while self.pending_messages and self.pending_messages[-1][0].done:\n            self.pending_messages.pop()\n\n\ndef _process_utility_output(output: UtilityOutput,\n                            utility_results: dict[int, AnyFuture]):\n    \"\"\"Set the result from a utility method in the waiting future\"\"\"\n    future = utility_results.pop(output.call_id)\n    if output.failure_message is not None:\n        future.set_exception(Exception(output.failure_message))\n    else:\n        future.set_result(output.result)\n\n\nclass SyncMPClient(MPClient):\n    \"\"\"Synchronous client for multi-proc EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n                 log_stats: bool):\n        super().__init__(\n            asyncio_mode=False,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=log_stats,\n        )\n\n        self.outputs_queue = queue.Queue[Union[EngineCoreOutputs, Exception]]()\n\n        # Ensure that the outputs socket processing thread does not have\n        # a ref to the client which prevents gc.\n        ctx = self.ctx\n        out_socket = self.resources.output_socket\n        assert out_socket is not None\n        decoder = self.decoder\n        utility_results = self.utility_results\n        outputs_queue = self.outputs_queue\n\n        shutdown_path = get_open_zmq_inproc_path()\n        resources = self.resources\n        resources.shutdown_path = shutdown_path\n\n        def process_outputs_socket():\n            shutdown_socket = ctx.socket(zmq.PAIR)\n            try:\n                shutdown_socket.bind(shutdown_path)\n                poller = zmq.Poller()\n                poller.register(shutdown_socket)\n                poller.register(out_socket)\n                while True:\n                    socks = poller.poll()\n                    if not socks:\n                        continue\n                    if len(socks) == 2 or socks[0][0] == shutdown_socket:\n                        # shutdown signal, exit thread.\n                        break\n\n                    frames = out_socket.recv_multipart(copy=False)\n                    resources.validate_alive(frames)\n                    outputs = decoder.decode(frames)\n                    if outputs.utility_output:\n                        _process_utility_output(outputs.utility_output,\n                                                utility_results)\n                    else:\n                        outputs_queue.put_nowait(outputs)\n            except Exception as e:\n                outputs_queue.put_nowait(e)\n            finally:\n                # Close sockets.\n                shutdown_socket.close(linger=0)\n                out_socket.close(linger=0)\n\n        # Process outputs from engine in separate thread.\n        self.output_queue_thread = Thread(target=process_outputs_socket,\n                                          name=\"EngineCoreOutputQueueThread\",\n                                          daemon=True)\n        self.output_queue_thread.start()\n\n        # The thread takes on responsibility for closing the socket.\n        self.resources.output_socket = None\n\n    def get_output(self) -> EngineCoreOutputs:\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        outputs = self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n            raise self._format_exception(outputs) from None\n        return outputs\n\n    def _send_input(self, request_type: EngineCoreRequestType, request: Any):\n        self.ensure_alive()\n        self.free_pending_messages()\n        # (Identity, RequestType, SerializedRequest)\n        msg = (self.core_engine.identity, request_type.value,\n               *self.encoder.encode(request))\n\n        if len(msg) <= 3:\n            # No auxiliary buffers => no tensor backing buffers in request.\n            self.input_socket.send_multipart(msg, copy=False)\n            return\n\n        tracker = self.input_socket.send_multipart(msg, copy=False, track=True)\n        self.add_pending_message(tracker, request)\n\n    def call_utility(self, method: str, *args) -> Any:\n        call_id = uuid.uuid1().int >> 64\n        future: Future[Any] = Future()\n        self.utility_results[call_id] = future\n        self._send_input(EngineCoreRequestType.UTILITY,\n                         (call_id, method, args))\n\n        return future.result()\n\n    def add_request(self, request: EngineCoreRequest) -> None:\n        self._send_input(EngineCoreRequestType.ADD, request)\n\n    def abort_requests(self, request_ids: list[str]) -> None:\n        if request_ids and not self.resources.engine_dead:\n            self._send_input(EngineCoreRequestType.ABORT, request_ids)\n\n    def profile(self, is_start: bool = True) -> None:\n        self.call_utility(\"profile\", is_start)\n\n    def reset_mm_cache(self) -> None:\n        self.call_utility(\"reset_mm_cache\")\n\n    def reset_prefix_cache(self) -> None:\n        self.call_utility(\"reset_prefix_cache\")\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        return self.call_utility(\"add_lora\", lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        return self.call_utility(\"remove_lora\", lora_id)\n\n    def list_loras(self) -> set[int]:\n        return self.call_utility(\"list_loras\")\n\n    def pin_lora(self, lora_id: int) -> bool:\n        return self.call_utility(\"pin_lora\", lora_id)\n\n    def sleep(self, level: int = 1) -> None:\n        self.call_utility(\"sleep\", level)\n\n    def wake_up(self, tags: Optional[list[str]] = None) -> None:\n        self.call_utility(\"wake_up\", tags)\n\n    def is_sleeping(self) -> bool:\n        return self.call_utility(\"is_sleeping\")\n\n    def execute_dummy_batch(self) -> None:\n        self.call_utility(\"execute_dummy_batch\")\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.call_utility(\"collective_rpc\", method, timeout, args,\n                                 kwargs)\n\n    def save_sharded_state(self,\n                           path: str,\n                           pattern: Optional[str] = None,\n                           max_size: Optional[int] = None) -> None:\n        self.call_utility(\"save_sharded_state\", path, pattern, max_size)\n\n\nclass AsyncMPClient(MPClient):\n    \"\"\"Asyncio-compatible client for multi-proc EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n                 log_stats: bool):\n        super().__init__(\n            asyncio_mode=True,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=log_stats,\n        )\n\n        self.outputs_queue = asyncio.Queue[Union[EngineCoreOutputs,\n                                                 Exception]]()\n        try:\n            # If we are running in an asyncio event loop, start the queue task.\n            # Otherwise, it will be started lazily. If it is not started here,\n            # we could miss EXECUTOR_FAILED messages from engine core if they\n            # occur prior to any requests being sent.\n            asyncio.get_running_loop()\n            self._ensure_output_queue_task()\n        except RuntimeError:\n            pass\n\n    def _ensure_output_queue_task(self):\n        resources = self.resources\n        if resources.output_queue_task is not None:\n            return\n\n        # Perform IO in separate task to parallelize as much as possible.\n        # Avoid task having direct reference back to the client.\n        decoder = self.decoder\n        utility_results = self.utility_results\n        outputs_queue = self.outputs_queue\n        output_handler: Optional[Callable[[AsyncMPClient, EngineCoreOutputs],\n                                          Awaitable[None]]] = getattr(\n                                              self.__class__,\n                                              \"process_engine_outputs\", None)\n        _self_ref = weakref.ref(self) if output_handler else None\n        output_socket = resources.output_socket\n        assert output_socket is not None\n\n        async def process_outputs_socket():\n            try:\n                while True:\n                    frames = await output_socket.recv_multipart(copy=False)\n                    resources.validate_alive(frames)\n                    outputs: EngineCoreOutputs = decoder.decode(frames)\n                    if outputs.utility_output:\n                        _process_utility_output(outputs.utility_output,\n                                                utility_results)\n                        continue\n\n                    if output_handler is not None:\n                        assert _self_ref is not None\n                        _self = _self_ref()\n                        if not _self:\n                            # Client has been garbage collected, abort.\n                            return\n                        await output_handler(_self, outputs)\n\n                    if outputs.outputs or outputs.scheduler_stats:\n                        outputs_queue.put_nowait(outputs)\n            except Exception as e:\n                outputs_queue.put_nowait(e)\n\n        resources.output_queue_task = asyncio.create_task(\n            process_outputs_socket(), name=\"EngineCoreOutputQueueTask\")\n\n    async def get_output_async(self) -> EngineCoreOutputs:\n        self._ensure_output_queue_task()\n        # If an exception arises in process_outputs_socket task,\n        # it is forwarded to the outputs_queue so we can raise it\n        # from this (run_output_handler) task to shut down the server.\n        assert self.outputs_queue is not None\n        outputs = await self.outputs_queue.get()\n        if isinstance(outputs, Exception):\n            raise self._format_exception(outputs) from None\n        return outputs\n\n    def _send_input(self,\n                    request_type: EngineCoreRequestType,\n                    request: Any,\n                    engine: Optional[CoreEngine] = None) -> Awaitable[Any]:\n        self.ensure_alive()\n        if engine is None:\n            engine = self.core_engine\n\n        message = (request_type.value, *self.encoder.encode(request))\n        return self._send_input_message(message, engine, request)\n\n    def _send_input_message(self, message: tuple[bytestr,\n                                                 ...], engine: CoreEngine,\n                            objects: Any) -> Awaitable[Any]:\n        \"\"\"\n        objects is a reference to retain until zmq is finished with the\n        buffers, in case they were extracted from tensors in the request.\n        \"\"\"\n        self.ensure_alive()\n        self.free_pending_messages()\n\n        msg = (engine.identity, ) + message\n        if not objects or len(msg) <= 3:\n            # No auxiliary buffers => no tensor backing buffers in request.\n            return self.input_socket.send_multipart(msg, copy=False)\n\n        future: asyncio.Future[zmq.MessageTracker]\n        future = self.input_socket.send_multipart(msg, copy=False, track=True)\n\n        def add_pending(f: asyncio.Future[zmq.MessageTracker]):\n            with contextlib.suppress(BaseException):\n                self.add_pending_message(f.result(), objects)\n\n        future.add_done_callback(add_pending)\n        return future\n\n    async def call_utility_async(self, method: str, *args) -> Any:\n        return await self._call_utility_async(method,\n                                              *args,\n                                              engine=self.core_engine)\n\n    async def _call_utility_async(self, method: str, *args,\n                                  engine: CoreEngine) -> Any:\n        call_id = uuid.uuid1().int >> 64\n        future = asyncio.get_running_loop().create_future()\n        self.utility_results[call_id] = future\n        message = (EngineCoreRequestType.UTILITY.value, *self.encoder.encode(\n            (call_id, method, args)))\n        await self._send_input_message(message, engine, args)\n        self._ensure_output_queue_task()\n        return await future\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        await self._send_input(EngineCoreRequestType.ADD, request)\n        self._ensure_output_queue_task()\n\n    async def abort_requests_async(self, request_ids: list[str]) -> None:\n        if request_ids and not self.resources.engine_dead:\n            await self._send_input(EngineCoreRequestType.ABORT, request_ids)\n\n    async def profile_async(self, is_start: bool = True) -> None:\n        await self.call_utility_async(\"profile\", is_start)\n\n    async def reset_mm_cache_async(self) -> None:\n        await self.call_utility_async(\"reset_mm_cache\")\n\n    async def reset_prefix_cache_async(self) -> None:\n        await self.call_utility_async(\"reset_prefix_cache\")\n\n    async def sleep_async(self, level: int = 1) -> None:\n        await self.call_utility_async(\"sleep\", level)\n\n    async def wake_up_async(self, tags: Optional[list[str]] = None) -> None:\n        await self.call_utility_async(\"wake_up\", tags)\n\n    async def is_sleeping_async(self) -> bool:\n        return await self.call_utility_async(\"is_sleeping\")\n\n    async def execute_dummy_batch_async(self) -> None:\n        await self.call_utility_async(\"execute_dummy_batch\")\n\n    async def add_lora_async(self, lora_request: LoRARequest) -> bool:\n        return await self.call_utility_async(\"add_lora\", lora_request)\n\n    async def remove_lora_async(self, lora_id: int) -> bool:\n        return await self.call_utility_async(\"remove_lora\", lora_id)\n\n    async def list_loras_async(self) -> set[int]:\n        return await self.call_utility_async(\"list_loras\")\n\n    async def pin_lora_async(self, lora_id: int) -> bool:\n        return await self.call_utility_async(\"pin_lora\", lora_id)\n\n    async def save_sharded_state_async(self,\n                                       path: str,\n                                       pattern: Optional[str] = None,\n                                       max_size: Optional[int] = None) -> None:\n        await self.call_utility_async(\"save_sharded_state\", path, pattern,\n                                      max_size)\n\n    async def collective_rpc_async(\n            self,\n            method: Union[str, Callable[..., _R]],\n            timeout: Optional[float] = None,\n            args: tuple = (),\n            kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return await self.call_utility_async(\"collective_rpc\", method, timeout,\n                                             args, kwargs)\n\n\nclass DPAsyncMPClient(AsyncMPClient):\n    \"\"\"Asyncio-compatible client for multi-proc, multi-engine (data parallel)\n    EngineCore.\"\"\"\n\n    def __init__(self, vllm_config: VllmConfig, executor_class: type[Executor],\n                 log_stats: bool):\n\n        self.current_wave = 0\n        self.engines_running = False\n        self.reqs_in_flight: dict[str, CoreEngine] = {}\n\n        super().__init__(vllm_config, executor_class, log_stats)\n\n        assert len(self.core_engines) > 1\n\n    async def call_utility_async(self, method: str, *args) -> Any:\n        # Only the result from the first engine is returned.\n        return (await asyncio.gather(*[\n            self._call_utility_async(method, *args, engine=engine)\n            for engine in self.core_engines\n        ]))[0]\n\n    async def add_request_async(self, request: EngineCoreRequest) -> None:\n        request.current_wave = self.current_wave\n\n        chosen_engine = self.get_core_engine_for_request()\n        self.reqs_in_flight[request.request_id] = chosen_engine\n        chosen_engine.num_reqs_in_flight += 1\n\n        to_await = self._send_input(EngineCoreRequestType.ADD, request,\n                                    chosen_engine)\n        if not self.engines_running:\n            # Send request to chosen engine and dp start loop\n            # control message to all other engines.\n            self.engines_running = True\n            to_await = asyncio.gather(\n                to_await,  # type: ignore[assignment]\n                *self._start_wave_coros(exclude_index=chosen_engine.index))\n\n        await to_await\n\n        self._ensure_output_queue_task()\n\n    def get_core_engine_for_request(self) -> CoreEngine:\n        return min(self.core_engines, key=lambda e: e.num_reqs_in_flight)\n\n    @staticmethod\n    async def process_engine_outputs(self: \"DPAsyncMPClient\",\n                                     outputs: EngineCoreOutputs):\n        if self.reqs_in_flight:\n            for req_id in outputs.finished_requests or ():\n                if engine := self.reqs_in_flight.pop(req_id, None):\n                    engine.num_reqs_in_flight -= 1\n\n        if outputs.wave_complete is not None:\n            # Current wave is complete, move to next wave number\n            # and mark engines as paused.\n            if self.current_wave <= outputs.wave_complete:\n                self.current_wave = outputs.wave_complete + 1\n                self.engines_running = False\n\n        elif outputs.start_wave is not None and (\n                outputs.start_wave > self.current_wave or\n            (outputs.start_wave == self.current_wave\n             and not self.engines_running)):\n            # Engine received request for a non-current wave so we must ensure\n            # that other engines progress to the next wave.\n            self.current_wave = outputs.start_wave\n            self.engines_running = True\n            await asyncio.gather(*self._start_wave_coros(\n                exclude_index=outputs.engine_index))\n\n    def _start_wave_coros(self, exclude_index: int) -> list[Awaitable[None]]:\n        logger.debug(\"Sending start DP wave %d.\", self.current_wave)\n        return [\n            self._send_input(EngineCoreRequestType.START_DP_WAVE,\n                             self.current_wave, engine)\n            for engine in self.core_engines if engine.index != exclude_index\n        ]\n\n    async def abort_requests_async(self, request_ids: list[str]) -> None:\n        if not request_ids:\n            return\n\n        if len(request_ids) == 1:\n            # Fast-path common case.\n            if engine := self.reqs_in_flight.get(request_ids[0]):\n                await self._abort_requests(request_ids, engine)\n            return\n\n        by_engine: dict[CoreEngine, list[str]] = {}\n        for req_id in request_ids:\n            if engine := self.reqs_in_flight.get(req_id):\n                by_engine.setdefault(engine, []).append(req_id)\n        for engine, req_ids in by_engine.items():\n            await self._abort_requests(req_ids, engine)\n\n    async def _abort_requests(self, request_ids: list[str],\n                              engine: CoreEngine) -> None:\n        if not self.resources.engine_dead:\n            await self._send_input(EngineCoreRequestType.ABORT, request_ids,\n                                   engine)\n", 1029], "/home/jeromeku/vllm/vllm/v1/serial_utils.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport dataclasses\nimport pickle\nfrom collections.abc import Sequence\nfrom inspect import isclass\nfrom types import FunctionType\nfrom typing import Any, Optional, Union\n\nimport cloudpickle\nimport numpy as np\nimport torch\nimport zmq\nfrom msgspec import msgpack\n\nfrom vllm import envs\nfrom vllm.logger import init_logger\nfrom vllm.multimodal.inputs import (BaseMultiModalField,\n                                    MultiModalBatchedField,\n                                    MultiModalFieldConfig, MultiModalFieldElem,\n                                    MultiModalFlatField, MultiModalKwargs,\n                                    MultiModalKwargsItem,\n                                    MultiModalSharedField, NestedTensors)\n\nlogger = init_logger(__name__)\n\nCUSTOM_TYPE_PICKLE = 1\nCUSTOM_TYPE_CLOUDPICKLE = 2\nCUSTOM_TYPE_RAW_VIEW = 3\n\n# MultiModalField class serialization type map.\n# These need to list all possible field types and match them\n# to factory methods in `MultiModalFieldConfig`.\nMMF_CLASS_TO_FACTORY: dict[type[BaseMultiModalField], str] = {\n    MultiModalFlatField: \"flat\",\n    MultiModalSharedField: \"shared\",\n    MultiModalBatchedField: \"batched\",\n}\n\nbytestr = Union[bytes, bytearray, memoryview, zmq.Frame]\n\n\ndef _log_insecure_serialization_warning():\n    logger.warning_once(\"Allowing insecure serialization using pickle due to \"\n                        \"VLLM_ALLOW_INSECURE_SERIALIZATION=1\")\n\n\nclass MsgpackEncoder:\n    \"\"\"Encoder with custom torch tensor and numpy array serialization.\n\n    Note that unlike vanilla `msgspec` Encoders, this interface is generally\n    not thread-safe when encoding tensors / numpy arrays.\n\n    By default, arrays below 256B are serialized inline Larger will get sent \n    via dedicated messages. Note that this is a per-tensor limit.\n    \"\"\"\n\n    def __init__(self, size_threshold: Optional[int] = None):\n        if size_threshold is None:\n            size_threshold = envs.VLLM_MSGPACK_ZERO_COPY_THRESHOLD\n        self.encoder = msgpack.Encoder(enc_hook=self.enc_hook)\n        # This is used as a local stash of buffers that we can then access from\n        # our custom `msgspec` hook, `enc_hook`. We don't have a way to\n        # pass custom data to the hook otherwise.\n        self.aux_buffers: Optional[list[bytestr]] = None\n        self.size_threshold = size_threshold\n        if envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            _log_insecure_serialization_warning()\n\n    def encode(self, obj: Any) -> Sequence[bytestr]:\n        try:\n            self.aux_buffers = bufs = [b'']\n            bufs[0] = self.encoder.encode(obj)\n            # This `bufs` list allows us to collect direct pointers to backing\n            # buffers of tensors and np arrays, and return them along with the\n            # top-level encoded buffer instead of copying their data into the\n            # new buffer.\n            return bufs\n        finally:\n            self.aux_buffers = None\n\n    def encode_into(self, obj: Any, buf: bytearray) -> Sequence[bytestr]:\n        try:\n            self.aux_buffers = [buf]\n            bufs = self.aux_buffers\n            self.encoder.encode_into(obj, buf)\n            return bufs\n        finally:\n            self.aux_buffers = None\n\n    def enc_hook(self, obj: Any) -> Any:\n        if isinstance(obj, torch.Tensor):\n            return self._encode_tensor(obj)\n\n        # Fall back to pickle for object or void kind ndarrays.\n        if isinstance(obj, np.ndarray) and obj.dtype.kind not in ('O', 'V'):\n            return self._encode_ndarray(obj)\n\n        if isinstance(obj, slice):\n            # We are assuming only int-based values will be used here.\n            return tuple(\n                int(v) if v is not None else None\n                for v in (obj.start, obj.stop, obj.step))\n\n        if isinstance(obj, MultiModalKwargs):\n            mm: MultiModalKwargs = obj\n            if not mm.modalities:\n                # just return the main dict if there are no modalities.\n                return dict(mm)\n\n            # ignore the main dict, it will be re-indexed.\n            # Encode a list of MultiModalKwargsItems as plain dicts\n            # + special handling for .field.\n            # Any tensors *not* indexed by modality will be ignored.\n            return [[{\n                \"modality\": elem.modality,\n                \"key\": elem.key,\n                \"data\": self._encode_nested_tensors(elem.data),\n                \"field\": self._encode_mm_field(elem.field),\n            } for elem in item.values()]\n                    for itemlist in mm._items_by_modality.values()\n                    for item in itemlist]\n\n        if not envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            raise TypeError(f\"Object of type {type(obj)} is not serializable\"\n                            \"Set VLLM_ALLOW_INSECURE_SERIALIZATION=1 to allow \"\n                            \"fallback to pickle-based serialization.\")\n\n        if isinstance(obj, FunctionType):\n            # `pickle` is generally faster than cloudpickle, but can have\n            # problems serializing methods.\n            return msgpack.Ext(CUSTOM_TYPE_CLOUDPICKLE, cloudpickle.dumps(obj))\n\n        return msgpack.Ext(CUSTOM_TYPE_PICKLE,\n                           pickle.dumps(obj, protocol=pickle.HIGHEST_PROTOCOL))\n\n    def _encode_ndarray(\n        self, obj: np.ndarray\n    ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n        assert self.aux_buffers is not None\n        # If the array is non-contiguous, we need to copy it first\n        arr_data = obj.data if obj.data.c_contiguous else obj.tobytes()\n        if not obj.shape or obj.nbytes < self.size_threshold:\n            # Encode small arrays and scalars inline. Using this extension type\n            # ensures we can avoid copying when decoding.\n            data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr_data)\n        else:\n            # Otherwise encode index of backing buffer to avoid copy.\n            data = len(self.aux_buffers)\n            self.aux_buffers.append(arr_data)\n\n        # We serialize the ndarray as a tuple of native types.\n        # The data is either inlined if small, or an index into a list of\n        # backing buffers that we've stashed in `aux_buffers`.\n        return obj.dtype.str, obj.shape, data\n\n    def _encode_tensor(\n        self, obj: torch.Tensor\n    ) -> tuple[str, tuple[int, ...], Union[int, memoryview]]:\n        assert self.aux_buffers is not None\n        # this creates a copy of the tensor if it's not already contiguous\n        obj = obj.contiguous()\n        #  view the tensor as a 1D array of bytes\n        arr = obj.view((obj.numel(), )).view(torch.uint8).numpy()\n        if obj.nbytes < self.size_threshold:\n            # Smaller tensors are encoded inline, just like ndarrays.\n            data = msgpack.Ext(CUSTOM_TYPE_RAW_VIEW, arr.data)\n        else:\n            # Otherwise encode index of backing buffer to avoid copy.\n            data = len(self.aux_buffers)\n            self.aux_buffers.append(arr.data)\n        dtype = str(obj.dtype)[6:]  # remove 'torch.' prefix\n        return dtype, obj.shape, data\n\n    def _encode_nested_tensors(self, nt: NestedTensors) -> Any:\n        if isinstance(nt, torch.Tensor):\n            return self._encode_tensor(nt)\n        if isinstance(nt, (int, float)):\n            # Although it violates NestedTensors type, MultiModalKwargs\n            # values are sometimes floats.\n            return nt\n        return [self._encode_nested_tensors(x) for x in nt]\n\n    def _encode_mm_field(self, field: BaseMultiModalField):\n        # Figure out the factory name for the field type.\n        name = MMF_CLASS_TO_FACTORY.get(field.__class__)\n        if not name:\n            raise TypeError(f\"Unsupported field type: {field.__class__}\")\n        # We just need to copy all of the field values in order\n        # which will be then used to reconstruct the field.\n        field_values = (getattr(field, f.name)\n                        for f in dataclasses.fields(field))\n        return name, *field_values\n\n\nclass MsgpackDecoder:\n    \"\"\"Decoder with custom torch tensor and numpy array serialization.\n\n    Note that unlike vanilla `msgspec` Decoders, this interface is generally\n    not thread-safe when encoding tensors / numpy arrays.\n    \"\"\"\n\n    def __init__(self, t: Optional[Any] = None):\n        args = () if t is None else (t, )\n        self.decoder = msgpack.Decoder(*args,\n                                       ext_hook=self.ext_hook,\n                                       dec_hook=self.dec_hook)\n        self.aux_buffers: Sequence[bytestr] = ()\n        if envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            _log_insecure_serialization_warning()\n\n    def decode(self, bufs: Union[bytestr, Sequence[bytestr]]) -> Any:\n        if isinstance(bufs, (bytes, bytearray, memoryview, zmq.Frame)):\n            # TODO - This check can become `isinstance(bufs, bytestr)`\n            # as of Python 3.10.\n            return self.decoder.decode(bufs)\n\n        self.aux_buffers = bufs\n        try:\n            return self.decoder.decode(bufs[0])\n        finally:\n            self.aux_buffers = ()\n\n    def dec_hook(self, t: type, obj: Any) -> Any:\n        # Given native types in `obj`, convert to type `t`.\n        if isclass(t):\n            if issubclass(t, np.ndarray):\n                return self._decode_ndarray(obj)\n            if issubclass(t, torch.Tensor):\n                return self._decode_tensor(obj)\n            if t is slice:\n                return slice(*obj)\n            if issubclass(t, MultiModalKwargs):\n                if isinstance(obj, list):\n                    return MultiModalKwargs.from_items(\n                        self._decode_mm_items(obj))\n                return MultiModalKwargs({\n                    k: self._decode_nested_tensors(v)\n                    for k, v in obj.items()\n                })\n        return obj\n\n    def _decode_ndarray(self, arr: Any) -> np.ndarray:\n        dtype, shape, data = arr\n        # zero-copy decode. We assume the ndarray will not be kept around,\n        # as it now locks the whole received message buffer in memory.\n        buffer = self.aux_buffers[data] if isinstance(data, int) else data\n        return np.ndarray(buffer=buffer, dtype=np.dtype(dtype), shape=shape)\n\n    def _decode_tensor(self, arr: Any) -> torch.Tensor:\n        dtype, shape, data = arr\n        # Copy from inline representation, to decouple the memory storage\n        # of the message from the original buffer. And also make Torch\n        # not complain about a readonly memoryview.\n        buffer = self.aux_buffers[data] if isinstance(data, int) \\\n            else bytearray(data)\n        # Create numpy wrapper around the bytes\n        arr = np.ndarray(buffer=buffer, dtype=np.uint8, shape=(len(buffer), ))\n        torch_dtype = getattr(torch, dtype)\n        assert isinstance(torch_dtype, torch.dtype)\n        # Convert back to proper shape & type\n        return torch.from_numpy(arr).view(torch_dtype).view(shape)\n\n    def _decode_mm_items(self, obj: list) -> list[MultiModalKwargsItem]:\n        decoded_items = []\n        for item in obj:\n            elems = []\n            for v in item:\n                v[\"data\"] = self._decode_nested_tensors(v[\"data\"])\n                # Reconstruct the field processor using MultiModalFieldConfig\n                factory_meth_name, *field_args = v[\"field\"]\n                factory_meth = getattr(MultiModalFieldConfig,\n                                       factory_meth_name)\n\n                # Special case: decode the union \"slices\" field of\n                # MultiModalFlatField\n                if factory_meth_name == \"flat\":\n                    field_args[0] = self._decode_nested_slices(field_args[0])\n\n                v[\"field\"] = factory_meth(None, *field_args).field\n                elems.append(MultiModalFieldElem(**v))\n            decoded_items.append(MultiModalKwargsItem.from_elems(elems))\n        return decoded_items\n\n    def _decode_nested_tensors(self, obj: Any) -> NestedTensors:\n        if isinstance(obj, (int, float)):\n            # Although it violates NestedTensors type, MultiModalKwargs\n            # values are sometimes floats.\n            return obj\n        if not isinstance(obj, list):\n            raise TypeError(f\"Unexpected NestedTensors contents: {type(obj)}\")\n        if obj and isinstance(obj[0], str):\n            return self._decode_tensor(obj)\n        return [self._decode_nested_tensors(x) for x in obj]\n\n    def _decode_nested_slices(self, obj: Any) -> Any:\n        assert isinstance(obj, (list, tuple))\n        if obj and not isinstance(obj[0], (list, tuple)):\n            return slice(*obj)\n        return [self._decode_nested_slices(x) for x in obj]\n\n    def ext_hook(self, code: int, data: memoryview) -> Any:\n        if code == CUSTOM_TYPE_RAW_VIEW:\n            return data\n\n        if envs.VLLM_ALLOW_INSECURE_SERIALIZATION:\n            if code == CUSTOM_TYPE_PICKLE:\n                return pickle.loads(data)\n            if code == CUSTOM_TYPE_CLOUDPICKLE:\n                return cloudpickle.loads(data)\n\n        raise NotImplementedError(\n            f\"Extension type code {code} is not supported\")\n", 313], "/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py": ["# SPDX-License-Identifier: Apache-2.0\n\nfrom collections.abc import Mapping\nfrom copy import copy\nfrom typing import Any, Callable, Optional, Union\n\nfrom typing_extensions import TypeVar\n\nimport vllm.envs as envs\nfrom vllm.config import ParallelConfig, VllmConfig\nfrom vllm.distributed import stateless_destroy_torch_distributed_process_group\nfrom vllm.engine.arg_utils import EngineArgs\nfrom vllm.inputs import PromptType\nfrom vllm.logger import init_logger\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MULTIMODAL_REGISTRY, MultiModalRegistry\nfrom vllm.outputs import RequestOutput\nfrom vllm.pooling_params import PoolingParams\nfrom vllm.prompt_adapter.request import PromptAdapterRequest\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.transformers_utils.tokenizer_group import (\n    TokenizerGroup, init_tokenizer_from_configs)\nfrom vllm.usage.usage_lib import UsageContext\nfrom vllm.utils import Device\nfrom vllm.v1.engine.core_client import EngineCoreClient\nfrom vllm.v1.engine.output_processor import OutputProcessor\nfrom vllm.v1.engine.parallel_sampling import ParentRequest\nfrom vllm.v1.engine.processor import Processor\nfrom vllm.v1.executor.abstract import Executor\nfrom vllm.v1.metrics.loggers import StatLoggerFactory\n\nlogger = init_logger(__name__)\n\n_R = TypeVar(\"_R\", default=Any)\n\n\nclass LLMEngine:\n    \"\"\"Legacy LLMEngine for backwards compatibility.\"\"\"\n\n    def __init__(\n        self,\n        vllm_config: VllmConfig,\n        executor_class: type[Executor],\n        log_stats: bool,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[list[StatLoggerFactory]] = None,\n        mm_registry: MultiModalRegistry = MULTIMODAL_REGISTRY,\n        use_cached_outputs: bool = False,\n        multiprocess_mode: bool = False,\n    ) -> None:\n        if not envs.VLLM_USE_V1:\n            raise ValueError(\n                \"Using V1 LLMEngine, but envs.VLLM_USE_V1=False. \"\n                \"This should not happen. As a workaround, try using \"\n                \"LLMEngine.from_vllm_config(...) or explicitly set \"\n                \"VLLM_USE_V1=0 or 1 and report this issue on Github.\")\n\n        if stat_loggers is not None:\n            raise NotImplementedError(\n                \"Passing StatLoggers to LLMEngine in V1 is not yet supported. \"\n                \"Set VLLM_USE_V1=0 and file and issue on Github.\")\n\n        self.vllm_config = vllm_config\n        self.model_config = vllm_config.model_config\n        self.cache_config = vllm_config.cache_config\n\n        # important: init dp group before init the engine_core\n        # In the decoupled engine case this is handled in EngineCoreProc.\n        parallel_config = vllm_config.parallel_config\n        if not multiprocess_mode and parallel_config.data_parallel_size > 1:\n            self.dp_group = parallel_config.stateless_init_dp_group()\n        else:\n            self.dp_group = None\n        self.should_execute_dummy_batch = False\n\n        # Tokenizer (+ ensure liveness if running in another process).\n        self.tokenizer = init_tokenizer_from_configs(\n            model_config=vllm_config.model_config,\n            scheduler_config=vllm_config.scheduler_config,\n            lora_config=vllm_config.lora_config)\n\n        # Processor (convert Inputs --> EngineCoreRequests)\n        self.processor = Processor(vllm_config=vllm_config,\n                                   tokenizer=self.tokenizer,\n                                   mm_registry=mm_registry)\n\n        # OutputProcessor (convert EngineCoreOutputs --> RequestOutput).\n        self.output_processor = OutputProcessor(self.tokenizer,\n                                                log_stats=False)\n\n        # EngineCore (gets EngineCoreRequests and gives EngineCoreOutputs)\n        self.engine_core = EngineCoreClient.make_client(\n            multiprocess_mode=multiprocess_mode,\n            asyncio_mode=False,\n            vllm_config=vllm_config,\n            executor_class=executor_class,\n            log_stats=False,  # FIXME: implement\n        )\n\n        if not multiprocess_mode:\n            # for v0 compatibility\n            self.model_executor = self.engine_core.engine_core.model_executor  # type: ignore\n\n        # Don't keep the dummy data in memory\n        self.reset_mm_cache()\n\n    @classmethod\n    def from_vllm_config(\n        cls,\n        vllm_config: VllmConfig,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[list[StatLoggerFactory]] = None,\n        disable_log_stats: bool = False,\n    ) -> \"LLMEngine\":\n        return cls(vllm_config=vllm_config,\n                   executor_class=Executor.get_class(vllm_config),\n                   log_stats=(not disable_log_stats),\n                   usage_context=usage_context,\n                   stat_loggers=stat_loggers,\n                   multiprocess_mode=envs.VLLM_ENABLE_V1_MULTIPROCESSING)\n\n    @classmethod\n    def from_engine_args(\n        cls,\n        engine_args: EngineArgs,\n        usage_context: UsageContext = UsageContext.ENGINE_CONTEXT,\n        stat_loggers: Optional[list[StatLoggerFactory]] = None,\n        enable_multiprocessing: bool = False,\n    ) -> \"LLMEngine\":\n        \"\"\"Creates an LLM engine from the engine arguments.\"\"\"\n\n        # Create the engine configs.\n        vllm_config = engine_args.create_engine_config(usage_context)\n        executor_class = Executor.get_class(vllm_config)\n\n        if envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n            logger.debug(\"Enabling multiprocessing for LLMEngine.\")\n            enable_multiprocessing = True\n\n        # Create the LLMEngine.\n        return cls(vllm_config=vllm_config,\n                   executor_class=executor_class,\n                   log_stats=not engine_args.disable_log_stats,\n                   usage_context=usage_context,\n                   stat_loggers=stat_loggers,\n                   multiprocess_mode=enable_multiprocessing)\n\n    def get_num_unfinished_requests(self) -> int:\n        return self.output_processor.get_num_unfinished_requests()\n\n    def has_unfinished_requests(self) -> bool:\n        has_unfinished = self.output_processor.has_unfinished_requests()\n        if self.dp_group is None:\n            return has_unfinished\n        return self.has_unfinished_requests_dp(has_unfinished)\n\n    def has_unfinished_requests_dp(self, has_unfinished: bool) -> bool:\n        aggregated_has_unfinished = ParallelConfig.has_unfinished_dp(\n            self.dp_group, has_unfinished)\n        if not has_unfinished and aggregated_has_unfinished:\n            self.should_execute_dummy_batch = True\n        return aggregated_has_unfinished\n\n    @classmethod\n    def validate_outputs(cls, outputs, output_type):\n        return outputs\n\n    def abort_request(self, request_ids: list[str]) -> None:\n        \"\"\"Remove request_ids from EngineCore and Detokenizer.\"\"\"\n\n        request_ids = self.output_processor.abort_requests(request_ids)\n        self.engine_core.abort_requests(request_ids)\n\n    def add_request(\n        self,\n        request_id: str,\n        prompt: PromptType,\n        params: Union[SamplingParams, PoolingParams],\n        arrival_time: Optional[float] = None,\n        lora_request: Optional[LoRARequest] = None,\n        tokenization_kwargs: Optional[dict[str, Any]] = None,\n        trace_headers: Optional[Mapping[str, str]] = None,\n        prompt_adapter_request: Optional[PromptAdapterRequest] = None,\n        priority: int = 0,\n    ) -> None:\n        # Process raw inputs into the request.\n        prompt_str, request = self.processor.process_inputs(\n            request_id, prompt, params, arrival_time, lora_request,\n            tokenization_kwargs, trace_headers, prompt_adapter_request,\n            priority)\n\n        n = params.n if isinstance(params, SamplingParams) else 1\n\n        if n == 1:\n            # Make a new RequestState and queue.\n            self.output_processor.add_request(request, prompt_str, None, 0)\n            # Add the request to EngineCore.\n            self.engine_core.add_request(request)\n            return\n\n        # Fan out child requests (for n>1).\n        parent_req = ParentRequest(request_id, params)\n        for idx in range(n):\n            request_id, params = parent_req.get_child_info(idx)\n            child_request = request if idx == n - 1 else copy(request)\n            child_request.request_id = request_id\n            child_request.sampling_params = params\n\n            # Make a new RequestState and queue.\n            self.output_processor.add_request(child_request, prompt_str,\n                                              parent_req, idx)\n            # Add the request to EngineCore.\n            self.engine_core.add_request(child_request)\n\n    def step(self) -> list[RequestOutput]:\n\n        if self.should_execute_dummy_batch:\n            self.should_execute_dummy_batch = False\n            self.engine_core.execute_dummy_batch()\n            return []\n\n        # 1) Get EngineCoreOutput from the EngineCore.\n        outputs = self.engine_core.get_output()\n\n        # 2) Process EngineCoreOutputs.\n        processed_outputs = self.output_processor.process_outputs(\n            outputs.outputs)\n\n        # 3) Abort any reqs that finished due to stop strings.\n        self.engine_core.abort_requests(processed_outputs.reqs_to_abort)\n\n        return processed_outputs.request_outputs\n\n    def get_vllm_config(self):\n        return self.vllm_config\n\n    def get_model_config(self):\n        return self.model_config\n\n    def start_profile(self):\n        self.engine_core.profile(True)\n\n    def stop_profile(self):\n        self.engine_core.profile(False)\n\n    def reset_mm_cache(self):\n        self.processor.mm_registry.reset_processor_cache()\n        self.processor.mm_input_cache_client.reset()\n        self.engine_core.reset_mm_cache()\n\n    def reset_prefix_cache(self, device: Optional[Device] = None):\n        self.engine_core.reset_prefix_cache()\n\n    def sleep(self, level: int = 1):\n        self.engine_core.sleep(level)\n\n    def wake_up(self, tags: Optional[list[str]] = None):\n        self.engine_core.wake_up(tags)\n\n    def is_sleeping(self) -> bool:\n        return self.engine_core.is_sleeping()\n\n    def get_tokenizer_group(self) -> TokenizerGroup:\n        if self.tokenizer is None:\n            raise ValueError(\"Unable to get tokenizer because \"\n                             \"skip_tokenizer_init is True\")\n\n        return self.tokenizer\n\n    def add_lora(self, lora_request: LoRARequest) -> bool:\n        \"\"\"Load a new LoRA adapter into the engine for future requests.\"\"\"\n        return self.engine_core.add_lora(lora_request)\n\n    def remove_lora(self, lora_id: int) -> bool:\n        \"\"\"Remove an already loaded LoRA adapter.\"\"\"\n        return self.engine_core.remove_lora(lora_id)\n\n    def list_loras(self) -> set[int]:\n        \"\"\"List all registered adapters.\"\"\"\n        return self.engine_core.list_loras()\n\n    def pin_lora(self, lora_id: int) -> bool:\n        \"\"\"Prevent an adapter from being evicted.\"\"\"\n        return self.engine_core.pin_lora(lora_id)\n\n    def collective_rpc(self,\n                       method: Union[str, Callable[..., _R]],\n                       timeout: Optional[float] = None,\n                       args: tuple = (),\n                       kwargs: Optional[dict[str, Any]] = None) -> list[_R]:\n        return self.engine_core.collective_rpc(method, timeout, args, kwargs)\n\n    def __del__(self):\n        if dp_group := getattr(self, \"dp_group\", None):\n            stateless_destroy_torch_distributed_process_group(dp_group)\n", 295], "/home/jeromeku/vllm/vllm/v1/engine/__init__.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport enum\nimport time\nfrom collections.abc import Sequence\nfrom typing import Any, Optional, Union\n\nimport msgspec\n\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal import MultiModalKwargs\nfrom vllm.multimodal.inputs import PlaceholderRange\nfrom vllm.sampling_params import SamplingParams\nfrom vllm.v1.metrics.stats import SchedulerStats\nfrom vllm.v1.outputs import LogprobsLists, LogprobsTensors\n\n# These are possible values of RequestOutput.finish_reason,\n# so form part of the external API.\nFINISH_REASON_STRINGS = (\"stop\", \"length\", \"abort\")\n\n\nclass FinishReason(enum.IntEnum):\n    \"\"\"\n    Reason a request finished - stop, length, or abort.\n\n    Int rather than Str for more compact serialization.\n\n    stop - a stop string was emitted\n    length - max_tokens was consumed, or max_model_len was reached\n    abort - aborted for another reason\n\n    \"\"\"\n    STOP = 0\n    LENGTH = 1\n    ABORT = 2\n\n    def __str__(self):\n        return FINISH_REASON_STRINGS[self.value]\n\n\nclass EngineCoreRequest(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True,  # type: ignore[call-arg]\n        gc=False):  # type: ignore[call-arg]\n\n    # NOTE: prompt and prompt_token_ids should be DecoderOnlyInput,\n    # but this object is currently not playing well with msgspec\n    # due to circular imports and typing we have in data.py\n\n    request_id: str\n    prompt_token_ids: list[int]\n    mm_inputs: Optional[Sequence[Optional[MultiModalKwargs]]]\n    mm_hashes: Optional[list[str]]\n    mm_placeholders: Optional[list[PlaceholderRange]]\n    sampling_params: SamplingParams\n    eos_token_id: Optional[int]\n    arrival_time: float\n    lora_request: Optional[LoRARequest]\n    cache_salt: Optional[str]\n\n    # Used in DP case to indicate which wave of requests this is expected to\n    # belong to, to cover a race condition where the request is sent before\n    # a wave finished notification is received.\n    current_wave: int = 0\n\n\nclass EngineCoreEventType(enum.IntEnum):\n    \"\"\"The type of engine core request event.\"\"\"\n    QUEUED = 1\n    SCHEDULED = 2\n    PREEMPTED = 3\n\n\nclass EngineCoreEvent(msgspec.Struct):\n    \"\"\"A timestamped engine core event associated with a request.\n\n    The timestamp is a monotonic timestamps and is used for by the engine\n    frontend to calculate intervals between engine core events. These\n    timestamps should not be compared with timestamps from other processes.\n    \"\"\"\n    type: EngineCoreEventType\n    timestamp: float\n\n    @classmethod\n    def new_event(cls,\n                  event_type: EngineCoreEventType,\n                  timestamp: Optional[float] = None) -> \"EngineCoreEvent\":\n        timestamp = time.monotonic() if timestamp is None else timestamp\n        return cls(event_type, timestamp)\n\n\nclass EngineCoreOutput(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True,  # type: ignore[call-arg]\n        gc=False):  # type: ignore[call-arg]\n\n    request_id: str\n    new_token_ids: list[int]\n\n    new_logprobs: Optional[LogprobsLists] = None\n    new_prompt_logprobs_tensors: Optional[LogprobsTensors] = None\n\n    finish_reason: Optional[FinishReason] = None\n    stop_reason: Union[int, str, None] = None\n    events: Optional[list[EngineCoreEvent]] = None\n    kv_transfer_params: Optional[dict[str, Any]] = None\n\n    @property\n    def finished(self) -> bool:\n        return self.finish_reason is not None\n\n\nclass UtilityOutput(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        gc=False):  # type: ignore[call-arg]\n\n    call_id: int\n\n    # Non-None implies the call failed, result should be None.\n    failure_message: Optional[str] = None\n    result: Any = None\n\n\nclass EngineCoreOutputs(\n        msgspec.Struct,\n        array_like=True,  # type: ignore[call-arg]\n        omit_defaults=True,  # type: ignore[call-arg]\n        gc=False):  # type: ignore[call-arg]\n\n    #NOTE(Nick): We could consider ways to make this more compact,\n    # e.g. columnwise layout\n\n    engine_index: int = 0\n\n    # [num_reqs]\n    outputs: list[EngineCoreOutput] = []\n    scheduler_stats: Optional[SchedulerStats] = None\n    timestamp: float = 0.0\n\n    utility_output: Optional[UtilityOutput] = None\n    finished_requests: Optional[set[str]] = None\n\n    # In DP case, used to signal that the current wave of requests\n    # has finished and the engines are paused.\n    wave_complete: Optional[int] = None\n    # In DP case, used to signal that a request was received for an\n    # \"old\" wave, so the next wave needs to be started in other engines.\n    start_wave: Optional[int] = None\n\n    def __post_init__(self):\n        if self.timestamp == 0.0:\n            self.timestamp = time.monotonic()\n\n\nclass EngineCoreRequestType(enum.Enum):\n    \"\"\"\n    Request types defined as hex byte strings, so it can be sent over sockets\n    without separate encoding step.\n    \"\"\"\n    ADD = b'\\x00'\n    ABORT = b'\\x01'\n    START_DP_WAVE = b'\\x02'\n    UTILITY = b'\\x03'\n    # Sentinel used within EngineCoreProc.\n    EXECUTOR_FAILED = b'\\x04'\n", 168], "/home/jeromeku/vllm/vllm/outputs.py": ["# SPDX-License-Identifier: Apache-2.0\n\nimport time\nfrom collections.abc import MutableSequence\nfrom collections.abc import Sequence as GenericSequence\nfrom dataclasses import dataclass\nfrom typing import Any, Generic, Optional, Union\n\nimport torch\nfrom typing_extensions import TypeVar, deprecated\n\nfrom vllm.lora.request import LoRARequest\nfrom vllm.multimodal.inputs import MultiModalPlaceholderDict\nfrom vllm.sampling_params import RequestOutputKind\nfrom vllm.sequence import (PromptLogprobs, RequestMetrics, SampleLogprobs,\n                           SequenceGroup, SequenceGroupBase, SequenceStatus)\n\n\n@dataclass\nclass CompletionOutput:\n    \"\"\"The output data of one completion output of a request.\n\n    Args:\n        index: The index of the output in the request.\n        text: The generated output text.\n        token_ids: The token IDs of the generated output text.\n        cumulative_logprob: The cumulative log probability of the generated\n            output text.\n        logprobs: The log probabilities of the top probability words at each\n            position if the logprobs are requested.\n        finish_reason: The reason why the sequence is finished.\n        stop_reason: The stop string or token id that caused the completion\n            to stop, None if the completion finished for some other reason\n            including encountering the EOS token.\n        lora_request: The LoRA request that was used to generate the output.\n    \"\"\"\n\n    index: int\n    text: str\n    token_ids: GenericSequence[int]\n    cumulative_logprob: Optional[float]\n    logprobs: Optional[SampleLogprobs]\n    finish_reason: Optional[str] = None\n    stop_reason: Union[int, str, None] = None\n    lora_request: Optional[LoRARequest] = None\n\n    def finished(self) -> bool:\n        return self.finish_reason is not None\n\n    def __repr__(self) -> str:\n        return (f\"CompletionOutput(index={self.index}, \"\n                f\"text={self.text!r}, \"\n                f\"token_ids={self.token_ids}, \"\n                f\"cumulative_logprob={self.cumulative_logprob}, \"\n                f\"logprobs={self.logprobs}, \"\n                f\"finish_reason={self.finish_reason}, \"\n                f\"stop_reason={self.stop_reason})\")\n\n\n@dataclass\nclass PoolingOutput:\n    \"\"\"The output data of one pooling output of a request.\n\n    Args:\n        data: The extracted hidden states.\n    \"\"\"\n    data: torch.Tensor\n\n    def __repr__(self) -> str:\n        return (f\"PoolingOutput(data={self.data})\")\n\n    def __eq__(self, other: object) -> bool:\n        return (isinstance(other, self.__class__) and bool(\n            (self.data == other.data).all()))\n\n    @property\n    @deprecated(\"`LLM.encode()` now stores raw outputs in the `data` \"\n                \"attribute. To return embeddings, use `LLM.embed()`. \"\n                \"To return class probabilities, use `LLM.classify()` \"\n                \"and access the `probs` attribute. \")\n    def embedding(self) -> list[float]:\n        return self.data.tolist()\n\n\nclass RequestOutput:\n    \"\"\"The output data of a completion request to the LLM.\n\n    Args:\n        request_id: The unique ID of the request.\n        prompt: The prompt string of the request.\n                For encoder/decoder models, this is the\n                decoder input prompt.\n        prompt_token_ids: The token IDs of the prompt.\n                          For encoder/decoder models, this is the\n                          decoder input prompt token ids.\n        prompt_logprobs: The log probabilities to return per prompt token.\n        outputs: The output sequences of the request.\n        finished: Whether the whole request is finished.\n        metrics: Metrics associated with the request.\n        lora_request: The LoRA request that was used to generate the output.\n        encoder_prompt: The encoder prompt string of the request.\n                        None if decoder-only.\n        encoder_prompt_token_ids: The token IDs of the encoder prompt.\n                                  None if decoder-only.\n        num_cached_tokens: The number of tokens with prefix cache hit.\n        kv_transfer_params: The params for remote K/V transfer.\n    \"\"\"\n\n    def __init__(\n        self,\n        request_id: str,\n        prompt: Optional[str],\n        prompt_token_ids: Optional[list[int]],\n        prompt_logprobs: Optional[PromptLogprobs],\n        outputs: list[CompletionOutput],\n        finished: bool,\n        metrics: Optional[RequestMetrics] = None,\n        lora_request: Optional[LoRARequest] = None,\n        encoder_prompt: Optional[str] = None,\n        encoder_prompt_token_ids: Optional[list[int]] = None,\n        num_cached_tokens: Optional[int] = None,\n        *,\n        multi_modal_placeholders: Optional[MultiModalPlaceholderDict] = None,\n        kv_transfer_params: Optional[dict[str, Any]] = None,\n    ) -> None:\n        self.request_id = request_id\n        self.prompt = prompt\n        self.prompt_token_ids = prompt_token_ids\n        self.multi_modal_placeholders = multi_modal_placeholders or {}\n        self.prompt_logprobs = prompt_logprobs\n        self.outputs = outputs\n        self.finished = finished\n        self.metrics = metrics\n        self.lora_request = lora_request\n        self.encoder_prompt = encoder_prompt\n        self.encoder_prompt_token_ids = encoder_prompt_token_ids\n        self.num_cached_tokens = num_cached_tokens\n        self.kv_transfer_params = kv_transfer_params\n\n    def add(self, next_output: \"RequestOutput\", aggregate: bool) -> None:\n        \"\"\"Merge subsequent RequestOutput into this one\"\"\"\n\n        self.finished |= next_output.finished\n        self.kv_transfer_params = next_output.kv_transfer_params\n\n        for next_completion in next_output.outputs:\n            for i, completion in enumerate(self.outputs):\n                if completion.index == next_completion.index:\n                    if aggregate:\n                        # Merge outputs with same index\n                        completion.text += next_completion.text\n                        if not isinstance(completion.token_ids,\n                                          MutableSequence):\n                            completion.token_ids = list(completion.token_ids)\n                        completion.token_ids.extend(next_completion.token_ids)\n                        if next_completion.logprobs:\n                            assert completion.logprobs is not None\n                            completion.logprobs.extend(\n                                next_completion.logprobs)\n                        completion.cumulative_logprob = (\n                            next_completion.cumulative_logprob)\n                        completion.finish_reason = next_completion.finish_reason\n                        completion.stop_reason = next_completion.stop_reason\n                    else:\n                        # Replace the output with the new one\n                        self.outputs[i] = next_completion\n                    break\n            else:\n                self.outputs.append(next_completion)\n\n    @classmethod\n    def from_seq_group(\n        cls, seq_group: SequenceGroup, use_cache: bool,\n        seq_id_to_seq_group: dict[str, SequenceGroupBase]\n    ) -> Optional[\"RequestOutput\"]:\n        finished = seq_group.is_finished()\n\n        if seq_group.request_id in seq_id_to_seq_group:\n            group: SequenceGroupBase = seq_id_to_seq_group[\n                seq_group.request_id]\n            assembled_seq_group = group.maybe_assemble_group(seq_group)\n            if finished:\n                group.finish_seq(seq_group)\n            if assembled_seq_group is None:\n                return None\n\n            # clear finished seq in seq_id_to_seq_group\n            if len(group.to_be_finished) == 0:\n                for sub_request_id in list(group.seq_id_to_index.keys()):\n                    if sub_request_id in seq_id_to_seq_group:\n                        del seq_id_to_seq_group[sub_request_id]\n\n            return cls.from_seq_group(assembled_seq_group, use_cache,\n                                      seq_id_to_seq_group)\n\n        sampling_params = seq_group.sampling_params\n        if sampling_params is None:\n            raise ValueError(\n                \"Sampling parameters are missing for a CompletionRequest.\")\n\n        if sampling_params.output_kind == RequestOutputKind.FINAL_ONLY and (\n                not finished):\n            return None\n\n        # Init cache (if needed)\n        if use_cache and seq_group.cached_request_output is None:\n            seq_group.cached_request_output = RequestOutput(  # type: ignore\n                request_id=\"\",\n                prompt=None,\n                prompt_token_ids=[],\n                prompt_logprobs=None,\n                outputs=[],\n                finished=False)\n\n        top_n_seqs = seq_group.get_seqs()\n\n        # Create the outputs.\n        # NOTE: We need omit logprobs here explicitly because the sequence\n        # always has the logprobs of the sampled tokens even if the\n        # logprobs are not requested.\n        include_logprobs = sampling_params.logprobs is not None\n        text_buffer_length = sampling_params.output_text_buffer_length\n        delta = sampling_params.output_kind == RequestOutputKind.DELTA\n\n        outputs = []\n        include_prompt = True\n        # num_cached_tokens should be the same for all the sequences\n        num_cached_tokens = None\n        for i, seq in enumerate(top_n_seqs):\n            output_text = seq.get_output_text_to_return(\n                text_buffer_length, delta)\n\n            output_token_ids = seq.get_output_token_ids_to_return(delta)\n            num_output_tokens = 1 if isinstance(output_token_ids,\n                                                int) else len(output_token_ids)\n            num_cached_tokens = seq.data.get_num_cached_tokens()\n\n            output_logprobs = seq.output_logprobs if include_logprobs else None\n\n            if delta:\n                # Slice logprobs delta if applicable\n                if output_logprobs:\n                    # num_output_tokens can be 0 when n > 1 and request finishes\n                    # before the others\n                    if num_output_tokens > 0:\n                        output_logprobs = output_logprobs[-num_output_tokens:]\n                    else:\n                        output_logprobs = None\n                # Don't include prompt if this is after the first output\n                # containing decode token ids\n                if include_prompt and seq.get_output_len() > num_output_tokens:\n                    include_prompt = False\n\n            if use_cache:\n                # Get cached output object\n                cached_outputs = seq_group.cached_request_output.outputs  # type: ignore\n                if i >= len(cached_outputs):\n                    cached_outputs.append(\n                        CompletionOutput(index=i,\n                                         text=\"\",\n                                         token_ids=[],\n                                         cumulative_logprob=None,\n                                         logprobs=None,\n                                         finish_reason=None,\n                                         stop_reason=None))\n                output = cached_outputs[i]\n\n                # Init cached output object\n                assert output.index == i\n                output.text = output_text\n\n                if isinstance(output_token_ids, int):\n                    output.token_ids.clear()\n                    output.token_ids.append(output_token_ids)\n                else:\n                    output.token_ids = output_token_ids\n\n                output.cumulative_logprob = seq.get_cumulative_logprob() \\\n                    if include_logprobs else None\n                output.logprobs = output_logprobs\n                output.finish_reason = SequenceStatus.get_finished_reason(\n                    seq.status)\n                output.stop_reason = seq.stop_reason\n\n            else:\n                output = CompletionOutput(\n                    top_n_seqs.index(seq), output_text, [output_token_ids]\n                    if isinstance(output_token_ids, int) else output_token_ids,\n                    seq.get_cumulative_logprob() if include_logprobs else None,\n                    output_logprobs,\n                    SequenceStatus.get_finished_reason(seq.status),\n                    seq.stop_reason)\n\n            outputs.append(output)\n\n        # Every sequence in the sequence group should have the same prompt.\n        if include_prompt:\n            prompt = seq_group.prompt\n            prompt_token_ids = seq_group.prompt_token_ids\n            encoder_prompt = seq_group.encoder_prompt\n            encoder_prompt_token_ids = seq_group.encoder_prompt_token_ids\n            prompt_logprobs = seq_group.prompt_logprobs\n        else:\n            prompt = None\n            prompt_token_ids = None\n            encoder_prompt = None\n            encoder_prompt_token_ids = None\n            prompt_logprobs = None\n        finished_time = time.time() if finished else None\n        seq_group.set_finished_time(finished_time)\n\n        init_kwargs = {\n            \"request_id\": seq_group.request_id,\n            \"prompt\": prompt,\n            \"prompt_token_ids\": prompt_token_ids,\n            \"prompt_logprobs\": prompt_logprobs,\n            \"outputs\": outputs,\n            \"finished\": finished,\n            \"metrics\": seq_group.metrics,\n            \"lora_request\": seq_group.lora_request,\n            \"encoder_prompt\": encoder_prompt,\n            \"encoder_prompt_token_ids\": encoder_prompt_token_ids,\n            \"num_cached_tokens\": num_cached_tokens,\n            \"multi_modal_placeholders\": seq_group.multi_modal_placeholders\n        }\n\n        if use_cache:\n            request_output = seq_group.cached_request_output\n            request_output.__init__(**init_kwargs)  # type: ignore\n        else:\n            request_output = cls(**init_kwargs)  # type: ignore\n\n        return request_output\n\n    def __repr__(self) -> str:\n        return (f\"RequestOutput(request_id={self.request_id}, \"\n                f\"prompt={self.prompt!r}, \"\n                f\"prompt_token_ids={self.prompt_token_ids}, \"\n                f\"encoder_prompt={self.encoder_prompt!r}, \"\n                f\"encoder_prompt_token_ids={self.encoder_prompt_token_ids}, \"\n                f\"prompt_logprobs={self.prompt_logprobs}, \"\n                f\"outputs={self.outputs}, \"\n                f\"finished={self.finished}, \"\n                f\"metrics={self.metrics}, \"\n                f\"lora_request={self.lora_request}, \"\n                f\"num_cached_tokens={self.num_cached_tokens}, \"\n                f\"multi_modal_placeholders={self.multi_modal_placeholders})\")\n\n\n_O = TypeVar(\"_O\", default=PoolingOutput)\n\n\nclass PoolingRequestOutput(Generic[_O]):\n    \"\"\"\n    The output data of a pooling request to the LLM.\n\n    Args:\n        request_id (str): A unique identifier for the pooling request.\n        outputs (PoolingOutput): The pooling results for the given input.\n        prompt_token_ids (list[int]): A list of token IDs used in the prompt.\n        finished (bool): A flag indicating whether the pooling is completed.\n    \"\"\"\n\n    def __init__(self, request_id: str, outputs: _O,\n                 prompt_token_ids: list[int], finished: bool):\n        self.request_id = request_id\n        self.prompt_token_ids = prompt_token_ids\n        self.finished = finished\n        self.outputs = outputs\n\n    @staticmethod\n    def from_seq_group(seq_group: SequenceGroup) -> \"PoolingRequestOutput\":\n        pooled_data = seq_group.pooled_data\n        assert pooled_data is not None\n\n        data = pooled_data.to(dtype=torch.float32, device=\"cpu\")\n        output = PoolingOutput(data)\n        prompt_token_ids = seq_group.prompt_token_ids\n        finished = seq_group.is_finished()\n\n        return PoolingRequestOutput(seq_group.request_id, output,\n                                    prompt_token_ids, finished)\n\n    def __repr__(self):\n        \"\"\"\n        Returns a string representation of an PoolingRequestOutput instance.\n\n        The representation includes the request_id and the number of outputs,\n        providing a quick overview of the pooling request's results.\n\n        Returns:\n            str: A string representation of the PoolingRequestOutput instance.\n        \"\"\"\n        return (f\"{type(self).__name__}(request_id={self.request_id!r}, \"\n                f\"outputs={self.outputs!r}, \"\n                f\"prompt_token_ids={self.prompt_token_ids}, \"\n                f\"finished={self.finished})\")\n\n\nclass RequestOutputFactory:\n\n    @staticmethod\n    def create(seq_group: SequenceGroup,\n               seq_id_to_seq_group: dict[str, SequenceGroupBase],\n               use_cache: bool = False):\n        if seq_group.pooled_data is not None:\n            return PoolingRequestOutput.from_seq_group(seq_group)\n        else:\n            return RequestOutput.from_seq_group(seq_group, use_cache,\n                                                seq_id_to_seq_group)\n\n\n@dataclass\nclass EmbeddingOutput:\n    \"\"\"The output data of one embedding output of a request.\n\n    Args:\n        embedding: The embedding vector, which is a list of floats.\n        Its length depends on the hidden dimension of the model.\n    \"\"\"\n    embedding: list[float]\n\n    @staticmethod\n    def from_base(pooling_output: PoolingOutput):\n        pooled_data = pooling_output.data\n        if pooled_data.ndim != 1:\n            raise ValueError(\"pooled_data should be a 1-D embedding vector\")\n\n        return EmbeddingOutput(pooled_data.tolist())\n\n    @property\n    def hidden_size(self) -> int:\n        return len(self.embedding)\n\n    def __repr__(self) -> str:\n        return f\"EmbeddingOutput(hidden_size={self.hidden_size})\"\n\n\nclass EmbeddingRequestOutput(PoolingRequestOutput[EmbeddingOutput]):\n\n    @staticmethod\n    def from_base(request_output: PoolingRequestOutput):\n        return EmbeddingRequestOutput(\n            request_id=request_output.request_id,\n            outputs=EmbeddingOutput.from_base(request_output.outputs),\n            prompt_token_ids=request_output.prompt_token_ids,\n            finished=request_output.finished,\n        )\n\n\n@dataclass\nclass ClassificationOutput:\n    \"\"\"The output data of one classification output of a request.\n\n    Args:\n        probs: The probability vector, which is a list of floats.\n        Its length depends on the number of classes.\n    \"\"\"\n    probs: list[float]\n\n    @staticmethod\n    def from_base(pooling_output: PoolingOutput):\n        pooled_data = pooling_output.data\n        if pooled_data.ndim != 1:\n            raise ValueError(\"pooled_data should be a 1-D probability vector\")\n\n        return ClassificationOutput(pooled_data.tolist())\n\n    @property\n    def num_classes(self) -> int:\n        return len(self.probs)\n\n    def __repr__(self) -> str:\n        return f\"ClassificationOutput(num_classes={self.num_classes})\"\n\n\nclass ClassificationRequestOutput(PoolingRequestOutput[ClassificationOutput]):\n\n    @staticmethod\n    def from_base(request_output: PoolingRequestOutput):\n        return ClassificationRequestOutput(\n            request_id=request_output.request_id,\n            outputs=ClassificationOutput.from_base(request_output.outputs),\n            prompt_token_ids=request_output.prompt_token_ids,\n            finished=request_output.finished,\n        )\n\n\n@dataclass\nclass ScoringOutput:\n    \"\"\"The output data of one scoring output of a request.\n\n    Args:\n        score: The similarity score, which is a scalar value.\n    \"\"\"\n    score: float\n\n    @staticmethod\n    def from_base(pooling_output: PoolingOutput):\n        pooled_data = pooling_output.data\n        if pooled_data.ndim != 0:\n            raise ValueError(\"pooled_data should be a scalar score\")\n\n        return ScoringOutput(pooled_data.item())\n\n    def __repr__(self) -> str:\n        return f\"ScoringOutput(score={self.score})\"\n\n    @property\n    @deprecated(\"`LLM.score()` now returns scalar scores. \"\n                \"Please access it via the `score` attribute. \")\n    def embedding(self) -> list[float]:\n        return [self.score]\n\n\nclass ScoringRequestOutput(PoolingRequestOutput[ScoringOutput]):\n\n    @staticmethod\n    def from_base(request_output: PoolingRequestOutput):\n        return ScoringRequestOutput(\n            request_id=request_output.request_id,\n            outputs=ScoringOutput.from_base(request_output.outputs),\n            prompt_token_ids=request_output.prompt_token_ids,\n            finished=request_output.finished,\n        )\n", 525]}, "functions": {"LLM.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:382)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 382], "ModelConfig.runner_type (/home/jeromeku/vllm/vllm/config.py:1365)": ["/home/jeromeku/vllm/vllm/config.py", 1365], "LLM._add_guided_params (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1422)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 1422], "Counter.__next__ (/home/jeromeku/vllm/vllm/utils.py:216)": ["/home/jeromeku/vllm/vllm/utils.py", 216], "Processor._validate_lora (/home/jeromeku/vllm/vllm/v1/engine/processor.py:145)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 145], "Processor._validate_logprobs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:61)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 61], "Processor._validate_structured_output (/home/jeromeku/vllm/vllm/v1/engine/processor.py:150)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 150], "Processor._validate_logit_bias (/home/jeromeku/vllm/vllm/v1/engine/processor.py:96)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 96], "Processor._validate_sampling_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:78)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 78], "Processor._validate_supported_sampling_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:116)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 116], "Processor._validate_params (/home/jeromeku/vllm/vllm/v1/engine/processor.py:128)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 128], "is_encoder_decoder (/home/jeromeku/vllm/vllm/transformers_utils/config.py:254)": ["/home/jeromeku/vllm/vllm/transformers_utils/config.py", 254], "ModelConfig.is_encoder_decoder (/home/jeromeku/vllm/vllm/config.py:1340)": ["/home/jeromeku/vllm/vllm/config.py", 1340], "is_explicit_encoder_decoder_prompt (/home/jeromeku/vllm/vllm/inputs/parse.py:135)": ["/home/jeromeku/vllm/vllm/inputs/parse.py", 135], "parse_singleton_prompt (/home/jeromeku/vllm/vllm/inputs/parse.py:117)": ["/home/jeromeku/vllm/vllm/inputs/parse.py", 117], "InputPreprocessor.get_tokenizer_group (/home/jeromeku/vllm/vllm/inputs/preprocess.py:42)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 42], "InputPreprocessor._get_tokenization_kw (/home/jeromeku/vllm/vllm/inputs/preprocess.py:182)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 182], "TokenizerGroup.get_lora_tokenizer (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:78)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py", 78], "encode_tokens (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:54)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py", 54], "TokenizerGroup._raise_if_input_too_long (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:34)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py", 34], "TokenizerGroup.encode (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py:46)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer_group.py", 46], "InputPreprocessor._tokenize_prompt (/home/jeromeku/vllm/vllm/inputs/preprocess.py:199)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 199], "token_inputs (/home/jeromeku/vllm/vllm/inputs/data.py:186)": ["/home/jeromeku/vllm/vllm/inputs/data.py", 186], "InputPreprocessor._process_text (/home/jeromeku/vllm/vllm/inputs/preprocess.py:389)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 389], "InputPreprocessor._prompt_to_llm_inputs (/home/jeromeku/vllm/vllm/inputs/preprocess.py:457)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 457], "InputPreprocessor._apply_prompt_adapter (/home/jeromeku/vllm/vllm/inputs/preprocess.py:170)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 170], "InputPreprocessor._build_decoder_only_llm_inputs (/home/jeromeku/vllm/vllm/inputs/preprocess.py:756)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 756], "InputPreprocessor._process_decoder_only_prompt (/home/jeromeku/vllm/vllm/inputs/preprocess.py:771)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 771], "InputPreprocessor.preprocess (/home/jeromeku/vllm/vllm/inputs/preprocess.py:828)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 828], "__getattr__ (/home/jeromeku/vllm/vllm/platforms/__init__.py:264)": ["/home/jeromeku/vllm/vllm/platforms/__init__.py", 264], "Platform.validate_request (/home/jeromeku/vllm/vllm/platforms/interface.py:456)": ["/home/jeromeku/vllm/vllm/platforms/interface.py", 456], "InputPreprocessor.get_eos_token_id (/home/jeromeku/vllm/vllm/inputs/preprocess.py:59)": ["/home/jeromeku/vllm/vllm/inputs/preprocess.py", 59], "split_enc_dec_inputs (/home/jeromeku/vllm/vllm/inputs/parse.py:140)": ["/home/jeromeku/vllm/vllm/inputs/parse.py", 140], "get_cached_tokenizer.<locals>.CachedTokenizer.max_token_id (/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py:121)": ["/home/jeromeku/vllm/vllm/transformers_utils/tokenizer.py", 121], "Processor._validate_model_input (/home/jeromeku/vllm/vllm/v1/engine/processor.py:346)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 346], "Processor._validate_model_inputs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:332)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 332], "SamplingParams.clone (/home/jeromeku/vllm/vllm/sampling_params.py:536)": ["/home/jeromeku/vllm/vllm/sampling_params.py", 536], "SamplingParams.update_from_generation_config (/home/jeromeku/vllm/vllm/sampling_params.py:457)": ["/home/jeromeku/vllm/vllm/sampling_params.py", 457], "SamplingParams.update_from_tokenizer (/home/jeromeku/vllm/vllm/sampling_params.py:483)": ["/home/jeromeku/vllm/vllm/sampling_params.py", 483], "Processor.process_inputs (/home/jeromeku/vllm/vllm/v1/engine/processor.py:203)": ["/home/jeromeku/vllm/vllm/v1/engine/processor.py", 203], "LogprobsProcessor.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:34)": ["/home/jeromeku/vllm/vllm/v1/engine/logprobs.py", 34], "IncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:22)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 22], "BaseIncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:60)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 60], "FastIncrementalDetokenizer.__init__ (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:154)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 154], "IncrementalDetokenizer.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:37)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 37], "RequestState.__init__ (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:74)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 74], "RequestState.from_new_request (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:107)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 107], "LoRARequestStates.get_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:188)": ["/home/jeromeku/vllm/vllm/v1/metrics/stats.py", 188], "LoRARequestStates.add_request (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:195)": ["/home/jeromeku/vllm/vllm/v1/metrics/stats.py", 195], "OutputProcessor.add_request (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:272)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 272], "MPClient.ensure_alive (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:561)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 561], "MPClient.free_pending_messages (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:569)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 569], "MsgpackEncoder.encode (/home/jeromeku/vllm/vllm/v1/serial_utils.py:70)": ["/home/jeromeku/vllm/vllm/v1/serial_utils.py", 70], "SyncMPClient._send_input (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:659)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 659], "SyncMPClient.add_request (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:683)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 683], "LLMEngine.add_request (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:174)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 174], "LLM._add_request (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1402)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 1402], "LLM._validate_and_add_requests (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1345)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 1345], "OutputProcessor.get_num_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:242)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 242], "LLMEngine.get_num_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:148)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 148], "OutputProcessor.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:245)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 245], "LLMEngine.has_unfinished_requests (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:151)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 151], "SyncMPClient.get_output (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:650)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 650], "OutputProcessor._update_stats_from_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:390)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 390], "FastIncrementalDetokenizer.decode_next (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:200)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 200], "BaseIncrementalDetokenizer.update (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:79)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 79], "LogprobsProcessor.update_from_output (/home/jeromeku/vllm/vllm/v1/engine/logprobs.py:194)": ["/home/jeromeku/vllm/vllm/v1/engine/logprobs.py", 194], "RequestState.make_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:144)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 144], "LoRARequestStates.update_iteration_stats (/home/jeromeku/vllm/vllm/v1/metrics/stats.py:228)": ["/home/jeromeku/vllm/vllm/v1/metrics/stats.py", 228], "OutputProcessor.process_outputs (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:297)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 297], "SyncMPClient.abort_requests (/home/jeromeku/vllm/vllm/v1/engine/core_client.py:686)": ["/home/jeromeku/vllm/vllm/v1/engine/core_client.py", 686], "LLMEngine.step (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:215)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 215], "BaseIncrementalDetokenizer.get_next_output_text (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:135)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 135], "IncrementalDetokenizer.output_token_ids (/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py:25)": ["/home/jeromeku/vllm/vllm/v1/engine/detokenizer.py", 25], "FinishReason.__str__ (/home/jeromeku/vllm/vllm/v1/engine/__init__.py:37)": ["/home/jeromeku/vllm/vllm/v1/engine/__init__.py", 37], "RequestState._new_completion_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:198)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 198], "RequestOutput.__init__ (/home/jeromeku/vllm/vllm/outputs.py:109)": ["/home/jeromeku/vllm/vllm/outputs.py", 109], "RequestState._new_request_output (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:174)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 174], "EngineCoreOutput.finished (/home/jeromeku/vllm/vllm/v1/engine/__init__.py:110)": ["/home/jeromeku/vllm/vllm/v1/engine/__init__.py", 110], "OutputProcessor._update_stats_from_finished (/home/jeromeku/vllm/vllm/v1/engine/output_processor.py:407)": ["/home/jeromeku/vllm/vllm/v1/engine/output_processor.py", 407], "LLM._run_engine.<locals>.<genexpr> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1475)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 1475], "LLM._run_engine.<locals>.<lambda> (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1491)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 1491], "LLM._run_engine (/home/jeromeku/vllm/vllm/entrypoints/llm.py:1445)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 1445], "LLMEngine.validate_outputs (/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py:164)": ["/home/jeromeku/vllm/vllm/v1/engine/llm_engine.py", 164], "LLM.generate (/home/jeromeku/vllm/vllm/entrypoints/llm.py:380)": ["/home/jeromeku/vllm/vllm/entrypoints/llm.py", 380], "deprecate_kwargs.<locals>.wrapper.<locals>.inner (/home/jeromeku/vllm/vllm/utils.py:1196)": ["/home/jeromeku/vllm/vllm/utils.py", 1196]}}}